[[[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nIf only my malediction is needed for that, I bestow it upon him! and his cathedral, and his mills!\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581If', '\u2581only', '\u2581my', '\u2581ma', 'led', 'iction', '\u2581is', '\u2581needed', '\u2581for', '\u2581that', ',', '\u2581I', '\u2581best', 'ow', '\u2581it', '\u2581upon', '\u2581him', '!', '\u2581and', '\u2581his', '\u2581c', 'athedral', ',', '\u2581and', '\u2581his', '\u2581mill', 's', '!']"}], "[\n  [\"\u2581If\", \"Si\"],\n  [\"\u2581only\", null],\n  [\"\u2581my\", null],\n  [\"\u2581ma\", \"mal\u00e9diction\"],\n  [\"led\", null],\n  [\"iction\", null],\n  [\"\u2581is\", null],\n  [\"\u2581needed\", \"n\u00e9cessaire\"],\n  [\"\u2581for\", null],\n  [\"\u2581that\", null],\n  [\",\", null],\n  [\"\u2581I\", null],\n  [\"\u2581best\", \"accorde\"],\n  [\"ow\", null],\n  [\"\u2581it\", null],\n  [\"\u2581upon\", null],\n  [\"\u2581him\", \"lui\"],\n  [\"!\", null],\n  [\"\u2581and\", null],\n  [\"\u2581his\", null],\n  [\"\u2581c\", \"cath\u00e9drale\"],\n  [\"athedral\", null],\n  [\",\", null],\n  [\"\u2581and\", null],\n  [\"\u2581his\", null],\n  [\"\u2581mill\", \"moulins\"],\n  [\"s\", null],\n  [\"!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nIf only my malediction is needed for that, I bestow it upon him! and his cathedral, and his mills!\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581If', '\u2581only', '\u2581my', '\u2581ma', 'led', 'iction', '\u2581is', '\u2581needed', '\u2581for', '\u2581that', ',', '\u2581I', '\u2581best', 'ow', '\u2581it', '\u2581upon', '\u2581him', '!', '\u2581and', '\u2581his', '\u2581c', 'athedral', ',', '\u2581and', '\u2581his', '\u2581mill', 's', '!']"}], "[\n  [\"\u2581If\", \"\u0415\u0441\u043b\u0438\"],\n  [\"\u2581only\", null],\n  [\"\u2581my\", null],\n  [\"\u2581ma\", \"\u043f\u0440\u043e\u043a\u043b\u044f\u0442\u0438\u0435\"],\n  [\"led\", null],\n  [\"iction\", null],\n  [\"\u2581is\", null],\n  [\"\u2581needed\", \"\u043d\u0443\u0436\u043d\u043e\"],\n  [\"\u2581for\", null],\n  [\"\u2581that\", \"\u044d\u0442\u043e\u0433\u043e\"],\n  [\",\", null],\n  [\"\u2581I\", \"\u044f\"],\n  [\"\u2581best\", \"\u0434\u0430\u0440\u0443\u044e\"],\n  [\"ow\", null],\n  [\"\u2581it\", \"\u0435\u0433\u043e\"],\n  [\"\u2581upon\", null],\n  [\"\u2581him\", \"\u0435\u043c\u0443\"],\n  [\"!\", null],\n  [\"\u2581and\", \"\u0438\"],\n  [\"\u2581his\", \"\u0435\u0433\u043e\"],\n  [\"\u2581c\", \"\u0441\u043e\u0431\u043e\u0440\"],\n  [\"athedral\", null],\n  [\",\", null],\n  [\"\u2581and\", \"\u0438\"],\n  [\"\u2581his\", \"\u0435\u0433\u043e\"],\n  [\"\u2581mill\", \"\u043c\u0435\u043b\u044c\u043d\u0438\u0446\u044b\"],\n  [\"s\", null],\n  [\"!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nHe bent down to her ear, and added (the spectators supposed that he was receiving her last confession): \"Will you have me?\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581He', '\u2581bent', '\u2581down', '\u2581to', '\u2581her', '\u2581ear', ',', '\u2581and', '\u2581added', '\u2581(', 'the', '\u2581spect', 'ators', '\u2581supposed', '\u2581that', '\u2581he', '\u2581was', '\u2581receiving', '\u2581her', '\u2581last', '\u2581conf', 'ession', '):', '\u2581\"', 'Will', '\u2581you', '\u2581have', '\u2581me', '?']"}], "[\n[\"\u2581He\", \"Il\"],\n[\"\u2581bent\", \"pencha\"],\n[\"\u2581down\", null],\n[\"\u2581to\", null],\n[\"\u2581her\", \"son\"],\n[\"\u2581ear\", \"oreille\"],\n[\",\", null],\n[\"\u2581and\", null],\n[\"\u2581added\", \"ajouta\"],\n[\"\u2581(\", null],\n[\"the\", null],\n[\"\u2581spect\", \"spectateurs\"],\n[\"ators\", null],\n[\"\u2581supposed\", \"suppos\u00e8rent\"],\n[\"\u2581that\", null],\n[\"\u2581he\", \"il\"],\n[\"\u2581was\", null],\n[\"\u2581receiving\", \"recevait\"],\n[\"\u2581her\", \"sa\"],\n[\"\u2581last\", \"derni\u00e8re\"],\n[\"\u2581conf\", \"confession\"],\n[\"ession\", null],\n[\"):\", null],\n[\"\u2581\\\"\", null],\n[\"Will\", \"Voulez\"],\n[\"\u2581you\", null],\n[\"\u2581have\", null],\n[\"\u2581me\", \"moi\"],\n[\"?\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nHe bent down to her ear, and added (the spectators supposed that he was receiving her last confession): \"Will you have me?\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581He', '\u2581bent', '\u2581down', '\u2581to', '\u2581her', '\u2581ear', ',', '\u2581and', '\u2581added', '\u2581(', 'the', '\u2581spect', 'ators', '\u2581supposed', '\u2581that', '\u2581he', '\u2581was', '\u2581receiving', '\u2581her', '\u2581last', '\u2581conf', 'ession', '):', '\u2581\"', 'Will', '\u2581you', '\u2581have', '\u2581me', '?']"}], "[\n  [\"\u2581He\", \"\u041e\u043d\"],\n  [\"\u2581bent\", \"\u043d\u0430\u043a\u043b\u043e\u043d\u0438\u043b\u0441\u044f\"],\n  [\"\u2581down\", null],\n  [\"\u2581to\", null],\n  [\"\u2581her\", \"\u0435\u0451\"],\n  [\"\u2581ear\", \"\u0443\u0445\u0443\"],\n  [\",\", null],\n  [\"\u2581and\", null],\n  [\"\u2581added\", \"\u0434\u043e\u0431\u0430\u0432\u0438\u043b\"],\n  [\"\u2581(\", null],\n  [\"the\", null],\n  [\"\u2581spect\", \"\u0437\u0440\u0438\u0442\u0435\u043b\u0438\"],\n  [\"ators\", null],\n  [\"\u2581supposed\", \"\u043f\u0440\u0435\u0434\u043f\u043e\u043b\u0430\u0433\u0430\u043b\u0438\"],\n  [\"\u2581that\", null],\n  [\"\u2581he\", \"\u043e\u043d\"],\n  [\"\u2581was\", null],\n  [\"\u2581receiving\", \"\u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u0442\"],\n  [\"\u2581her\", \"\u0435\u0451\"],\n  [\"\u2581last\", \"\u043f\u043e\u0441\u043b\u0435\u0434\u043d\u044e\u044e\"],\n  [\"\u2581conf\", \"\u0438\u0441\u043f\u043e\u0432\u0435\u0434\u044c\"],\n  [\"ession\", null],\n  [\"):\", null],\n  [\"\u2581\\\"\", null],\n  [\"Will\", \"\u0412\u044b\u0439\u0434\u0435\u0448\u044c\"],\n  [\"\u2581you\", null],\n  [\"\u2581have\", null],\n  [\"\u2581me\", \"\u0437\u0430\u043c\u0443\u0436 \u0437\u0430 \u043c\u0435\u043d\u044f\"],\n  [\"?\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nLa Gr\u00e8ve avait d\u00e8s lors cet aspect sinistre que lui conservent encore aujourd\u2019hui l\u2019id\u00e9e ex\u00e9crable qu\u2019elle r\u00e9veille et le sombre H\u00f4tel de Ville de Dominique Boccador, qui a remplac\u00e9 la Maison-aux-Piliers.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581La', '\u2581Gr', '\u00e8ve', '\u2581avait', '\u2581d\u00e8s', '\u2581lors', '\u2581cet', '\u2581aspect', '\u2581sin', 'istre', '\u2581que', '\u2581lui', '\u2581conser', 'vent', '\u2581encore', '\u2581aujourd', '\u2019', 'hui', '\u2581l', '\u2019', 'id', '\u00e9e', '\u2581ex', '\u00e9c', 'rable', '\u2581qu', '\u2019', 'elle', '\u2581r\u00e9', 've', 'ille', '\u2581et', '\u2581le', '\u2581som', 'bre', '\u2581H', '\u00f4tel', '\u2581de', '\u2581Ville', '\u2581de', '\u2581Domin', 'ique', '\u2581B', 'occ', 'ador', ',', '\u2581qui', '\u2581a', '\u2581remp', 'lac', '\u00e9', '\u2581la', '\u2581M', 'aison', '-', 'aux', '-', 'P', 'ili', 'ers', '.']"}], "[\n[\"\u2581La\", \"The\"],\n[\"\u2581Gr\", \"Square\"],\n[\"\u00e8ve\", null],\n[\"\u2581avait\", \"had\"],\n[\"\u2581d\u00e8s\", \"from\"],\n[\"\u2581lors\", \"then\"],\n[\"\u2581cet\", \"this\"],\n[\"\u2581aspect\", \"appearance\"],\n[\"\u2581sin\", \"sinister\"],\n[\"istre\", null],\n[\"\u2581que\", null],\n[\"\u2581lui\", null],\n[\"\u2581conser\", \"preserve\"],\n[\"vent\", null],\n[\"\u2581encore\", \"still\"],\n[\"\u2581aujourd\", \"today\"],\n[\"'\", null],\n[\"hui\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"id\", \"idea\"],\n[\"\u00e9e\", null],\n[\"\u2581ex\", \"execrable\"],\n[\"\u00e9c\", null],\n[\"rable\", null],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"elle\", \"it\"],\n[\"\u2581r\u00e9\", \"awakens\"],\n[\"ve\", null],\n[\"ille\", null],\n[\"\u2581et\", \"and\"],\n[\"\u2581le\", \"the\"],\n[\"\u2581som\", \"somber\"],\n[\"bre\", null],\n[\"\u2581H\", \"City\"],\n[\"\u00f4tel\", null],\n[\"\u2581de\", null],\n[\"\u2581Ville\", \"Hall\"],\n[\"\u2581de\", \"of\"],\n[\"\u2581Domin\", \"Dominique\"],\n[\"ique\", null],\n[\"\u2581B\", \"Boccador\"],\n[\"occ\", null],\n[\"ador\", null],\n[\",\", null],\n[\"\u2581qui\", \"which\"],\n[\"\u2581a\", \"has\"],\n[\"\u2581remp\", \"replaced\"],\n[\"lac\", null],\n[\"\u00e9\", null],\n[\"\u2581la\", \"the\"],\n[\"\u2581M\", \"House\"],\n[\"aison\", null],\n[\"-\", null],\n[\"aux\", null],\n[\"-\", null],\n[\"P\", \"Pillars\"],\n[\"ili\", null],\n[\"ers\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nLa Gr\u00e8ve avait d\u00e8s lors cet aspect sinistre que lui conservent encore aujourd\u2019hui l\u2019id\u00e9e ex\u00e9crable qu\u2019elle r\u00e9veille et le sombre H\u00f4tel de Ville de Dominique Boccador, qui a remplac\u00e9 la Maison-aux-Piliers.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581La', '\u2581Gr', '\u00e8ve', '\u2581avait', '\u2581d\u00e8s', '\u2581lors', '\u2581cet', '\u2581aspect', '\u2581sin', 'istre', '\u2581que', '\u2581lui', '\u2581conser', 'vent', '\u2581encore', '\u2581aujourd', '\u2019', 'hui', '\u2581l', '\u2019', 'id', '\u00e9e', '\u2581ex', '\u00e9c', 'rable', '\u2581qu', '\u2019', 'elle', '\u2581r\u00e9', 've', 'ille', '\u2581et', '\u2581le', '\u2581som', 'bre', '\u2581H', '\u00f4tel', '\u2581de', '\u2581Ville', '\u2581de', '\u2581Domin', 'ique', '\u2581B', 'occ', 'ador', ',', '\u2581qui', '\u2581a', '\u2581remp', 'lac', '\u00e9', '\u2581la', '\u2581M', 'aison', '-', 'aux', '-', 'P', 'ili', 'ers', '.']"}], "[\n[\"\u2581La\", \"\u041f\u043b\u043e\u0449\u0430\u0434\u044c\"],\n[\"\u2581Gr\", null],\n[\"\u00e8ve\", null],\n[\"\u2581avait\", \"\u0438\u043c\u0435\u043b\u0430\"],\n[\"\u2581d\u00e8s\", \"\u0441\"],\n[\"\u2581lors\", \"\u0442\u0435\u0445 \u043f\u043e\u0440\"],\n[\"\u2581cet\", \"\u044d\u0442\u043e\u0442\"],\n[\"\u2581aspect\", \"\u0432\u0438\u0434\"],\n[\"\u2581sin\", null],\n[\"istre\", null],\n[\"\u2581que\", null],\n[\"\u2581lui\", null],\n[\"\u2581conser\", null],\n[\"vent\", null],\n[\"\u2581encore\", \"\u0432\u0441\u0435 \u0435\u0449\u0435\"],\n[\"\u2581aujourd\", null],\n[\"'\", null],\n[\"hui\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"id\", null],\n[\"\u00e9e\", null],\n[\"\u2581ex\", null],\n[\"\u00e9c\", null],\n[\"rable\", null],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"elle\", null],\n[\"\u2581r\u00e9\", null],\n[\"ve\", null],\n[\"ille\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581le\", null],\n[\"\u2581som\", null],\n[\"bre\", null],\n[\"\u2581H\", null],\n[\"\u00f4tel\", null],\n[\"\u2581de\", null],\n[\"\u2581Ville\", \"\u0420\u0430\u0442\u0443\u0448\u0430\"],\n[\"\u2581de\", null],\n[\"\u2581Domin\", null],\n[\"ique\", null],\n[\"\u2581B\", null],\n[\"occ\", null],\n[\"ador\", null],\n[\",\", null],\n[\"\u2581qui\", \"\u043a\u043e\u0442\u043e\u0440\u0430\u044f\"],\n[\"\u2581a\", null],\n[\"\u2581remp\", null],\n[\"lac\", null],\n[\"\u00e9\", null],\n[\"\u2581la\", null],\n[\"\u2581M\", null],\n[\"aison\", null],\n[\"-\", null],\n[\"aux\", null],\n[\"-\", null],\n[\"\u2581P\", null],\n[\"ili\", null],\n[\"ers\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nQuand ma\u00eetre Philippe Lheulier, avocat extraordinaire du roi, avait dit \u00e0 la pauvre Esmeralda : Il se meurt, c\u2019\u00e9tait par erreur ou par plaisanterie.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Qu', 'and', '\u2581ma', '\u00eetre', '\u2581Philippe', '\u2581L', 'he', 'ul', 'ier', ',', '\u2581av', 'oc', 'at', '\u2581extra', 'ord', 'inaire', '\u2581du', '\u2581roi', ',', '\u2581avait', '\u2581dit', '\u2581\u00e0', '\u2581la', '\u2581pau', 'vre', '\u2581Es', 'mer', 'al', 'da', '\u2581:', '\u2581Il', '\u2581se', '\u2581me', 'urt', ',', '\u2581c', '\u2019', '\u00e9tait', '\u2581par', '\u2581erre', 'ur', '\u2581ou', '\u2581par', '\u2581pla', 'is', 'anterie', '.']"}], "[\n    [\"\u2581Qu\", \"When\"],\n    [\"and\", null],\n    [\"\u2581ma\", \"master\"],\n    [\"\u00eetre\", null],\n    [\"\u2581Philippe\", \"Philippe\"],\n    [\"\u2581L\", \"Lheulier\"],\n    [\"he\", null],\n    [\"ul\", null],\n    [\"ier\", null],\n    [\",\", null],\n    [\"\u2581av\", \"lawyer\"],\n    [\"oc\", null],\n    [\"at\", null],\n    [\"\u2581extra\", \"extraordinary\"],\n    [\"ord\", null],\n    [\"inaire\", null],\n    [\"\u2581du\", null],\n    [\"\u2581roi\", \"king\"],\n    [\",\", null],\n    [\"\u2581avait\", \"had\"],\n    [\"\u2581dit\", \"said\"],\n    [\"\u2581\u00e0\", \"to\"],\n    [\"\u2581la\", \"the\"],\n    [\"\u2581pau\", \"poor\"],\n    [\"vre\", null],\n    [\"\u2581Es\", \"Esmeralda\"],\n    [\"mer\", null],\n    [\"al\", null],\n    [\"da\", null],\n    [\"\u2581:\", null],\n    [\"\u2581Il\", \"He\"],\n    [\"\u2581se\", null],\n    [\"\u2581me\", \"is dying\"],\n    [\"urt\", null],\n    [\",\", null],\n    [\"\u2581c\", \"it\"],\n    [\"'\", null],\n    [\"\u00e9tait\", \"was\"],\n    [\"\u2581par\", \"by\"],\n    [\"\u2581erre\", \"mistake\"],\n    [\"ur\", null],\n    [\"\u2581ou\", \"or\"],\n    [\"\u2581par\", \"by\"],\n    [\"\u2581pla\", \"joke\"],\n    [\"is\", null],\n    [\"anterie\", null],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nQuand ma\u00eetre Philippe Lheulier, avocat extraordinaire du roi, avait dit \u00e0 la pauvre Esmeralda : Il se meurt, c\u2019\u00e9tait par erreur ou par plaisanterie.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Qu', 'and', '\u2581ma', '\u00eetre', '\u2581Philippe', '\u2581L', 'he', 'ul', 'ier', ',', '\u2581av', 'oc', 'at', '\u2581extra', 'ord', 'inaire', '\u2581du', '\u2581roi', ',', '\u2581avait', '\u2581dit', '\u2581\u00e0', '\u2581la', '\u2581pau', 'vre', '\u2581Es', 'mer', 'al', 'da', '\u2581:', '\u2581Il', '\u2581se', '\u2581me', 'urt', ',', '\u2581c', '\u2019', '\u00e9tait', '\u2581par', '\u2581erre', 'ur', '\u2581ou', '\u2581par', '\u2581pla', 'is', 'anterie', '.']"}], "[\n[\"\u2581Qu\", \"\u041a\u043e\u0433\u0434\u0430\"],\n[\"and\", null],\n[\"\u2581ma\", null],\n[\"\u00eetre\", null],\n[\"\u2581Philippe\", \"\u0424\u0438\u043b\u0438\u043f\u043f\"],\n[\"\u2581L\", null],\n[\"he\", null],\n[\"ul\", null],\n[\"ier\", null],\n[\",\", null],\n[\"\u2581av\", null],\n[\"oc\", null],\n[\"at\", null],\n[\"\u2581extra\", null],\n[\"ord\", null],\n[\"inaire\", null],\n[\"\u2581du\", null],\n[\"\u2581roi\", \"\u043a\u043e\u0440\u043e\u043b\u044f\"],\n[\",\", null],\n[\"\u2581avait\", \"\u0441\u043a\u0430\u0437\u0430\u043b\"],\n[\"\u2581dit\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581la\", null],\n[\"\u2581pau\", null],\n[\"vre\", null],\n[\"\u2581Es\", null],\n[\"mer\", null],\n[\"al\", null],\n[\"da\", null],\n[\"\u2581:\", null],\n[\"\u2581Il\", \"\u041e\u043d\"],\n[\"\u2581se\", null],\n[\"\u2581me\", null],\n[\"urt\", null],\n[\",\", null],\n[\"\u2581c\", null],\n[\"'\", null],\n[\"\u00e9tait\", \"\u0431\u044b\u043b\u043e\"],\n[\"\u2581par\", null],\n[\"\u2581erre\", null],\n[\"ur\", null],\n[\"\u2581ou\", \"\u0438\u043b\u0438\"],\n[\"\u2581par\", \"\u0438\u0437\"],\n[\"\u2581pla\", null],\n[\"is\", null],\n[\"anterie\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nLe voyant ouvrir la porte du jardin, je montai rapidement dans ma chambre.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Le', '\u2581voy', 'ant', '\u2581ouv', 'rir', '\u2581la', '\u2581porte', '\u2581du', '\u2581j', 'ardin', ',', '\u2581je', '\u2581mont', 'ai', '\u2581rapide', 'ment', '\u2581dans', '\u2581ma', '\u2581cham', 'bre', '.']"}], "[\n    [\"\u2581Le\", null],\n    [\"\u2581voy\", \"seeing\"],\n    [\"ant\", null],\n    [\"\u2581ouv\", \"open\"],\n    [\"rir\", null],\n    [\"\u2581la\", null],\n    [\"\u2581porte\", \"door\"],\n    [\"\u2581du\", null],\n    [\"\u2581j\", \"garden\"],\n    [\"ardin\", null],\n    [\",\", null],\n    [\"\u2581je\", null],\n    [\"\u2581mont\", \"went up\"],\n    [\"ai\", null],\n    [\"\u2581rapide\", \"quickly\"],\n    [\"ment\", null],\n    [\"\u2581dans\", null],\n    [\"\u2581ma\", null],\n    [\"\u2581cham\", \"room\"],\n    [\"bre\", null],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nLe voyant ouvrir la porte du jardin, je montai rapidement dans ma chambre.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Le', '\u2581voy', 'ant', '\u2581ouv', 'rir', '\u2581la', '\u2581porte', '\u2581du', '\u2581j', 'ardin', ',', '\u2581je', '\u2581mont', 'ai', '\u2581rapide', 'ment', '\u2581dans', '\u2581ma', '\u2581cham', 'bre', '.']"}], "[\n    [\"\u2581Le\", null],\n    [\"\u2581voy\", \"\u0423\u0432\u0438\u0434\u0435\u0432\"],\n    [\"ant\", null],\n    [\"\u2581ouv\", \"\u043e\u0442\u043a\u0440\u044b\u0432\u0430\u0442\u044c\"],\n    [\"rir\", null],\n    [\"\u2581la\", null],\n    [\"\u2581porte\", \"\u0434\u0432\u0435\u0440\u044c\"],\n    [\"\u2581du\", null],\n    [\"\u2581j\", \"\u0441\u0430\u0434\"],\n    [\"ardin\", null],\n    [\",\", null],\n    [\"\u2581je\", null],\n    [\"\u2581mont\", \"\u043f\u043e\u0434\u043d\u044f\u043b\u0441\u044f\"],\n    [\"ai\", null],\n    [\"\u2581rapide\", \"\u0431\u044b\u0441\u0442\u0440\u043e\"],\n    [\"ment\", null],\n    [\"\u2581dans\", null],\n    [\"\u2581ma\", null],\n    [\"\u2581cham\", \"\u043a\u043e\u043c\u043d\u0430\u0442\u0443\"],\n    [\"bre\", null],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nLa rencontre de Glenarvan et Ayrton avait \u00e9videmment produit chez tous deux une \u00e9motion r\u00e9ciproque.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581La', '\u2581rencontre', '\u2581de', '\u2581Glen', 'ar', 'van', '\u2581et', '\u2581A', 'yr', 'ton', '\u2581avait', '\u2581\u00e9v', 'ide', 'm', 'ment', '\u2581produit', '\u2581chez', '\u2581tous', '\u2581deux', '\u2581une', '\u2581\u00e9', 'm', 'otion', '\u2581r\u00e9', 'ci', 'pro', 'que', '.']"}], "[\n  [\"\u2581La\", null],\n  [\"\u2581rencontre\", \"meeting\"],\n  [\"\u2581de\", null],\n  [\"\u2581Glen\", \"Glenarvan\"],\n  [\"ar\", null],\n  [\"van\", null],\n  [\"\u2581et\", null],\n  [\"\u2581A\", \"Ayrton\"],\n  [\"yr\", null],\n  [\"ton\", null],\n  [\"\u2581avait\", \"had\"],\n  [\"\u2581\u00e9v\", \"obviously\"],\n  [\"ide\", null],\n  [\"m\", null],\n  [\"ment\", null],\n  [\"\u2581produit\", \"produced\"],\n  [\"\u2581chez\", \"in\"],\n  [\"\u2581tous\", \"both\"],\n  [\"\u2581deux\", null],\n  [\"\u2581une\", null],\n  [\"\u2581\u00e9\", \"emotion\"],\n  [\"m\", null],\n  [\"otion\", null],\n  [\"\u2581r\u00e9\", \"reciprocal\"],\n  [\"ci\", null],\n  [\"pro\", null],\n  [\"que\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nLa rencontre de Glenarvan et Ayrton avait \u00e9videmment produit chez tous deux une \u00e9motion r\u00e9ciproque.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581La', '\u2581rencontre', '\u2581de', '\u2581Glen', 'ar', 'van', '\u2581et', '\u2581A', 'yr', 'ton', '\u2581avait', '\u2581\u00e9v', 'ide', 'm', 'ment', '\u2581produit', '\u2581chez', '\u2581tous', '\u2581deux', '\u2581une', '\u2581\u00e9', 'm', 'otion', '\u2581r\u00e9', 'ci', 'pro', 'que', '.']"}], "[\n  [\"\u2581La\", null],\n  [\"\u2581rencontre\", \"\u0432\u0441\u0442\u0440\u0435\u0447\u0430\"],\n  [\"\u2581de\", null],\n  [\"\u2581Glen\", \"\u0413\u043b\u0435\u043d\u0430\u0440\u0432\u0430\u043d\"],\n  [\"ar\", null],\n  [\"van\", null],\n  [\"\u2581et\", null],\n  [\"\u2581A\", \"\u0410\u0439\u0440\u0442\u043e\u043d\"],\n  [\"yr\", null],\n  [\"ton\", null],\n  [\"\u2581avait\", null],\n  [\"\u2581\u00e9v\", \"\u043e\u0447\u0435\u0432\u0438\u0434\u043d\u043e\"],\n  [\"ide\", null],\n  [\"m\", null],\n  [\"ment\", null],\n  [\"\u2581produit\", \"\u0432\u044b\u0437\u0432\u0430\u043b\u0430\"],\n  [\"\u2581chez\", \"\u0443\"],\n  [\"\u2581tous\", \"\u043e\u0431\u043e\u0438\u0445\"],\n  [\"\u2581deux\", null],\n  [\"\u2581une\", null],\n  [\"\u2581\u00e9\", \"\u044d\u043c\u043e\u0446\u0438\u044e\"],\n  [\"m\", null],\n  [\"otion\", null],\n  [\"\u2581r\u00e9\", \"\u0432\u0437\u0430\u0438\u043c\u043d\u0443\u044e\"],\n  [\"ci\", null],\n  [\"pro\", null],\n  [\"que\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0415\u0441\u043b\u0438 \u0431 \u044f \u043c\u043e\u0433 \u043b\u044e\u0431\u0438\u0442\u044c \u0432\u0441\u0435 \u044d\u0442\u043e \u0434\u0435\u043b\u043e, \u043a\u0430\u043a \u044f \u043b\u044e\u0431\u043b\u044e \u0442\u0435\u0431\u044f... \u0430 \u0442\u043e \u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435 \u0432\u0440\u0435\u043c\u044f \u0434\u0435\u043b\u0430\u044e, \u043a\u0430\u043a \u0437\u0430\u0434\u0430\u043d\u043d\u044b\u0439 \u0443\u0440\u043e\u043a.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0415', '\u0441\u043b\u0438', '\u2581\u0431', '\u2581\u044f', '\u2581\u043c\u043e', '\u0433', '\u2581\u043b\u044e', '\u0431\u0438', '\u0442\u044c', '\u2581\u0432\u0441\u0435', '\u2581\u044d\u0442\u043e', '\u2581\u0434\u0435', '\u043b\u043e', ',', '\u2581\u043a\u0430\u043a', '\u2581\u044f', '\u2581\u043b\u044e', '\u0431', '\u043b\u044e', '\u2581\u0442\u0435', '\u0431\u044f', '...', '\u2581\u0430', '\u2581\u0442\u043e', '\u2581\u044f', '\u2581\u043f\u043e\u0441\u043b\u0435\u0434', '\u043d\u0435\u0435', '\u2581\u0432\u0440\u0435\u043c\u044f', '\u2581\u0434\u0435\u043b\u0430', '\u044e', ',', '\u2581\u043a\u0430\u043a', '\u2581\u0437\u0430', '\u0434\u0430\u043d', '\u043d\u044b\u0439', '\u2581\u0443', '\u0440\u043e\u043a', '.']"}], "[\n[\"\u2581\u0415\", \"If\"],\n[\"\u0441\u043b\u0438\", null],\n[\"\u2581\u0431\", null],\n[\"\u2581\u044f\", null],\n[\"\u2581\u043c\u043e\", \"could\"],\n[\"\u0433\", null],\n[\"\u2581\u043b\u044e\", \"love\"],\n[\"\u0431\u0438\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0432\u0441\u0435\", \"all\"],\n[\"\u2581\u044d\u0442\u043e\", \"this\"],\n[\"\u2581\u0434\u0435\", \"work\"],\n[\"\u043b\u043e\", null],\n[\",\" , null],\n[\"\u2581\u043a\u0430\u043a\", \"as\"],\n[\"\u2581\u044f\", null],\n[\"\u2581\u043b\u044e\", \"love\"],\n[\"\u0431\", null],\n[\"\u043b\u044e\", null],\n[\"\u2581\u0442\u0435\", \"you\"],\n[\"\u0431\u044f\", null],\n[\"...\", null],\n[\"\u2581\u0430\", \"but\"],\n[\"\u2581\u0442\u043e\", null],\n[\"\u2581\u044f\", null],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\u0434\", \"lately\"],\n[\"\u043d\u0435\u0435\", null],\n[\"\u2581\u0432\u0440\u0435\u043c\u044f\", \"time\"],\n[\"\u2581\u0434\u0435\u043b\u0430\", \"do\"],\n[\"\u044e\", null],\n[\",\", null],\n[\"\u2581\u043a\u0430\u043a\", \"as\"],\n[\"\u2581\u0437\u0430\", \"assigned\"],\n[\"\u0434\u0430\u043d\", null],\n[\"\u043d\u044b\u0439\", null],\n[\"\u2581\u0443\", \"lesson\"],\n[\"\u0440\u043e\u043a\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u0415\u0441\u043b\u0438 \u0431 \u044f \u043c\u043e\u0433 \u043b\u044e\u0431\u0438\u0442\u044c \u0432\u0441\u0435 \u044d\u0442\u043e \u0434\u0435\u043b\u043e, \u043a\u0430\u043a \u044f \u043b\u044e\u0431\u043b\u044e \u0442\u0435\u0431\u044f... \u0430 \u0442\u043e \u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435 \u0432\u0440\u0435\u043c\u044f \u0434\u0435\u043b\u0430\u044e, \u043a\u0430\u043a \u0437\u0430\u0434\u0430\u043d\u043d\u044b\u0439 \u0443\u0440\u043e\u043a.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0415', '\u0441\u043b\u0438', '\u2581\u0431', '\u2581\u044f', '\u2581\u043c\u043e', '\u0433', '\u2581\u043b\u044e', '\u0431\u0438', '\u0442\u044c', '\u2581\u0432\u0441\u0435', '\u2581\u044d\u0442\u043e', '\u2581\u0434\u0435', '\u043b\u043e', ',', '\u2581\u043a\u0430\u043a', '\u2581\u044f', '\u2581\u043b\u044e', '\u0431', '\u043b\u044e', '\u2581\u0442\u0435', '\u0431\u044f', '...', '\u2581\u0430', '\u2581\u0442\u043e', '\u2581\u044f', '\u2581\u043f\u043e\u0441\u043b\u0435\u0434', '\u043d\u0435\u0435', '\u2581\u0432\u0440\u0435\u043c\u044f', '\u2581\u0434\u0435\u043b\u0430', '\u044e', ',', '\u2581\u043a\u0430\u043a', '\u2581\u0437\u0430', '\u0434\u0430\u043d', '\u043d\u044b\u0439', '\u2581\u0443', '\u0440\u043e\u043a', '.']"}], "[\n[\"\u2581\u0415\", null],\n[\"\u0441\u043b\u0438\", \"Si\"],\n[\"\u2581\u0431\", null],\n[\"\u2581\u044f\", null],\n[\"\u2581\u043c\u043e\", \"pouvais\"],\n[\"\u0433\", null],\n[\"\u2581\u043b\u044e\", \"aimer\"],\n[\"\u0431\u0438\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0432\u0441\u0435\", \"tout\"],\n[\"\u2581\u044d\u0442\u043e\", \"cela\"],\n[\"\u2581\u0434\u0435\", \"affaire\"],\n[\"\u043b\u043e\", null],\n[\",\", null],\n[\"\u2581\u043a\u0430\u043a\", \"comme\"],\n[\"\u2581\u044f\", null],\n[\"\u2581\u043b\u044e\", \"aime\"],\n[\"\u0431\", null],\n[\"\u043b\u044e\", null],\n[\"\u2581\u0442\u0435\", \"toi\"],\n[\"\u0431\u044f\", null],\n[\"...\", null],\n[\"\u2581\u0430\", \"mais\"],\n[\"\u2581\u0442\u043e\", null],\n[\"\u2581\u044f\", null],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\u0434\", \"derni\u00e8rement\"],\n[\"\u043d\u0435\u0435\", null],\n[\"\u2581\u0432\u0440\u0435\u043c\u044f\", \"temps\"],\n[\"\u2581\u0434\u0435\u043b\u0430\", \"fais\"],\n[\"\u044e\", null],\n[\",\", null],\n[\"\u2581\u043a\u0430\u043a\", \"comme\"],\n[\"\u2581\u0437\u0430\", \"devoir\"],\n[\"\u0434\u0430\u043d\", null],\n[\"\u043d\u044b\u0439\", null],\n[\"\u2581\u0443\", \"le\u00e7on\"],\n[\"\u0440\u043e\u043a\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n-- \u0410 \u043d\u0430 \u044d\u0442\u0438 \u0434\u0435\u043d\u044c\u0433\u0438 \u043e\u043d \u0431\u044b \u043d\u0430\u043a\u0443\u043f\u0438\u043b \u0441\u043a\u043e\u0442\u0430 \u0438\u043b\u0438 \u0437\u0435\u043c\u043b\u0438\u0446\u0443 \u043a\u0443\u043f\u0438\u043b \u0431\u044b \u0437\u0430 \u0431\u0435\u0441\u0446\u0435\u043d\u043e\u043a \u0438 \u043c\u0443\u0436\u0438\u043a\u0430\u043c \u0440\u043e\u0437\u0434\u0430\u043b \u0431\u044b \u0432\u043d\u0430\u0439\u043c\u044b, -- \u0441 \u0443\u043b\u044b\u0431\u043a\u043e\u0439 \u0434\u043e\u043a\u043e\u043d\u0447\u0438\u043b \u041b\u0435\u0432\u0438\u043d, \u043e\u0447\u0435\u0432\u0438\u0434\u043d\u043e \u043d\u0435 \u0440\u0430\u0437 \u0443\u0436\u0435 \u0441\u0442\u0430\u043b\u043a\u0438\u0432\u0430\u0432\u0448\u0438\u0439\u0441\u044f \u0441 \u043f\u043e\u0434\u043e\u0431\u043d\u044b\u043c\u0438 \u0440\u0430\u0441\u0447\u0435\u0442\u0430\u043c\u0438. -- \u0418 \u043e\u043d \u0441\u043e\u0441\u0442\u0430\u0432\u0438\u0442 \u0441\u0435\u0431\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581--', '\u2581\u0410', '\u2581\u043d\u0430', '\u2581\u044d\u0442\u0438', '\u2581\u0434\u0435\u043d\u044c', '\u0433\u0438', '\u2581\u043e\u043d', '\u2581\u0431\u044b', '\u2581\u043d\u0430', '\u043a\u0443', '\u043f\u0438', '\u043b', '\u2581\u0441\u043a\u043e', '\u0442\u0430', '\u2581\u0438\u043b\u0438', '\u2581\u0437\u0435\u043c', '\u043b\u0438', '\u0446\u0443', '\u2581\u043a\u0443', '\u043f\u0438', '\u043b', '\u2581\u0431\u044b', '\u2581\u0437\u0430', '\u2581\u0431', '\u0435\u0441', '\u0446\u0435', '\u043d\u043e\u043a', '\u2581\u0438', '\u2581\u043c\u0443', '\u0436\u0438', '\u043a\u0430\u043c', '\u2581\u0440\u043e\u0437', '\u0434\u0430', '\u043b', '\u2581\u0431\u044b', '\u2581\u0432', '\u043d\u0430', '\u0439', '\u043c\u044b', ',', '\u2581--', '\u2581\u0441', '\u2581\u0443', '\u043b\u044b', '\u0431', '\u043a\u043e\u0439', '\u2581\u0434\u043e', '\u043a\u043e\u043d', '\u0447\u0438\u043b', '\u2581\u041b\u0435', '\u0432\u0438\u043d', ',', '\u2581\u043e', '\u0447\u0435', '\u0432\u0438', '\u0434\u043d\u043e', '\u2581\u043d\u0435', '\u2581\u0440\u0430\u0437', '\u2581\u0443\u0436\u0435', '\u2581\u0441\u0442\u0430', '\u043b\u043a\u0438', '\u0432\u0430', '\u0432\u0448\u0438\u0439', '\u0441\u044f', '\u2581\u0441', '\u2581\u043f\u043e\u0434\u043e', '\u0431', '\u043d\u044b\u043c\u0438', '\u2581\u0440\u0430\u0441', '\u0447\u0435', '\u0442\u0430\u043c\u0438', '.', '\u2581--', '\u2581\u0418', '\u2581\u043e\u043d', '\u2581\u0441\u043e', '\u0441\u0442\u0430\u0432\u0438', '\u0442', '\u2581\u0441\u0435\u0431\u0435', '\u2581\u0441\u043e\u0441\u0442\u043e\u044f', '\u043d\u0438\u0435', '.']"}], "[\n[\"\u2581--\", null],\n[\"\u2581\u0410\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u044d\u0442\u0438\", \"these\"],\n[\"\u2581\u0434\u0435\u043d\u044c\", \"money\"],\n[\"\u0433\u0438\", null],\n[\"\u2581\u043e\u043d\", \"he\"],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u043a\u0443\", \"buy\"],\n[\"\u043f\u0438\", null],\n[\"\u043b\", null],\n[\"\u2581\u0441\u043a\u043e\", \"cattle\"],\n[\"\u0442\u0430\", null],\n[\"\u2581\u0438\u043b\u0438\", \"or\"],\n[\"\u2581\u0437\u0435\u043c\", \"land\"],\n[\"\u043b\u0438\", null],\n[\"\u0446\u0443\", null],\n[\"\u2581\u043a\u0443\", \"buy\"],\n[\"\u043f\u0438\", null],\n[\"\u043b\", null],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u0437\u0430\", \"for\"],\n[\"\u2581\u0431\", \"dirt\"],\n[\"\u0435\u0441\", null],\n[\"\u0446\u0435\", null],\n[\"\u043d\u043e\u043a\", null],\n[\"\u2581\u0438\", \"and\"],\n[\"\u2581\u043c\u0443\", \"peasants\"],\n[\"\u0436\u0438\", null],\n[\"\u043a\u0430\u043c\", null],\n[\"\u2581\u0440\u043e\u0437\", \"distribute\"],\n[\"\u0434\u0430\", null],\n[\"\u043b\", null],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u0432\", null],\n[\"\u043d\u0430\", \"rent\"],\n[\"\u0439\", null],\n[\"\u043c\u044b\", null],\n[\",\", null],\n[\"\u2581--\", null],\n[\"\u2581\u0441\", \"with\"],\n[\"\u2581\u0443\", \"smile\"],\n[\"\u043b\u044b\", null],\n[\"\u0431\", null],\n[\"\u043a\u043e\u0439\", null],\n[\"\u2581\u0434\u043e\", \"finished\"],\n[\"\u043a\u043e\u043d\", null],\n[\"\u0447\u0438\u043b\", null],\n[\"\u2581\u041b\u0435\", \"Levin\"],\n[\"\u0432\u0438\u043d\", null],\n[\",\", null],\n[\"\u2581\u043e\", \"obviously\"],\n[\"\u0447\u0435\", null],\n[\"\u0432\u0438\", null],\n[\"\u0434\u043d\u043e\", null],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0440\u0430\u0437\", \"repeatedly\"],\n[\"\u2581\u0443\u0436\u0435\", \"already\"],\n[\"\u2581\u0441\u0442\u0430\", \"encountered\"],\n[\"\u043b\u043a\u0438\", null],\n[\"\u0432\u0430\", null],\n[\"\u0432\u0448\u0438\u0439\", null],\n[\"\u0441\u044f\", null],\n[\"\u2581\u0441\", \"with\"],\n[\"\u2581\u043f\u043e\u0434\u043e\", \"similar\"],\n[\"\u0431\", null],\n[\"\u043d\u044b\u043c\u0438\", null],\n[\"\u2581\u0440\u0430\u0441\", \"calculations\"],\n[\"\u0447\u0435\", null],\n[\"\u0442\u0430\u043c\u0438\", null],\n[\".\", null],\n[\"\u2581--\", null],\n[\"\u2581\u0418\", \"And\"],\n[\"\u2581\u043e\u043d\", \"he\"],\n[\"\u2581\u0441\u043e\", \"will\"],\n[\"\u0441\u0442\u0430\u0432\u0438\", \"make\"],\n[\"\u0442\", null],\n[\"\u2581\u0441\u0435\u0431\u0435\", \"himself\"],\n[\"\u2581\u0441\u043e\u0441\u0442\u043e\u044f\", \"fortune\"],\n[\"\u043d\u0438\u0435\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n-- \u0410 \u043d\u0430 \u044d\u0442\u0438 \u0434\u0435\u043d\u044c\u0433\u0438 \u043e\u043d \u0431\u044b \u043d\u0430\u043a\u0443\u043f\u0438\u043b \u0441\u043a\u043e\u0442\u0430 \u0438\u043b\u0438 \u0437\u0435\u043c\u043b\u0438\u0446\u0443 \u043a\u0443\u043f\u0438\u043b \u0431\u044b \u0437\u0430 \u0431\u0435\u0441\u0446\u0435\u043d\u043e\u043a \u0438 \u043c\u0443\u0436\u0438\u043a\u0430\u043c \u0440\u043e\u0437\u0434\u0430\u043b \u0431\u044b \u0432\u043d\u0430\u0439\u043c\u044b, -- \u0441 \u0443\u043b\u044b\u0431\u043a\u043e\u0439 \u0434\u043e\u043a\u043e\u043d\u0447\u0438\u043b \u041b\u0435\u0432\u0438\u043d, \u043e\u0447\u0435\u0432\u0438\u0434\u043d\u043e \u043d\u0435 \u0440\u0430\u0437 \u0443\u0436\u0435 \u0441\u0442\u0430\u043b\u043a\u0438\u0432\u0430\u0432\u0448\u0438\u0439\u0441\u044f \u0441 \u043f\u043e\u0434\u043e\u0431\u043d\u044b\u043c\u0438 \u0440\u0430\u0441\u0447\u0435\u0442\u0430\u043c\u0438. -- \u0418 \u043e\u043d \u0441\u043e\u0441\u0442\u0430\u0432\u0438\u0442 \u0441\u0435\u0431\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581--', '\u2581\u0410', '\u2581\u043d\u0430', '\u2581\u044d\u0442\u0438', '\u2581\u0434\u0435\u043d\u044c', '\u0433\u0438', '\u2581\u043e\u043d', '\u2581\u0431\u044b', '\u2581\u043d\u0430', '\u043a\u0443', '\u043f\u0438', '\u043b', '\u2581\u0441\u043a\u043e', '\u0442\u0430', '\u2581\u0438\u043b\u0438', '\u2581\u0437\u0435\u043c', '\u043b\u0438', '\u0446\u0443', '\u2581\u043a\u0443', '\u043f\u0438', '\u043b', '\u2581\u0431\u044b', '\u2581\u0437\u0430', '\u2581\u0431', '\u0435\u0441', '\u0446\u0435', '\u043d\u043e\u043a', '\u2581\u0438', '\u2581\u043c\u0443', '\u0436\u0438', '\u043a\u0430\u043c', '\u2581\u0440\u043e\u0437', '\u0434\u0430', '\u043b', '\u2581\u0431\u044b', '\u2581\u0432', '\u043d\u0430', '\u0439', '\u043c\u044b', ',', '\u2581--', '\u2581\u0441', '\u2581\u0443', '\u043b\u044b', '\u0431', '\u043a\u043e\u0439', '\u2581\u0434\u043e', '\u043a\u043e\u043d', '\u0447\u0438\u043b', '\u2581\u041b\u0435', '\u0432\u0438\u043d', ',', '\u2581\u043e', '\u0447\u0435', '\u0432\u0438', '\u0434\u043d\u043e', '\u2581\u043d\u0435', '\u2581\u0440\u0430\u0437', '\u2581\u0443\u0436\u0435', '\u2581\u0441\u0442\u0430', '\u043b\u043a\u0438', '\u0432\u0430', '\u0432\u0448\u0438\u0439', '\u0441\u044f', '\u2581\u0441', '\u2581\u043f\u043e\u0434\u043e', '\u0431', '\u043d\u044b\u043c\u0438', '\u2581\u0440\u0430\u0441', '\u0447\u0435', '\u0442\u0430\u043c\u0438', '.', '\u2581--', '\u2581\u0418', '\u2581\u043e\u043d', '\u2581\u0441\u043e', '\u0441\u0442\u0430\u0432\u0438', '\u0442', '\u2581\u0441\u0435\u0431\u0435', '\u2581\u0441\u043e\u0441\u0442\u043e\u044f', '\u043d\u0438\u0435', '.']"}], "[\n[\"\u2581--\", null],\n[\"\u2581\u0410\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u044d\u0442\u0438\", \"ces\"],\n[\"\u2581\u0434\u0435\u043d\u044c\", \"argent\"],\n[\"\u0433\u0438\", null],\n[\"\u2581\u043e\u043d\", \"il\"],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u043a\u0443\", \"acheter\"],\n[\"\u043f\u0438\", null],\n[\"\u043b\", null],\n[\"\u2581\u0441\u043a\u043e\", \"b\u00e9tail\"],\n[\"\u0442\u0430\", null],\n[\"\u2581\u0438\u043b\u0438\", \"ou\"],\n[\"\u2581\u0437\u0435\u043c\", \"terre\"],\n[\"\u043b\u0438\", null],\n[\"\u0446\u0443\", null],\n[\"\u2581\u043a\u0443\", \"acheter\"],\n[\"\u043f\u0438\", null],\n[\"\u043b\", null],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u0437\u0430\", \"pour\"],\n[\"\u2581\u0431\", \"rien\"],\n[\"\u0435\u0441\", null],\n[\"\u0446\u0435\", null],\n[\"\u043d\u043e\u043a\", null],\n[\"\u2581\u0438\", \"et\"],\n[\"\u2581\u043c\u0443\", \"paysans\"],\n[\"\u0436\u0438\", null],\n[\"\u043a\u0430\u043c\", null],\n[\"\u2581\u0440\u043e\u0437\", \"distribuer\"],\n[\"\u0434\u0430\", null],\n[\"\u043b\", null],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u0432\", null],\n[\"\u043d\u0430\", \"location\"],\n[\"\u0439\", null],\n[\"\u043c\u044b\", null],\n[\",\", null],\n[\"\u2581--\", null],\n[\"\u2581\u0441\", \"avec\"],\n[\"\u2581\u0443\", \"sourire\"],\n[\"\u043b\u044b\", null],\n[\"\u0431\", null],\n[\"\u043a\u043e\u0439\", null],\n[\"\u2581\u0434\u043e\", \"terminer\"],\n[\"\u043a\u043e\u043d\", null],\n[\"\u0447\u0438\u043b\", null],\n[\"\u2581\u041b\u0435\", \"L\u00e9vine\"],\n[\"\u0432\u0438\u043d\", null],\n[\",\", null],\n[\"\u2581\u043e\", \"\u00e9videmment\"],\n[\"\u0447\u0435\", null],\n[\"\u0432\u0438\", null],\n[\"\u0434\u043d\u043e\", null],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0440\u0430\u0437\", \"plusieurs fois\"],\n[\"\u2581\u0443\u0436\u0435\", \"d\u00e9j\u00e0\"],\n[\"\u2581\u0441\u0442\u0430\", \"confront\u00e9\"],\n[\"\u043b\u043a\u0438\", null],\n[\"\u0432\u0430\", null],\n[\"\u0432\u0448\u0438\u0439\", null],\n[\"\u0441\u044f\", null],\n[\"\u2581\u0441\", \"\u00e0\"],\n[\"\u2581\u043f\u043e\u0434\u043e\", \"similaires\"],\n[\"\u0431\", null],\n[\"\u043d\u044b\u043c\u0438\", null],\n[\"\u2581\u0440\u0430\u0441\", \"calculs\"],\n[\"\u0447\u0435\", null],\n[\"\u0442\u0430\u043c\u0438\", null],\n[\".\", null],\n[\"\u2581--\", null],\n[\"\u2581\u0418\", \"Et\"],\n[\"\u2581\u043e\u043d\", \"il\"],\n[\"\u2581\u0441\u043e\", \"constituer\"],\n[\"\u0441\u0442\u0430\u0432\u0438\", null],\n[\"\u0442\", null],\n[\"\u2581\u0441\u0435\u0431\u0435\", \"soi\"],\n[\"\u2581\u0441\u043e\u0441\u0442\u043e\u044f\", \"fortune\"],\n[\"\u043d\u0438\u0435\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nIf only my malediction is needed for that, I bestow it upon him! and his cathedral, and his mills!\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581If', '\u2581only', '\u2581my', '\u2581m', 'aled', 'iction', '\u2581is', '\u2581needed', '\u2581for', '\u2581that', ',', '\u2581I', '\u2581best', 'ow', '\u2581it', '\u2581upon', '\u2581him', '!', '\u2581and', '\u2581his', '\u2581cat', 'hedral', ',', '\u2581and', '\u2581his', '\u2581mill', 's', '!']"}], "[\n  [\"\u2581If\", \"Si\"],\n  [\"\u2581only\", null],\n  [\"\u2581my\", null],\n  [\"\u2581m\", \"mal\u00e9diction\"],\n  [\"aled\", null],\n  [\"iction\", null],\n  [\"\u2581is\", null],\n  [\"\u2581needed\", \"n\u00e9cessaire\"],\n  [\"\u2581for\", null],\n  [\"\u2581that\", null],\n  [\",\", null],\n  [\"\u2581I\", null],\n  [\"\u2581best\", \"accorde\"],\n  [\"ow\", null],\n  [\"\u2581it\", null],\n  [\"\u2581upon\", null],\n  [\"\u2581him\", \"lui\"],\n  [\"!\", null],\n  [\"\u2581and\", null],\n  [\"\u2581his\", null],\n  [\"\u2581cat\", \"cath\u00e9drale\"],\n  [\"hedral\", null],\n  [\",\", null],\n  [\"\u2581and\", null],\n  [\"\u2581his\", null],\n  [\"\u2581mill\", \"moulins\"],\n  [\"s\", null],\n  [\"!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nIf only my malediction is needed for that, I bestow it upon him! and his cathedral, and his mills!\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581If', '\u2581only', '\u2581my', '\u2581m', 'aled', 'iction', '\u2581is', '\u2581needed', '\u2581for', '\u2581that', ',', '\u2581I', '\u2581best', 'ow', '\u2581it', '\u2581upon', '\u2581him', '!', '\u2581and', '\u2581his', '\u2581cat', 'hedral', ',', '\u2581and', '\u2581his', '\u2581mill', 's', '!']"}], "[\n    [\"\u2581If\", \"\u0415\u0441\u043b\u0438\"],\n    [\"\u2581only\", null],\n    [\"\u2581my\", null],\n    [\"\u2581m\", \"\u043f\u0440\u043e\u043a\u043b\u044f\u0442\u0438\u0435\"],\n    [\"aled\", null],\n    [\"iction\", null],\n    [\"\u2581is\", null],\n    [\"\u2581needed\", \"\u043d\u0443\u0436\u043d\u043e\"],\n    [\"\u2581for\", null],\n    [\"\u2581that\", \"\u044d\u0442\u043e\u0433\u043e\"],\n    [\",\", null],\n    [\"\u2581I\", \"\u044f\"],\n    [\"\u2581best\", \"\u0434\u0430\u0440\u0443\u044e\"],\n    [\"ow\", null],\n    [\"\u2581it\", \"\u0435\u0433\u043e\"],\n    [\"\u2581upon\", null],\n    [\"\u2581him\", \"\u0435\u043c\u0443\"],\n    [\"!\", null],\n    [\"\u2581and\", \"\u0438\"],\n    [\"\u2581his\", \"\u0435\u0433\u043e\"],\n    [\"\u2581cat\", \"\u0441\u043e\u0431\u043e\u0440\"],\n    [\"hedral\", null],\n    [\",\", null],\n    [\"\u2581and\", \"\u0438\"],\n    [\"\u2581his\", \"\u0435\u0433\u043e\"],\n    [\"\u2581mill\", \"\u043c\u0435\u043b\u044c\u043d\u0438\u0446\u044b\"],\n    [\"s\", null],\n    [\"!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nLa Gr\u00e8ve avait d\u00e8s lors cet aspect sinistre que lui conservent encore aujourd\u2019hui l\u2019id\u00e9e ex\u00e9crable qu\u2019elle r\u00e9veille et le sombre H\u00f4tel de Ville de Dominique Boccador, qui a remplac\u00e9 la Maison-aux-Piliers.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581La', '\u2581Gr', '\u00e8ve', '\u2581avait', '\u2581d', '\u00e8s', '\u2581lors', '\u2581cet', '\u2581aspect', '\u2581sin', 'ist', 're', '\u2581que', '\u2581lui', '\u2581con', 'ser', 'vent', '\u2581encore', '\u2581au', 'jour', 'd', '\u2019', 'hui', '\u2581l', '\u2019', 'id', '\u00e9e', '\u2581ex', '\u00e9', 'cr', 'able', '\u2581qu', '\u2019', 'elle', '\u2581r\u00e9', 've', 'ille', '\u2581et', '\u2581le', '\u2581som', 'bre', '\u2581H', '\u00f4t', 'el', '\u2581de', '\u2581V', 'ille', '\u2581de', '\u2581Domin', 'ique', '\u2581B', 'occ', 'ador', ',', '\u2581qui', '\u2581a', '\u2581rem', 'plac', '\u00e9', '\u2581la', '\u2581M', 'aison', '-', 'aux', '-', 'P', 'ili', 'ers', '.']"}], "[\n[\"\u2581La\", \"The\"],\n[\"\u2581Gr\", \"Square\"],\n[\"\u00e8ve\", null],\n[\"\u2581avait\", \"had\"],\n[\"\u2581d\", null],\n[\"\u00e8s\", null],\n[\"\u2581lors\", \"then\"],\n[\"\u2581cet\", \"that\"],\n[\"\u2581aspect\", \"appearance\"],\n[\"\u2581sin\", \"sinister\"],\n[\"ist\", null],\n[\"re\", null],\n[\"\u2581que\", \"that\"],\n[\"\u2581lui\", null],\n[\"\u2581con\", \"preserve\"],\n[\"ser\", null],\n[\"vent\", null],\n[\"\u2581encore\", \"still\"],\n[\"\u2581au\", null],\n[\"jour\", \"today\"],\n[\"d\", null],\n[\"'\", null],\n[\"hui\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"id\", \"idea\"],\n[\"\u00e9e\", null],\n[\"\u2581ex\", \"execrable\"],\n[\"\u00e9\", null],\n[\"cr\", null],\n[\"able\", null],\n[\"\u2581qu\", \"that\"],\n[\"'\", null],\n[\"elle\", \"it\"],\n[\"\u2581r\u00e9\", \"awakens\"],\n[\"ve\", null],\n[\"ille\", null],\n[\"\u2581et\", \"and\"],\n[\"\u2581le\", \"the\"],\n[\"\u2581som\", \"somber\"],\n[\"bre\", null],\n[\"\u2581H\", \"City\"],\n[\"\u00f4t\", null],\n[\"el\", null],\n[\"\u2581de\", null],\n[\"\u2581V\", null],\n[\"ille\", null],\n[\"\u2581de\", \"of\"],\n[\"\u2581Domin\", \"Dominique\"],\n[\"ique\", null],\n[\"\u2581B\", \"Boccador\"],\n[\"occ\", null],\n[\"ador\", null],\n[\",\", null],\n[\"\u2581qui\", \"which\"],\n[\"\u2581a\", \"has\"],\n[\"\u2581rem\", \"replaced\"],\n[\"plac\", null],\n[\"\u00e9\", null],\n[\"\u2581la\", \"the\"],\n[\"\u2581M\", \"House\"],\n[\"aison\", null],\n[\"-\", null],\n[\"aux\", null],\n[\"-\", null],\n[\"\u2581P\", \"Pillars\"],\n[\"ili\", null],\n[\"ers\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nLa Gr\u00e8ve avait d\u00e8s lors cet aspect sinistre que lui conservent encore aujourd\u2019hui l\u2019id\u00e9e ex\u00e9crable qu\u2019elle r\u00e9veille et le sombre H\u00f4tel de Ville de Dominique Boccador, qui a remplac\u00e9 la Maison-aux-Piliers.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581La', '\u2581Gr', '\u00e8ve', '\u2581avait', '\u2581d', '\u00e8s', '\u2581lors', '\u2581cet', '\u2581aspect', '\u2581sin', 'ist', 're', '\u2581que', '\u2581lui', '\u2581con', 'ser', 'vent', '\u2581encore', '\u2581au', 'jour', 'd', '\u2019', 'hui', '\u2581l', '\u2019', 'id', '\u00e9e', '\u2581ex', '\u00e9', 'cr', 'able', '\u2581qu', '\u2019', 'elle', '\u2581r\u00e9', 've', 'ille', '\u2581et', '\u2581le', '\u2581som', 'bre', '\u2581H', '\u00f4t', 'el', '\u2581de', '\u2581V', 'ille', '\u2581de', '\u2581Domin', 'ique', '\u2581B', 'occ', 'ador', ',', '\u2581qui', '\u2581a', '\u2581rem', 'plac', '\u00e9', '\u2581la', '\u2581M', 'aison', '-', 'aux', '-', 'P', 'ili', 'ers', '.']"}], "[\n[\"\u2581La\", \"\u041f\u043b\u043e\u0449\u0430\u0434\u044c\"],\n[\"\u2581Gr\", null],\n[\"\u00e8ve\", null],\n[\"\u2581avait\", \"\u0438\u043c\u0435\u043b\u0430\"],\n[\"\u2581d\", null],\n[\"\u00e8s\", null],\n[\"\u2581lors\", \"\u0442\u043e\u0433\u0434\u0430\"],\n[\"\u2581cet\", \"\u044d\u0442\u043e\u0442\"],\n[\"\u2581aspect\", \"\u0432\u0438\u0434\"],\n[\"\u2581sin\", null],\n[\"ist\", null],\n[\"re\", null],\n[\"\u2581que\", null],\n[\"\u2581lui\", null],\n[\"\u2581con\", null],\n[\"ser\", null],\n[\"vent\", null],\n[\"\u2581encore\", \"\u0434\u043e \u0441\u0438\u0445 \u043f\u043e\u0440\"],\n[\"\u2581au\", null],\n[\"jour\", null],\n[\"d\", null],\n[\"'\", null],\n[\"hui\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"id\", null],\n[\"\u00e9e\", null],\n[\"\u2581ex\", null],\n[\"\u00e9\", null],\n[\"cr\", null],\n[\"able\", null],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"elle\", null],\n[\"\u2581r\u00e9\", null],\n[\"ve\", null],\n[\"ille\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581le\", null],\n[\"\u2581som\", null],\n[\"bre\", null],\n[\"\u2581H\", null],\n[\"\u00f4t\", null],\n[\"el\", null],\n[\"\u2581de\", null],\n[\"\u2581V\", null],\n[\"ille\", null],\n[\"\u2581de\", null],\n[\"\u2581Domin\", null],\n[\"ique\", null],\n[\"\u2581B\", null],\n[\"occ\", null],\n[\"ador\", null],\n[\",\", null],\n[\"\u2581qui\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u0439\"],\n[\"\u2581a\", null],\n[\"\u2581rem\", null],\n[\"plac\", null],\n[\"\u00e9\", null],\n[\"\u2581la\", null],\n[\"\u2581M\", null],\n[\"aison\", null],\n[\"-\", null],\n[\"aux\", null],\n[\"-\", null],\n[\"\u2581P\", null],\n[\"ili\", null],\n[\"ers\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nQuand ma\u00eetre Philippe Lheulier, avocat extraordinaire du roi, avait dit \u00e0 la pauvre Esmeralda : Il se meurt, c\u2019\u00e9tait par erreur ou par plaisanterie.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Qu', 'and', '\u2581ma', '\u00eetre', '\u2581Philippe', '\u2581L', 'he', 'ul', 'ier', ',', '\u2581av', 'oc', 'at', '\u2581extraord', 'ina', 'ire', '\u2581du', '\u2581roi', ',', '\u2581avait', '\u2581dit', '\u2581\u00e0', '\u2581la', '\u2581p', 'au', 'vre', '\u2581Es', 'mer', 'al', 'da', '\u2581:', '\u2581Il', '\u2581se', '\u2581me', 'urt', ',', '\u2581c', '\u2019', '\u00e9tait', '\u2581par', '\u2581erre', 'ur', '\u2581ou', '\u2581par', '\u2581pl', 'ais', 'anter', 'ie', '.']"}], "[\n  [\"\u2581Qu\", \"When\"],\n  [\"and\", null],\n  [\"\u2581ma\", \"master\"],\n  [\"\u00eetre\", null],\n  [\"\u2581Philippe\", \"Philippe\"],\n  [\"\u2581L\", \"Lheulier\"],\n  [\"he\", null],\n  [\"ul\", null],\n  [\"ier\", null],\n  [\",\", null],\n  [\"\u2581av\", \"lawyer\"],\n  [\"oc\", null],\n  [\"at\", null],\n  [\"\u2581extraord\", \"extraordinary\"],\n  [\"ina\", null],\n  [\"ire\", null],\n  [\"\u2581du\", null],\n  [\"\u2581roi\", \"king\"],\n  [\",\", null],\n  [\"\u2581avait\", \"had\"],\n  [\"\u2581dit\", \"said\"],\n  [\"\u2581\u00e0\", \"to\"],\n  [\"\u2581la\", \"the\"],\n  [\"\u2581p\", \"poor\"],\n  [\"au\", null],\n  [\"vre\", null],\n  [\"\u2581Es\", \"Esmeralda\"],\n  [\"mer\", null],\n  [\"al\", null],\n  [\"da\", null],\n  [\"\u2581:\", null],\n  [\"\u2581Il\", \"He\"],\n  [\"\u2581se\", null],\n  [\"\u2581me\", \"is dying\"],\n  [\"urt\", null],\n  [\",\", null],\n  [\"\u2581c\", \"it\"],\n  [\"'\", null],\n  [\"\u00e9tait\", \"was\"],\n  [\"\u2581par\", \"by\"],\n  [\"\u2581erre\", \"mistake\"],\n  [\"ur\", null],\n  [\"\u2581ou\", \"or\"],\n  [\"\u2581par\", \"by\"],\n  [\"\u2581pl\", \"jest\"],\n  [\"ais\", null],\n  [\"anter\", null],\n  [\"ie\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nQuand ma\u00eetre Philippe Lheulier, avocat extraordinaire du roi, avait dit \u00e0 la pauvre Esmeralda : Il se meurt, c\u2019\u00e9tait par erreur ou par plaisanterie.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Qu', 'and', '\u2581ma', '\u00eetre', '\u2581Philippe', '\u2581L', 'he', 'ul', 'ier', ',', '\u2581av', 'oc', 'at', '\u2581extraord', 'ina', 'ire', '\u2581du', '\u2581roi', ',', '\u2581avait', '\u2581dit', '\u2581\u00e0', '\u2581la', '\u2581p', 'au', 'vre', '\u2581Es', 'mer', 'al', 'da', '\u2581:', '\u2581Il', '\u2581se', '\u2581me', 'urt', ',', '\u2581c', '\u2019', '\u00e9tait', '\u2581par', '\u2581erre', 'ur', '\u2581ou', '\u2581par', '\u2581pl', 'ais', 'anter', 'ie', '.']"}], "[\n  [\"\u2581Qu\", \"\u041a\u043e\u0433\u0434\u0430\"],\n  [\"and\", null],\n  [\"\u2581ma\", \"\u043c\u044d\u0442\u0440\"],\n  [\"\u00eetre\", null],\n  [\"\u2581Philippe\", \"\u0424\u0438\u043b\u0438\u043f\u043f\"],\n  [\"\u2581L\", \"\u041b\u0451\u043b\u044c\u0435\"],\n  [\"he\", null],\n  [\"ul\", null],\n  [\"ier\", null],\n  [\",\", null],\n  [\"\u2581av\", \"\u0430\u0434\u0432\u043e\u043a\u0430\u0442\"],\n  [\"oc\", null],\n  [\"at\", null],\n  [\"\u2581extraord\", \"\u0447\u0440\u0435\u0437\u0432\u044b\u0447\u0430\u0439\u043d\u044b\u0439\"],\n  [\"ina\", null],\n  [\"ire\", null],\n  [\"\u2581du\", null],\n  [\"\u2581roi\", \"\u043a\u043e\u0440\u043e\u043b\u044f\"],\n  [\",\", null],\n  [\"\u2581avait\", \"\u0441\u043a\u0430\u0437\u0430\u043b\"],\n  [\"\u2581dit\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581la\", null],\n  [\"\u2581p\", \"\u0431\u0435\u0434\u043d\u043e\u0439\"],\n  [\"au\", null],\n  [\"vre\", null],\n  [\"\u2581Es\", \"\u042d\u0441\u043c\u0435\u0440\u0430\u043b\u044c\u0434\u0435\"],\n  [\"mer\", null],\n  [\"al\", null],\n  [\"da\", null],\n  [\"\u2581:\", null],\n  [\"\u2581Il\", \"\u041e\u043d\"],\n  [\"\u2581se\", null],\n  [\"\u2581me\", \"\u0443\u043c\u0438\u0440\u0430\u0435\u0442\"],\n  [\"urt\", null],\n  [\",\", null],\n  [\"\u2581c\", \"\u044d\u0442\u043e\"],\n  [\"'\", null],\n  [\"\u00e9tait\", \"\u0431\u044b\u043b\u043e\"],\n  [\"\u2581par\", \"\u043f\u043e\"],\n  [\"\u2581erre\", \"\u043e\u0448\u0438\u0431\u043a\u0435\"],\n  [\"ur\", null],\n  [\"\u2581ou\", \"\u0438\u043b\u0438\"],\n  [\"\u2581par\", \"\u0438\u0437\"],\n  [\"\u2581pl\", \"\u0448\u0443\u0442\u043a\u0438\"],\n  [\"ais\", null],\n  [\"anter\", null],\n  [\"ie\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nLe voyant ouvrir la porte du jardin, je montai rapidement dans ma chambre.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Le', '\u2581voy', 'ant', '\u2581ou', 'vr', 'ir', '\u2581la', '\u2581porte', '\u2581du', '\u2581j', 'ard', 'in', ',', '\u2581je', '\u2581mont', 'ai', '\u2581rapid', 'ement', '\u2581dans', '\u2581ma', '\u2581ch', 'amb', 're', '.']"}], "[\n  [\"\u2581Le\", null],\n  [\"\u2581voy\", \"Seeing\"],\n  [\"ant\", null],\n  [\"\u2581ou\", null],\n  [\"vr\", null],\n  [\"ir\", null],\n  [\"\u2581la\", null],\n  [\"\u2581porte\", \"door\"],\n  [\"\u2581du\", null],\n  [\"\u2581j\", \"garden\"],\n  [\"ard\", null],\n  [\"in\", null],\n  [\",\", null],\n  [\"\u2581je\", null],\n  [\"\u2581mont\", \"went\"],\n  [\"ai\", null],\n  [\"\u2581rapid\", \"quickly\"],\n  [\"ement\", null],\n  [\"\u2581dans\", null],\n  [\"\u2581ma\", null],\n  [\"\u2581ch\", \"room\"],\n  [\"amb\", null],\n  [\"re\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nLe voyant ouvrir la porte du jardin, je montai rapidement dans ma chambre.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Le', '\u2581voy', 'ant', '\u2581ou', 'vr', 'ir', '\u2581la', '\u2581porte', '\u2581du', '\u2581j', 'ard', 'in', ',', '\u2581je', '\u2581mont', 'ai', '\u2581rapid', 'ement', '\u2581dans', '\u2581ma', '\u2581ch', 'amb', 're', '.']"}], "[\n  [\"\u2581Le\", null],\n  [\"\u2581voy\", \"\u0423\u0432\u0438\u0434\u0435\u0432\"],\n  [\"ant\", null],\n  [\"\u2581ou\", null],\n  [\"vr\", null],\n  [\"ir\", null],\n  [\"\u2581la\", null],\n  [\"\u2581porte\", \"\u0434\u0432\u0435\u0440\u044c\"],\n  [\"\u2581du\", null],\n  [\"\u2581j\", \"\u0441\u0430\u0434\u0430\"],\n  [\"ard\", null],\n  [\"in\", null],\n  [\",\", null],\n  [\"\u2581je\", null],\n  [\"\u2581mont\", \"\u043f\u043e\u0434\u043d\u044f\u043b\u0441\u044f\"],\n  [\"ai\", null],\n  [\"\u2581rapid\", \"\u0431\u044b\u0441\u0442\u0440\u043e\"],\n  [\"ement\", null],\n  [\"\u2581dans\", \"\u0432\"],\n  [\"\u2581ma\", null],\n  [\"\u2581ch\", \"\u043a\u043e\u043c\u043d\u0430\u0442\u0443\"],\n  [\"amb\", null],\n  [\"re\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nLa rencontre de Glenarvan et Ayrton avait \u00e9videmment produit chez tous deux une \u00e9motion r\u00e9ciproque.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581La', '\u2581ren', 'cont', 're', '\u2581de', '\u2581Glen', 'ar', 'van', '\u2581et', '\u2581A', 'yr', 'ton', '\u2581avait', '\u2581\u00e9', 'vid', 'em', 'ment', '\u2581produ', 'it', '\u2581chez', '\u2581tous', '\u2581deux', '\u2581une', '\u2581\u00e9', 'm', 'otion', '\u2581r\u00e9', 'cip', 'ro', 'que', '.']"}], "[\n  [\"\u2581La\", \"The\"],\n  [\"\u2581ren\", \"meeting\"],\n  [\"cont\", null],\n  [\"re\", null],\n  [\"\u2581de\", null],\n  [\"\u2581Glen\", \"Glenarvan\"],\n  [\"ar\", null],\n  [\"van\", null],\n  [\"\u2581et\", null],\n  [\"\u2581A\", \"Ayrton\"],\n  [\"yr\", null],\n  [\"ton\", null],\n  [\"\u2581avait\", \"had\"],\n  [\"\u2581\u00e9\", \"obviously\"],\n  [\"vid\", null],\n  [\"em\", null],\n  [\"ment\", null],\n  [\"\u2581produ\", \"produced\"],\n  [\"it\", null],\n  [\"\u2581chez\", \"in\"],\n  [\"\u2581tous\", \"both\"],\n  [\"\u2581deux\", null],\n  [\"\u2581une\", \"a\"],\n  [\"\u2581\u00e9\", \"emotion\"],\n  [\"m\", null],\n  [\"otion\", null],\n  [\"\u2581r\u00e9\", \"reciprocal\"],\n  [\"cip\", null],\n  [\"ro\", null],\n  [\"que\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nLa rencontre de Glenarvan et Ayrton avait \u00e9videmment produit chez tous deux une \u00e9motion r\u00e9ciproque.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581La', '\u2581ren', 'cont', 're', '\u2581de', '\u2581Glen', 'ar', 'van', '\u2581et', '\u2581A', 'yr', 'ton', '\u2581avait', '\u2581\u00e9', 'vid', 'em', 'ment', '\u2581produ', 'it', '\u2581chez', '\u2581tous', '\u2581deux', '\u2581une', '\u2581\u00e9', 'm', 'otion', '\u2581r\u00e9', 'cip', 'ro', 'que', '.']"}], "[\n  [\"\u2581La\", null],\n  [\"\u2581ren\", \"\u0412\u0441\u0442\u0440\u0435\u0447\u0430\"],\n  [\"cont\", null],\n  [\"re\", null],\n  [\"\u2581de\", null],\n  [\"\u2581Glen\", \"\u0413\u043b\u0435\u043d\u0430\u0440\u0432\u0430\u043d\u0430\"],\n  [\"ar\", null],\n  [\"van\", null],\n  [\"\u2581et\", null],\n  [\"\u2581A\", \"\u042d\u0439\u0440\u0442\u043e\u043d\u0430\"],\n  [\"yr\", null],\n  [\"ton\", null],\n  [\"\u2581avait\", \"\u043e\u0447\u0435\u0432\u0438\u0434\u043d\u043e\"],\n  [\"\u2581\u00e9\", \"\u0432\u044b\u0437\u0432\u0430\u043b\u0430\"],\n  [\"vid\", null],\n  [\"em\", null],\n  [\"ment\", null],\n  [\"\u2581produ\", null],\n  [\"it\", null],\n  [\"\u2581chez\", \"\u0443\"],\n  [\"\u2581tous\", \"\u043e\u0431\u043e\u0438\u0445\"],\n  [\"\u2581deux\", null],\n  [\"\u2581une\", null],\n  [\"\u2581\u00e9\", \"\u0432\u0437\u0430\u0438\u043c\u043d\u043e\u0435\"],\n  [\"m\", null],\n  [\"otion\", \"\u0432\u043e\u043b\u043d\u0435\u043d\u0438\u0435\"],\n  [\"\u2581r\u00e9\", null],\n  [\"cip\", null],\n  [\"ro\", null],\n  [\"que\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0415\u0441\u043b\u0438 \u0431 \u044f \u043c\u043e\u0433 \u043b\u044e\u0431\u0438\u0442\u044c \u0432\u0441\u0435 \u044d\u0442\u043e \u0434\u0435\u043b\u043e, \u043a\u0430\u043a \u044f \u043b\u044e\u0431\u043b\u044e \u0442\u0435\u0431\u044f... \u0430 \u0442\u043e \u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435 \u0432\u0440\u0435\u043c\u044f \u0434\u0435\u043b\u0430\u044e, \u043a\u0430\u043a \u0437\u0430\u0434\u0430\u043d\u043d\u044b\u0439 \u0443\u0440\u043e\u043a.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0415', '\u0441\u043b\u0438', '\u2581\u0431', '\u2581\u044f', '\u2581\u043c\u043e', '\u0433', '\u2581\u043b\u044e', '\u0431\u0438', '\u0442\u044c', '\u2581\u0432\u0441\u0435', '\u2581\u044d\u0442\u043e', '\u2581\u0434\u0435', '\u043b\u043e', ',', '\u2581\u043a\u0430\u043a', '\u2581\u044f', '\u2581\u043b\u044e', '\u0431', '\u043b\u044e', '\u2581\u0442\u0435', '\u0431', '\u044f', '...', '\u2581\u0430', '\u2581\u0442\u043e', '\u2581\u044f', '\u2581\u043f\u043e\u0441\u043b\u0435\u0434', '\u043d\u0435\u0435', '\u2581\u0432\u0440\u0435\u043c\u044f', '\u2581\u0434\u0435', '\u043b\u0430', '\u044e', ',', '\u2581\u043a\u0430\u043a', '\u2581\u0437\u0430', '\u0434\u0430\u043d', '\u043d\u044b\u0439', '\u2581\u0443', '\u0440\u043e\u043a', '.']"}], "[\n[\"\u2581\u0415\", \"If\"],\n[\"\u0441\u043b\u0438\", null],\n[\"\u2581\u0431\", null],\n[\"\u2581\u044f\", null],\n[\"\u2581\u043c\u043e\", \"could\"],\n[\"\u0433\", null],\n[\"\u2581\u043b\u044e\", \"love\"],\n[\"\u0431\u0438\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0432\u0441\u0435\", \"all\"],\n[\"\u2581\u044d\u0442\u043e\", \"this\"],\n[\"\u2581\u0434\u0435\", \"business\"],\n[\"\u043b\u043e\", null],\n[\",\", null],\n[\"\u2581\u043a\u0430\u043a\", \"as\"],\n[\"\u2581\u044f\", null],\n[\"\u2581\u043b\u044e\", \"love\"],\n[\"\u0431\", null],\n[\"\u043b\u044e\", null],\n[\"\u2581\u0442\u0435\", \"you\"],\n[\"\u0431\", null],\n[\"\u044f\", null],\n[\"...\", null],\n[\"\u2581\u0430\", \"but\"],\n[\"\u2581\u0442\u043e\", null],\n[\"\u2581\u044f\", null],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\u0434\", \"lately\"],\n[\"\u043d\u0435\u0435\", null],\n[\"\u2581\u0432\u0440\u0435\u043c\u044f\", \"time\"],\n[\"\u2581\u0434\u0435\", \"do\"],\n[\"\u043b\u0430\", null],\n[\"\u044e\", null],\n[\",\", null],\n[\"\u2581\u043a\u0430\u043a\", \"as\"],\n[\"\u2581\u0437\u0430\", \"assigned\"],\n[\"\u0434\u0430\u043d\", null],\n[\"\u043d\u044b\u0439\", null],\n[\"\u2581\u0443\", \"lesson\"],\n[\"\u0440\u043e\u043a\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u0415\u0441\u043b\u0438 \u0431 \u044f \u043c\u043e\u0433 \u043b\u044e\u0431\u0438\u0442\u044c \u0432\u0441\u0435 \u044d\u0442\u043e \u0434\u0435\u043b\u043e, \u043a\u0430\u043a \u044f \u043b\u044e\u0431\u043b\u044e \u0442\u0435\u0431\u044f... \u0430 \u0442\u043e \u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435 \u0432\u0440\u0435\u043c\u044f \u0434\u0435\u043b\u0430\u044e, \u043a\u0430\u043a \u0437\u0430\u0434\u0430\u043d\u043d\u044b\u0439 \u0443\u0440\u043e\u043a.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0415', '\u0441\u043b\u0438', '\u2581\u0431', '\u2581\u044f', '\u2581\u043c\u043e', '\u0433', '\u2581\u043b\u044e', '\u0431\u0438', '\u0442\u044c', '\u2581\u0432\u0441\u0435', '\u2581\u044d\u0442\u043e', '\u2581\u0434\u0435', '\u043b\u043e', ',', '\u2581\u043a\u0430\u043a', '\u2581\u044f', '\u2581\u043b\u044e', '\u0431', '\u043b\u044e', '\u2581\u0442\u0435', '\u0431', '\u044f', '...', '\u2581\u0430', '\u2581\u0442\u043e', '\u2581\u044f', '\u2581\u043f\u043e\u0441\u043b\u0435\u0434', '\u043d\u0435\u0435', '\u2581\u0432\u0440\u0435\u043c\u044f', '\u2581\u0434\u0435', '\u043b\u0430', '\u044e', ',', '\u2581\u043a\u0430\u043a', '\u2581\u0437\u0430', '\u0434\u0430\u043d', '\u043d\u044b\u0439', '\u2581\u0443', '\u0440\u043e\u043a', '.']"}], "[\n[\"\u2581\u0415\", \"Si\"],\n[\"\u0441\u043b\u0438\", null],\n[\"\u2581\u0431\", null],\n[\"\u2581\u044f\", null],\n[\"\u2581\u043c\u043e\", \"pouvoir\"],\n[\"\u0433\", null],\n[\"\u2581\u043b\u044e\", \"aimer\"],\n[\"\u0431\u0438\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0432\u0441\u0435\", \"tout\"],\n[\"\u2581\u044d\u0442\u043e\", \"cela\"],\n[\"\u2581\u0434\u0435\", \"affaire\"],\n[\"\u043b\u043e\", null],\n[\",\", null],\n[\"\u2581\u043a\u0430\u043a\", \"comme\"],\n[\"\u2581\u044f\", null],\n[\"\u2581\u043b\u044e\", \"aimer\"],\n[\"\u0431\", null],\n[\"\u043b\u044e\", null],\n[\"\u2581\u0442\u0435\", \"toi\"],\n[\"\u0431\", null],\n[\"\u044f\", null],\n[\"...\", null],\n[\"\u2581\u0430\", \"mais\"],\n[\"\u2581\u0442\u043e\", null],\n[\"\u2581\u044f\", null],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\u0434\", \"dernier\"],\n[\"\u043d\u0435\u0435\", null],\n[\"\u2581\u0432\u0440\u0435\u043c\u044f\", \"temps\"],\n[\"\u2581\u0434\u0435\", \"faire\"],\n[\"\u043b\u0430\", null],\n[\"\u044e\", null],\n[\",\", null],\n[\"\u2581\u043a\u0430\u043a\", \"comme\"],\n[\"\u2581\u0437\u0430\", \"devoir\"],\n[\"\u0434\u0430\u043d\", null],\n[\"\u043d\u044b\u0439\", null],\n[\"\u2581\u0443\", \"le\u00e7on\"],\n[\"\u0440\u043e\u043a\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n-- \u0410 \u043d\u0430 \u044d\u0442\u0438 \u0434\u0435\u043d\u044c\u0433\u0438 \u043e\u043d \u0431\u044b \u043d\u0430\u043a\u0443\u043f\u0438\u043b \u0441\u043a\u043e\u0442\u0430 \u0438\u043b\u0438 \u0437\u0435\u043c\u043b\u0438\u0446\u0443 \u043a\u0443\u043f\u0438\u043b \u0431\u044b \u0437\u0430 \u0431\u0435\u0441\u0446\u0435\u043d\u043e\u043a \u0438 \u043c\u0443\u0436\u0438\u043a\u0430\u043c \u0440\u043e\u0437\u0434\u0430\u043b \u0431\u044b \u0432\u043d\u0430\u0439\u043c\u044b, -- \u0441 \u0443\u043b\u044b\u0431\u043a\u043e\u0439 \u0434\u043e\u043a\u043e\u043d\u0447\u0438\u043b \u041b\u0435\u0432\u0438\u043d, \u043e\u0447\u0435\u0432\u0438\u0434\u043d\u043e \u043d\u0435 \u0440\u0430\u0437 \u0443\u0436\u0435 \u0441\u0442\u0430\u043b\u043a\u0438\u0432\u0430\u0432\u0448\u0438\u0439\u0441\u044f \u0441 \u043f\u043e\u0434\u043e\u0431\u043d\u044b\u043c\u0438 \u0440\u0430\u0441\u0447\u0435\u0442\u0430\u043c\u0438. -- \u0418 \u043e\u043d \u0441\u043e\u0441\u0442\u0430\u0432\u0438\u0442 \u0441\u0435\u0431\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581--', '\u2581\u0410', '\u2581\u043d\u0430', '\u2581\u044d\u0442\u0438', '\u2581\u0434', '\u0435\u043d\u044c', '\u0433\u0438', '\u2581\u043e\u043d', '\u2581\u0431\u044b', '\u2581\u043d\u0430', '\u043a\u0443', '\u043f\u0438', '\u043b', '\u2581\u0441\u043a\u043e', '\u0442\u0430', '\u2581\u0438\u043b\u0438', '\u2581\u0437\u0435\u043c', '\u043b\u0438', '\u0446\u0443', '\u2581\u043a\u0443', '\u043f\u0438', '\u043b', '\u2581\u0431\u044b', '\u2581\u0437\u0430', '\u2581\u0431\u0435', '\u0441', '\u0446\u0435', '\u043d\u043e\u043a', '\u2581\u0438', '\u2581\u043c\u0443', '\u0436\u0438', '\u043a\u0430', '\u043c', '\u2581\u0440\u043e\u0437', '\u0434\u0430', '\u043b', '\u2581\u0431\u044b', '\u2581\u0432', '\u043d\u0430', '\u0439', '\u043c\u044b', ',', '\u2581--', '\u2581\u0441', '\u2581\u0443', '\u043b\u044b', '\u0431', '\u043a\u043e\u0439', '\u2581\u0434\u043e', '\u043a\u043e\u043d', '\u0447\u0438', '\u043b', '\u2581\u041b\u0435', '\u0432\u0438\u043d', ',', '\u2581\u043e', '\u0447\u0435', '\u0432\u0438', '\u0434\u043d\u043e', '\u2581\u043d\u0435', '\u2581\u0440\u0430\u0437', '\u2581\u0443', '\u0436\u0435', '\u2581\u0441\u0442\u0430\u043b', '\u043a\u0438', '\u0432\u0430', '\u0432\u0448\u0438\u0439', '\u0441\u044f', '\u2581\u0441', '\u2581\u043f\u043e', '\u0434\u043e', '\u0431', '\u043d\u044b\u043c\u0438', '\u2581\u0440\u0430\u0441', '\u0447\u0435', '\u0442\u0430\u043c\u0438', '.', '\u2581--', '\u2581\u0418', '\u2581\u043e\u043d', '\u2581\u0441\u043e', '\u0441\u0442\u0430\u0432\u0438', '\u0442', '\u2581\u0441\u0435', '\u0431\u0435', '\u2581\u0441\u043e\u0441\u0442\u043e\u044f', '\u043d\u0438\u0435', '.']"}], "[\n[\"\u2581--\", null],\n[\"\u2581\u0410\", \"And\"],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u044d\u0442\u0438\", \"these\"],\n[\"\u2581\u0434\", null],\n[\"\u0435\u043d\u044c\", null],\n[\"\u0433\u0438\", \"money\"],\n[\"\u2581\u043e\u043d\", \"he\"],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u043a\u0443\", null],\n[\"\u043f\u0438\", null],\n[\"\u043b\", \"buy\"],\n[\"\u2581\u0441\u043a\u043e\", null],\n[\"\u0442\u0430\", \"cattle\"],\n[\"\u2581\u0438\u043b\u0438\", \"or\"],\n[\"\u2581\u0437\u0435\u043c\", null],\n[\"\u043b\u0438\", null],\n[\"\u0446\u0443\", \"land\"],\n[\"\u2581\u043a\u0443\", null],\n[\"\u043f\u0438\", null],\n[\"\u043b\", \"buy\"],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u0437\u0430\", \"for\"],\n[\"\u2581\u0431\u0435\", null],\n[\"\u0441\", null],\n[\"\u0446\u0435\", null],\n[\"\u043d\u043e\u043a\", \"pittance\"],\n[\"\u2581\u0438\", \"and\"],\n[\"\u2581\u043c\u0443\", null],\n[\"\u0436\u0438\", null],\n[\"\u043a\u0430\", null],\n[\"\u043c\", \"peasants\"],\n[\"\u2581\u0440\u043e\u0437\", null],\n[\"\u0434\u0430\", null],\n[\"\u043b\", \"give\"],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u0432\", null],\n[\"\u043d\u0430\", null],\n[\"\u0439\", null],\n[\"\u043c\u044b\", \"rent\"],\n[\",\", null],\n[\"\u2581--\", null],\n[\"\u2581\u0441\", \"with\"],\n[\"\u2581\u0443\", null],\n[\"\u043b\u044b\", null],\n[\"\u0431\", null],\n[\"\u043a\u043e\u0439\", \"smile\"],\n[\"\u2581\u0434\u043e\", null],\n[\"\u043a\u043e\u043d\", null],\n[\"\u0447\u0438\", null],\n[\"\u043b\", \"finished\"],\n[\"\u2581\u041b\u0435\", null],\n[\"\u0432\u0438\u043d\", \"Levin\"],\n[\",\", null],\n[\"\u2581\u043e\", null],\n[\"\u0447\u0435\", null],\n[\"\u0432\u0438\", null],\n[\"\u0434\u043d\u043e\", \"obviously\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0440\u0430\u0437\", \"times\"],\n[\"\u2581\u0443\", null],\n[\"\u0436\u0435\", \"already\"],\n[\"\u2581\u0441\u0442\u0430\u043b\", null],\n[\"\u043a\u0438\", null],\n[\"\u0432\u0430\", null],\n[\"\u0432\u0448\u0438\u0439\", \"encountered\"],\n[\"\u0441\u044f\", null],\n[\"\u2581\u0441\", \"with\"],\n[\"\u2581\u043f\u043e\", null],\n[\"\u0434\u043e\", null],\n[\"\u0431\", null],\n[\"\u043d\u044b\u043c\u0438\", \"similar\"],\n[\"\u2581\u0440\u0430\u0441\", null],\n[\"\u0447\u0435\", null],\n[\"\u0442\u0430\u043c\u0438\", \"calculations\"],\n[\".\", null],\n[\"\u2581--\", null],\n[\"\u2581\u0418\", \"And\"],\n[\"\u2581\u043e\u043d\", \"he\"],\n[\"\u2581\u0441\u043e\", null],\n[\"\u0441\u0442\u0430\u0432\u0438\", null],\n[\"\u0442\", \"will make\"],\n[\"\u2581\u0441\u0435\", null],\n[\"\u0431\u0435\", \"himself\"],\n[\"\u2581\u0441\u043e\u0441\u0442\u043e\u044f\", null],\n[\"\u043d\u0438\u0435\", \"fortune\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n-- \u0410 \u043d\u0430 \u044d\u0442\u0438 \u0434\u0435\u043d\u044c\u0433\u0438 \u043e\u043d \u0431\u044b \u043d\u0430\u043a\u0443\u043f\u0438\u043b \u0441\u043a\u043e\u0442\u0430 \u0438\u043b\u0438 \u0437\u0435\u043c\u043b\u0438\u0446\u0443 \u043a\u0443\u043f\u0438\u043b \u0431\u044b \u0437\u0430 \u0431\u0435\u0441\u0446\u0435\u043d\u043e\u043a \u0438 \u043c\u0443\u0436\u0438\u043a\u0430\u043c \u0440\u043e\u0437\u0434\u0430\u043b \u0431\u044b \u0432\u043d\u0430\u0439\u043c\u044b, -- \u0441 \u0443\u043b\u044b\u0431\u043a\u043e\u0439 \u0434\u043e\u043a\u043e\u043d\u0447\u0438\u043b \u041b\u0435\u0432\u0438\u043d, \u043e\u0447\u0435\u0432\u0438\u0434\u043d\u043e \u043d\u0435 \u0440\u0430\u0437 \u0443\u0436\u0435 \u0441\u0442\u0430\u043b\u043a\u0438\u0432\u0430\u0432\u0448\u0438\u0439\u0441\u044f \u0441 \u043f\u043e\u0434\u043e\u0431\u043d\u044b\u043c\u0438 \u0440\u0430\u0441\u0447\u0435\u0442\u0430\u043c\u0438. -- \u0418 \u043e\u043d \u0441\u043e\u0441\u0442\u0430\u0432\u0438\u0442 \u0441\u0435\u0431\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581--', '\u2581\u0410', '\u2581\u043d\u0430', '\u2581\u044d\u0442\u0438', '\u2581\u0434', '\u0435\u043d\u044c', '\u0433\u0438', '\u2581\u043e\u043d', '\u2581\u0431\u044b', '\u2581\u043d\u0430', '\u043a\u0443', '\u043f\u0438', '\u043b', '\u2581\u0441\u043a\u043e', '\u0442\u0430', '\u2581\u0438\u043b\u0438', '\u2581\u0437\u0435\u043c', '\u043b\u0438', '\u0446\u0443', '\u2581\u043a\u0443', '\u043f\u0438', '\u043b', '\u2581\u0431\u044b', '\u2581\u0437\u0430', '\u2581\u0431\u0435', '\u0441', '\u0446\u0435', '\u043d\u043e\u043a', '\u2581\u0438', '\u2581\u043c\u0443', '\u0436\u0438', '\u043a\u0430', '\u043c', '\u2581\u0440\u043e\u0437', '\u0434\u0430', '\u043b', '\u2581\u0431\u044b', '\u2581\u0432', '\u043d\u0430', '\u0439', '\u043c\u044b', ',', '\u2581--', '\u2581\u0441', '\u2581\u0443', '\u043b\u044b', '\u0431', '\u043a\u043e\u0439', '\u2581\u0434\u043e', '\u043a\u043e\u043d', '\u0447\u0438', '\u043b', '\u2581\u041b\u0435', '\u0432\u0438\u043d', ',', '\u2581\u043e', '\u0447\u0435', '\u0432\u0438', '\u0434\u043d\u043e', '\u2581\u043d\u0435', '\u2581\u0440\u0430\u0437', '\u2581\u0443', '\u0436\u0435', '\u2581\u0441\u0442\u0430\u043b', '\u043a\u0438', '\u0432\u0430', '\u0432\u0448\u0438\u0439', '\u0441\u044f', '\u2581\u0441', '\u2581\u043f\u043e', '\u0434\u043e', '\u0431', '\u043d\u044b\u043c\u0438', '\u2581\u0440\u0430\u0441', '\u0447\u0435', '\u0442\u0430\u043c\u0438', '.', '\u2581--', '\u2581\u0418', '\u2581\u043e\u043d', '\u2581\u0441\u043e', '\u0441\u0442\u0430\u0432\u0438', '\u0442', '\u2581\u0441\u0435', '\u0431\u0435', '\u2581\u0441\u043e\u0441\u0442\u043e\u044f', '\u043d\u0438\u0435', '.']"}], "[\n[\"\u2581--\", null],\n[\"\u2581\u0410\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u044d\u0442\u0438\", \"ces\"],\n[\"\u2581\u0434\", null],\n[\"\u0435\u043d\u044c\", null],\n[\"\u0433\u0438\", \"argent\"],\n[\"\u2581\u043e\u043d\", \"il\"],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u043a\u0443\", null],\n[\"\u043f\u0438\", null],\n[\"\u043b\", \"acheter\"],\n[\"\u2581\u0441\u043a\u043e\", null],\n[\"\u0442\u0430\", \"b\u00e9tail\"],\n[\"\u2581\u0438\u043b\u0438\", \"ou\"],\n[\"\u2581\u0437\u0435\u043c\", null],\n[\"\u043b\u0438\", null],\n[\"\u0446\u0443\", \"terre\"],\n[\"\u2581\u043a\u0443\", null],\n[\"\u043f\u0438\", null],\n[\"\u043b\", \"acheter\"],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u0437\u0430\", \"pour\"],\n[\"\u2581\u0431\u0435\", null],\n[\"\u0441\", null],\n[\"\u0446\u0435\", null],\n[\"\u043d\u043e\u043a\", \"rien\"],\n[\"\u2581\u0438\", \"et\"],\n[\"\u2581\u043c\u0443\", null],\n[\"\u0436\u0438\", null],\n[\"\u043a\u0430\", null],\n[\"\u043c\", \"paysans\"],\n[\"\u2581\u0440\u043e\u0437\", null],\n[\"\u0434\u0430\", null],\n[\"\u043b\", \"distribuer\"],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u0432\", null],\n[\"\u043d\u0430\", null],\n[\"\u0439\", null],\n[\"\u043c\u044b\", \"louer\"],\n[\",\", null],\n[\"\u2581--\", null],\n[\"\u2581\u0441\", \"avec\"],\n[\"\u2581\u0443\", null],\n[\"\u043b\u044b\", null],\n[\"\u0431\", null],\n[\"\u043a\u043e\u0439\", \"sourire\"],\n[\"\u2581\u0434\u043e\", null],\n[\"\u043a\u043e\u043d\", null],\n[\"\u0447\u0438\", null],\n[\"\u043b\", \"terminer\"],\n[\"\u2581\u041b\u0435\", null],\n[\"\u0432\u0438\u043d\", \"L\u00e9vine\"],\n[\",\", null],\n[\"\u2581\u043e\", null],\n[\"\u0447\u0435\", null],\n[\"\u0432\u0438\", null],\n[\"\u0434\u043d\u043e\", \"\u00e9videmment\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0440\u0430\u0437\", \"plusieurs fois\"],\n[\"\u2581\u0443\", null],\n[\"\u0436\u0435\", \"d\u00e9j\u00e0\"],\n[\"\u2581\u0441\u0442\u0430\u043b\", null],\n[\"\u043a\u0438\", null],\n[\"\u0432\u0430\", null],\n[\"\u0432\u0448\u0438\u0439\", null],\n[\"\u0441\u044f\", \"rencontrer\"],\n[\"\u2581\u0441\", \"avec\"],\n[\"\u2581\u043f\u043e\", null],\n[\"\u0434\u043e\", null],\n[\"\u0431\", null],\n[\"\u043d\u044b\u043c\u0438\", \"similaires\"],\n[\"\u2581\u0440\u0430\u0441\", null],\n[\"\u0447\u0435\", null],\n[\"\u0442\u0430\u043c\u0438\", \"calculs\"],\n[\".\", null],\n[\"\u2581--\", null],\n[\"\u2581\u0418\", \"Et\"],\n[\"\u2581\u043e\u043d\", \"il\"],\n[\"\u2581\u0441\u043e\", null],\n[\"\u0441\u0442\u0430\u0432\u0438\", null],\n[\"\u0442\", \"constituera\"],\n[\"\u2581\u0441\u0435\", null],\n[\"\u0431\u0435\", \"se\"],\n[\"\u2581\u0441\u043e\u0441\u0442\u043e\u044f\", null],\n[\"\u043d\u0438\u0435\", \"fortune\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0422\u043e\u043b\u044c\u043a\u043e \u0432 \u0441\u0430\u043c\u043e\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435 \u0432\u0440\u0435\u043c\u044f, \u043f\u043e \u043f\u043e\u0432\u043e\u0434\u0443 \u0441\u0432\u043e\u0438\u0445 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0439 \u043a \u0410\u043d\u043d\u0435, \u0412\u0440\u043e\u043d\u0441\u043a\u0438\u0439 \u043d\u0430\u0447\u0438\u043d\u0430\u043b \u0447\u0443\u0432\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c, \u0447\u0442\u043e \u0441\u0432\u043e\u0434 \u0435\u0433\u043e \u043f\u0440\u0430\u0432\u0438\u043b \u043d\u0435 \u0432\u043f\u043e\u043b\u043d\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u043b \u0432\u0441\u0435 \u0443\u0441\u043b\u043e\u0432\u0438\u044f, \u0438 \u0432 \u0431\u0443\u0434\u0443\u0449\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u043b\u0438\u0441\u044c \u0442\u0440\u0443\u0434\u043d\u043e\u0441\u0442\u0438 \u0438 \u0441\u043e\u043c\u043d\u0435\u043d\u0438\u044f, \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0412\u0440\u043e\u043d\u0441\u043a\u0438\u0439 \u0443\u0436\u0435 \u043d\u0435 \u043d\u0430\u0445\u043e\u0434\u0438\u043b \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u044f\u0449\u0435\u0439 \u043d\u0438\u0442\u0438.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0422\u043e', '\u043b\u044c\u043a\u043e', '\u2581\u0432', '\u2581\u0441\u0430\u043c\u043e', '\u0435', '\u2581\u043f\u043e\u0441\u043b\u0435\u0434', '\u043d\u0435\u0435', '\u2581\u0432\u0440\u0435\u043c\u044f', ',', '\u2581\u043f\u043e', '\u2581\u043f\u043e', '\u0432\u043e', '\u0434\u0443', '\u2581\u0441\u0432\u043e\u0438', '\u0445', '\u2581\u043e\u0442\u043d\u043e', '\u0448\u0435', '\u043d\u0438\u0439', '\u2581\u043a', '\u2581\u0410\u043d', '\u043d\u0435', ',', '\u2581\u0412', '\u0440\u043e\u043d', '\u0441\u043a\u0438\u0439', '\u2581\u043d\u0430', '\u0447\u0438\u043d\u0430', '\u043b', '\u2581\u0447', '\u0443', '\u0432', '\u0441\u0442\u0432\u043e', '\u0432\u0430\u0442\u044c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0441\u0432\u043e', '\u0434', '\u2581\u0435\u0433\u043e', '\u2581\u043f\u0440\u0430\u0432\u0438', '\u043b', '\u2581\u043d\u0435', '\u2581\u0432', '\u043f\u043e\u043b', '\u043d\u0435', '\u2581\u043e', '\u043f\u0440\u0435\u0434\u0435', '\u043b\u044f', '\u043b', '\u2581\u0432\u0441\u0435', '\u2581\u0443', '\u0441\u043b\u043e', '\u0432\u0438\u044f', ',', '\u2581\u0438', '\u2581\u0432', '\u2581\u0431\u0443', '\u0434\u0443', '\u0449\u0435', '\u043c', '\u2581\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432', '\u043b\u044f', '\u043b\u0438\u0441\u044c', '\u2581\u0442', '\u0440\u0443\u0434', '\u043d\u043e\u0441\u0442\u0438', '\u2581\u0438', '\u2581\u0441\u043e', '\u043c', '\u043d\u0435\u043d\u0438\u044f', ',', '\u2581\u0432', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0445', '\u2581\u0412', '\u0440\u043e\u043d', '\u0441\u043a\u0438\u0439', '\u2581\u0443', '\u0436\u0435', '\u2581\u043d\u0435', '\u2581\u043d\u0430\u0445\u043e\u0434\u0438', '\u043b', '\u2581\u0440\u0443\u043a\u043e', '\u0432\u043e\u0434', '\u044f', '\u0449\u0435', '\u0439', '\u2581\u043d\u0438', '\u0442\u0438', '.']"}], "[\n[\"\u2581\u0422\u043e\", \"Only\"],\n[\"\u043b\u044c\u043a\u043e\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0441\u0430\u043c\u043e\", \"recent\"],\n[\"\u0435\", null],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\u0434\", null],\n[\"\u043d\u0435\u0435\", null],\n[\"\u2581\u0432\u0440\u0435\u043c\u044f\", \"time\"],\n[\",\", null],\n[\"\u2581\u043f\u043e\", null],\n[\"\u2581\u043f\u043e\", null],\n[\"\u0432\u043e\", null],\n[\"\u0434\u0443\", null],\n[\"\u2581\u0441\u0432\u043e\u0438\", \"his\"],\n[\"\u0445\", null],\n[\"\u2581\u043e\u0442\u043d\u043e\", \"relationship\"],\n[\"\u0448\u0435\", null],\n[\"\u043d\u0438\u0439\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u0410\u043d\", \"Anna\"],\n[\"\u043d\u0435\", null],\n[\",\", null],\n[\"\u2581\u0412\", \"Vronsky\"],\n[\"\u0440\u043e\u043d\", null],\n[\"\u0441\u043a\u0438\u0439\", null],\n[\"\u2581\u043d\u0430\", \"began\"],\n[\"\u0447\u0438\u043d\u0430\", null],\n[\"\u043b\", null],\n[\"\u2581\u0447\", \"feel\"],\n[\"\u0443\", null],\n[\"\u0432\", null],\n[\"\u0441\u0442\u0432\u043e\", null],\n[\"\u0432\u0430\u0442\u044c\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", \"that\"],\n[\"\u2581\u0441\u0432\u043e\", \"code\"],\n[\"\u0434\", null],\n[\"\u2581\u0435\u0433\u043e\", \"his\"],\n[\"\u2581\u043f\u0440\u0430\u0432\u0438\", \"rules\"],\n[\"\u043b\", null],\n[\"\u2581\u043d\u0435\", \"not\"],\n[\"\u2581\u0432\", null],\n[\"\u043f\u043e\u043b\", null],\n[\"\u043d\u0435\", null],\n[\"\u2581\u043e\", \"define\"],\n[\"\u043f\u0440\u0435\u0434\u0435\", null],\n[\"\u043b\u044f\", null],\n[\"\u043b\", null],\n[\"\u2581\u0432\u0441\u0435\", \"all\"],\n[\"\u2581\u0443\", \"conditions\"],\n[\"\u0441\u043b\u043e\", null],\n[\"\u0432\u0438\u044f\", null],\n[\",\", null],\n[\"\u2581\u0438\", \"and\"],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0431\u0443\", \"future\"],\n[\"\u0434\u0443\", null],\n[\"\u0449\u0435\", null],\n[\"\u043c\", null],\n[\"\u2581\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\", \"presented\"],\n[\"\u043b\u044f\", null],\n[\"\u043b\u0438\u0441\u044c\", null],\n[\"\u2581\u0442\", \"difficulties\"],\n[\"\u0440\u0443\u0434\", null],\n[\"\u043d\u043e\u0441\u0442\u0438\", null],\n[\"\u2581\u0438\", \"and\"],\n[\"\u2581\u0441\u043e\", \"doubts\"],\n[\"\u043c\", null],\n[\"\u043d\u0435\u043d\u0438\u044f\", null],\n[\",\", null],\n[\"\u2581\u0432\", \"in\"],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0445\", \"which\"],\n[\"\u2581\u0412\", \"Vronsky\"],\n[\"\u0440\u043e\u043d\", null],\n[\"\u0441\u043a\u0438\u0439\", null],\n[\"\u2581\u0443\", null],\n[\"\u0436\u0435\", null],\n[\"\u2581\u043d\u0435\", \"not\"],\n[\"\u2581\u043d\u0430\u0445\u043e\u0434\u0438\", \"find\"],\n[\"\u043b\", null],\n[\"\u2581\u0440\u0443\u043a\u043e\", \"guiding\"],\n[\"\u0432\u043e\u0434\", null],\n[\"\u044f\", null],\n[\"\u0449\u0435\", null],\n[\"\u0439\", null],\n[\"\u2581\u043d\u0438\", \"thread\"],\n[\"\u0442\u0438\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u0422\u043e\u043b\u044c\u043a\u043e \u0432 \u0441\u0430\u043c\u043e\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435 \u0432\u0440\u0435\u043c\u044f, \u043f\u043e \u043f\u043e\u0432\u043e\u0434\u0443 \u0441\u0432\u043e\u0438\u0445 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0439 \u043a \u0410\u043d\u043d\u0435, \u0412\u0440\u043e\u043d\u0441\u043a\u0438\u0439 \u043d\u0430\u0447\u0438\u043d\u0430\u043b \u0447\u0443\u0432\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c, \u0447\u0442\u043e \u0441\u0432\u043e\u0434 \u0435\u0433\u043e \u043f\u0440\u0430\u0432\u0438\u043b \u043d\u0435 \u0432\u043f\u043e\u043b\u043d\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u043b \u0432\u0441\u0435 \u0443\u0441\u043b\u043e\u0432\u0438\u044f, \u0438 \u0432 \u0431\u0443\u0434\u0443\u0449\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u043b\u0438\u0441\u044c \u0442\u0440\u0443\u0434\u043d\u043e\u0441\u0442\u0438 \u0438 \u0441\u043e\u043c\u043d\u0435\u043d\u0438\u044f, \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0412\u0440\u043e\u043d\u0441\u043a\u0438\u0439 \u0443\u0436\u0435 \u043d\u0435 \u043d\u0430\u0445\u043e\u0434\u0438\u043b \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u044f\u0449\u0435\u0439 \u043d\u0438\u0442\u0438.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0422\u043e', '\u043b\u044c\u043a\u043e', '\u2581\u0432', '\u2581\u0441\u0430\u043c\u043e', '\u0435', '\u2581\u043f\u043e\u0441\u043b\u0435\u0434', '\u043d\u0435\u0435', '\u2581\u0432\u0440\u0435\u043c\u044f', ',', '\u2581\u043f\u043e', '\u2581\u043f\u043e', '\u0432\u043e', '\u0434\u0443', '\u2581\u0441\u0432\u043e\u0438', '\u0445', '\u2581\u043e\u0442\u043d\u043e', '\u0448\u0435', '\u043d\u0438\u0439', '\u2581\u043a', '\u2581\u0410\u043d', '\u043d\u0435', ',', '\u2581\u0412', '\u0440\u043e\u043d', '\u0441\u043a\u0438\u0439', '\u2581\u043d\u0430', '\u0447\u0438\u043d\u0430', '\u043b', '\u2581\u0447', '\u0443', '\u0432', '\u0441\u0442\u0432\u043e', '\u0432\u0430\u0442\u044c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0441\u0432\u043e', '\u0434', '\u2581\u0435\u0433\u043e', '\u2581\u043f\u0440\u0430\u0432\u0438', '\u043b', '\u2581\u043d\u0435', '\u2581\u0432', '\u043f\u043e\u043b', '\u043d\u0435', '\u2581\u043e', '\u043f\u0440\u0435\u0434\u0435', '\u043b\u044f', '\u043b', '\u2581\u0432\u0441\u0435', '\u2581\u0443', '\u0441\u043b\u043e', '\u0432\u0438\u044f', ',', '\u2581\u0438', '\u2581\u0432', '\u2581\u0431\u0443', '\u0434\u0443', '\u0449\u0435', '\u043c', '\u2581\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432', '\u043b\u044f', '\u043b\u0438\u0441\u044c', '\u2581\u0442', '\u0440\u0443\u0434', '\u043d\u043e\u0441\u0442\u0438', '\u2581\u0438', '\u2581\u0441\u043e', '\u043c', '\u043d\u0435\u043d\u0438\u044f', ',', '\u2581\u0432', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0445', '\u2581\u0412', '\u0440\u043e\u043d', '\u0441\u043a\u0438\u0439', '\u2581\u0443', '\u0436\u0435', '\u2581\u043d\u0435', '\u2581\u043d\u0430\u0445\u043e\u0434\u0438', '\u043b', '\u2581\u0440\u0443\u043a\u043e', '\u0432\u043e\u0434', '\u044f', '\u0449\u0435', '\u0439', '\u2581\u043d\u0438', '\u0442\u0438', '.']"}], "[\n[\"\u2581\u0422\u043e\", \"Seulement\"],\n[\"\u043b\u044c\u043a\u043e\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0441\u0430\u043c\u043e\", \"tout\"],\n[\"\u0435\", null],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\u0434\", \"dernier\"],\n[\"\u043d\u0435\u0435\", null],\n[\"\u2581\u0432\u0440\u0435\u043c\u044f\", \"temps\"],\n[\",\", null],\n[\"\u2581\u043f\u043e\", null],\n[\"\u2581\u043f\u043e\", null],\n[\"\u0432\u043e\", null],\n[\"\u0434\u0443\", null],\n[\"\u2581\u0441\u0432\u043e\u0438\", \"ses\"],\n[\"\u0445\", null],\n[\"\u2581\u043e\u0442\u043d\u043e\", \"relations\"],\n[\"\u0448\u0435\", null],\n[\"\u043d\u0438\u0439\", null],\n[\"\u2581\u043a\", \"avec\"],\n[\"\u2581\u0410\u043d\", \"Anna\"],\n[\"\u043d\u0435\", null],\n[\",\", null],\n[\"\u2581\u0412\", \"Vronski\"],\n[\"\u0440\u043e\u043d\", null],\n[\"\u0441\u043a\u0438\u0439\", null],\n[\"\u2581\u043d\u0430\", \"commen\u00e7ait\"],\n[\"\u0447\u0438\u043d\u0430\", null],\n[\"\u043b\", null],\n[\"\u2581\u0447\", \"sentir\"],\n[\"\u0443\", null],\n[\"\u0432\", null],\n[\"\u0441\u0442\u0432\u043e\", null],\n[\"\u0432\u0430\u0442\u044c\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", \"que\"],\n[\"\u2581\u0441\u0432\u043e\", \"code\"],\n[\"\u0434\", null],\n[\"\u2581\u0435\u0433\u043e\", \"ses\"],\n[\"\u2581\u043f\u0440\u0430\u0432\u0438\", \"r\u00e8gles\"],\n[\"\u043b\", null],\n[\"\u2581\u043d\u0435\", \"ne\"],\n[\"\u2581\u0432\", null],\n[\"\u043f\u043e\u043b\", null],\n[\"\u043d\u0435\", null],\n[\"\u2581\u043e\", \"d\u00e9finissait\"],\n[\"\u043f\u0440\u0435\u0434\u0435\", null],\n[\"\u043b\u044f\", null],\n[\"\u043b\", null],\n[\"\u2581\u0432\u0441\u0435\", \"toutes\"],\n[\"\u2581\u0443\", \"conditions\"],\n[\"\u0441\u043b\u043e\", null],\n[\"\u0432\u0438\u044f\", null],\n[\",\", null],\n[\"\u2581\u0438\", \"et\"],\n[\"\u2581\u0432\", \"dans\"],\n[\"\u2581\u0431\u0443\", \"futur\"],\n[\"\u0434\u0443\", null],\n[\"\u0449\u0435\", null],\n[\"\u043c\", null],\n[\"\u2581\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\", \"apparaissaient\"],\n[\"\u043b\u044f\", null],\n[\"\u043b\u0438\u0441\u044c\", null],\n[\"\u2581\u0442\", \"difficult\u00e9s\"],\n[\"\u0440\u0443\u0434\", null],\n[\"\u043d\u043e\u0441\u0442\u0438\", null],\n[\"\u2581\u0438\", \"et\"],\n[\"\u2581\u0441\u043e\", \"doutes\"],\n[\"\u043c\", null],\n[\"\u043d\u0435\u043d\u0438\u044f\", null],\n[\",\", null],\n[\"\u2581\u0432\", \"dans\"],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0445\", \"lesquels\"],\n[\"\u2581\u0412\", \"Vronski\"],\n[\"\u0440\u043e\u043d\", null],\n[\"\u0441\u043a\u0438\u0439\", null],\n[\"\u2581\u0443\", null],\n[\"\u0436\u0435\", null],\n[\"\u2581\u043d\u0435\", \"ne\"],\n[\"\u2581\u043d\u0430\u0445\u043e\u0434\u0438\", \"trouvait\"],\n[\"\u043b\", null],\n[\"\u2581\u0440\u0443\u043a\u043e\", \"fil\"],\n[\"\u0432\u043e\u0434\", null],\n[\"\u044f\", null],\n[\"\u0449\u0435\", null],\n[\"\u0439\", null],\n[\"\u2581\u043d\u0438\", null],\n[\"\u0442\u0438\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nIf only my malediction is needed for that, I bestow it upon him! and his cathedral, and his mills!\n\n\n---=== TOKENIZED TEXT ===---\n\n['If', '\u2581only', '\u2581my', '\u2581mal', 'ed', 'iction', '\u2581is', '\u2581needed', '\u2581for', '\u2581that', ',', '\u2581I', '\u2581bestow', '\u2581it', '\u2581upon', '\u2581him', '!', '\u2581and', '\u2581his', '\u2581cathedral', ',', '\u2581and', '\u2581his', '\u2581mills', '!']"}], "[\n  [\"If\", \"Si\"],\n  [\"\u2581only\", \"seulement\"],\n  [\"\u2581my\", null],\n  [\"\u2581mal\", \"mal\u00e9diction\"],\n  [\"ed\", null],\n  [\"iction\", null],\n  [\"\u2581is\", null],\n  [\"\u2581needed\", \"n\u00e9cessaire\"],\n  [\"\u2581for\", null],\n  [\"\u2581that\", \"cela\"],\n  [\",\", null],\n  [\"\u2581I\", null],\n  [\"\u2581bestow\", \"accorde\"],\n  [\"\u2581it\", null],\n  [\"\u2581upon\", null],\n  [\"\u2581him\", \"lui\"],\n  [\"!\", null],\n  [\"\u2581and\", null],\n  [\"\u2581his\", null],\n  [\"\u2581cathedral\", \"cath\u00e9drale\"],\n  [\",\", null],\n  [\"\u2581and\", null],\n  [\"\u2581his\", null],\n  [\"\u2581mills\", \"moulins\"],\n  [\"!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nIf only my malediction is needed for that, I bestow it upon him! and his cathedral, and his mills!\n\n\n---=== TOKENIZED TEXT ===---\n\n['If', '\u2581only', '\u2581my', '\u2581mal', 'ed', 'iction', '\u2581is', '\u2581needed', '\u2581for', '\u2581that', ',', '\u2581I', '\u2581bestow', '\u2581it', '\u2581upon', '\u2581him', '!', '\u2581and', '\u2581his', '\u2581cathedral', ',', '\u2581and', '\u2581his', '\u2581mills', '!']"}], "[\n    [\"If\", \"\u0415\u0441\u043b\u0438\"],\n    [\"\u2581only\", null],\n    [\"\u2581my\", null],\n    [\"\u2581mal\", \"\u043f\u0440\u043e\u043a\u043b\u044f\u0442\u0438\u0435\"],\n    [\"ed\", null],\n    [\"iction\", null],\n    [\"\u2581is\", null],\n    [\"\u2581needed\", \"\u043d\u0443\u0436\u043d\u043e\"],\n    [\"\u2581for\", null],\n    [\"\u2581that\", \"\u044d\u0442\u043e\u0433\u043e\"],\n    [\",\", null],\n    [\"\u2581I\", null],\n    [\"\u2581bestow\", \"\u043d\u0430\u0434\u0435\u043b\u044f\u044e\"],\n    [\"\u2581it\", null],\n    [\"\u2581upon\", null],\n    [\"\u2581him\", \"\u0435\u0433\u043e\"],\n    [\"!\", null],\n    [\"\u2581and\", null],\n    [\"\u2581his\", null],\n    [\"\u2581cathedral\", \"\u0441\u043e\u0431\u043e\u0440\"],\n    [\",\", null],\n    [\"\u2581and\", null],\n    [\"\u2581his\", null],\n    [\"\u2581mills\", \"\u043c\u0435\u043b\u044c\u043d\u0438\u0446\u044b\"],\n    [\"!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nHe bent down to her ear, and added (the spectators supposed that he was receiving her last confession): \"Will you have me?\n\n\n---=== TOKENIZED TEXT ===---\n\n['He', '\u2581bent', '\u2581down', '\u2581to', '\u2581her', '\u2581ear', ',', '\u2581and', '\u2581added', '\u2581(', 'the', '\u2581spectators', '\u2581supposed', '\u2581that', '\u2581he', '\u2581was', '\u2581receiving', '\u2581her', '\u2581last', '\u2581confession', '):', '\u2581\"', 'Will', '\u2581you', '\u2581have', '\u2581me', '?']"}], "[\n[\"He\", \"Il\"],\n[\"\u2581bent\", \"pencha\"],\n[\"\u2581down\", null],\n[\"\u2581to\", null],\n[\"\u2581her\", \"son\"],\n[\"\u2581ear\", \"oreille\"],\n[\",\", null],\n[\"\u2581and\", null],\n[\"\u2581added\", \"ajouta\"],\n[\"\u2581(\", null],\n[\"the\", null],\n[\"\u2581spectators\", \"spectateurs\"],\n[\"\u2581supposed\", \"supposaient\"],\n[\"\u2581that\", null],\n[\"\u2581he\", null],\n[\"\u2581was\", null],\n[\"\u2581receiving\", \"recevait\"],\n[\"\u2581her\", \"sa\"],\n[\"\u2581last\", \"derni\u00e8re\"],\n[\"\u2581confession\", \"confession\"],\n[\"):\", null],\n[\"\u2581\\\"\", null],\n[\"Will\", \"Voulez\"],\n[\"\u2581you\", null],\n[\"\u2581have\", null],\n[\"\u2581me\", \"m'\u00e9pouser\"],\n[\"?\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nHe bent down to her ear, and added (the spectators supposed that he was receiving her last confession): \"Will you have me?\n\n\n---=== TOKENIZED TEXT ===---\n\n['He', '\u2581bent', '\u2581down', '\u2581to', '\u2581her', '\u2581ear', ',', '\u2581and', '\u2581added', '\u2581(', 'the', '\u2581spectators', '\u2581supposed', '\u2581that', '\u2581he', '\u2581was', '\u2581receiving', '\u2581her', '\u2581last', '\u2581confession', '):', '\u2581\"', 'Will', '\u2581you', '\u2581have', '\u2581me', '?']"}], "[\n[\"He\", \"\u041e\u043d\"],\n[\"\u2581bent\", \"\u043d\u0430\u043a\u043b\u043e\u043d\u0438\u043b\u0441\u044f\"],\n[\"\u2581down\", null],\n[\"\u2581to\", null],\n[\"\u2581her\", \"\u0435\u0451\"],\n[\"\u2581ear\", \"\u0443\u0445\u0443\"],\n[\",\", null],\n[\"\u2581and\", null],\n[\"\u2581added\", \"\u0434\u043e\u0431\u0430\u0432\u0438\u043b\"],\n[\"\u2581(\", null],\n[\"the\", null],\n[\"\u2581spectators\", \"\u0437\u0440\u0438\u0442\u0435\u043b\u0438\"],\n[\"\u2581supposed\", \"\u043f\u0440\u0435\u0434\u043f\u043e\u043b\u0430\u0433\u0430\u043b\u0438\"],\n[\"\u2581that\", null],\n[\"\u2581he\", null],\n[\"\u2581was\", null],\n[\"\u2581receiving\", \"\u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u0442\"],\n[\"\u2581her\", \"\u0435\u0451\"],\n[\"\u2581last\", \"\u043f\u043e\u0441\u043b\u0435\u0434\u043d\u044e\u044e\"],\n[\"\u2581confession\", \"\u0438\u0441\u043f\u043e\u0432\u0435\u0434\u044c\"],\n[\"):\", null],\n[\"\u2581\\\"\", null],\n[\"Will\", \"\u0412\u044b\u0439\u0434\u0435\u0448\u044c\"],\n[\"\u2581you\", null],\n[\"\u2581have\", null],\n[\"\u2581me\", null],\n[\"?\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nLa Gr\u00e8ve avait d\u00e8s lors cet aspect sinistre que lui conservent encore aujourd\u2019hui l\u2019id\u00e9e ex\u00e9crable qu\u2019elle r\u00e9veille et le sombre H\u00f4tel de Ville de Dominique Boccador, qui a remplac\u00e9 la Maison-aux-Piliers.\n\n\n---=== TOKENIZED TEXT ===---\n\n['La', '\u2581Gr', '\u00e8ve', '\u2581avait', '\u2581d\u00e8s', '\u2581lors', '\u2581cet', '\u2581aspect', '\u2581sin', 'istre', '\u2581que', '\u2581lui', '\u2581conserv', 'ent', '\u2581encore', '\u2581aujourd', '\u2019', 'hui', '\u2581l', '\u2019', 'id\u00e9e', '\u2581ex', '\u00e9c', 'rable', '\u2581qu', '\u2019', 'elle', '\u2581r\u00e9', 'veille', '\u2581et', '\u2581le', '\u2581sombre', '\u2581H\u00f4tel', '\u2581de', '\u2581Ville', '\u2581de', '\u2581Dominique', '\u2581Boc', 'cador', ',', '\u2581qui', '\u2581a', '\u2581remplac\u00e9', '\u2581la', '\u2581Maison', '-', 'aux', '-', 'P', 'ili', 'ers', '.']"}], "[\n  [\"La\", null],\n  [\"\u2581Gr\", \"Square\"],\n  [\"\u00e8ve\", null],\n  [\"\u2581avait\", \"had\"],\n  [\"\u2581d\u00e8s\", null],\n  [\"\u2581lors\", \"then\"],\n  [\"\u2581cet\", \"this\"],\n  [\"\u2581aspect\", \"appearance\"],\n  [\"\u2581sin\", \"sini\"],\n  [\"istre\", null],\n  [\"\u2581que\", null],\n  [\"\u2581lui\", null],\n  [\"\u2581conserv\", \"still\"],\n  [\"ent\", null],\n  [\"\u2581encore\", null],\n  [\"\u2581aujourd\", \"today\"],\n  [\"'\", null],\n  [\"hui\", null],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"id\u00e9e\", \"idea\"],\n  [\"\u2581ex\", \"execr\"],\n  [\"\u00e9c\", null],\n  [\"rable\", null],\n  [\"\u2581qu\", null],\n  [\"'\", null],\n  [\"elle\", \"it\"],\n  [\"\u2581r\u00e9\", \"awak\"],\n  [\"veille\", null],\n  [\"\u2581et\", \"and\"],\n  [\"\u2581le\", null],\n  [\"\u2581sombre\", \"gloomy\"],\n  [\"\u2581H\u00f4tel\", \"City\"],\n  [\"\u2581de\", null],\n  [\"\u2581Ville\", \"Hall\"],\n  [\"\u2581de\", \"of\"],\n  [\"\u2581Dominique\", \"Dominique\"],\n  [\"\u2581Boc\", \"Boc\"],\n  [\"cador\", null],\n  [\",\", null],\n  [\"\u2581qui\", \"which\"],\n  [\"\u2581a\", \"has\"],\n  [\"\u2581remplac\u00e9\", \"replaced\"],\n  [\"\u2581la\", null],\n  [\"\u2581Maison\", \"House\"],\n  [\"-\", null],\n  [\"aux\", null],\n  [\"-\", null],\n  [\"\u2581P\", \"Pil\"],\n  [\"ili\", null],\n  [\"ers\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nLa Gr\u00e8ve avait d\u00e8s lors cet aspect sinistre que lui conservent encore aujourd\u2019hui l\u2019id\u00e9e ex\u00e9crable qu\u2019elle r\u00e9veille et le sombre H\u00f4tel de Ville de Dominique Boccador, qui a remplac\u00e9 la Maison-aux-Piliers.\n\n\n---=== TOKENIZED TEXT ===---\n\n['La', '\u2581Gr', '\u00e8ve', '\u2581avait', '\u2581d\u00e8s', '\u2581lors', '\u2581cet', '\u2581aspect', '\u2581sin', 'istre', '\u2581que', '\u2581lui', '\u2581conserv', 'ent', '\u2581encore', '\u2581aujourd', '\u2019', 'hui', '\u2581l', '\u2019', 'id\u00e9e', '\u2581ex', '\u00e9c', 'rable', '\u2581qu', '\u2019', 'elle', '\u2581r\u00e9', 'veille', '\u2581et', '\u2581le', '\u2581sombre', '\u2581H\u00f4tel', '\u2581de', '\u2581Ville', '\u2581de', '\u2581Dominique', '\u2581Boc', 'cador', ',', '\u2581qui', '\u2581a', '\u2581remplac\u00e9', '\u2581la', '\u2581Maison', '-', 'aux', '-', 'P', 'ili', 'ers', '.']"}], "[\n[\"La\", \"\u0413\u0440\u0435\u0432\u0441\u043a\u0430\u044f\"],\n[\"\u2581Gr\", null],\n[\"\u00e8ve\", null],\n[\"\u2581avait\", \"\u0438\u043c\u0435\u043b\u0430\"],\n[\"\u2581d\u00e8s\", \"\u0441\"],\n[\"\u2581lors\", \"\u0442\u0435\u0445 \u043f\u043e\u0440\"],\n[\"\u2581cet\", \"\u044d\u0442\u043e\u0442\"],\n[\"\u2581aspect\", \"\u0432\u0438\u0434\"],\n[\"\u2581sin\", null],\n[\"istre\", null],\n[\"\u2581que\", null],\n[\"\u2581lui\", null],\n[\"\u2581conserv\", null],\n[\"ent\", null],\n[\"\u2581encore\", \"\u0434\u043e \u0441\u0438\u0445 \u043f\u043e\u0440\"],\n[\"\u2581aujourd\", null],\n[\"'\", null],\n[\"hui\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"id\u00e9e\", \"\u043c\u044b\u0441\u043b\u044c\"],\n[\"\u2581ex\", null],\n[\"\u00e9c\", null],\n[\"rable\", null],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"elle\", \"\u043e\u043d\u0430\"],\n[\"\u2581r\u00e9\", null],\n[\"veille\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581le\", null],\n[\"\u2581sombre\", \"\u043c\u0440\u0430\u0447\u043d\u0430\u044f\"],\n[\"\u2581H\u00f4tel\", \"\u0420\u0430\u0442\u0443\u0448\u0430\"],\n[\"\u2581de\", null],\n[\"\u2581Ville\", null],\n[\"\u2581de\", null],\n[\"\u2581Dominique\", \"\u0414\u043e\u043c\u0438\u043d\u0438\u043a\u0430\"],\n[\"\u2581Boc\", null],\n[\"cador\", null],\n[\",\", null],\n[\"\u2581qui\", \"\u043a\u043e\u0442\u043e\u0440\u0430\u044f\"],\n[\"\u2581a\", null],\n[\"\u2581remplac\u00e9\", \"\u0437\u0430\u043c\u0435\u043d\u0438\u043b\u0430\"],\n[\"\u2581la\", null],\n[\"\u2581Maison\", \"\u0414\u043e\u043c\"],\n[\"-\", null],\n[\"aux\", null],\n[\"-\", null],\n[\"P\", null],\n[\"ili\", null],\n[\"ers\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nQuand ma\u00eetre Philippe Lheulier, avocat extraordinaire du roi, avait dit \u00e0 la pauvre Esmeralda : Il se meurt, c\u2019\u00e9tait par erreur ou par plaisanterie.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Quand', '\u2581ma\u00eetre', '\u2581Philippe', '\u2581L', 'he', 'ulier', ',', '\u2581avocat', '\u2581extraordinaire', '\u2581du', '\u2581roi', ',', '\u2581avait', '\u2581dit', '\u2581\u00e0', '\u2581la', '\u2581pauvre', '\u2581Esmeralda', '\u2581:', '\u2581Il', '\u2581se', '\u2581meurt', ',', '\u2581c', '\u2019', '\u00e9tait', '\u2581par', '\u2581erreur', '\u2581ou', '\u2581par', '\u2581pla', 'isan', 'terie', '.']"}], "[\n[\"Quand\", \"When\"],\n[\"\u2581ma\u00eetre\", \"master\"],\n[\"\u2581Philippe\", \"Philippe\"],\n[\"\u2581L\", null],\n[\"he\", null],\n[\"ulier\", null],\n[\",\", null],\n[\"\u2581avocat\", \"lawyer\"],\n[\"\u2581extraordinaire\", \"extraordinary\"],\n[\"\u2581du\", null],\n[\"\u2581roi\", \"king\"],\n[\",\", null],\n[\"\u2581avait\", null],\n[\"\u2581dit\", \"said\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581la\", null],\n[\"\u2581pauvre\", \"poor\"],\n[\"\u2581Esmeralda\", \"Esmeralda\"],\n[\"\u2581:\", null],\n[\"\u2581Il\", \"He\"],\n[\"\u2581se\", null],\n[\"\u2581meurt\", \"is dying\"],\n[\",\", null],\n[\"\u2581c\", null],\n[\"'\", null],\n[\"\u00e9tait\", null],\n[\"\u2581par\", \"by\"],\n[\"\u2581erreur\", \"mistake\"],\n[\"\u2581ou\", \"or\"],\n[\"\u2581par\", \"by\"],\n[\"\u2581pla\", null],\n[\"isan\", null],\n[\"terie\", \"joke\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nQuand ma\u00eetre Philippe Lheulier, avocat extraordinaire du roi, avait dit \u00e0 la pauvre Esmeralda : Il se meurt, c\u2019\u00e9tait par erreur ou par plaisanterie.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Quand', '\u2581ma\u00eetre', '\u2581Philippe', '\u2581L', 'he', 'ulier', ',', '\u2581avocat', '\u2581extraordinaire', '\u2581du', '\u2581roi', ',', '\u2581avait', '\u2581dit', '\u2581\u00e0', '\u2581la', '\u2581pauvre', '\u2581Esmeralda', '\u2581:', '\u2581Il', '\u2581se', '\u2581meurt', ',', '\u2581c', '\u2019', '\u00e9tait', '\u2581par', '\u2581erreur', '\u2581ou', '\u2581par', '\u2581pla', 'isan', 'terie', '.']"}], "[\n  [\"Quand\", \"\u041a\u043e\u0433\u0434\u0430\"],\n  [\"\u2581ma\u00eetre\", \"\u043c\u044d\u0442\u0440\"],\n  [\"\u2581Philippe\", \"\u0424\u0438\u043b\u0438\u043f\u043f\"],\n  [\"\u2581L\", \"\u041b\"],\n  [\"he\", null],\n  [\"ulier\", null],\n  [\",\", null],\n  [\"\u2581avocat\", \"\u0430\u0434\u0432\u043e\u043a\u0430\u0442\"],\n  [\"\u2581extraordinaire\", \"\u0447\u0440\u0435\u0437\u0432\u044b\u0447\u0430\u0439\u043d\u044b\u0439\"],\n  [\"\u2581du\", null],\n  [\"\u2581roi\", \"\u043a\u043e\u0440\u043e\u043b\u044f\"],\n  [\",\", null],\n  [\"\u2581avait\", null],\n  [\"\u2581dit\", \"\u0441\u043a\u0430\u0437\u0430\u043b\"],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581la\", null],\n  [\"\u2581pauvre\", \"\u0431\u0435\u0434\u043d\u043e\u0439\"],\n  [\"\u2581Esmeralda\", \"\u042d\u0441\u043c\u0435\u0440\u0430\u043b\u044c\u0434\u0435\"],\n  [\"\u2581:\", null],\n  [\"\u2581Il\", \"\u041e\u043d\"],\n  [\"\u2581se\", null],\n  [\"\u2581meurt\", \"\u0443\u043c\u0438\u0440\u0430\u0435\u0442\"],\n  [\",\", null],\n  [\"\u2581c\", \"\u044d\u0442\u043e\"],\n  [\"'\", null],\n  [\"\u00e9tait\", null],\n  [\"\u2581par\", null],\n  [\"\u2581erreur\", \"\u043e\u0448\u0438\u0431\u043a\u0430\"],\n  [\"\u2581ou\", \"\u0438\u043b\u0438\"],\n  [\"\u2581par\", null],\n  [\"\u2581pla\", \"\u0448\u0443\u0442\u043a\u0430\"],\n  [\"isan\", null],\n  [\"terie\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nLe voyant ouvrir la porte du jardin, je montai rapidement dans ma chambre.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Le', '\u2581voyant', '\u2581ouvrir', '\u2581la', '\u2581porte', '\u2581du', '\u2581jardin', ',', '\u2581je', '\u2581mon', 'tai', '\u2581rapidement', '\u2581dans', '\u2581ma', '\u2581chambre', '.']"}], "[\n  [\"Le\", null],\n  [\"\u2581voyant\", \"seeing\"],\n  [\"\u2581ouvrir\", \"open\"],\n  [\"\u2581la\", null],\n  [\"\u2581porte\", \"door\"],\n  [\"\u2581du\", null],\n  [\"\u2581jardin\", \"garden\"],\n  [\",\", null],\n  [\"\u2581je\", null],\n  [\"\u2581mon\", \"went\"],\n  [\"tai\", null],\n  [\"\u2581rapidement\", \"quickly\"],\n  [\"\u2581dans\", null],\n  [\"\u2581ma\", null],\n  [\"\u2581chambre\", \"room\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nLe voyant ouvrir la porte du jardin, je montai rapidement dans ma chambre.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Le', '\u2581voyant', '\u2581ouvrir', '\u2581la', '\u2581porte', '\u2581du', '\u2581jardin', ',', '\u2581je', '\u2581mon', 'tai', '\u2581rapidement', '\u2581dans', '\u2581ma', '\u2581chambre', '.']"}], "[\n[\"Le\", null],\n[\"\u2581voyant\", \"\u0423\u0432\u0438\u0434\u0435\u0432\"],\n[\"\u2581ouvrir\", \"\u043e\u0442\u043a\u0440\u044b\u0432\u0430\u0442\u044c\"],\n[\"\u2581la\", null],\n[\"\u2581porte\", \"\u0434\u0432\u0435\u0440\u044c\"],\n[\"\u2581du\", null],\n[\"\u2581jardin\", \"\u0441\u0430\u0434\u0430\"],\n[\",\", null],\n[\"\u2581je\", null],\n[\"\u2581mon\", \"\u043f\u043e\u0434\u043d\u044f\u043b\u0441\u044f\"],\n[\"tai\", null],\n[\"\u2581rapidement\", \"\u0431\u044b\u0441\u0442\u0440\u043e\"],\n[\"\u2581dans\", null],\n[\"\u2581ma\", null],\n[\"\u2581chambre\", \"\u043a\u043e\u043c\u043d\u0430\u0442\u0443\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nLa rencontre de Glenarvan et Ayrton avait \u00e9videmment produit chez tous deux une \u00e9motion r\u00e9ciproque.\n\n\n---=== TOKENIZED TEXT ===---\n\n['La', '\u2581rencontre', '\u2581de', '\u2581Glen', 'ar', 'van', '\u2581et', '\u2581Ayr', 'ton', '\u2581avait', '\u2581\u00e9videmment', '\u2581produit', '\u2581chez', '\u2581tous', '\u2581deux', '\u2581une', '\u2581\u00e9motion', '\u2581r\u00e9', 'ci', 'pro', 'que', '.']"}], "[\n  [\"La\", null],\n  [\"\u2581rencontre\", \"meeting\"],\n  [\"\u2581de\", null],\n  [\"\u2581Glen\", \"Glenarvan\"],\n  [\"ar\", null],\n  [\"van\", null],\n  [\"\u2581et\", null],\n  [\"\u2581Ayr\", \"Ayrton\"],\n  [\"ton\", null],\n  [\"\u2581avait\", \"had\"],\n  [\"\u2581\u00e9videmment\", \"obviously\"],\n  [\"\u2581produit\", \"produced\"],\n  [\"\u2581chez\", \"in\"],\n  [\"\u2581tous\", \"both\"],\n  [\"\u2581deux\", null],\n  [\"\u2581une\", null],\n  [\"\u2581\u00e9motion\", \"emotion\"],\n  [\"\u2581r\u00e9\", \"mutual\"],\n  [\"ci\", null],\n  [\"pro\", null],\n  [\"que\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nLa rencontre de Glenarvan et Ayrton avait \u00e9videmment produit chez tous deux une \u00e9motion r\u00e9ciproque.\n\n\n---=== TOKENIZED TEXT ===---\n\n['La', '\u2581rencontre', '\u2581de', '\u2581Glen', 'ar', 'van', '\u2581et', '\u2581Ayr', 'ton', '\u2581avait', '\u2581\u00e9videmment', '\u2581produit', '\u2581chez', '\u2581tous', '\u2581deux', '\u2581une', '\u2581\u00e9motion', '\u2581r\u00e9', 'ci', 'pro', 'que', '.']"}], "[\n[\"La\", null],\n[\"\u2581rencontre\", \"\u0412\u0441\u0442\u0440\u0435\u0447\u0430\"],\n[\"\u2581de\", null],\n[\"\u2581Glen\", \"\u0413\u043b\u0435\u043d\u0430\u0440\u0432\u0430\u043d\"],\n[\"ar\", null],\n[\"van\", null],\n[\"\u2581et\", null],\n[\"\u2581Ayr\", \"\u0410\u0439\u0440\u0442\u043e\u043d\"],\n[\"ton\", null],\n[\"\u2581avait\", null],\n[\"\u2581\u00e9videmment\", \"\u043e\u0447\u0435\u0432\u0438\u0434\u043d\u043e\"],\n[\"\u2581produit\", \"\u0432\u044b\u0437\u0432\u0430\u043b\u0430\"],\n[\"\u2581chez\", \"\u0443\"],\n[\"\u2581tous\", \"\u043e\u0431\u043e\u0438\u0445\"],\n[\"\u2581deux\", null],\n[\"\u2581une\", null],\n[\"\u2581\u00e9motion\", \"\u044d\u043c\u043e\u0446\u0438\u044e\"],\n[\"\u2581r\u00e9\", \"\u0432\u0437\u0430\u0438\u043c\u043d\u0443\u044e\"],\n[\"ci\", null],\n[\"pro\", null],\n[\"que\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0415\u0441\u043b\u0438 \u0431 \u044f \u043c\u043e\u0433 \u043b\u044e\u0431\u0438\u0442\u044c \u0432\u0441\u0435 \u044d\u0442\u043e \u0434\u0435\u043b\u043e, \u043a\u0430\u043a \u044f \u043b\u044e\u0431\u043b\u044e \u0442\u0435\u0431\u044f... \u0430 \u0442\u043e \u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435 \u0432\u0440\u0435\u043c\u044f \u0434\u0435\u043b\u0430\u044e, \u043a\u0430\u043a \u0437\u0430\u0434\u0430\u043d\u043d\u044b\u0439 \u0443\u0440\u043e\u043a.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0415\u0441\u043b\u0438', '\u2581\u0431', '\u2581\u044f', '\u2581\u043c\u043e\u0433', '\u2581\u043b\u044e\u0431\u0438\u0442\u044c', '\u2581\u0432\u0441\u0435', '\u2581\u044d\u0442\u043e', '\u2581\u0434\u0435\u043b\u043e', ',', '\u2581\u043a\u0430\u043a', '\u2581\u044f', '\u2581\u043b\u044e\u0431\u043b\u044e', '\u2581\u0442\u0435\u0431\u044f', '...', '\u2581\u0430', '\u2581\u0442\u043e', '\u2581\u044f', '\u2581\u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435', '\u2581\u0432\u0440\u0435\u043c\u044f', '\u2581\u0434\u0435\u043b\u0430', '\u044e', ',', '\u2581\u043a\u0430\u043a', '\u2581\u0437\u0430\u0434\u0430', '\u043d\u043d\u044b\u0439', '\u2581\u0443\u0440\u043e\u043a', '.']"}], "[\n    [\"\u0415\u0441\u043b\u0438\", \"If\"],\n    [\"\u2581\u0431\", null],\n    [\"\u2581\u044f\", null],\n    [\"\u2581\u043c\u043e\u0433\", \"could\"],\n    [\"\u2581\u043b\u044e\u0431\u0438\u0442\u044c\", \"love\"],\n    [\"\u2581\u0432\u0441\u0435\", \"all\"],\n    [\"\u2581\u044d\u0442\u043e\", \"this\"],\n    [\"\u2581\u0434\u0435\u043b\u043e\", \"matter\"],\n    [\",\", null],\n    [\"\u2581\u043a\u0430\u043a\", \"as\"],\n    [\"\u2581\u044f\", null],\n    [\"\u2581\u043b\u044e\u0431\u043b\u044e\", \"love\"],\n    [\"\u2581\u0442\u0435\u0431\u044f\", \"you\"],\n    [\"...\", null],\n    [\"\u2581\u0430\", \"but\"],\n    [\"\u2581\u0442\u043e\", null],\n    [\"\u2581\u044f\", null],\n    [\"\u2581\u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435\", \"lately\"],\n    [\"\u2581\u0432\u0440\u0435\u043c\u044f\", \"time\"],\n    [\"\u2581\u0434\u0435\u043b\u0430\", \"do\"],\n    [\"\u044e\", null],\n    [\",\", null],\n    [\"\u2581\u043a\u0430\u043a\", \"like\"],\n    [\"\u2581\u0437\u0430\u0434\u0430\", \"assigned\"],\n    [\"\u043d\u043d\u044b\u0439\", null],\n    [\"\u2581\u0443\u0440\u043e\u043a\", \"task\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u0415\u0441\u043b\u0438 \u0431 \u044f \u043c\u043e\u0433 \u043b\u044e\u0431\u0438\u0442\u044c \u0432\u0441\u0435 \u044d\u0442\u043e \u0434\u0435\u043b\u043e, \u043a\u0430\u043a \u044f \u043b\u044e\u0431\u043b\u044e \u0442\u0435\u0431\u044f... \u0430 \u0442\u043e \u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435 \u0432\u0440\u0435\u043c\u044f \u0434\u0435\u043b\u0430\u044e, \u043a\u0430\u043a \u0437\u0430\u0434\u0430\u043d\u043d\u044b\u0439 \u0443\u0440\u043e\u043a.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0415\u0441\u043b\u0438', '\u2581\u0431', '\u2581\u044f', '\u2581\u043c\u043e\u0433', '\u2581\u043b\u044e\u0431\u0438\u0442\u044c', '\u2581\u0432\u0441\u0435', '\u2581\u044d\u0442\u043e', '\u2581\u0434\u0435\u043b\u043e', ',', '\u2581\u043a\u0430\u043a', '\u2581\u044f', '\u2581\u043b\u044e\u0431\u043b\u044e', '\u2581\u0442\u0435\u0431\u044f', '...', '\u2581\u0430', '\u2581\u0442\u043e', '\u2581\u044f', '\u2581\u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435', '\u2581\u0432\u0440\u0435\u043c\u044f', '\u2581\u0434\u0435\u043b\u0430', '\u044e', ',', '\u2581\u043a\u0430\u043a', '\u2581\u0437\u0430\u0434\u0430', '\u043d\u043d\u044b\u0439', '\u2581\u0443\u0440\u043e\u043a', '.']"}], "[\n    [\"\u0415\u0441\u043b\u0438\", \"Si\"],\n    [\"\u2581\u0431\", null],\n    [\"\u2581\u044f\", null],\n    [\"\u2581\u043c\u043e\u0433\", \"pouvais\"],\n    [\"\u2581\u043b\u044e\u0431\u0438\u0442\u044c\", \"aimer\"],\n    [\"\u2581\u0432\u0441\u0435\", \"tout\"],\n    [\"\u2581\u044d\u0442\u043e\", \"cela\"],\n    [\"\u2581\u0434\u0435\u043b\u043e\", \"affaire\"],\n    [\",\", null],\n    [\"\u2581\u043a\u0430\u043a\", \"comme\"],\n    [\"\u2581\u044f\", null],\n    [\"\u2581\u043b\u044e\u0431\u043b\u044e\", \"aime\"],\n    [\"\u2581\u0442\u0435\u0431\u044f\", \"toi\"],\n    [\"...\", null],\n    [\"\u2581\u0430\", \"mais\"],\n    [\"\u2581\u0442\u043e\", null],\n    [\"\u2581\u044f\", null],\n    [\"\u2581\u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435\", \"derni\u00e8rement\"],\n    [\"\u2581\u0432\u0440\u0435\u043c\u044f\", \"temps\"],\n    [\"\u2581\u0434\u0435\u043b\u0430\", \"fais\"],\n    [\"\u044e\", null],\n    [\",\", null],\n    [\"\u2581\u043a\u0430\u043a\", \"comme\"],\n    [\"\u2581\u0437\u0430\u0434\u0430\", \"devoir\"],\n    [\"\u043d\u043d\u044b\u0439\", null],\n    [\"\u2581\u0443\u0440\u043e\u043a\", \"le\u00e7on\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n-- \u0410 \u043d\u0430 \u044d\u0442\u0438 \u0434\u0435\u043d\u044c\u0433\u0438 \u043e\u043d \u0431\u044b \u043d\u0430\u043a\u0443\u043f\u0438\u043b \u0441\u043a\u043e\u0442\u0430 \u0438\u043b\u0438 \u0437\u0435\u043c\u043b\u0438\u0446\u0443 \u043a\u0443\u043f\u0438\u043b \u0431\u044b \u0437\u0430 \u0431\u0435\u0441\u0446\u0435\u043d\u043e\u043a \u0438 \u043c\u0443\u0436\u0438\u043a\u0430\u043c \u0440\u043e\u0437\u0434\u0430\u043b \u0431\u044b \u0432\u043d\u0430\u0439\u043c\u044b, -- \u0441 \u0443\u043b\u044b\u0431\u043a\u043e\u0439 \u0434\u043e\u043a\u043e\u043d\u0447\u0438\u043b \u041b\u0435\u0432\u0438\u043d, \u043e\u0447\u0435\u0432\u0438\u0434\u043d\u043e \u043d\u0435 \u0440\u0430\u0437 \u0443\u0436\u0435 \u0441\u0442\u0430\u043b\u043a\u0438\u0432\u0430\u0432\u0448\u0438\u0439\u0441\u044f \u0441 \u043f\u043e\u0434\u043e\u0431\u043d\u044b\u043c\u0438 \u0440\u0430\u0441\u0447\u0435\u0442\u0430\u043c\u0438. -- \u0418 \u043e\u043d \u0441\u043e\u0441\u0442\u0430\u0432\u0438\u0442 \u0441\u0435\u0431\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435.\n\n\n---=== TOKENIZED TEXT ===---\n\n['--', '\u2581\u0410', '\u2581\u043d\u0430', '\u2581\u044d\u0442\u0438', '\u2581\u0434\u0435\u043d\u044c\u0433\u0438', '\u2581\u043e\u043d', '\u2581\u0431\u044b', '\u2581\u043d\u0430', '\u043a\u0443', '\u043f\u0438\u043b', '\u2581\u0441\u043a\u043e', '\u0442\u0430', '\u2581\u0438\u043b\u0438', '\u2581\u0437\u0435\u043c\u043b\u0438', '\u0446\u0443', '\u2581\u043a\u0443\u043f\u0438', '\u043b', '\u2581\u0431\u044b', '\u2581\u0437\u0430', '\u2581\u0431\u0435\u0441', '\u0446\u0435', '\u043d\u043e\u043a', '\u2581\u0438', '\u2581\u043c\u0443', '\u0436\u0438', '\u043a\u0430\u043c', '\u2581\u0440\u043e', '\u0437\u0434\u0430', '\u043b', '\u2581\u0431\u044b', '\u2581\u0432', '\u043d\u0430\u0439', '\u043c\u044b', ',', '\u2581--', '\u2581\u0441', '\u2581\u0443\u043b\u044b\u0431', '\u043a\u043e\u0439', '\u2581\u0434\u043e', '\u043a\u043e\u043d', '\u0447\u0438\u043b', '\u2581\u041b\u0435', '\u0432\u0438\u043d', ',', '\u2581\u043e\u0447\u0435', '\u0432\u0438\u0434\u043d\u043e', '\u2581\u043d\u0435', '\u2581\u0440\u0430\u0437', '\u2581\u0443\u0436\u0435', '\u2581\u0441\u0442\u0430', '\u043b\u043a\u0438', '\u0432\u0430', '\u0432\u0448\u0438\u0439\u0441\u044f', '\u2581\u0441', '\u2581\u043f\u043e\u0434\u043e\u0431', '\u043d\u044b\u043c\u0438', '\u2581\u0440\u0430\u0441', '\u0447\u0435', '\u0442\u0430\u043c\u0438', '.', '\u2581--', '\u2581\u0418', '\u2581\u043e\u043d', '\u2581\u0441\u043e\u0441\u0442\u0430\u0432\u0438', '\u0442', '\u2581\u0441\u0435\u0431\u0435', '\u2581\u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435', '.']"}], "[\n[\"--\", null],\n[\"\u2581\u0410\", \"And\"],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u044d\u0442\u0438\", \"these\"],\n[\"\u2581\u0434\u0435\u043d\u044c\u0433\u0438\", \"money\"],\n[\"\u2581\u043e\u043d\", \"he\"],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u043a\u0443\", null],\n[\"\u043f\u0438\u043b\", \"buy\"],\n[\"\u2581\u0441\u043a\u043e\", null],\n[\"\u0442\u0430\", \"cattle\"],\n[\"\u2581\u0438\u043b\u0438\", \"or\"],\n[\"\u2581\u0437\u0435\u043c\u043b\u0438\", \"land\"],\n[\"\u0446\u0443\", null],\n[\"\u2581\u043a\u0443\u043f\u0438\", null],\n[\"\u043b\", \"buy\"],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u0437\u0430\", \"for\"],\n[\"\u2581\u0431\u0435\u0441\", null],\n[\"\u0446\u0435\", null],\n[\"\u043d\u043e\u043a\", \"pittance\"],\n[\"\u2581\u0438\", \"and\"],\n[\"\u2581\u043c\u0443\", null],\n[\"\u0436\u0438\", null],\n[\"\u043a\u0430\u043c\", \"peasants\"],\n[\"\u2581\u0440\u043e\", null],\n[\"\u0437\u0434\u0430\", null],\n[\"\u043b\", \"distribute\"],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u0432\", null],\n[\"\u043d\u0430\u0439\", null],\n[\"\u043c\u044b\", \"rent\"],\n[\",\", null],\n[\"\u2581--\", null],\n[\"\u2581\u0441\", \"with\"],\n[\"\u2581\u0443\u043b\u044b\u0431\", null],\n[\"\u043a\u043e\u0439\", \"smile\"],\n[\"\u2581\u0434\u043e\", null],\n[\"\u043a\u043e\u043d\", null],\n[\"\u0447\u0438\u043b\", \"finished\"],\n[\"\u2581\u041b\u0435\", null],\n[\"\u0432\u0438\u043d\", \"Levin\"],\n[\",\", null],\n[\"\u2581\u043e\u0447\u0435\", null],\n[\"\u0432\u0438\u0434\u043d\u043e\", \"obviously\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0440\u0430\u0437\", \"repeatedly\"],\n[\"\u2581\u0443\u0436\u0435\", \"already\"],\n[\"\u2581\u0441\u0442\u0430\", null],\n[\"\u043b\u043a\u0438\", null],\n[\"\u0432\u0430\", null],\n[\"\u0432\u0448\u0438\u0439\u0441\u044f\", \"encountered\"],\n[\"\u2581\u0441\", \"with\"],\n[\"\u2581\u043f\u043e\u0434\u043e\u0431\", null],\n[\"\u043d\u044b\u043c\u0438\", \"similar\"],\n[\"\u2581\u0440\u0430\u0441\", null],\n[\"\u0447\u0435\", null],\n[\"\u0442\u0430\u043c\u0438\", \"calculations\"],\n[\".\", null],\n[\"\u2581--\", null],\n[\"\u2581\u0418\", \"And\"],\n[\"\u2581\u043e\u043d\", \"he\"],\n[\"\u2581\u0441\u043e\u0441\u0442\u0430\u0432\u0438\", null],\n[\"\u0442\", \"will make\"],\n[\"\u2581\u0441\u0435\u0431\u0435\", \"himself\"],\n[\"\u2581\u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435\", \"fortune\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n-- \u0410 \u043d\u0430 \u044d\u0442\u0438 \u0434\u0435\u043d\u044c\u0433\u0438 \u043e\u043d \u0431\u044b \u043d\u0430\u043a\u0443\u043f\u0438\u043b \u0441\u043a\u043e\u0442\u0430 \u0438\u043b\u0438 \u0437\u0435\u043c\u043b\u0438\u0446\u0443 \u043a\u0443\u043f\u0438\u043b \u0431\u044b \u0437\u0430 \u0431\u0435\u0441\u0446\u0435\u043d\u043e\u043a \u0438 \u043c\u0443\u0436\u0438\u043a\u0430\u043c \u0440\u043e\u0437\u0434\u0430\u043b \u0431\u044b \u0432\u043d\u0430\u0439\u043c\u044b, -- \u0441 \u0443\u043b\u044b\u0431\u043a\u043e\u0439 \u0434\u043e\u043a\u043e\u043d\u0447\u0438\u043b \u041b\u0435\u0432\u0438\u043d, \u043e\u0447\u0435\u0432\u0438\u0434\u043d\u043e \u043d\u0435 \u0440\u0430\u0437 \u0443\u0436\u0435 \u0441\u0442\u0430\u043b\u043a\u0438\u0432\u0430\u0432\u0448\u0438\u0439\u0441\u044f \u0441 \u043f\u043e\u0434\u043e\u0431\u043d\u044b\u043c\u0438 \u0440\u0430\u0441\u0447\u0435\u0442\u0430\u043c\u0438. -- \u0418 \u043e\u043d \u0441\u043e\u0441\u0442\u0430\u0432\u0438\u0442 \u0441\u0435\u0431\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435.\n\n\n---=== TOKENIZED TEXT ===---\n\n['--', '\u2581\u0410', '\u2581\u043d\u0430', '\u2581\u044d\u0442\u0438', '\u2581\u0434\u0435\u043d\u044c\u0433\u0438', '\u2581\u043e\u043d', '\u2581\u0431\u044b', '\u2581\u043d\u0430', '\u043a\u0443', '\u043f\u0438\u043b', '\u2581\u0441\u043a\u043e', '\u0442\u0430', '\u2581\u0438\u043b\u0438', '\u2581\u0437\u0435\u043c\u043b\u0438', '\u0446\u0443', '\u2581\u043a\u0443\u043f\u0438', '\u043b', '\u2581\u0431\u044b', '\u2581\u0437\u0430', '\u2581\u0431\u0435\u0441', '\u0446\u0435', '\u043d\u043e\u043a', '\u2581\u0438', '\u2581\u043c\u0443', '\u0436\u0438', '\u043a\u0430\u043c', '\u2581\u0440\u043e', '\u0437\u0434\u0430', '\u043b', '\u2581\u0431\u044b', '\u2581\u0432', '\u043d\u0430\u0439', '\u043c\u044b', ',', '\u2581--', '\u2581\u0441', '\u2581\u0443\u043b\u044b\u0431', '\u043a\u043e\u0439', '\u2581\u0434\u043e', '\u043a\u043e\u043d', '\u0447\u0438\u043b', '\u2581\u041b\u0435', '\u0432\u0438\u043d', ',', '\u2581\u043e\u0447\u0435', '\u0432\u0438\u0434\u043d\u043e', '\u2581\u043d\u0435', '\u2581\u0440\u0430\u0437', '\u2581\u0443\u0436\u0435', '\u2581\u0441\u0442\u0430', '\u043b\u043a\u0438', '\u0432\u0430', '\u0432\u0448\u0438\u0439\u0441\u044f', '\u2581\u0441', '\u2581\u043f\u043e\u0434\u043e\u0431', '\u043d\u044b\u043c\u0438', '\u2581\u0440\u0430\u0441', '\u0447\u0435', '\u0442\u0430\u043c\u0438', '.', '\u2581--', '\u2581\u0418', '\u2581\u043e\u043d', '\u2581\u0441\u043e\u0441\u0442\u0430\u0432\u0438', '\u0442', '\u2581\u0441\u0435\u0431\u0435', '\u2581\u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435', '.']"}], "[\n[\"--\", null],\n[\"\u2581\u0410\", \"Et\"],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u044d\u0442\u0438\", \"ces\"],\n[\"\u2581\u0434\u0435\u043d\u044c\u0433\u0438\", \"argent\"],\n[\"\u2581\u043e\u043d\", \"il\"],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u043a\u0443\", \"ach\u00e8terait\"],\n[\"\u043f\u0438\u043b\", null],\n[\"\u2581\u0441\u043a\u043e\", \"b\u00e9tail\"],\n[\"\u0442\u0430\", null],\n[\"\u2581\u0438\u043b\u0438\", \"ou\"],\n[\"\u2581\u0437\u0435\u043c\u043b\u0438\", \"terre\"],\n[\"\u0446\u0443\", null],\n[\"\u2581\u043a\u0443\u043f\u0438\", \"ach\u00e8terait\"],\n[\"\u043b\", null],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u0437\u0430\", \"pour\"],\n[\"\u2581\u0431\u0435\u0441\", \"une bouch\u00e9e\"],\n[\"\u0446\u0435\", null],\n[\"\u043d\u043e\u043a\", null],\n[\"\u2581\u0438\", \"et\"],\n[\"\u2581\u043c\u0443\", \"paysans\"],\n[\"\u0436\u0438\", null],\n[\"\u043a\u0430\u043c\", null],\n[\"\u2581\u0440\u043e\", \"distribuerait\"],\n[\"\u0437\u0434\u0430\", null],\n[\"\u043b\", null],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u0432\", null],\n[\"\u043d\u0430\u0439\", \"en location\"],\n[\"\u043c\u044b\", null],\n[\",\", null],\n[\"\u2581--\", null],\n[\"\u2581\u0441\", \"avec\"],\n[\"\u2581\u0443\u043b\u044b\u0431\", \"sourire\"],\n[\"\u043a\u043e\u0439\", null],\n[\"\u2581\u0434\u043e\", \"termina\"],\n[\"\u043a\u043e\u043d\", null],\n[\"\u0447\u0438\u043b\", null],\n[\"\u2581\u041b\u0435\", \"L\u00e9vine\"],\n[\"\u0432\u0438\u043d\", null],\n[\",\", null],\n[\"\u2581\u043e\u0447\u0435\", \"\u00e9videmment\"],\n[\"\u0432\u0438\u0434\u043d\u043e\", null],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0440\u0430\u0437\", \"plusieurs fois\"],\n[\"\u2581\u0443\u0436\u0435\", \"d\u00e9j\u00e0\"],\n[\"\u2581\u0441\u0442\u0430\", \"confront\u00e9\"],\n[\"\u043b\u043a\u0438\", null],\n[\"\u0432\u0430\", null],\n[\"\u0432\u0448\u0438\u0439\u0441\u044f\", null],\n[\"\u2581\u0441\", \"\u00e0\"],\n[\"\u2581\u043f\u043e\u0434\u043e\u0431\", \"de tels\"],\n[\"\u043d\u044b\u043c\u0438\", null],\n[\"\u2581\u0440\u0430\u0441\", \"calculs\"],\n[\"\u0447\u0435\", null],\n[\"\u0442\u0430\u043c\u0438\", null],\n[\".\", null],\n[\"\u2581--\", null],\n[\"\u2581\u0418\", \"Et\"],\n[\"\u2581\u043e\u043d\", \"il\"],\n[\"\u2581\u0441\u043e\u0441\u0442\u0430\u0432\u0438\", \"constituerait\"],\n[\"\u0442\", null],\n[\"\u2581\u0441\u0435\u0431\u0435\", \"pour lui-m\u00eame\"],\n[\"\u2581\u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435\", \"fortune\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nBut this is the way we manage it; we collect a crowd like this one here, then each personin turn passes his head through a hole, and makes a grimace at the rest; time one who makes the ugliest, is elected pope by general acclamation; that's the way it is.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581But', '\u2581this', '\u2581is', '\u2581the', '\u2581way', '\u2581we', '\u2581manage', '\u2581it', ';', '\u2581we', '\u2581collect', '\u2581a', '\u2581crowd', '\u2581like', '\u2581this', '\u2581one', '\u2581here', ',', '\u2581then', '\u2581each', '\u2581person', 'in', '\u2581turn', '\u2581passes', '\u2581his', '\u2581head', '\u2581through', '\u2581a', '\u2581hole', ',', '\u2581and', '\u2581makes', '\u2581a', '\u2581grim', 'ace', '\u2581at', '\u2581the', '\u2581rest', ';', '\u2581time', '\u2581one', '\u2581who', '\u2581makes', '\u2581the', '\u2581u', 'g', 'liest', ',', '\u2581is', '\u2581elected', '\u2581po', 'pe', '\u2581by', '\u2581general', '\u2581ac', 'clam', 'ation', ';', '\u2581that', \"'\", 's', '\u2581the', '\u2581way', '\u2581it', '\u2581is', '.']"}], "[\n[\"\u2581But\", \"Mais\"],\n[\"\u2581this\", \"ceci\"],\n[\"\u2581is\", null],\n[\"\u2581the\", null],\n[\"\u2581way\", \"fa\u00e7on\"],\n[\"\u2581we\", \"nous\"],\n[\"\u2581manage\", \"g\u00e9rons\"],\n[\"\u2581it\", null],\n[\";\", null],\n[\"\u2581we\", \"nous\"],\n[\"\u2581collect\", \"rassemblons\"],\n[\"\u2581a\", null],\n[\"\u2581crowd\", \"foule\"],\n[\"\u2581like\", \"comme\"],\n[\"\u2581this\", \"celle-ci\"],\n[\"\u2581one\", null],\n[\"\u2581here\", \"ici\"],\n[\",\", null],\n[\"\u2581then\", \"puis\"],\n[\"\u2581each\", \"chaque\"],\n[\"\u2581person\", \"personne\"],\n[\"in\", null],\n[\"\u2581turn\", \"tour\"],\n[\"\u2581passes\", \"passe\"],\n[\"\u2581his\", \"sa\"],\n[\"\u2581head\", \"t\u00eate\"],\n[\"\u2581through\", \"\u00e0 travers\"],\n[\"\u2581a\", null],\n[\"\u2581hole\", \"trou\"],\n[\",\", null],\n[\"\u2581and\", \"et\"],\n[\"\u2581makes\", \"fait\"],\n[\"\u2581a\", null],\n[\"\u2581grim\", \"grimace\"],\n[\"ace\", null],\n[\"\u2581at\", \"\u00e0\"],\n[\"\u2581the\", null],\n[\"\u2581rest\", \"reste\"],\n[\";\", null],\n[\"\u2581time\", \"celui\"],\n[\"\u2581one\", null],\n[\"\u2581who\", \"qui\"],\n[\"\u2581makes\", \"fait\"],\n[\"\u2581the\", null],\n[\"\u2581u\", \"plus\"],\n[\"g\", null],\n[\"liest\", null],\n[\",\", null],\n[\"\u2581is\", null],\n[\"\u2581elected\", \"\u00e9lu\"],\n[\"\u2581po\", \"pape\"],\n[\"pe\", null],\n[\"\u2581by\", \"par\"],\n[\"\u2581general\", \"acclamation\"],\n[\"\u2581ac\", null],\n[\"clam\", null],\n[\"ation\", null],\n[\";\", null],\n[\"\u2581that\", \"c'est\"],\n[\"'\", null],\n[\"s\", null],\n[\"\u2581the\", null],\n[\"\u2581way\", \"fa\u00e7on\"],\n[\"\u2581it\", null],\n[\"\u2581is\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nBut this is the way we manage it; we collect a crowd like this one here, then each personin turn passes his head through a hole, and makes a grimace at the rest; time one who makes the ugliest, is elected pope by general acclamation; that's the way it is.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581But', '\u2581this', '\u2581is', '\u2581the', '\u2581way', '\u2581we', '\u2581manage', '\u2581it', ';', '\u2581we', '\u2581collect', '\u2581a', '\u2581crowd', '\u2581like', '\u2581this', '\u2581one', '\u2581here', ',', '\u2581then', '\u2581each', '\u2581person', 'in', '\u2581turn', '\u2581passes', '\u2581his', '\u2581head', '\u2581through', '\u2581a', '\u2581hole', ',', '\u2581and', '\u2581makes', '\u2581a', '\u2581grim', 'ace', '\u2581at', '\u2581the', '\u2581rest', ';', '\u2581time', '\u2581one', '\u2581who', '\u2581makes', '\u2581the', '\u2581u', 'g', 'liest', ',', '\u2581is', '\u2581elected', '\u2581po', 'pe', '\u2581by', '\u2581general', '\u2581ac', 'clam', 'ation', ';', '\u2581that', \"'\", 's', '\u2581the', '\u2581way', '\u2581it', '\u2581is', '.']"}], "[\n[\"\u2581But\", \"\u041d\u043e\"],\n[\"\u2581this\", \"\u044d\u0442\u043e\"],\n[\"\u2581is\", null],\n[\"\u2581the\", null],\n[\"\u2581way\", \"\u0441\u043f\u043e\u0441\u043e\u0431\"],\n[\"\u2581we\", \"\u043c\u044b\"],\n[\"\u2581manage\", \"\u0443\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u043c\"],\n[\"\u2581it\", \"\u044d\u0442\u0438\u043c\"],\n[\";\", null],\n[\"\u2581we\", \"\u043c\u044b\"],\n[\"\u2581collect\", \"\u0441\u043e\u0431\u0438\u0440\u0430\u0435\u043c\"],\n[\"\u2581a\", null],\n[\"\u2581crowd\", \"\u0442\u043e\u043b\u043f\u0443\"],\n[\"\u2581like\", \"\u043a\u0430\u043a\"],\n[\"\u2581this\", \"\u044d\u0442\u0443\"],\n[\"\u2581one\", \"\u043e\u0434\u043d\u0443\"],\n[\"\u2581here\", \"\u0437\u0434\u0435\u0441\u044c\"],\n[\",\", null],\n[\"\u2581then\", \"\u0437\u0430\u0442\u0435\u043c\"],\n[\"\u2581each\", \"\u043a\u0430\u0436\u0434\u044b\u0439\"],\n[\"\u2581person\", \"\u0447\u0435\u043b\u043e\u0432\u0435\u043a\"],\n[\"in\", null],\n[\"\u2581turn\", \"\u043f\u043e \u043e\u0447\u0435\u0440\u0435\u0434\u0438\"],\n[\"\u2581passes\", \"\u043f\u0440\u043e\u0441\u043e\u0432\u044b\u0432\u0430\u0435\u0442\"],\n[\"\u2581his\", \"\u0441\u0432\u043e\u044e\"],\n[\"\u2581head\", \"\u0433\u043e\u043b\u043e\u0432\u0443\"],\n[\"\u2581through\", \"\u0447\u0435\u0440\u0435\u0437\"],\n[\"\u2581a\", null],\n[\"\u2581hole\", \"\u0434\u044b\u0440\u0443\"],\n[\",\", null],\n[\"\u2581and\", \"\u0438\"],\n[\"\u2581makes\", \"\u043a\u043e\u0440\u0447\u0438\u0442\"],\n[\"\u2581a\", null],\n[\"\u2581grim\", \"\u0433\u0440\u0438\u043c\u0430\u0441\u0443\"],\n[\"ace\", null],\n[\"\u2581at\", null],\n[\"\u2581the\", null],\n[\"\u2581rest\", \"\u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u043c\"],\n[\";\", null],\n[\"\u2581time\", \"\u0442\u043e\u0442\"],\n[\"\u2581one\", null],\n[\"\u2581who\", \"\u043a\u0442\u043e\"],\n[\"\u2581makes\", \"\u0441\u0434\u0435\u043b\u0430\u0435\u0442\"],\n[\"\u2581the\", null],\n[\"\u2581u\", \"\u0441\u0430\u043c\u0443\u044e\"],\n[\"g\", null],\n[\"liest\", null],\n[\",\", null],\n[\"\u2581is\", null],\n[\"\u2581elected\", \"\u0438\u0437\u0431\u0438\u0440\u0430\u0435\u0442\u0441\u044f\"],\n[\"\u2581po\", \"\u043f\u0430\u043f\u043e\u0439\"],\n[\"pe\", null],\n[\"\u2581by\", null],\n[\"\u2581general\", \"\u0432\u0441\u0435\u043e\u0431\u0449\u0438\u043c\"],\n[\"\u2581ac\", \"\u043e\u0434\u043e\u0431\u0440\u0435\u043d\u0438\u0435\u043c\"],\n[\"clam\", null],\n[\"ation\", null],\n[\";\", null],\n[\"\u2581that\", \"\u0432\u043e\u0442\"],\n[\"'\", null],\n[\"s\", null],\n[\"\u2581the\", null],\n[\"\u2581way\", \"\u043a\u0430\u043a\"],\n[\"\u2581it\", \"\u044d\u0442\u043e\"],\n[\"\u2581is\", \"\u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nHe too was silent for a while and then again began in a less squeaky voice, coldly emphasizing certain chance words that had no special importance.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581He', '\u2581too', '\u2581was', '\u2581silent', '\u2581for', '\u2581a', '\u2581while', '\u2581and', '\u2581then', '\u2581again', '\u2581began', '\u2581in', '\u2581a', '\u2581less', '\u2581sque', 'aky', '\u2581voice', ',', '\u2581cold', 'ly', '\u2581emphas', 'izing', '\u2581certain', '\u2581chance', '\u2581words', '\u2581that', '\u2581had', '\u2581no', '\u2581special', '\u2581importance', '.']"}], "[\n  [\"\u2581He\", \"Il\"],\n  [\"\u2581too\", \"aussi\"],\n  [\"\u2581was\", null],\n  [\"\u2581silent\", \"silencieux\"],\n  [\"\u2581for\", null],\n  [\"\u2581a\", null],\n  [\"\u2581while\", \"moment\"],\n  [\"\u2581and\", null],\n  [\"\u2581then\", \"puis\"],\n  [\"\u2581again\", \"\u00e0 nouveau\"],\n  [\"\u2581began\", \"commen\u00e7a\"],\n  [\"\u2581in\", null],\n  [\"\u2581a\", null],\n  [\"\u2581less\", \"moins\"],\n  [\"\u2581sque\", \"grin\u00e7ante\"],\n  [\"aky\", null],\n  [\"\u2581voice\", \"voix\"],\n  [\",\", null],\n  [\"\u2581cold\", \"froide\"],\n  [\"ly\", null],\n  [\"\u2581emphas\", \"soulignant\"],\n  [\"izing\", null],\n  [\"\u2581certain\", \"certains\"],\n  [\"\u2581chance\", \"hasard\"],\n  [\"\u2581words\", \"mots\"],\n  [\"\u2581that\", null],\n  [\"\u2581had\", null],\n  [\"\u2581no\", \"aucune\"],\n  [\"\u2581special\", \"particuli\u00e8re\"],\n  [\"\u2581importance\", \"importance\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nHe too was silent for a while and then again began in a less squeaky voice, coldly emphasizing certain chance words that had no special importance.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581He', '\u2581too', '\u2581was', '\u2581silent', '\u2581for', '\u2581a', '\u2581while', '\u2581and', '\u2581then', '\u2581again', '\u2581began', '\u2581in', '\u2581a', '\u2581less', '\u2581sque', 'aky', '\u2581voice', ',', '\u2581cold', 'ly', '\u2581emphas', 'izing', '\u2581certain', '\u2581chance', '\u2581words', '\u2581that', '\u2581had', '\u2581no', '\u2581special', '\u2581importance', '.']"}], "[\n[\"\u2581He\", \"\u041e\u043d\"],\n[\"\u2581too\", \"\u0442\u043e\u0436\u0435\"],\n[\"\u2581was\", null],\n[\"\u2581silent\", \"\u043c\u043e\u043b\u0447\u0430\u043b\"],\n[\"\u2581for\", null],\n[\"\u2581a\", null],\n[\"\u2581while\", \"\u043d\u0435\u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u0432\u0440\u0435\u043c\u044f\"],\n[\"\u2581and\", null],\n[\"\u2581then\", \"\u0437\u0430\u0442\u0435\u043c\"],\n[\"\u2581again\", \"\u0441\u043d\u043e\u0432\u0430\"],\n[\"\u2581began\", \"\u043d\u0430\u0447\u0430\u043b\"],\n[\"\u2581in\", null],\n[\"\u2581a\", null],\n[\"\u2581less\", \"\u043c\u0435\u043d\u0435\u0435\"],\n[\"\u2581sque\", \"\u043f\u0438\u0441\u043a\u043b\u044f\u0432\u044b\u043c\"],\n[\"aky\", null],\n[\"\u2581voice\", \"\u0433\u043e\u043b\u043e\u0441\u043e\u043c\"],\n[\",\", null],\n[\"\u2581cold\", \"\u0445\u043e\u043b\u043e\u0434\u043d\u043e\"],\n[\"ly\", null],\n[\"\u2581emphas\", \"\u043f\u043e\u0434\u0447\u0435\u0440\u043a\u0438\u0432\u0430\u044f\"],\n[\"izing\", null],\n[\"\u2581certain\", \"\u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435\"],\n[\"\u2581chance\", \"\u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0435\"],\n[\"\u2581words\", \"\u0441\u043b\u043e\u0432\u0430\"],\n[\"\u2581that\", null],\n[\"\u2581had\", null],\n[\"\u2581no\", null],\n[\"\u2581special\", \"\u043e\u0441\u043e\u0431\u043e\u0433\u043e\"],\n[\"\u2581importance\", \"\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nHave you no sense to devise a system which will make you independent of all efforts, and all wills, but your own?\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Have', '\u2581you', '\u2581no', '\u2581sense', '\u2581to', '\u2581dev', 'ise', '\u2581a', '\u2581system', '\u2581which', '\u2581will', '\u2581make', '\u2581you', '\u2581independent', '\u2581of', '\u2581all', '\u2581efforts', ',', '\u2581and', '\u2581all', '\u2581will', 's', ',', '\u2581but', '\u2581your', '\u2581own', '?']"}], "[\n[\"\u2581Have\", \"Avez\"],\n[\"\u2581you\", null],\n[\"\u2581no\", null],\n[\"\u2581sense\", \"sens\"],\n[\"\u2581to\", null],\n[\"\u2581dev\", \"concevoir\"],\n[\"ise\", null],\n[\"\u2581a\", null],\n[\"\u2581system\", \"syst\u00e8me\"],\n[\"\u2581which\", null],\n[\"\u2581will\", null],\n[\"\u2581make\", \"rendra\"],\n[\"\u2581you\", null],\n[\"\u2581independent\", \"ind\u00e9pendant\"],\n[\"\u2581of\", null],\n[\"\u2581all\", null],\n[\"\u2581efforts\", \"efforts\"],\n[\",\", null],\n[\"\u2581and\", null],\n[\"\u2581all\", null],\n[\"\u2581will\", \"volont\u00e9s\"],\n[\"s\", null],\n[\",\", null],\n[\"\u2581but\", null],\n[\"\u2581your\", null],\n[\"\u2581own\", \"propre\"],\n[\"?\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nHave you no sense to devise a system which will make you independent of all efforts, and all wills, but your own?\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Have', '\u2581you', '\u2581no', '\u2581sense', '\u2581to', '\u2581dev', 'ise', '\u2581a', '\u2581system', '\u2581which', '\u2581will', '\u2581make', '\u2581you', '\u2581independent', '\u2581of', '\u2581all', '\u2581efforts', ',', '\u2581and', '\u2581all', '\u2581will', 's', ',', '\u2581but', '\u2581your', '\u2581own', '?']"}], "[\n[\"\u2581Have\", \"\u041d\u0435\u0442\"],\n[\"\u2581you\", null],\n[\"\u2581no\", null],\n[\"\u2581sense\", \"\u0441\u043c\u044b\u0441\u043b\u0430\"],\n[\"\u2581to\", null],\n[\"\u2581dev\", \"\u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c\"],\n[\"ise\", null],\n[\"\u2581a\", null],\n[\"\u2581system\", \"\u0441\u0438\u0441\u0442\u0435\u043c\u0443\"],\n[\"\u2581which\", null],\n[\"\u2581will\", null],\n[\"\u2581make\", \"\u0441\u0434\u0435\u043b\u0430\u0435\u0442\"],\n[\"\u2581you\", null],\n[\"\u2581independent\", \"\u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u044b\u043c\"],\n[\"\u2581of\", null],\n[\"\u2581all\", \"\u0432\u0441\u0435\u0445\"],\n[\"\u2581efforts\", \"\u0443\u0441\u0438\u043b\u0438\u0439\"],\n[\",\", null],\n[\"\u2581and\", null],\n[\"\u2581all\", \"\u0432\u0441\u0435\u0445\"],\n[\"\u2581will\", \"\u0432\u043e\u043b\u044c\"],\n[\"s\", null],\n[\",\", null],\n[\"\u2581but\", null],\n[\"\u2581your\", null],\n[\"\u2581own\", \"\u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0439\"],\n[\"?\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nThe manor-house of Ferndean was a building of considerable antiquity, moderate size, and no architectural pretensions, deep buried in a wood.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581man', 'or', '-', 'house', '\u2581of', '\u2581Fer', 'nde', 'an', '\u2581was', '\u2581a', '\u2581building', '\u2581of', '\u2581considerable', '\u2581ant', 'iqu', 'ity', ',', '\u2581moderate', '\u2581size', ',', '\u2581and', '\u2581no', '\u2581architect', 'ural', '\u2581pret', 'ensions', ',', '\u2581deep', '\u2581buried', '\u2581in', '\u2581a', '\u2581wood', '.']"}], "[\n  [\"\u2581The\", \"Le\"],\n  [\"\u2581man\", \"manoir\"],\n  [\"or\", null],\n  [\"-\", null],\n  [\"house\", null],\n  [\"\u2581of\", null],\n  [\"\u2581Fer\", \"Ferndean\"],\n  [\"nde\", null],\n  [\"an\", null],\n  [\"\u2581was\", null],\n  [\"\u2581a\", null],\n  [\"\u2581building\", \"b\u00e2timent\"],\n  [\"\u2581of\", null],\n  [\"\u2581considerable\", \"consid\u00e9rable\"],\n  [\"\u2581ant\", \"antiquit\u00e9\"],\n  [\"iqu\", null],\n  [\"ity\", null],\n  [\",\", null],\n  [\"\u2581moderate\", \"mod\u00e9r\u00e9e\"],\n  [\"\u2581size\", \"taille\"],\n  [\",\", null],\n  [\"\u2581and\", null],\n  [\"\u2581no\", null],\n  [\"\u2581architect\", \"architecturales\"],\n  [\"ural\", null],\n  [\"\u2581pret\", \"pr\u00e9tentions\"],\n  [\"ensions\", null],\n  [\",\", null],\n  [\"\u2581deep\", \"profond\u00e9ment\"],\n  [\"\u2581buried\", \"enfoui\"],\n  [\"\u2581in\", null],\n  [\"\u2581a\", null],\n  [\"\u2581wood\", \"bois\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThe manor-house of Ferndean was a building of considerable antiquity, moderate size, and no architectural pretensions, deep buried in a wood.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581man', 'or', '-', 'house', '\u2581of', '\u2581Fer', 'nde', 'an', '\u2581was', '\u2581a', '\u2581building', '\u2581of', '\u2581considerable', '\u2581ant', 'iqu', 'ity', ',', '\u2581moderate', '\u2581size', ',', '\u2581and', '\u2581no', '\u2581architect', 'ural', '\u2581pret', 'ensions', ',', '\u2581deep', '\u2581buried', '\u2581in', '\u2581a', '\u2581wood', '.']"}], "[\n[\"\u2581The\", \"\u0423\u0441\u0430\u0434\u0435\u0431\u043d\u044b\u0439\"],\n[\"\u2581man\", \"\u0434\u043e\u043c\"],\n[\"or\", null],\n[\"-\", null],\n[\"house\", null],\n[\"\u2581of\", null],\n[\"\u2581Fer\", \"\u0424\u0435\u0440\u043d\u0434\u0438\u043d\"],\n[\"nde\", null],\n[\"an\", null],\n[\"\u2581was\", null],\n[\"\u2581a\", null],\n[\"\u2581building\", \"\u0437\u0434\u0430\u043d\u0438\u0435\"],\n[\"\u2581of\", null],\n[\"\u2581considerable\", \"\u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439\"],\n[\"\u2581ant\", \"\u0434\u0440\u0435\u0432\u043d\u043e\u0441\u0442\u0438\"],\n[\"iqu\", null],\n[\"ity\", null],\n[\",\", null],\n[\"\u2581moderate\", \"\u0443\u043c\u0435\u0440\u0435\u043d\u043d\u044b\u0445\"],\n[\"\u2581size\", \"\u0440\u0430\u0437\u043c\u0435\u0440\u043e\u0432\"],\n[\",\", null],\n[\"\u2581and\", null],\n[\"\u2581no\", \"\u0431\u0435\u0437\"],\n[\"\u2581architect\", \"\u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u043d\u044b\u0445\"],\n[\"ural\", null],\n[\"\u2581pret\", \"\u043f\u0440\u0435\u0442\u0435\u043d\u0437\u0438\u0439\"],\n[\"ensions\", null],\n[\",\", null],\n[\"\u2581deep\", \"\u0433\u043b\u0443\u0431\u043e\u043a\u043e\"],\n[\"\u2581buried\", \"\u0441\u043a\u0440\u044b\u0442\u043e\u0435\"],\n[\"\u2581in\", null],\n[\"\u2581a\", null],\n[\"\u2581wood\", \"\u043b\u0435\u0441\u0443\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nThe fear of failure in these points harassed me worse than the physical hardships of my lot; though these were no trifles.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581fear', '\u2581of', '\u2581failure', '\u2581in', '\u2581these', '\u2581points', '\u2581harass', 'ed', '\u2581me', '\u2581worse', '\u2581than', '\u2581the', '\u2581physical', '\u2581hard', 'ships', '\u2581of', '\u2581my', '\u2581lot', ';', '\u2581though', '\u2581these', '\u2581were', '\u2581no', '\u2581tr', 'if', 'les', '.']"}], "[\n  [\"\u2581The\", null],\n  [\"\u2581fear\", \"peur\"],\n  [\"\u2581of\", null],\n  [\"\u2581failure\", \"\u00e9chec\"],\n  [\"\u2581in\", null],\n  [\"\u2581these\", null],\n  [\"\u2581points\", \"points\"],\n  [\"\u2581harass\", \"harcelait\"],\n  [\"ed\", null],\n  [\"\u2581me\", null],\n  [\"\u2581worse\", \"davantage\"],\n  [\"\u2581than\", null],\n  [\"\u2581the\", null],\n  [\"\u2581physical\", \"physiques\"],\n  [\"\u2581hard\", \"difficult\u00e9s\"],\n  [\"ships\", null],\n  [\"\u2581of\", null],\n  [\"\u2581my\", null],\n  [\"\u2581lot\", \"sort\"],\n  [\";\", null],\n  [\"\u2581though\", \"bien que\"],\n  [\"\u2581these\", null],\n  [\"\u2581were\", null],\n  [\"\u2581no\", null],\n  [\"\u2581tr\", \"bagatelles\"],\n  [\"if\", null],\n  [\"les\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThe fear of failure in these points harassed me worse than the physical hardships of my lot; though these were no trifles.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581fear', '\u2581of', '\u2581failure', '\u2581in', '\u2581these', '\u2581points', '\u2581harass', 'ed', '\u2581me', '\u2581worse', '\u2581than', '\u2581the', '\u2581physical', '\u2581hard', 'ships', '\u2581of', '\u2581my', '\u2581lot', ';', '\u2581though', '\u2581these', '\u2581were', '\u2581no', '\u2581tr', 'if', 'les', '.']"}], "[\n[\"\u2581The\", null],\n[\"\u2581fear\", \"\u0421\u0442\u0440\u0430\u0445\"],\n[\"\u2581of\", null],\n[\"\u2581failure\", \"\u043d\u0435\u0443\u0434\u0430\u0447\u0438\"],\n[\"\u2581in\", null],\n[\"\u2581these\", null],\n[\"\u2581points\", \"\u043c\u043e\u043c\u0435\u043d\u0442\u0430\u0445\"],\n[\"\u2581harass\", \"\u043f\u0440\u0435\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043b\"],\n[\"ed\", null],\n[\"\u2581me\", \"\u043c\u0435\u043d\u044f\"],\n[\"\u2581worse\", \"\u0441\u0438\u043b\u044c\u043d\u0435\u0435\"],\n[\"\u2581than\", null],\n[\"\u2581the\", null],\n[\"\u2581physical\", \"\u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0438\u0445\"],\n[\"\u2581hard\", \"\u0442\u0440\u0443\u0434\u043d\u043e\u0441\u0442\u0435\u0439\"],\n[\"ships\", null],\n[\"\u2581of\", null],\n[\"\u2581my\", \"\u043c\u043e\u0435\u0439\"],\n[\"\u2581lot\", \"\u0443\u0447\u0430\u0441\u0442\u0438\"],\n[\";\", null],\n[\"\u2581though\", \"\u0445\u043e\u0442\u044f\"],\n[\"\u2581these\", null],\n[\"\u2581were\", null],\n[\"\u2581no\", null],\n[\"\u2581tr\", \"\u043f\u0443\u0441\u0442\u044f\u043a\u0430\u043c\u0438\"],\n[\"if\", null],\n[\"les\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nThe archdeacon maintained silence for several minutes, during which Gringoire had time to observe him.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581arch', 'de', 'acon', '\u2581maintained', '\u2581silence', '\u2581for', '\u2581several', '\u2581minutes', ',', '\u2581during', '\u2581which', '\u2581G', 'ring', 'oire', '\u2581had', '\u2581time', '\u2581to', '\u2581observe', '\u2581him', '.']"}], "[\n  [\"\u2581The\", null],\n  [\"\u2581arch\", \"archidiacre\"],\n  [\"de\", null],\n  [\"acon\", null],\n  [\"\u2581maintained\", \"garda\"],\n  [\"\u2581silence\", \"silence\"],\n  [\"\u2581for\", null],\n  [\"\u2581several\", \"plusieurs\"],\n  [\"\u2581minutes\", \"minutes\"],\n  [\",\", null],\n  [\"\u2581during\", \"pendant\"],\n  [\"\u2581which\", null],\n  [\"\u2581G\", \"Gringoire\"],\n  [\"ring\", null],\n  [\"oire\", null],\n  [\"\u2581had\", \"eut\"],\n  [\"\u2581time\", \"temps\"],\n  [\"\u2581to\", null],\n  [\"\u2581observe\", \"observer\"],\n  [\"\u2581him\", \"le\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThe archdeacon maintained silence for several minutes, during which Gringoire had time to observe him.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581arch', 'de', 'acon', '\u2581maintained', '\u2581silence', '\u2581for', '\u2581several', '\u2581minutes', ',', '\u2581during', '\u2581which', '\u2581G', 'ring', 'oire', '\u2581had', '\u2581time', '\u2581to', '\u2581observe', '\u2581him', '.']"}], "[\n  [\"\u2581The\", null],\n  [\"\u2581arch\", \"\u0410\u0440\u0445\u0438\u0434\u044c\u044f\u043a\u043e\u043d\"],\n  [\"de\", null],\n  [\"acon\", null],\n  [\"\u2581maintained\", \"\u0441\u043e\u0445\u0440\u0430\u043d\u044f\u043b\"],\n  [\"\u2581silence\", \"\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u0435\"],\n  [\"\u2581for\", null],\n  [\"\u2581several\", \"\u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e\"],\n  [\"\u2581minutes\", \"\u043c\u0438\u043d\u0443\u0442\"],\n  [\",\", null],\n  [\"\u2581during\", \"\u0432 \u0442\u0435\u0447\u0435\u043d\u0438\u0435\"],\n  [\"\u2581which\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u0445\"],\n  [\"\u2581G\", \"\u0413\u0440\u0438\u043d\u0433\u0443\u0430\u0440\"],\n  [\"ring\", null],\n  [\"oire\", null],\n  [\"\u2581had\", \"\u0438\u043c\u0435\u043b\"],\n  [\"\u2581time\", \"\u0432\u0440\u0435\u043c\u044f\"],\n  [\"\u2581to\", null],\n  [\"\u2581observe\", \"\u043d\u0430\u0431\u043b\u044e\u0434\u0430\u0442\u044c\"],\n  [\"\u2581him\", \"\u0437\u0430 \u043d\u0438\u043c\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nEach of those faces that with so much searching, so many faults and corrections, he had evolved with its own character, each representing so much pain and pleasure, and all of them so often placed and replaced to obtain harmony; all the shades of colour and tone elaborated with such effort all this, regarded as a whole from those others' point of view, now seemed trivialities a thousand times repeated.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Each', '\u2581of', '\u2581those', '\u2581faces', '\u2581that', '\u2581with', '\u2581so', '\u2581much', '\u2581searching', ',', '\u2581so', '\u2581many', '\u2581fault', 's', '\u2581and', '\u2581corrections', ',', '\u2581he', '\u2581had', '\u2581evolved', '\u2581with', '\u2581its', '\u2581own', '\u2581character', ',', '\u2581each', '\u2581representing', '\u2581so', '\u2581much', '\u2581pain', '\u2581and', '\u2581pleasure', ',', '\u2581and', '\u2581all', '\u2581of', '\u2581them', '\u2581so', '\u2581often', '\u2581placed', '\u2581and', '\u2581replaced', '\u2581to', '\u2581obtain', '\u2581harmony', ';', '\u2581all', '\u2581the', '\u2581sh', 'ades', '\u2581of', '\u2581colour', '\u2581and', '\u2581tone', '\u2581elabor', 'ated', '\u2581with', '\u2581such', '\u2581effort', '\u2581all', '\u2581this', ',', '\u2581regarded', '\u2581as', '\u2581a', '\u2581whole', '\u2581from', '\u2581those', '\u2581others', \"'\", '\u2581point', '\u2581of', '\u2581view', ',', '\u2581now', '\u2581seemed', '\u2581trivial', 'ities', '\u2581a', '\u2581thousand', '\u2581times', '\u2581repeated', '.']"}], "[\n[\"\u2581Each\", \"Chacun\"],\n[\"\u2581of\", null],\n[\"\u2581those\", \"ces\"],\n[\"\u2581faces\", \"visages\"],\n[\"\u2581that\", null],\n[\"\u2581with\", null],\n[\"\u2581so\", null],\n[\"\u2581much\", null],\n[\"\u2581searching\", \"recherche\"],\n[\",\", null],\n[\"\u2581so\", null],\n[\"\u2581many\", null],\n[\"\u2581fault\", \"d\u00e9fauts\"],\n[\"s\", null],\n[\"\u2581and\", null],\n[\"\u2581corrections\", \"corrections\"],\n[\",\", null],\n[\"\u2581he\", \"il\"],\n[\"\u2581had\", null],\n[\"\u2581evolved\", \"avait \u00e9volu\u00e9\"],\n[\"\u2581with\", null],\n[\"\u2581its\", null],\n[\"\u2581own\", null],\n[\"\u2581character\", \"caract\u00e8re\"],\n[\",\", null],\n[\"\u2581each\", \"chacun\"],\n[\"\u2581representing\", \"repr\u00e9sentant\"],\n[\"\u2581so\", null],\n[\"\u2581much\", null],\n[\"\u2581pain\", \"douleur\"],\n[\"\u2581and\", null],\n[\"\u2581pleasure\", \"plaisir\"],\n[\",\", null],\n[\"\u2581and\", null],\n[\"\u2581all\", \"tous\"],\n[\"\u2581of\", null],\n[\"\u2581them\", null],\n[\"\u2581so\", null],\n[\"\u2581often\", \"souvent\"],\n[\"\u2581placed\", \"plac\u00e9s\"],\n[\"\u2581and\", null],\n[\"\u2581replaced\", \"replac\u00e9s\"],\n[\"\u2581to\", null],\n[\"\u2581obtain\", \"obtenir\"],\n[\"\u2581harmony\", \"harmonie\"],\n[\";\", null],\n[\"\u2581all\", \"tous\"],\n[\"\u2581the\", null],\n[\"\u2581sh\", \"nuances\"],\n[\"ades\", null],\n[\"\u2581of\", null],\n[\"\u2581colour\", \"couleur\"],\n[\"\u2581and\", null],\n[\"\u2581tone\", \"ton\"],\n[\"\u2581elabor\", \"\u00e9labor\u00e9s\"],\n[\"ated\", null],\n[\"\u2581with\", null],\n[\"\u2581such\", null],\n[\"\u2581effort\", \"effort\"],\n[\"\u2581all\", \"tout\"],\n[\"\u2581this\", \"cela\"],\n[\",\", null],\n[\"\u2581regarded\", \"consid\u00e9r\u00e9\"],\n[\"\u2581as\", null],\n[\"\u2581a\", null],\n[\"\u2581whole\", \"ensemble\"],\n[\"\u2581from\", null],\n[\"\u2581those\", \"ces\"],\n[\"\u2581others\", \"autres\"],\n[\"'\", null],\n[\"\u2581point\", \"point\"],\n[\"\u2581of\", null],\n[\"\u2581view\", \"vue\"],\n[\",\", null],\n[\"\u2581now\", \"maintenant\"],\n[\"\u2581seemed\", \"semblait\"],\n[\"\u2581trivial\", \"banalit\u00e9s\"],\n[\"ities\", null],\n[\"\u2581a\", null],\n[\"\u2581thousand\", \"mille\"],\n[\"\u2581times\", \"fois\"],\n[\"\u2581repeated\", \"r\u00e9p\u00e9t\u00e9es\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nEach of those faces that with so much searching, so many faults and corrections, he had evolved with its own character, each representing so much pain and pleasure, and all of them so often placed and replaced to obtain harmony; all the shades of colour and tone elaborated with such effort all this, regarded as a whole from those others' point of view, now seemed trivialities a thousand times repeated.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Each', '\u2581of', '\u2581those', '\u2581faces', '\u2581that', '\u2581with', '\u2581so', '\u2581much', '\u2581searching', ',', '\u2581so', '\u2581many', '\u2581fault', 's', '\u2581and', '\u2581corrections', ',', '\u2581he', '\u2581had', '\u2581evolved', '\u2581with', '\u2581its', '\u2581own', '\u2581character', ',', '\u2581each', '\u2581representing', '\u2581so', '\u2581much', '\u2581pain', '\u2581and', '\u2581pleasure', ',', '\u2581and', '\u2581all', '\u2581of', '\u2581them', '\u2581so', '\u2581often', '\u2581placed', '\u2581and', '\u2581replaced', '\u2581to', '\u2581obtain', '\u2581harmony', ';', '\u2581all', '\u2581the', '\u2581sh', 'ades', '\u2581of', '\u2581colour', '\u2581and', '\u2581tone', '\u2581elabor', 'ated', '\u2581with', '\u2581such', '\u2581effort', '\u2581all', '\u2581this', ',', '\u2581regarded', '\u2581as', '\u2581a', '\u2581whole', '\u2581from', '\u2581those', '\u2581others', \"'\", '\u2581point', '\u2581of', '\u2581view', ',', '\u2581now', '\u2581seemed', '\u2581trivial', 'ities', '\u2581a', '\u2581thousand', '\u2581times', '\u2581repeated', '.']"}], "[\n[\"\u2581Each\", \"\u041a\u0430\u0436\u0434\u043e\u0435\"],\n[\"\u2581of\", null],\n[\"\u2581those\", \"\u0442\u0435\u0445\"],\n[\"\u2581faces\", \"\u043b\u0438\u0446\"],\n[\"\u2581that\", null],\n[\"\u2581with\", null],\n[\"\u2581so\", null],\n[\"\u2581much\", null],\n[\"\u2581searching\", \"\u043f\u043e\u0438\u0441\u043a\u043e\u0432\"],\n[\",\", null],\n[\"\u2581so\", null],\n[\"\u2581many\", \"\u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u043e\u043c\"],\n[\"\u2581fault\", \"\u043d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043a\u043e\u0432\"],\n[\"s\", null],\n[\"\u2581and\", null],\n[\"\u2581corrections\", \"\u0438\u0441\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0439\"],\n[\",\", null],\n[\"\u2581he\", \"\u043e\u043d\"],\n[\"\u2581had\", null],\n[\"\u2581evolved\", \"\u0440\u0430\u0437\u0432\u0438\u043b\"],\n[\"\u2581with\", null],\n[\"\u2581its\", null],\n[\"\u2581own\", \"\u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u043c\"],\n[\"\u2581character\", \"\u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u043e\u043c\"],\n[\",\", null],\n[\"\u2581each\", \"\u043a\u0430\u0436\u0434\u043e\u0435\"],\n[\"\u2581representing\", \"\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0449\u0435\u0435\"],\n[\"\u2581so\", null],\n[\"\u2581much\", null],\n[\"\u2581pain\", \"\u0431\u043e\u043b\u044c\"],\n[\"\u2581and\", null],\n[\"\u2581pleasure\", \"\u0443\u0434\u043e\u0432\u043e\u043b\u044c\u0441\u0442\u0432\u0438\u0435\"],\n[\",\", null],\n[\"\u2581and\", null],\n[\"\u2581all\", \"\u0432\u0441\u0435\"],\n[\"\u2581of\", null],\n[\"\u2581them\", \"\u043e\u043d\u0438\"],\n[\"\u2581so\", null],\n[\"\u2581often\", \"\u0447\u0430\u0441\u0442\u043e\"],\n[\"\u2581placed\", \"\u0440\u0430\u0437\u043c\u0435\u0449\u0435\u043d\u043d\u044b\u0435\"],\n[\"\u2581and\", null],\n[\"\u2581replaced\", \"\u043f\u0435\u0440\u0435\u043c\u0435\u0449\u0435\u043d\u043d\u044b\u0435\"],\n[\"\u2581to\", null],\n[\"\u2581obtain\", \"\u0434\u043e\u0441\u0442\u0438\u0447\u044c\"],\n[\"\u2581harmony\", \"\u0433\u0430\u0440\u043c\u043e\u043d\u0438\u0438\"],\n[\";\", null],\n[\"\u2581all\", \"\u0432\u0441\u0435\"],\n[\"\u2581the\", null],\n[\"\u2581sh\", \"\u043e\u0442\u0442\u0435\u043d\u043a\u0438\"],\n[\"ades\", null],\n[\"\u2581of\", null],\n[\"\u2581colour\", \"\u0446\u0432\u0435\u0442\u0430\"],\n[\"\u2581and\", null],\n[\"\u2581tone\", \"\u0442\u043e\u043d\u0430\"],\n[\"\u2581elabor\", \"\u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u044b\u0435\"],\n[\"ated\", null],\n[\"\u2581with\", null],\n[\"\u2581such\", \"\u0442\u0430\u043a\u0438\u043c\"],\n[\"\u2581effort\", \"\u0443\u0441\u0438\u043b\u0438\u0435\u043c\"],\n[\"\u2581all\", \"\u0432\u0441\u0435\"],\n[\"\u2581this\", \"\u044d\u0442\u043e\"],\n[\",\", null],\n[\"\u2581regarded\", \"\u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0435\u043c\u043e\u0435\"],\n[\"\u2581as\", null],\n[\"\u2581a\", null],\n[\"\u2581whole\", \"\u0446\u0435\u043b\u043e\u0435\"],\n[\"\u2581from\", null],\n[\"\u2581those\", \"\u0442\u0435\u0445\"],\n[\"\u2581others\", \"\u0434\u0440\u0443\u0433\u0438\u0445\"],\n[\"'\", null],\n[\"\u2581point\", \"\u0442\u043e\u0447\u043a\u0438\"],\n[\"\u2581of\", null],\n[\"\u2581view\", \"\u0437\u0440\u0435\u043d\u0438\u044f\"],\n[\",\", null],\n[\"\u2581now\", \"\u0442\u0435\u043f\u0435\u0440\u044c\"],\n[\"\u2581seemed\", \"\u043a\u0430\u0437\u0430\u043b\u043e\u0441\u044c\"],\n[\"\u2581trivial\", \"\u043c\u0435\u043b\u043e\u0447\u0430\u043c\u0438\"],\n[\"ities\", null],\n[\"\u2581a\", null],\n[\"\u2581thousand\", \"\u0442\u044b\u0441\u044f\u0447\u0443\"],\n[\"\u2581times\", \"\u0440\u0430\u0437\"],\n[\"\u2581repeated\", \"\u043f\u043e\u0432\u0442\u043e\u0440\u0435\u043d\u043d\u044b\u043c\u0438\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nFor instance, it ought to consider the education of women injurious, yet it established courses of lectures and universities for women.'\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581For', '\u2581instance', ',', '\u2581it', '\u2581ought', '\u2581to', '\u2581consider', '\u2581the', '\u2581education', '\u2581of', '\u2581women', '\u2581inj', 'urious', ',', '\u2581yet', '\u2581it', '\u2581established', '\u2581courses', '\u2581of', '\u2581lect', 'ures', '\u2581and', '\u2581universities', '\u2581for', '\u2581women', \".'\"]"}], "[\n    [\"\u2581For\", \"Par\"],\n    [\"\u2581instance\", \"exemple\"],\n    [\",\", null],\n    [\"\u2581it\", null],\n    [\"\u2581ought\", \"devrait\"],\n    [\"\u2581to\", null],\n    [\"\u2581consider\", \"consid\u00e9rer\"],\n    [\"\u2581the\", null],\n    [\"\u2581education\", \"\u00e9ducation\"],\n    [\"\u2581of\", null],\n    [\"\u2581women\", \"femmes\"],\n    [\"\u2581inj\", \"nuisible\"],\n    [\"urious\", null],\n    [\",\", null],\n    [\"\u2581yet\", \"pourtant\"],\n    [\"\u2581it\", null],\n    [\"\u2581established\", \"\u00e9tabli\"],\n    [\"\u2581courses\", \"cours\"],\n    [\"\u2581of\", null],\n    [\"\u2581lect\", \"conf\u00e9rences\"],\n    [\"ures\", null],\n    [\"\u2581and\", \"et\"],\n    [\"\u2581universities\", \"universit\u00e9s\"],\n    [\"\u2581for\", \"pour\"],\n    [\"\u2581women\", \"femmes\"],\n    [\".'\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nFor instance, it ought to consider the education of women injurious, yet it established courses of lectures and universities for women.'\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581For', '\u2581instance', ',', '\u2581it', '\u2581ought', '\u2581to', '\u2581consider', '\u2581the', '\u2581education', '\u2581of', '\u2581women', '\u2581inj', 'urious', ',', '\u2581yet', '\u2581it', '\u2581established', '\u2581courses', '\u2581of', '\u2581lect', 'ures', '\u2581and', '\u2581universities', '\u2581for', '\u2581women', \".'\"]"}], "[\n  [\"\u2581For\", \"\u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440\"],\n  [\"\u2581instance\", null],\n  [\",\", null],\n  [\"\u2581it\", null],\n  [\"\u2581ought\", \"\u0434\u043e\u043b\u0436\u043d\u043e\"],\n  [\"\u2581to\", null],\n  [\"\u2581consider\", \"\u0441\u0447\u0438\u0442\u0430\u0442\u044c\"],\n  [\"\u2581the\", null],\n  [\"\u2581education\", \"\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435\"],\n  [\"\u2581of\", null],\n  [\"\u2581women\", \"\u0436\u0435\u043d\u0449\u0438\u043d\"],\n  [\"\u2581inj\", \"\u0432\u0440\u0435\u0434\u043d\u044b\u043c\"],\n  [\"urious\", null],\n  [\",\", null],\n  [\"\u2581yet\", \"\u043e\u0434\u043d\u0430\u043a\u043e\"],\n  [\"\u2581it\", null],\n  [\"\u2581established\", \"\u0443\u0447\u0440\u0435\u0434\u0438\u043b\u043e\"],\n  [\"\u2581courses\", \"\u043a\u0443\u0440\u0441\u044b\"],\n  [\"\u2581of\", null],\n  [\"\u2581lect\", \"\u043b\u0435\u043a\u0446\u0438\u0439\"],\n  [\"ures\", null],\n  [\"\u2581and\", \"\u0438\"],\n  [\"\u2581universities\", \"\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442\u044b\"],\n  [\"\u2581for\", \"\u0434\u043b\u044f\"],\n  [\"\u2581women\", \"\u0436\u0435\u043d\u0449\u0438\u043d\"],\n  [\".'\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\n'Oh, how sweet he is \u2013 don't frighten him!' exclaimed Kitty unexpectedly, looking at a sparrow that had settled on the railing, turned a raspberry stalk over, and was pecking at it.\n\n\n---=== TOKENIZED TEXT ===---\n\n[\"\u2581'\", 'Oh', ',', '\u2581how', '\u2581sweet', '\u2581he', '\u2581is', '\u2581\u2013', '\u2581don', \"'\", 't', '\u2581fright', 'en', '\u2581him', \"!'\", '\u2581excl', 'aimed', '\u2581K', 'itty', '\u2581unexpected', 'ly', ',', '\u2581looking', '\u2581at', '\u2581a', '\u2581sp', 'arrow', '\u2581that', '\u2581had', '\u2581settled', '\u2581on', '\u2581the', '\u2581rail', 'ing', ',', '\u2581turned', '\u2581a', '\u2581r', 'asp', 'berry', '\u2581stalk', '\u2581over', ',', '\u2581and', '\u2581was', '\u2581pe', 'ck', 'ing', '\u2581at', '\u2581it', '.']"}], "[\n[\"\u2581'\", null],\n[\"Oh\", \"Oh\"],\n[\",\", null],\n[\"\u2581how\", \"comment\"],\n[\"\u2581sweet\", \"doux\"],\n[\"\u2581he\", \"il\"],\n[\"\u2581is\", \"est\"],\n[\"\u2581\u2013\", null],\n[\"\u2581don\", \"ne\"],\n[\"'\", null],\n[\"t\", null],\n[\"\u2581fright\", \"effrayez\"],\n[\"en\", null],\n[\"\u2581him\", \"le\"],\n[\"!'\", null],\n[\"\u2581excl\", \"s'exclama\"],\n[\"aimed\", null],\n[\"\u2581K\", \"Kitty\"],\n[\"itty\", null],\n[\"\u2581unexpected\", \"inopin\u00e9ment\"],\n[\"ly\", null],\n[\",\", null],\n[\"\u2581looking\", \"regardant\"],\n[\"\u2581at\", null],\n[\"\u2581a\", \"un\"],\n[\"\u2581sp\", \"moineau\"],\n[\"arrow\", null],\n[\"\u2581that\", \"qui\"],\n[\"\u2581had\", \"s'\u00e9tait\"],\n[\"\u2581settled\", \"pos\u00e9\"],\n[\"\u2581on\", \"sur\"],\n[\"\u2581the\", \"la\"],\n[\"\u2581rail\", \"balustrade\"],\n[\"ing\", null],\n[\",\", null],\n[\"\u2581turned\", \"retourn\u00e9\"],\n[\"\u2581a\", \"une\"],\n[\"\u2581r\", \"tige\"],\n[\"asp\", null],\n[\"berry\", \"framboise\"],\n[\"\u2581stalk\", null],\n[\"\u2581over\", null],\n[\",\", null],\n[\"\u2581and\", \"et\"],\n[\"\u2581was\", null],\n[\"\u2581pe\", \"picorait\"],\n[\"ck\", null],\n[\"ing\", null],\n[\"\u2581at\", null],\n[\"\u2581it\", \"la\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\n'Oh, how sweet he is \u2013 don't frighten him!' exclaimed Kitty unexpectedly, looking at a sparrow that had settled on the railing, turned a raspberry stalk over, and was pecking at it.\n\n\n---=== TOKENIZED TEXT ===---\n\n[\"\u2581'\", 'Oh', ',', '\u2581how', '\u2581sweet', '\u2581he', '\u2581is', '\u2581\u2013', '\u2581don', \"'\", 't', '\u2581fright', 'en', '\u2581him', \"!'\", '\u2581excl', 'aimed', '\u2581K', 'itty', '\u2581unexpected', 'ly', ',', '\u2581looking', '\u2581at', '\u2581a', '\u2581sp', 'arrow', '\u2581that', '\u2581had', '\u2581settled', '\u2581on', '\u2581the', '\u2581rail', 'ing', ',', '\u2581turned', '\u2581a', '\u2581r', 'asp', 'berry', '\u2581stalk', '\u2581over', ',', '\u2581and', '\u2581was', '\u2581pe', 'ck', 'ing', '\u2581at', '\u2581it', '.']"}], "[\n[\"\u2581'\", null],\n[\"Oh\", \"\u041e\u0439\"],\n[\",\", null],\n[\"\u2581how\", null],\n[\"\u2581sweet\", \"\u043c\u0438\u043b\u044b\u0439\"],\n[\"\u2581he\", \"\u043e\u043d\"],\n[\"\u2581is\", null],\n[\"\u2581\u2013\", null],\n[\"\u2581don\", \"\u043d\u0435\"],\n[\"'\", null],\n[\"t\", null],\n[\"\u2581fright\", \"\u043f\u0443\u0433\u0430\u0439\"],\n[\"en\", null],\n[\"\u2581him\", \"\u0435\u0433\u043e\"],\n[\"!'\", null],\n[\"\u2581excl\", \"\u0432\u043e\u0441\u043a\u043b\u0438\u043a\u043d\u0443\u043b\u0430\"],\n[\"aimed\", null],\n[\"\u2581K\", \"\u041a\u0438\u0442\u0442\u0438\"],\n[\"itty\", null],\n[\"\u2581unexpected\", \"\u043d\u0435\u043e\u0436\u0438\u0434\u0430\u043d\u043d\u043e\"],\n[\"ly\", null],\n[\",\", null],\n[\"\u2581looking\", \"\u0433\u043b\u044f\u0434\u044f\"],\n[\"\u2581at\", \"\u043d\u0430\"],\n[\"\u2581a\", null],\n[\"\u2581sp\", \"\u0432\u043e\u0440\u043e\u0431\u044c\u044f\"],\n[\"arrow\", null],\n[\"\u2581that\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u0439\"],\n[\"\u2581had\", null],\n[\"\u2581settled\", \"\u0441\u0435\u043b\"],\n[\"\u2581on\", \"\u043d\u0430\"],\n[\"\u2581the\", null],\n[\"\u2581rail\", \"\u043f\u0435\u0440\u0438\u043b\u0430\"],\n[\"ing\", null],\n[\",\", null],\n[\"\u2581turned\", \"\u043f\u043e\u0432\u0435\u0440\u043d\u0443\u043b\"],\n[\"\u2581a\", null],\n[\"\u2581r\", \"\u043c\u0430\u043b\u0438\u043d\u043e\u0432\u044b\u0439\"],\n[\"asp\", null],\n[\"berry\", null],\n[\"\u2581stalk\", \"\u0441\u0442\u0435\u0431\u0435\u043b\u0451\u043a\"],\n[\"\u2581over\", null],\n[\",\", null],\n[\"\u2581and\", \"\u0438\"],\n[\"\u2581was\", null],\n[\"\u2581pe\", \"\u043a\u043b\u0435\u0432\u0430\u043b\"],\n[\"ck\", null],\n[\"ing\", null],\n[\"\u2581at\", null],\n[\"\u2581it\", \"\u0435\u0433\u043e\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\n'There is one thing I want to say...' the Princess began, and from her serious look Kitty guessed what was coming.\n\n\n---=== TOKENIZED TEXT ===---\n\n[\"\u2581'\", 'There', '\u2581is', '\u2581one', '\u2581thing', '\u2581I', '\u2581want', '\u2581to', '\u2581say', \"...'\", '\u2581the', '\u2581Princess', '\u2581began', ',', '\u2581and', '\u2581from', '\u2581her', '\u2581serious', '\u2581look', '\u2581K', 'itty', '\u2581guessed', '\u2581what', '\u2581was', '\u2581coming', '.']"}], "[\n[\"\u2581'\", null],\n[\"There\", \"Il\"],\n[\"\u2581is\", null],\n[\"\u2581one\", \"une\"],\n[\"\u2581thing\", \"chose\"],\n[\"\u2581I\", \"je\"],\n[\"\u2581want\", \"veux\"],\n[\"\u2581to\", null],\n[\"\u2581say\", \"dire\"],\n[\"...'\", null],\n[\"\u2581the\", null],\n[\"\u2581Princess\", \"Princesse\"],\n[\"\u2581began\", \"commen\u00e7a\"],\n[\",\", null],\n[\"\u2581and\", null],\n[\"\u2581from\", null],\n[\"\u2581her\", \"son\"],\n[\"\u2581serious\", \"s\u00e9rieux\"],\n[\"\u2581look\", \"regard\"],\n[\"\u2581K\", \"Kitty\"],\n[\"itty\", null],\n[\"\u2581guessed\", \"devina\"],\n[\"\u2581what\", \"ce\"],\n[\"\u2581was\", null],\n[\"\u2581coming\", \"venait\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\n'There is one thing I want to say...' the Princess began, and from her serious look Kitty guessed what was coming.\n\n\n---=== TOKENIZED TEXT ===---\n\n[\"\u2581'\", 'There', '\u2581is', '\u2581one', '\u2581thing', '\u2581I', '\u2581want', '\u2581to', '\u2581say', \"...'\", '\u2581the', '\u2581Princess', '\u2581began', ',', '\u2581and', '\u2581from', '\u2581her', '\u2581serious', '\u2581look', '\u2581K', 'itty', '\u2581guessed', '\u2581what', '\u2581was', '\u2581coming', '.']"}], "[\n[\"\u2581'\", null],\n[\"There\", \"\u0415\u0441\u0442\u044c\"],\n[\"\u2581is\", null],\n[\"\u2581one\", \"\u043e\u0434\u043d\u0430\"],\n[\"\u2581thing\", \"\u0432\u0435\u0449\u044c\"],\n[\"\u2581I\", \"\u044f\"],\n[\"\u2581want\", \"\u0445\u043e\u0447\u0443\"],\n[\"\u2581to\", null],\n[\"\u2581say\", \"\u0441\u043a\u0430\u0437\u0430\u0442\u044c\"],\n[\"...'\", null],\n[\"\u2581the\", null],\n[\"\u2581Princess\", \"\u041a\u043d\u044f\u0433\u0438\u043d\u044f\"],\n[\"\u2581began\", \"\u043d\u0430\u0447\u0430\u043b\u0430\"],\n[\",\", null],\n[\"\u2581and\", \"\u0438\"],\n[\"\u2581from\", \"\u043f\u043e\"],\n[\"\u2581her\", \"\u0435\u0435\"],\n[\"\u2581serious\", \"\u0441\u0435\u0440\u044c\u0435\u0437\u043d\u043e\u043c\u0443\"],\n[\"\u2581look\", \"\u0432\u0438\u0434\u0443\"],\n[\"\u2581K\", \"\u041a\u0438\u0442\u0442\u0438\"],\n[\"itty\", null],\n[\"\u2581guessed\", \"\u0434\u043e\u0433\u0430\u0434\u0430\u043b\u0430\u0441\u044c\"],\n[\"\u2581what\", \"\u0447\u0442\u043e\"],\n[\"\u2581was\", null],\n[\"\u2581coming\", \"\u043f\u043e\u0441\u043b\u0435\u0434\u0443\u0435\u0442\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nCharles remained alone the whole afternoon; they had taken Berthe to Madame Homais'; Felicite was in the room upstairs with Madame Lefrancois.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Charles', '\u2581remained', '\u2581alone', '\u2581the', '\u2581whole', '\u2581afternoon', ';', '\u2581they', '\u2581had', '\u2581taken', '\u2581Bert', 'he', '\u2581to', '\u2581Mad', 'ame', '\u2581Hom', 'ais', \"';\", '\u2581Fel', 'ic', 'ite', '\u2581was', '\u2581in', '\u2581the', '\u2581room', '\u2581upstairs', '\u2581with', '\u2581Mad', 'ame', '\u2581L', 'ef', 'r', 'anc', 'ois', '.']"}], "[\n[\"\u2581Charles\", \"Charles\"],\n[\"\u2581remained\", \"resta\"],\n[\"\u2581alone\", \"seul\"],\n[\"\u2581the\", null],\n[\"\u2581whole\", \"tout\"],\n[\"\u2581afternoon\", \"apr\u00e8s-midi\"],\n[\";\", null],\n[\"\u2581they\", \"ils\"],\n[\"\u2581had\", null],\n[\"\u2581taken\", \"avaient emmen\u00e9\"],\n[\"\u2581Bert\", \"Berthe\"],\n[\"he\", null],\n[\"\u2581to\", null],\n[\"\u2581Mad\", \"Madame\"],\n[\"ame\", null],\n[\"\u2581Hom\", \"Homais\"],\n[\"ais\", null],\n[\"'\", null],\n[\"\u2581Fel\", \"F\u00e9licit\u00e9\"],\n[\"ic\", null],\n[\"ite\", null],\n[\"\u2581was\", \"\u00e9tait\"],\n[\"\u2581in\", \"dans\"],\n[\"\u2581the\", null],\n[\"\u2581room\", \"chambre\"],\n[\"\u2581upstairs\", \"\u00e0 l'\u00e9tage\"],\n[\"\u2581with\", \"avec\"],\n[\"\u2581Mad\", \"Madame\"],\n[\"ame\", null],\n[\"\u2581L\", \"Lefran\u00e7ois\"],\n[\"ef\", null],\n[\"r\", null],\n[\"anc\", null],\n[\"ois\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nCharles remained alone the whole afternoon; they had taken Berthe to Madame Homais'; Felicite was in the room upstairs with Madame Lefrancois.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Charles', '\u2581remained', '\u2581alone', '\u2581the', '\u2581whole', '\u2581afternoon', ';', '\u2581they', '\u2581had', '\u2581taken', '\u2581Bert', 'he', '\u2581to', '\u2581Mad', 'ame', '\u2581Hom', 'ais', \"';\", '\u2581Fel', 'ic', 'ite', '\u2581was', '\u2581in', '\u2581the', '\u2581room', '\u2581upstairs', '\u2581with', '\u2581Mad', 'ame', '\u2581L', 'ef', 'r', 'anc', 'ois', '.']"}], "[\n[\"\u2581Charles\", \"\u0428\u0430\u0440\u043b\u044c\"],\n[\"\u2581remained\", \"\u043e\u0441\u0442\u0430\u0432\u0430\u043b\u0441\u044f\"],\n[\"\u2581alone\", \"\u043e\u0434\u0438\u043d\"],\n[\"\u2581the\", null],\n[\"\u2581whole\", \"\u0432\u0435\u0441\u044c\"],\n[\"\u2581afternoon\", \"\u0434\u0435\u043d\u044c\"],\n[\";\", null],\n[\"\u2581they\", \"\u043e\u043d\u0438\"],\n[\"\u2581had\", null],\n[\"\u2581taken\", \"\u043e\u0442\u0432\u0435\u043b\u0438\"],\n[\"\u2581Bert\", \"\u0411\u0435\u0440\u0442\u0443\"],\n[\"he\", null],\n[\"\u2581to\", null],\n[\"\u2581Mad\", \"\u043c\u0430\u0434\u0430\u043c\"],\n[\"ame\", null],\n[\"\u2581Hom\", \"\u041e\u043c\"],\n[\"ais\", null],\n[\"';\", null],\n[\"\u2581Fel\", \"\u0424\u0435\u043b\u0438\u0441\u0438\u0442\u0435\"],\n[\"ic\", null],\n[\"ite\", null],\n[\"\u2581was\", null],\n[\"\u2581in\", \"\u0432\"],\n[\"\u2581the\", null],\n[\"\u2581room\", \"\u043a\u043e\u043c\u043d\u0430\u0442\u0435\"],\n[\"\u2581upstairs\", \"\u043d\u0430\u0432\u0435\u0440\u0445\u0443\"],\n[\"\u2581with\", \"\u0441\"],\n[\"\u2581Mad\", \"\u043c\u0430\u0434\u0430\u043c\"],\n[\"ame\", null],\n[\"\u2581L\", \"\u041b\u0435\u0444\u0440\u0430\u043d\u0441\u0443\u0430\"],\n[\"ef\", null],\n[\"r\", null],\n[\"anc\", null],\n[\"ois\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nA splendid Midsummer shone over England: skies so pure, suns so radiant as were then seen in long succession, seldom favour even singly, our wave- girt land.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581A', '\u2581spl', 'end', 'id', '\u2581Mid', 'sum', 'mer', '\u2581sh', 'one', '\u2581over', '\u2581England', ':', '\u2581sk', 'ies', '\u2581so', '\u2581pure', ',', '\u2581sun', 's', '\u2581so', '\u2581rad', 'iant', '\u2581as', '\u2581were', '\u2581then', '\u2581seen', '\u2581in', '\u2581long', '\u2581success', 'ion', ',', '\u2581sel', 'dom', '\u2581favour', '\u2581even', '\u2581sing', 'ly', ',', '\u2581our', '\u2581wave', '-', '\u2581g', 'irt', '\u2581land', '.']"}], "[\n[\"\u2581A\", \"Un\"],\n[\"\u2581spl\", \"splendide\"],\n[\"end\", null],\n[\"id\", null],\n[\"\u2581Mid\", \"Mi-\u00e9t\u00e9\"],\n[\"sum\", null],\n[\"mer\", null],\n[\"\u2581sh\", \"brillait\"],\n[\"one\", null],\n[\"\u2581over\", \"sur\"],\n[\"\u2581England\", \"l'Angleterre\"],\n[\":\", null],\n[\"\u2581sk\", \"ciels\"],\n[\"ies\", null],\n[\"\u2581so\", \"si\"],\n[\"\u2581pure\", \"purs\"],\n[\",\", null],\n[\"\u2581sun\", \"soleils\"],\n[\"s\", null],\n[\"\u2581so\", \"si\"],\n[\"\u2581rad\", \"radieux\"],\n[\"iant\", null],\n[\"\u2581as\", \"comme\"],\n[\"\u2581were\", \"\u00e9taient\"],\n[\"\u2581then\", \"alors\"],\n[\"\u2581seen\", \"vus\"],\n[\"\u2581in\", \"en\"],\n[\"\u2581long\", \"longue\"],\n[\"\u2581success\", \"succession\"],\n[\"ion\", null],\n[\",\", null],\n[\"\u2581sel\", \"rarement\"],\n[\"dom\", null],\n[\"\u2581favour\", \"favorisent\"],\n[\"\u2581even\", \"m\u00eame\"],\n[\"\u2581sing\", \"individuellement\"],\n[\"ly\", null],\n[\",\", null],\n[\"\u2581our\", \"notre\"],\n[\"\u2581wave\", \"entour\u00e9e\"],\n[\"-\", null],\n[\"\u2581g\", \"de\"],\n[\"irt\", null],\n[\"\u2581land\", \"terre\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nA splendid Midsummer shone over England: skies so pure, suns so radiant as were then seen in long succession, seldom favour even singly, our wave- girt land.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581A', '\u2581spl', 'end', 'id', '\u2581Mid', 'sum', 'mer', '\u2581sh', 'one', '\u2581over', '\u2581England', ':', '\u2581sk', 'ies', '\u2581so', '\u2581pure', ',', '\u2581sun', 's', '\u2581so', '\u2581rad', 'iant', '\u2581as', '\u2581were', '\u2581then', '\u2581seen', '\u2581in', '\u2581long', '\u2581success', 'ion', ',', '\u2581sel', 'dom', '\u2581favour', '\u2581even', '\u2581sing', 'ly', ',', '\u2581our', '\u2581wave', '-', '\u2581g', 'irt', '\u2581land', '.']"}], "[\n[\"\u2581A\", null],\n[\"\u2581spl\", \"\u0412\u0435\u043b\u0438\u043a\u043e\u043b\u0435\u043f\u043d\u043e\u0435\"],\n[\"end\", null],\n[\"id\", null],\n[\"\u2581Mid\", \"\u0421\u0435\u0440\u0435\u0434\u0438\u043d\u0430\"],\n[\"sum\", null],\n[\"mer\", null],\n[\"\u2581sh\", \"\u0441\u0438\u044f\u043b\u043e\"],\n[\"one\", null],\n[\"\u2581over\", \"\u043d\u0430\u0434\"],\n[\"\u2581England\", \"\u0410\u043d\u0433\u043b\u0438\u0435\u0439\"],\n[\":\", null],\n[\"\u2581sk\", \"\u043d\u0435\u0431\u0435\u0441\u0430\"],\n[\"ies\", null],\n[\"\u2581so\", \"\u0442\u0430\u043a\u0438\u0435\"],\n[\"\u2581pure\", \"\u0447\u0438\u0441\u0442\u044b\u0435\"],\n[\",\", null],\n[\"\u2581sun\", \"\u0441\u043e\u043b\u043d\u0446\u0430\"],\n[\"s\", null],\n[\"\u2581so\", \"\u0442\u0430\u043a\u0438\u0435\"],\n[\"\u2581rad\", \"\u044f\u0440\u043a\u0438\u0435\"],\n[\"iant\", null],\n[\"\u2581as\", \"\u043a\u0430\u043a\u0438\u0435\"],\n[\"\u2581were\", \"\u0431\u044b\u043b\u0438\"],\n[\"\u2581then\", \"\u0442\u043e\u0433\u0434\u0430\"],\n[\"\u2581seen\", \"\u0432\u0438\u0434\u043d\u044b\"],\n[\"\u2581in\", \"\u0432\"],\n[\"\u2581long\", \"\u0434\u043e\u043b\u0433\u043e\u0439\"],\n[\"\u2581success\", \"\u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438\"],\n[\"ion\", null],\n[\",\", null],\n[\"\u2581sel\", \"\u0440\u0435\u0434\u043a\u043e\"],\n[\"dom\", null],\n[\"\u2581favour\", \"\u0431\u043b\u0430\u0433\u043e\u043f\u0440\u0438\u044f\u0442\u0441\u0442\u0432\u0443\u044e\u0442\"],\n[\"\u2581even\", \"\u0434\u0430\u0436\u0435\"],\n[\"\u2581sing\", \"\u043f\u043e \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438\"],\n[\"ly\", null],\n[\",\", null],\n[\"\u2581our\", \"\u043d\u0430\u0448\u0435\u0439\"],\n[\"\u2581wave\", \"\u043e\u043a\u0440\u0443\u0436\u0435\u043d\u043d\u043e\u0439\"],\n[\"-\", null],\n[\"\u2581g\", \"\u0432\u043e\u043b\u043d\u0430\u043c\u0438\"],\n[\"irt\", null],\n[\"\u2581land\", \"\u0437\u0435\u043c\u043b\u0435\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nAt the end, speaking about the entrance he says, 'Now I'll go and close it', although at the beginning of the story it says the door to the law is open as it always is, but if it's always open - always - that means it's open independently of the lifespan of the man it's intended for, and not even the doorkeeper will be able to close it.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581At', '\u2581the', '\u2581end', ',', '\u2581speaking', '\u2581about', '\u2581the', '\u2581entrance', '\u2581he', '\u2581says', ',', \"\u2581'\", 'Now', '\u2581I', \"'\", 'll', '\u2581go', '\u2581and', '\u2581close', '\u2581it', \"',\", '\u2581although', '\u2581at', '\u2581the', '\u2581beginning', '\u2581of', '\u2581the', '\u2581story', '\u2581it', '\u2581says', '\u2581the', '\u2581door', '\u2581to', '\u2581the', '\u2581law', '\u2581is', '\u2581open', '\u2581as', '\u2581it', '\u2581always', '\u2581is', ',', '\u2581but', '\u2581if', '\u2581it', \"'\", 's', '\u2581always', '\u2581open', '\u2581-', '\u2581always', '\u2581-', '\u2581that', '\u2581means', '\u2581it', \"'\", 's', '\u2581open', '\u2581independently', '\u2581of', '\u2581the', '\u2581lif', 'es', 'pan', '\u2581of', '\u2581the', '\u2581man', '\u2581it', \"'\", 's', '\u2581intended', '\u2581for', ',', '\u2581and', '\u2581not', '\u2581even', '\u2581the', '\u2581door', 'keeper', '\u2581will', '\u2581be', '\u2581able', '\u2581to', '\u2581close', '\u2581it', '.']"}], "[[\"\u2581At\", \"\u00c0\"],\n[\"\u2581the\", null],\n[\"\u2581end\", \"fin\"],\n[\",\", null],\n[\"\u2581speaking\", \"parlant\"],\n[\"\u2581about\", null],\n[\"\u2581the\", null],\n[\"\u2581entrance\", \"entr\u00e9e\"],\n[\"\u2581he\", \"il\"],\n[\"\u2581says\", \"dit\"],\n[\",\", null],\n[\"\u2581'\", null],\n[\"Now\", \"Maintenant\"],\n[\"\u2581I\", \"je\"],\n[\"'\", null],\n[\"ll\", null],\n[\"\u2581go\", \"vais\"],\n[\"\u2581and\", null],\n[\"\u2581close\", \"fermer\"],\n[\"\u2581it\", \"la\"],\n[\"',\", null],\n[\"\u2581although\", \"bien que\"],\n[\"\u2581at\", null],\n[\"\u2581the\", null],\n[\"\u2581beginning\", \"d\u00e9but\"],\n[\"\u2581of\", null],\n[\"\u2581the\", null],\n[\"\u2581story\", \"histoire\"],\n[\"\u2581it\", \"elle\"],\n[\"\u2581says\", \"dit\"],\n[\"\u2581the\", null],\n[\"\u2581door\", \"porte\"],\n[\"\u2581to\", null],\n[\"\u2581the\", null],\n[\"\u2581law\", \"loi\"],\n[\"\u2581is\", \"est\"],\n[\"\u2581open\", \"ouverte\"],\n[\"\u2581as\", \"comme\"],\n[\"\u2581it\", \"elle\"],\n[\"\u2581always\", \"toujours\"],\n[\"\u2581is\", \"est\"],\n[\",\", null],\n[\"\u2581but\", \"mais\"],\n[\"\u2581if\", \"si\"],\n[\"\u2581it\", \"elle\"],\n[\"'\", null],\n[\"s\", null],\n[\"\u2581always\", \"toujours\"],\n[\"\u2581open\", \"ouverte\"],\n[\"\u2581-\", null],\n[\"\u2581always\", \"toujours\"],\n[\"\u2581-\", null],\n[\"\u2581that\", \"cela\"],\n[\"\u2581means\", \"signifie\"],\n[\"\u2581it\", \"elle\"],\n[\"'\", null],\n[\"s\", null],\n[\"\u2581open\", \"ouverte\"],\n[\"\u2581independently\", \"ind\u00e9pendamment\"],\n[\"\u2581of\", null],\n[\"\u2581the\", null],\n[\"\u2581lif\", \"dur\u00e9e\"],\n[\"es\", null],\n[\"pan\", null],\n[\"\u2581of\", null],\n[\"\u2581the\", null],\n[\"\u2581man\", \"homme\"],\n[\"\u2581it\", \"elle\"],\n[\"'\", null],\n[\"s\", null],\n[\"\u2581intended\", \"destin\u00e9e\"],\n[\"\u2581for\", null],\n[\",\", null],\n[\"\u2581and\", \"et\"],\n[\"\u2581not\", \"pas\"],\n[\"\u2581even\", \"m\u00eame\"],\n[\"\u2581the\", null],\n[\"\u2581door\", \"gardien\"],\n[\"keeper\", null],\n[\"\u2581will\", null],\n[\"\u2581be\", \"sera\"],\n[\"\u2581able\", \"capable\"],\n[\"\u2581to\", null],\n[\"\u2581close\", \"fermer\"],\n[\"\u2581it\", \"la\"],\n[\".\", null]]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nAt the end, speaking about the entrance he says, 'Now I'll go and close it', although at the beginning of the story it says the door to the law is open as it always is, but if it's always open - always - that means it's open independently of the lifespan of the man it's intended for, and not even the doorkeeper will be able to close it.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581At', '\u2581the', '\u2581end', ',', '\u2581speaking', '\u2581about', '\u2581the', '\u2581entrance', '\u2581he', '\u2581says', ',', \"\u2581'\", 'Now', '\u2581I', \"'\", 'll', '\u2581go', '\u2581and', '\u2581close', '\u2581it', \"',\", '\u2581although', '\u2581at', '\u2581the', '\u2581beginning', '\u2581of', '\u2581the', '\u2581story', '\u2581it', '\u2581says', '\u2581the', '\u2581door', '\u2581to', '\u2581the', '\u2581law', '\u2581is', '\u2581open', '\u2581as', '\u2581it', '\u2581always', '\u2581is', ',', '\u2581but', '\u2581if', '\u2581it', \"'\", 's', '\u2581always', '\u2581open', '\u2581-', '\u2581always', '\u2581-', '\u2581that', '\u2581means', '\u2581it', \"'\", 's', '\u2581open', '\u2581independently', '\u2581of', '\u2581the', '\u2581lif', 'es', 'pan', '\u2581of', '\u2581the', '\u2581man', '\u2581it', \"'\", 's', '\u2581intended', '\u2581for', ',', '\u2581and', '\u2581not', '\u2581even', '\u2581the', '\u2581door', 'keeper', '\u2581will', '\u2581be', '\u2581able', '\u2581to', '\u2581close', '\u2581it', '.']"}], "[\n  [\"\u2581At\", null],\n  [\"\u2581the\", null],\n  [\"\u2581end\", \"\u043a\u043e\u043d\u0446\u0435\"],\n  [\",\", null],\n  [\"\u2581speaking\", \"\u0433\u043e\u0432\u043e\u0440\u044f\"],\n  [\"\u2581about\", null],\n  [\"\u2581the\", null],\n  [\"\u2581entrance\", \"\u0432\u0445\u043e\u0434\u0435\"],\n  [\"\u2581he\", null],\n  [\"\u2581says\", \"\u0433\u043e\u0432\u043e\u0440\u0438\u0442\"],\n  [\",\", null],\n  [\"\u2581'\", null],\n  [\"Now\", \"\u0422\u0435\u043f\u0435\u0440\u044c\"],\n  [\"\u2581I\", null],\n  [\"'\", null],\n  [\"ll\", null],\n  [\"\u2581go\", \"\u043f\u043e\u0439\u0434\u0443\"],\n  [\"\u2581and\", null],\n  [\"\u2581close\", \"\u0437\u0430\u043a\u0440\u043e\u044e\"],\n  [\"\u2581it\", null],\n  [\"',\", null],\n  [\"\u2581although\", \"\u0445\u043e\u0442\u044f\"],\n  [\"\u2581at\", null],\n  [\"\u2581the\", null],\n  [\"\u2581beginning\", \"\u043d\u0430\u0447\u0430\u043b\u0435\"],\n  [\"\u2581of\", null],\n  [\"\u2581the\", null],\n  [\"\u2581story\", \"\u0438\u0441\u0442\u043e\u0440\u0438\u0438\"],\n  [\"\u2581it\", null],\n  [\"\u2581says\", \"\u0433\u043e\u0432\u043e\u0440\u0438\u0442\u0441\u044f\"],\n  [\"\u2581the\", null],\n  [\"\u2581door\", \"\u0434\u0432\u0435\u0440\u044c\"],\n  [\"\u2581to\", null],\n  [\"\u2581the\", null],\n  [\"\u2581law\", \"\u0437\u0430\u043a\u043e\u043d\u0443\"],\n  [\"\u2581is\", null],\n  [\"\u2581open\", \"\u043e\u0442\u043a\u0440\u044b\u0442\u0430\"],\n  [\"\u2581as\", null],\n  [\"\u2581it\", null],\n  [\"\u2581always\", \"\u0432\u0441\u0435\u0433\u0434\u0430\"],\n  [\"\u2581is\", null],\n  [\",\", null],\n  [\"\u2581but\", \"\u043d\u043e\"],\n  [\"\u2581if\", \"\u0435\u0441\u043b\u0438\"],\n  [\"\u2581it\", null],\n  [\"'\", null],\n  [\"s\", null],\n  [\"\u2581always\", \"\u0432\u0441\u0435\u0433\u0434\u0430\"],\n  [\"\u2581open\", \"\u043e\u0442\u043a\u0440\u044b\u0442\u0430\"],\n  [\"\u2581-\", null],\n  [\"\u2581always\", \"\u0432\u0441\u0435\u0433\u0434\u0430\"],\n  [\"\u2581-\", null],\n  [\"\u2581that\", null],\n  [\"\u2581means\", \"\u0437\u043d\u0430\u0447\u0438\u0442\"],\n  [\"\u2581it\", null],\n  [\"'\", null],\n  [\"s\", null],\n  [\"\u2581open\", \"\u043e\u0442\u043a\u0440\u044b\u0442\u0430\"],\n  [\"\u2581independently\", \"\u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\"],\n  [\"\u2581of\", null],\n  [\"\u2581the\", null],\n  [\"\u2581lif\", \"\u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438\"],\n  [\"es\", null],\n  [\"pan\", null],\n  [\"\u2581of\", null],\n  [\"\u2581the\", null],\n  [\"\u2581man\", \"\u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430\"],\n  [\"\u2581it\", null],\n  [\"'\", null],\n  [\"s\", null],\n  [\"\u2581intended\", \"\u043f\u0440\u0435\u0434\u043d\u0430\u0437\u043d\u0430\u0447\u0435\u043d\u0430\"],\n  [\"\u2581for\", null],\n  [\",\", null],\n  [\"\u2581and\", null],\n  [\"\u2581not\", \"\u043d\u0435\"],\n  [\"\u2581even\", \"\u0434\u0430\u0436\u0435\"],\n  [\"\u2581the\", null],\n  [\"\u2581door\", \"\u043f\u0440\u0438\u0432\u0440\u0430\u0442\u043d\u0438\u043a\"],\n  [\"keeper\", null],\n  [\"\u2581will\", null],\n  [\"\u2581be\", null],\n  [\"\u2581able\", \"\u0441\u043c\u043e\u0436\u0435\u0442\"],\n  [\"\u2581to\", null],\n  [\"\u2581close\", \"\u0437\u0430\u043a\u0440\u044b\u0442\u044c\"],\n  [\"\u2581it\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\n\"You damned ... ,\" in the first gurglings of his excitement his words could hardly be understood, K. was startled even though he had been expecting something of the sort and ran to his uncle with the intention, no doubt, of closing his mouth with both his hands.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\"', 'You', '\u2581damned', '\u2581...', '\u2581', ',\"', '\u2581in', '\u2581the', '\u2581first', '\u2581g', 'ur', 'gl', 'ings', '\u2581of', '\u2581his', '\u2581excitement', '\u2581his', '\u2581words', '\u2581could', '\u2581hardly', '\u2581be', '\u2581understood', ',', '\u2581K', '.', '\u2581was', '\u2581start', 'led', '\u2581even', '\u2581though', '\u2581he', '\u2581had', '\u2581been', '\u2581expecting', '\u2581something', '\u2581of', '\u2581the', '\u2581sort', '\u2581and', '\u2581ran', '\u2581to', '\u2581his', '\u2581uncle', '\u2581with', '\u2581the', '\u2581intention', ',', '\u2581no', '\u2581doubt', ',', '\u2581of', '\u2581closing', '\u2581his', '\u2581mouth', '\u2581with', '\u2581both', '\u2581his', '\u2581hands', '.']"}], "[\n[\"\u2581\\\"\", null],\n[\"You\", \"Vous\"],\n[\"\u2581damned\", \"maudit\"],\n[\"\u2581...\", null],\n[\"\u2581\", null],\n[\",\\\"\", null],\n[\"\u2581in\", null],\n[\"\u2581the\", null],\n[\"\u2581first\", \"premiers\"],\n[\"\u2581g\", \"gargouillis\"],\n[\"ur\", null],\n[\"gl\", null],\n[\"ings\", null],\n[\"\u2581of\", null],\n[\"\u2581his\", null],\n[\"\u2581excitement\", \"excitation\"],\n[\"\u2581his\", null],\n[\"\u2581words\", \"mots\"],\n[\"\u2581could\", null],\n[\"\u2581hardly\", \"difficilement\"],\n[\"\u2581be\", null],\n[\"\u2581understood\", \"compris\"],\n[\",\", null],\n[\"\u2581K\", \"K\"],\n[\".\", null],\n[\"\u2581was\", null],\n[\"\u2581start\", \"surpris\"],\n[\"led\", null],\n[\"\u2581even\", \"m\u00eame\"],\n[\"\u2581though\", \"si\"],\n[\"\u2581he\", null],\n[\"\u2581had\", null],\n[\"\u2581been\", null],\n[\"\u2581expecting\", \"attendait\"],\n[\"\u2581something\", \"quelque chose\"],\n[\"\u2581of\", null],\n[\"\u2581the\", null],\n[\"\u2581sort\", \"genre\"],\n[\"\u2581and\", null],\n[\"\u2581ran\", \"courut\"],\n[\"\u2581to\", null],\n[\"\u2581his\", null],\n[\"\u2581uncle\", \"oncle\"],\n[\"\u2581with\", null],\n[\"\u2581the\", null],\n[\"\u2581intention\", \"intention\"],\n[\",\", null],\n[\"\u2581no\", null],\n[\"\u2581doubt\", \"doute\"],\n[\",\", null],\n[\"\u2581of\", null],\n[\"\u2581closing\", \"fermer\"],\n[\"\u2581his\", null],\n[\"\u2581mouth\", \"bouche\"],\n[\"\u2581with\", null],\n[\"\u2581both\", \"deux\"],\n[\"\u2581his\", null],\n[\"\u2581hands\", \"mains\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\n\"You damned ... ,\" in the first gurglings of his excitement his words could hardly be understood, K. was startled even though he had been expecting something of the sort and ran to his uncle with the intention, no doubt, of closing his mouth with both his hands.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\"', 'You', '\u2581damned', '\u2581...', '\u2581', ',\"', '\u2581in', '\u2581the', '\u2581first', '\u2581g', 'ur', 'gl', 'ings', '\u2581of', '\u2581his', '\u2581excitement', '\u2581his', '\u2581words', '\u2581could', '\u2581hardly', '\u2581be', '\u2581understood', ',', '\u2581K', '.', '\u2581was', '\u2581start', 'led', '\u2581even', '\u2581though', '\u2581he', '\u2581had', '\u2581been', '\u2581expecting', '\u2581something', '\u2581of', '\u2581the', '\u2581sort', '\u2581and', '\u2581ran', '\u2581to', '\u2581his', '\u2581uncle', '\u2581with', '\u2581the', '\u2581intention', ',', '\u2581no', '\u2581doubt', ',', '\u2581of', '\u2581closing', '\u2581his', '\u2581mouth', '\u2581with', '\u2581both', '\u2581his', '\u2581hands', '.']"}], "[\n[\"\u2581\\\"\", null],\n[\"You\", \"\u0422\u044b\"],\n[\"\u2581damned\", \"\u043f\u0440\u043e\u043a\u043b\u044f\u0442\u044b\u0439\"],\n[\"\u2581...\", null],\n[\"\u2581\", null],\n[\",\\\"\", null],\n[\"\u2581in\", null],\n[\"\u2581the\", null],\n[\"\u2581first\", \"\u043f\u0435\u0440\u0432\u044b\u0435\"],\n[\"\u2581g\", \"\u0431\u0443\u043b\u044c\u043a\u0430\u043d\u044c\u044f\"],\n[\"ur\", null],\n[\"gl\", null],\n[\"ings\", null],\n[\"\u2581of\", null],\n[\"\u2581his\", \"\u0435\u0433\u043e\"],\n[\"\u2581excitement\", \"\u0432\u043e\u0437\u0431\u0443\u0436\u0434\u0435\u043d\u0438\u044f\"],\n[\"\u2581his\", \"\u0435\u0433\u043e\"],\n[\"\u2581words\", \"\u0441\u043b\u043e\u0432\u0430\"],\n[\"\u2581could\", \"\u043c\u043e\u0433\u043b\u0438\"],\n[\"\u2581hardly\", \"\u0435\u0434\u0432\u0430\"],\n[\"\u2581be\", null],\n[\"\u2581understood\", \"\u043f\u043e\u043d\u0438\u043c\u0430\u0442\u044c\u0441\u044f\"],\n[\",\", null],\n[\"\u2581K\", \"\u041a\"],\n[\".\", null],\n[\"\u2581was\", null],\n[\"\u2581start\", \"\u0432\u0437\u0434\u0440\u043e\u0433\u043d\u0443\u043b\"],\n[\"led\", null],\n[\"\u2581even\", \"\u0434\u0430\u0436\u0435\"],\n[\"\u2581though\", \"\u0445\u043e\u0442\u044f\"],\n[\"\u2581he\", \"\u043e\u043d\"],\n[\"\u2581had\", null],\n[\"\u2581been\", null],\n[\"\u2581expecting\", \"\u043e\u0436\u0438\u0434\u0430\u043b\"],\n[\"\u2581something\", \"\u0447\u0435\u0433\u043e-\u0442\u043e\"],\n[\"\u2581of\", null],\n[\"\u2581the\", null],\n[\"\u2581sort\", \"\u043f\u043e\u0434\u043e\u0431\u043d\u043e\u0433\u043e\"],\n[\"\u2581and\", \"\u0438\"],\n[\"\u2581ran\", \"\u043f\u043e\u0431\u0435\u0436\u0430\u043b\"],\n[\"\u2581to\", \"\u043a\"],\n[\"\u2581his\", \"\u0441\u0432\u043e\u0435\u043c\u0443\"],\n[\"\u2581uncle\", \"\u0434\u044f\u0434\u0435\"],\n[\"\u2581with\", \"\u0441\"],\n[\"\u2581the\", null],\n[\"\u2581intention\", \"\u043d\u0430\u043c\u0435\u0440\u0435\u043d\u0438\u0435\u043c\"],\n[\",\", null],\n[\"\u2581no\", \"\u043d\u0435\u0441\u043e\u043c\u043d\u0435\u043d\u043d\u043e\"],\n[\"\u2581doubt\", null],\n[\",\", null],\n[\"\u2581of\", null],\n[\"\u2581closing\", \"\u0437\u0430\u043a\u0440\u044b\u0442\u044c\"],\n[\"\u2581his\", \"\u0435\u0433\u043e\"],\n[\"\u2581mouth\", \"\u0440\u043e\u0442\"],\n[\"\u2581with\", null],\n[\"\u2581both\", \"\u043e\u0431\u0435\u0438\u043c\u0438\"],\n[\"\u2581his\", \"\u0441\u0432\u043e\u0438\u043c\u0438\"],\n[\"\u2581hands\", \"\u0440\u0443\u043a\u0430\u043c\u0438\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nHence, the man to whom it had belonged had come thither and had died there.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Hence', ',', '\u2581the', '\u2581man', '\u2581to', '\u2581whom', '\u2581it', '\u2581had', '\u2581belonged', '\u2581had', '\u2581come', '\u2581th', 'ither', '\u2581and', '\u2581had', '\u2581died', '\u2581there', '.']"}], "[\n  [\"\u2581Hence\", \"Donc\"],\n  [\",\", null],\n  [\"\u2581the\", null],\n  [\"\u2581man\", \"homme\"],\n  [\"\u2581to\", null],\n  [\"\u2581whom\", \"qui\"],\n  [\"\u2581it\", null],\n  [\"\u2581had\", null],\n  [\"\u2581belonged\", \"appartenu\"],\n  [\"\u2581had\", null],\n  [\"\u2581come\", \"venu\"],\n  [\"\u2581th\", \"l\u00e0\"],\n  [\"ither\", null],\n  [\"\u2581and\", null],\n  [\"\u2581had\", null],\n  [\"\u2581died\", \"mort\"],\n  [\"\u2581there\", \"l\u00e0\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nHence, the man to whom it had belonged had come thither and had died there.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Hence', ',', '\u2581the', '\u2581man', '\u2581to', '\u2581whom', '\u2581it', '\u2581had', '\u2581belonged', '\u2581had', '\u2581come', '\u2581th', 'ither', '\u2581and', '\u2581had', '\u2581died', '\u2581there', '.']"}], "[\n[\"\u2581Hence\", \"\u0421\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\"],\n[\",\", null],\n[\"\u2581the\", null],\n[\"\u2581man\", \"\u0447\u0435\u043b\u043e\u0432\u0435\u043a\"],\n[\"\u2581to\", null],\n[\"\u2581whom\", \"\u043a\u043e\u0442\u043e\u0440\u043e\u043c\u0443\"],\n[\"\u2581it\", null],\n[\"\u2581had\", null],\n[\"\u2581belonged\", \"\u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u0430\u043b\u043e\"],\n[\"\u2581had\", null],\n[\"\u2581come\", \"\u043f\u0440\u0438\u0448\u0435\u043b\"],\n[\"\u2581th\", \"\u0442\u0443\u0434\u0430\"],\n[\"ither\", null],\n[\"\u2581and\", null],\n[\"\u2581had\", null],\n[\"\u2581died\", \"\u0443\u043c\u0435\u0440\"],\n[\"\u2581there\", \"\u0442\u0430\u043c\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nVoil\u00e0 tout. \u00bb Un soldat montra au roi une serpe qu\u2019on avait saisie sur le truand.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Vo', 'il', '\u00e0', '\u2581tout', '.', '\u2581\u00bb', '\u2581Un', '\u2581sold', 'at', '\u2581mon', 'tra', '\u2581au', '\u2581roi', '\u2581une', '\u2581ser', 'pe', '\u2581qu', '\u2019', 'on', '\u2581avait', '\u2581sa', 'is', 'ie', '\u2581sur', '\u2581le', '\u2581tr', 'u', 'and', '.']"}], "[\n[\"\u2581Vo\", \"That\"],\n[\"il\", null],\n[\"\u00e0\", null],\n[\"\u2581tout\", \"all\"],\n[\".\", null],\n[\"\u2581\u00bb\", null],\n[\"\u2581Un\", \"A\"],\n[\"\u2581sold\", \"soldier\"],\n[\"at\", null],\n[\"\u2581mon\", \"showed\"],\n[\"tra\", null],\n[\"\u2581au\", null],\n[\"\u2581roi\", \"king\"],\n[\"\u2581une\", \"a\"],\n[\"\u2581ser\", \"sickle\"],\n[\"pe\", null],\n[\"\u2581qu\", \"that\"],\n[\"'\", null],\n[\"on\", null],\n[\"\u2581avait\", \"had\"],\n[\"\u2581sa\", \"seized\"],\n[\"is\", null],\n[\"ie\", null],\n[\"\u2581sur\", \"from\"],\n[\"\u2581le\", null],\n[\"\u2581tr\", \"vagabond\"],\n[\"u\", null],\n[\"and\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nVoil\u00e0 tout. \u00bb Un soldat montra au roi une serpe qu\u2019on avait saisie sur le truand.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Vo', 'il', '\u00e0', '\u2581tout', '.', '\u2581\u00bb', '\u2581Un', '\u2581sold', 'at', '\u2581mon', 'tra', '\u2581au', '\u2581roi', '\u2581une', '\u2581ser', 'pe', '\u2581qu', '\u2019', 'on', '\u2581avait', '\u2581sa', 'is', 'ie', '\u2581sur', '\u2581le', '\u2581tr', 'u', 'and', '.']"}], "[\n  [\"\u2581Vo\", \"\u0412\u043e\u0442\"],\n  [\"il\", null],\n  [\"\u00e0\", null],\n  [\"\u2581tout\", \"\u0432\u0441\u0451\"],\n  [\".\", null],\n  [\"\u2581\u00bb\", null],\n  [\"\u2581Un\", \"\u041e\u0434\u0438\u043d\"],\n  [\"\u2581sold\", \"\u0441\u043e\u043b\u0434\u0430\u0442\"],\n  [\"at\", null],\n  [\"\u2581mon\", \"\u043f\u043e\u043a\u0430\u0437\u0430\u043b\"],\n  [\"tra\", null],\n  [\"\u2581au\", null],\n  [\"\u2581roi\", \"\u043a\u043e\u0440\u043e\u043b\u044e\"],\n  [\"\u2581une\", null],\n  [\"\u2581ser\", \"\u0441\u0435\u0440\u043f\"],\n  [\"pe\", null],\n  [\"\u2581qu\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u0439\"],\n  [\"'\", null],\n  [\"on\", null],\n  [\"\u2581avait\", \"\u0431\u044b\u043b\"],\n  [\"\u2581sa\", \"\u0438\u0437\u044a\u044f\u0442\"],\n  [\"is\", null],\n  [\"ie\", null],\n  [\"\u2581sur\", \"\u0443\"],\n  [\"\u2581le\", null],\n  [\"\u2581tr\", \"\u0431\u0440\u043e\u0434\u044f\u0433\u0438\"],\n  [\"u\", null],\n  [\"and\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nL\u2019autre eut un geste d\u00e9daigneux plein de cordialit\u00e9, et, prenant son chapeau:\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581L', '\u2019', 'autre', '\u2581e', 'ut', '\u2581un', '\u2581gest', 'e', '\u2581d\u00e9', 'da', 'igne', 'ux', '\u2581ple', 'in', '\u2581de', '\u2581cord', 'ial', 'it\u00e9', ',', '\u2581et', ',', '\u2581pre', 'nant', '\u2581son', '\u2581ch', 'ape', 'au', ':']"}], "[\n  [\"\u2581L\", null],\n  [\"'\", null],\n  [\"autre\", \"other\"],\n  [\"\u2581e\", null],\n  [\"ut\", null],\n  [\"\u2581un\", null],\n  [\"\u2581gest\", \"gesture\"],\n  [\"e\", null],\n  [\"\u2581d\u00e9\", \"disdainful\"],\n  [\"da\", null],\n  [\"igne\", null],\n  [\"ux\", null],\n  [\"\u2581ple\", \"full\"],\n  [\"in\", null],\n  [\"\u2581de\", null],\n  [\"\u2581cord\", \"cordiality\"],\n  [\"ial\", null],\n  [\"it\u00e9\", null],\n  [\",\", null],\n  [\"\u2581et\", null],\n  [\",\", null],\n  [\"\u2581pre\", \"taking\"],\n  [\"nant\", null],\n  [\"\u2581son\", null],\n  [\"\u2581ch\", \"hat\"],\n  [\"ape\", null],\n  [\"au\", null],\n  [\":\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nL\u2019autre eut un geste d\u00e9daigneux plein de cordialit\u00e9, et, prenant son chapeau:\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581L', '\u2019', 'autre', '\u2581e', 'ut', '\u2581un', '\u2581gest', 'e', '\u2581d\u00e9', 'da', 'igne', 'ux', '\u2581ple', 'in', '\u2581de', '\u2581cord', 'ial', 'it\u00e9', ',', '\u2581et', ',', '\u2581pre', 'nant', '\u2581son', '\u2581ch', 'ape', 'au', ':']"}], "[\n  [\"\u2581L\", \"\u0414\u0440\u0443\u0433\u043e\u0439\"],\n  [\"'\", null],\n  [\"autre\", null],\n  [\"\u2581e\", null],\n  [\"ut\", null],\n  [\"\u2581un\", null],\n  [\"\u2581gest\", \"\u0436\u0435\u0441\u0442\"],\n  [\"e\", null],\n  [\"\u2581d\u00e9\", null],\n  [\"da\", null],\n  [\"igne\", null],\n  [\"ux\", null],\n  [\"\u2581ple\", \"\u043f\u043e\u043b\u043d\u044b\u0439\"],\n  [\"in\", null],\n  [\"\u2581de\", null],\n  [\"\u2581cord\", \"\u0441\u0435\u0440\u0434\u0435\u0447\u043d\u043e\u0441\u0442\u0438\"],\n  [\"ial\", null],\n  [\"it\u00e9\", null],\n  [\",\", null],\n  [\"\u2581et\", \"\u0438\"],\n  [\",\", null],\n  [\"\u2581pre\", \"\u0432\u0437\u044f\u0432\"],\n  [\"nant\", null],\n  [\"\u2581son\", \"\u0441\u0432\u043e\u044e\"],\n  [\"\u2581ch\", \"\u0448\u043b\u044f\u043f\u0443\"],\n  [\"ape\", null],\n  [\"au\", null],\n  [\":\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nJe vous dirai sur la voyageuse que vous avez recueillie tout ce que je puis dire sans compromettre la paix de mon esprit, ma propre s\u00e9curit\u00e9 morale et physique, et surtout celle des autres.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Je', '\u2581vous', '\u2581dir', 'ai', '\u2581sur', '\u2581la', '\u2581voyage', 'use', '\u2581que', '\u2581vous', '\u2581ave', 'z', '\u2581rec', 'ue', 'ill', 'ie', '\u2581tout', '\u2581ce', '\u2581que', '\u2581je', '\u2581puis', '\u2581dire', '\u2581sans', '\u2581comprom', 'ett', 're', '\u2581la', '\u2581pa', 'ix', '\u2581de', '\u2581mon', '\u2581es', 'prit', ',', '\u2581ma', '\u2581pro', 'pre', '\u2581s\u00e9', 'cur', 'it\u00e9', '\u2581mor', 'ale', '\u2581et', '\u2581phys', 'ique', ',', '\u2581et', '\u2581sur', 't', 'out', '\u2581celle', '\u2581des', '\u2581autres', '.']"}], "[\n[\"\u2581Je\", \"I\"],\n[\"\u2581vous\", null],\n[\"\u2581dir\", \"tell\"],\n[\"ai\", null],\n[\"\u2581sur\", null],\n[\"\u2581la\", null],\n[\"\u2581voyage\", \"traveler\"],\n[\"use\", null],\n[\"\u2581que\", null],\n[\"\u2581vous\", null],\n[\"\u2581ave\", \"have\"],\n[\"z\", null],\n[\"\u2581rec\", \"taken\"],\n[\"ue\", null],\n[\"ill\", null],\n[\"ie\", null],\n[\"\u2581tout\", \"everything\"],\n[\"\u2581ce\", null],\n[\"\u2581que\", null],\n[\"\u2581je\", null],\n[\"\u2581puis\", \"can\"],\n[\"\u2581dire\", \"say\"],\n[\"\u2581sans\", \"without\"],\n[\"\u2581comprom\", \"compromising\"],\n[\"ett\", null],\n[\"re\", null],\n[\"\u2581la\", null],\n[\"\u2581pa\", \"peace\"],\n[\"ix\", null],\n[\"\u2581de\", null],\n[\"\u2581mon\", null],\n[\"\u2581es\", \"mind\"],\n[\"prit\", null],\n[\",\" , null],\n[\"\u2581ma\", null],\n[\"\u2581pro\", \"own\"],\n[\"pre\", null],\n[\"\u2581s\u00e9\", \"security\"],\n[\"cur\", null],\n[\"it\u00e9\", null],\n[\"\u2581mor\", \"moral\"],\n[\"ale\", null],\n[\"\u2581et\", null],\n[\"\u2581phys\", \"physical\"],\n[\"ique\", null],\n[\",\" , null],\n[\"\u2581et\", null],\n[\"\u2581sur\", \"above\"],\n[\"t\", null],\n[\"out\", null],\n[\"\u2581celle\", \"that\"],\n[\"\u2581des\", null],\n[\"\u2581autres\", \"others\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nJe vous dirai sur la voyageuse que vous avez recueillie tout ce que je puis dire sans compromettre la paix de mon esprit, ma propre s\u00e9curit\u00e9 morale et physique, et surtout celle des autres.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Je', '\u2581vous', '\u2581dir', 'ai', '\u2581sur', '\u2581la', '\u2581voyage', 'use', '\u2581que', '\u2581vous', '\u2581ave', 'z', '\u2581rec', 'ue', 'ill', 'ie', '\u2581tout', '\u2581ce', '\u2581que', '\u2581je', '\u2581puis', '\u2581dire', '\u2581sans', '\u2581comprom', 'ett', 're', '\u2581la', '\u2581pa', 'ix', '\u2581de', '\u2581mon', '\u2581es', 'prit', ',', '\u2581ma', '\u2581pro', 'pre', '\u2581s\u00e9', 'cur', 'it\u00e9', '\u2581mor', 'ale', '\u2581et', '\u2581phys', 'ique', ',', '\u2581et', '\u2581sur', 't', 'out', '\u2581celle', '\u2581des', '\u2581autres', '.']"}], "[\n[\"\u2581Je\", \"\u042f\"],\n[\"\u2581vous\", null],\n[\"\u2581dir\", \"\u0441\u043a\u0430\u0436\u0443\"],\n[\"ai\", null],\n[\"\u2581sur\", null],\n[\"\u2581la\", null],\n[\"\u2581voyage\", \"\u043f\u0443\u0442\u0435\u0448\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u0438\u0446\u0435\"],\n[\"use\", null],\n[\"\u2581que\", null],\n[\"\u2581vous\", null],\n[\"\u2581ave\", \"\u043f\u0440\u0438\u044e\u0442\u0438\u043b\u0438\"],\n[\"z\", null],\n[\"\u2581rec\", null],\n[\"ue\", null],\n[\"ill\", null],\n[\"ie\", null],\n[\"\u2581tout\", \"\u0432\u0441\u0451\"],\n[\"\u2581ce\", null],\n[\"\u2581que\", null],\n[\"\u2581je\", null],\n[\"\u2581puis\", \"\u043c\u043e\u0433\u0443\"],\n[\"\u2581dire\", \"\u0441\u043a\u0430\u0437\u0430\u0442\u044c\"],\n[\"\u2581sans\", \"\u043d\u0435\"],\n[\"\u2581comprom\", \"\u043f\u043e\u0434\u0432\u0435\u0440\u0433\u0430\u044f\"],\n[\"ett\", null],\n[\"re\", null],\n[\"\u2581la\", null],\n[\"\u2581pa\", \"\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u0438\"],\n[\"ix\", null],\n[\"\u2581de\", null],\n[\"\u2581mon\", null],\n[\"\u2581es\", \"\u0440\u0430\u0437\u0443\u043c\"],\n[\"prit\", null],\n[\",\" , null],\n[\"\u2581ma\", null],\n[\"\u2581pro\", \"\u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u0443\u044e\"],\n[\"pre\", null],\n[\"\u2581s\u00e9\", \"\u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u044c\"],\n[\"cur\", null],\n[\"it\u00e9\", null],\n[\"\u2581mor\", \"\u043c\u043e\u0440\u0430\u043b\u044c\u043d\u0443\u044e\"],\n[\"ale\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581phys\", \"\u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0443\u044e\"],\n[\"ique\", null],\n[\",\" , null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581sur\", \"\u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\"],\n[\"t\", null],\n[\"out\", null],\n[\"\u2581celle\", \"\u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u044c\"],\n[\"\u2581des\", null],\n[\"\u2581autres\", \"\u0434\u0440\u0443\u0433\u0438\u0445\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n\u00c7\u00e0 et l\u00e0 \u00e9tincelaient des points lumineux, qui se croisaient dans tous les sens, s\u2019\u00e9teignaient et se rallumaient tour \u00e0 tour.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u00c7', '\u00e0', '\u2581et', '\u2581l\u00e0', '\u2581\u00e9t', 'inc', 'ela', 'ient', '\u2581des', '\u2581points', '\u2581lum', 'ine', 'ux', ',', '\u2581qui', '\u2581se', '\u2581cro', 'isa', 'ient', '\u2581dans', '\u2581tous', '\u2581les', '\u2581sens', ',', '\u2581s', '\u2019', '\u00e9', 'te', 'ign', 'aient', '\u2581et', '\u2581se', '\u2581r', 'all', 'uma', 'ient', '\u2581tour', '\u2581\u00e0', '\u2581tour', '.']"}], "[\n  [\"\u2581\", null],\n  [\"\u00c7\", \"Here\"],\n  [\"\u00e0\", null],\n  [\"\u2581et\", \"and\"],\n  [\"\u2581l\u00e0\", \"there\"],\n  [\"\u2581\u00e9t\", \"sparkled\"],\n  [\"inc\", null],\n  [\"ela\", null],\n  [\"ient\", null],\n  [\"\u2581des\", null],\n  [\"\u2581points\", \"points\"],\n  [\"\u2581lum\", \"luminous\"],\n  [\"ine\", null],\n  [\"ux\", null],\n  [\",\", null],\n  [\"\u2581qui\", \"which\"],\n  [\"\u2581se\", null],\n  [\"\u2581cro\", \"crossed\"],\n  [\"isa\", null],\n  [\"ient\", null],\n  [\"\u2581dans\", \"in\"],\n  [\"\u2581tous\", \"all\"],\n  [\"\u2581les\", null],\n  [\"\u2581sens\", \"directions\"],\n  [\",\", null],\n  [\"\u2581s\", null],\n  [\"'\", null],\n  [\"\u00e9\", \"extinguished\"],\n  [\"te\", null],\n  [\"ign\", null],\n  [\"aient\", null],\n  [\"\u2581et\", \"and\"],\n  [\"\u2581se\", null],\n  [\"\u2581r\", \"rekindled\"],\n  [\"all\", null],\n  [\"uma\", null],\n  [\"ient\", null],\n  [\"\u2581tour\", \"turn\"],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581tour\", \"by turn\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n\u00c7\u00e0 et l\u00e0 \u00e9tincelaient des points lumineux, qui se croisaient dans tous les sens, s\u2019\u00e9teignaient et se rallumaient tour \u00e0 tour.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u00c7', '\u00e0', '\u2581et', '\u2581l\u00e0', '\u2581\u00e9t', 'inc', 'ela', 'ient', '\u2581des', '\u2581points', '\u2581lum', 'ine', 'ux', ',', '\u2581qui', '\u2581se', '\u2581cro', 'isa', 'ient', '\u2581dans', '\u2581tous', '\u2581les', '\u2581sens', ',', '\u2581s', '\u2019', '\u00e9', 'te', 'ign', 'aient', '\u2581et', '\u2581se', '\u2581r', 'all', 'uma', 'ient', '\u2581tour', '\u2581\u00e0', '\u2581tour', '.']"}], "[\n[\"\u2581\", null],\n[\"\u00c7\", null],\n[\"\u00e0\", null],\n[\"\u2581et\", null],\n[\"\u2581l\u00e0\", \"\u0442\u0430\u043c\"],\n[\"\u2581\u00e9t\", \"\u0441\u0432\u0435\u0440\u043a\u0430\u043b\u0438\"],\n[\"inc\", null],\n[\"ela\", null],\n[\"ient\", null],\n[\"\u2581des\", null],\n[\"\u2581points\", \"\u0442\u043e\u0447\u043a\u0438\"],\n[\"\u2581lum\", \"\u0441\u0432\u0435\u0442\u044f\u0449\u0438\u0435\u0441\u044f\"],\n[\"ine\", null],\n[\"ux\", null],\n[\",\", null],\n[\"\u2581qui\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u0435\"],\n[\"\u2581se\", null],\n[\"\u2581cro\", \"\u043f\u0435\u0440\u0435\u0441\u0435\u043a\u0430\u043b\u0438\u0441\u044c\"],\n[\"isa\", null],\n[\"ient\", null],\n[\"\u2581dans\", \"\u0432\u043e\"],\n[\"\u2581tous\", \"\u0432\u0441\u0435\u0445\"],\n[\"\u2581les\", null],\n[\"\u2581sens\", \"\u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f\u0445\"],\n[\",\", null],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"\u00e9\", \"\u0433\u0430\u0441\u043b\u0438\"],\n[\"te\", null],\n[\"ign\", null],\n[\"aient\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581se\", null],\n[\"\u2581r\", \"\u0437\u0430\u0436\u0438\u0433\u0430\u043b\u0438\u0441\u044c\"],\n[\"all\", null],\n[\"uma\", null],\n[\"ient\", null],\n[\"\u2581tour\", \"\u043f\u043e\u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581tour\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n\u00c0 ces hardies paroles, lord Glenarvan avait tendu les bras \u00e0 sa jeune femme; il souriait, il la pressait sur son coeur, tandis que Mary et Robert lui baisaient les mains.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00c0', '\u2581ces', '\u2581hard', 'ies', '\u2581par', 'oles', ',', '\u2581lord', '\u2581Glen', 'ar', 'van', '\u2581avait', '\u2581tend', 'u', '\u2581les', '\u2581bras', '\u2581\u00e0', '\u2581sa', '\u2581je', 'une', '\u2581femme', ';', '\u2581il', '\u2581sou', 'ri', 'ait', ',', '\u2581il', '\u2581la', '\u2581press', 'ait', '\u2581sur', '\u2581son', '\u2581co', 'eur', ',', '\u2581tand', 'is', '\u2581que', '\u2581Mary', '\u2581et', '\u2581Robert', '\u2581lui', '\u2581b', 'ais', 'aient', '\u2581les', '\u2581m', 'ains', '.']"}], "[\n[\"\u2581\u00c0\", \"At\"],\n[\"\u2581ces\", null],\n[\"\u2581hard\", \"bold\"],\n[\"ies\", null],\n[\"\u2581par\", \"words\"],\n[\"oles\", null],\n[\",\", null],\n[\"\u2581lord\", \"Lord\"],\n[\"\u2581Glen\", \"Glenarvan\"],\n[\"ar\", null],\n[\"van\", null],\n[\"\u2581avait\", null],\n[\"\u2581tend\", \"extended\"],\n[\"u\", null],\n[\"\u2581les\", null],\n[\"\u2581bras\", \"arms\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581sa\", null],\n[\"\u2581je\", \"young\"],\n[\"une\", null],\n[\"\u2581femme\", \"wife\"],\n[\";\", null],\n[\"\u2581il\", null],\n[\"\u2581sou\", \"smiled\"],\n[\"ri\", null],\n[\"ait\", null],\n[\",\", null],\n[\"\u2581il\", null],\n[\"\u2581la\", null],\n[\"\u2581press\", \"pressed\"],\n[\"ait\", null],\n[\"\u2581sur\", null],\n[\"\u2581son\", null],\n[\"\u2581co\", \"heart\"],\n[\"eur\", null],\n[\",\", null],\n[\"\u2581tand\", \"while\"],\n[\"is\", null],\n[\"\u2581que\", null],\n[\"\u2581Mary\", \"Mary\"],\n[\"\u2581et\", null],\n[\"\u2581Robert\", \"Robert\"],\n[\"\u2581lui\", null],\n[\"\u2581b\", \"kissed\"],\n[\"ais\", null],\n[\"aient\", null],\n[\"\u2581les\", null],\n[\"\u2581m\", \"hands\"],\n[\"ains\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n\u00c0 ces hardies paroles, lord Glenarvan avait tendu les bras \u00e0 sa jeune femme; il souriait, il la pressait sur son coeur, tandis que Mary et Robert lui baisaient les mains.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00c0', '\u2581ces', '\u2581hard', 'ies', '\u2581par', 'oles', ',', '\u2581lord', '\u2581Glen', 'ar', 'van', '\u2581avait', '\u2581tend', 'u', '\u2581les', '\u2581bras', '\u2581\u00e0', '\u2581sa', '\u2581je', 'une', '\u2581femme', ';', '\u2581il', '\u2581sou', 'ri', 'ait', ',', '\u2581il', '\u2581la', '\u2581press', 'ait', '\u2581sur', '\u2581son', '\u2581co', 'eur', ',', '\u2581tand', 'is', '\u2581que', '\u2581Mary', '\u2581et', '\u2581Robert', '\u2581lui', '\u2581b', 'ais', 'aient', '\u2581les', '\u2581m', 'ains', '.']"}], "[\n[\"\u2581\u00c0\", \"\u041d\u0430\"],\n[\"\u2581ces\", null],\n[\"\u2581hard\", \"\u0441\u043c\u0435\u043b\u044b\u0435\"],\n[\"ies\", null],\n[\"\u2581par\", \"\u0441\u043b\u043e\u0432\u0430\"],\n[\"oles\", null],\n[\",\", null],\n[\"\u2581lord\", \"\u043b\u043e\u0440\u0434\"],\n[\"\u2581Glen\", \"\u0413\u043b\u0435\u043d\u0430\u0440\u0432\u0430\u043d\"],\n[\"ar\", null],\n[\"van\", null],\n[\"\u2581avait\", null],\n[\"\u2581tend\", \"\u043f\u0440\u043e\u0442\u044f\u043d\u0443\u043b\"],\n[\"u\", null],\n[\"\u2581les\", null],\n[\"\u2581bras\", \"\u0440\u0443\u043a\u0438\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581sa\", null],\n[\"\u2581je\", \"\u043c\u043e\u043b\u043e\u0434\u043e\u0439\"],\n[\"une\", null],\n[\"\u2581femme\", \"\u0436\u0435\u043d\u0435\"],\n[\";\", null],\n[\"\u2581il\", null],\n[\"\u2581sou\", \"\u0443\u043b\u044b\u0431\u0430\u043b\u0441\u044f\"],\n[\"ri\", null],\n[\"ait\", null],\n[\",\", null],\n[\"\u2581il\", null],\n[\"\u2581la\", null],\n[\"\u2581press\", \"\u043f\u0440\u0438\u0436\u0438\u043c\u0430\u043b\"],\n[\"ait\", null],\n[\"\u2581sur\", null],\n[\"\u2581son\", null],\n[\"\u2581co\", \"\u0441\u0435\u0440\u0434\u0446\u0443\"],\n[\"eur\", null],\n[\",\", null],\n[\"\u2581tand\", \"\u043f\u043e\u043a\u0430\"],\n[\"is\", null],\n[\"\u2581que\", null],\n[\"\u2581Mary\", \"\u041c\u044d\u0440\u0438\"],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581Robert\", \"\u0420\u043e\u0431\u0435\u0440\u0442\"],\n[\"\u2581lui\", null],\n[\"\u2581b\", \"\u0446\u0435\u043b\u043e\u0432\u0430\u043b\u0438\"],\n[\"ais\", null],\n[\"aient\", null],\n[\"\u2581les\", null],\n[\"\u2581m\", \"\u0440\u0443\u043a\u0438\"],\n[\"ains\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nCette force du sexe et du sang de l\u2019homme que, fol adolescent, j\u2019avais cru \u00e9touffer pour la vie, avait plus d\u2019une fois soulev\u00e9 convulsivement la cha\u00eene des v\u0153ux de fer qui me scellent, mis\u00e9rable, aux froides pierres de l\u2019autel.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Cette', '\u2581force', '\u2581du', '\u2581sex', 'e', '\u2581et', '\u2581du', '\u2581sang', '\u2581de', '\u2581l', '\u2019', 'hom', 'me', '\u2581que', ',', '\u2581fol', '\u2581adoles', 'cent', ',', '\u2581j', '\u2019', 'av', 'ais', '\u2581cru', '\u2581\u00e9t', 'ou', 'ff', 'er', '\u2581pour', '\u2581la', '\u2581vie', ',', '\u2581avait', '\u2581plus', '\u2581d', '\u2019', 'une', '\u2581fois', '\u2581sou', 'lev', '\u00e9', '\u2581conv', 'uls', 'ive', 'ment', '\u2581la', '\u2581cha', '\u00ee', 'ne', '\u2581des', '\u2581v', '\u0153', 'ux', '\u2581de', '\u2581fer', '\u2581qui', '\u2581me', '\u2581sc', 'ell', 'ent', ',', '\u2581mis', '\u00e9r', 'able', ',', '\u2581aux', '\u2581fro', 'ides', '\u2581pier', 'res', '\u2581de', '\u2581l', '\u2019', 'aut', 'el', '.']"}], "[\n[\"\u2581Cette\", \"This\"],\n[\"\u2581force\", \"force\"],\n[\"\u2581du\", null],\n[\"\u2581sex\", \"sex\"],\n[\"e\", null],\n[\"\u2581et\", null],\n[\"\u2581du\", null],\n[\"\u2581sang\", \"blood\"],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"hom\", \"man\"],\n[\"me\", null],\n[\"\u2581que\", null],\n[\",\", null],\n[\"\u2581fol\", \"foolish\"],\n[\"\u2581adoles\", \"adolescent\"],\n[\"cent\", null],\n[\",\", null],\n[\"\u2581j\", \"I\"],\n[\"'\", null],\n[\"av\", \"had\"],\n[\"ais\", null],\n[\"\u2581cru\", \"believed\"],\n[\"\u2581\u00e9t\", \"stifle\"],\n[\"ou\", null],\n[\"ff\", null],\n[\"er\", null],\n[\"\u2581pour\", null],\n[\"\u2581la\", null],\n[\"\u2581vie\", \"life\"],\n[\",\", null],\n[\"\u2581avait\", \"had\"],\n[\"\u2581plus\", \"more\"],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"une\", \"one\"],\n[\"\u2581fois\", \"time\"],\n[\"\u2581sou\", \"raised\"],\n[\"lev\", null],\n[\"\u00e9\", null],\n[\"\u2581conv\", \"convulsively\"],\n[\"uls\", null],\n[\"ive\", null],\n[\"ment\", null],\n[\"\u2581la\", null],\n[\"\u2581cha\", \"chain\"],\n[\"\u00ee\", null],\n[\"ne\", null],\n[\"\u2581des\", null],\n[\"\u2581v\", \"vows\"],\n[\"\u0153\", null],\n[\"ux\", null],\n[\"\u2581de\", null],\n[\"\u2581fer\", \"iron\"],\n[\"\u2581qui\", null],\n[\"\u2581me\", null],\n[\"\u2581sc\", \"seal\"],\n[\"ell\", null],\n[\"ent\", null],\n[\",\", null],\n[\"\u2581mis\", \"miserable\"],\n[\"\u00e9r\", null],\n[\"able\", null],\n[\",\", null],\n[\"\u2581aux\", null],\n[\"\u2581fro\", \"cold\"],\n[\"ides\", null],\n[\"\u2581pier\", \"stones\"],\n[\"res\", null],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"aut\", \"altar\"],\n[\"el\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nCette force du sexe et du sang de l\u2019homme que, fol adolescent, j\u2019avais cru \u00e9touffer pour la vie, avait plus d\u2019une fois soulev\u00e9 convulsivement la cha\u00eene des v\u0153ux de fer qui me scellent, mis\u00e9rable, aux froides pierres de l\u2019autel.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Cette', '\u2581force', '\u2581du', '\u2581sex', 'e', '\u2581et', '\u2581du', '\u2581sang', '\u2581de', '\u2581l', '\u2019', 'hom', 'me', '\u2581que', ',', '\u2581fol', '\u2581adoles', 'cent', ',', '\u2581j', '\u2019', 'av', 'ais', '\u2581cru', '\u2581\u00e9t', 'ou', 'ff', 'er', '\u2581pour', '\u2581la', '\u2581vie', ',', '\u2581avait', '\u2581plus', '\u2581d', '\u2019', 'une', '\u2581fois', '\u2581sou', 'lev', '\u00e9', '\u2581conv', 'uls', 'ive', 'ment', '\u2581la', '\u2581cha', '\u00ee', 'ne', '\u2581des', '\u2581v', '\u0153', 'ux', '\u2581de', '\u2581fer', '\u2581qui', '\u2581me', '\u2581sc', 'ell', 'ent', ',', '\u2581mis', '\u00e9r', 'able', ',', '\u2581aux', '\u2581fro', 'ides', '\u2581pier', 'res', '\u2581de', '\u2581l', '\u2019', 'aut', 'el', '.']"}], "[\n[\"\u2581Cette\", \"\u042d\u0442\u0430\"],\n[\"\u2581force\", \"\u0441\u0438\u043b\u0430\"],\n[\"\u2581du\", null],\n[\"\u2581sex\", \"\u043f\u043e\u043b\"],\n[\"e\", null],\n[\"\u2581et\", null],\n[\"\u2581du\", null],\n[\"\u2581sang\", \"\u043a\u0440\u043e\u0432\u0438\"],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"hom\", \"\u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430\"],\n[\"me\", null],\n[\"\u2581que\", null],\n[\",\", null],\n[\"\u2581fol\", \"\u0431\u0435\u0437\u0443\u043c\u043d\u044b\u0439\"],\n[\"\u2581adoles\", \"\u043f\u043e\u0434\u0440\u043e\u0441\u0442\u043e\u043a\"],\n[\"cent\", null],\n[\",\", null],\n[\"\u2581j\", null],\n[\"'\", null],\n[\"av\", \"\u0434\u0443\u043c\u0430\u043b\"],\n[\"ais\", null],\n[\"\u2581cru\", null],\n[\"\u2581\u00e9t\", \"\u043f\u043e\u0434\u0430\u0432\u0438\u0442\u044c\"],\n[\"ou\", null],\n[\"ff\", null],\n[\"er\", null],\n[\"\u2581pour\", null],\n[\"\u2581la\", null],\n[\"\u2581vie\", \"\u0436\u0438\u0437\u043d\u044c\"],\n[\",\", null],\n[\"\u2581avait\", null],\n[\"\u2581plus\", \"\u0431\u043e\u043b\u0435\u0435\"],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"une\", \"\u043e\u0434\u043d\u043e\u0433\u043e\"],\n[\"\u2581fois\", \"\u0440\u0430\u0437\u0430\"],\n[\"\u2581sou\", \"\u043f\u043e\u0434\u043d\u0438\u043c\u0430\u043b\u0430\"],\n[\"lev\", null],\n[\"\u00e9\", null],\n[\"\u2581conv\", \"\u0441\u0443\u0434\u043e\u0440\u043e\u0436\u043d\u043e\"],\n[\"uls\", null],\n[\"ive\", null],\n[\"ment\", null],\n[\"\u2581la\", null],\n[\"\u2581cha\", \"\u0446\u0435\u043f\u044c\"],\n[\"\u00ee\", null],\n[\"ne\", null],\n[\"\u2581des\", null],\n[\"\u2581v\", \"\u043e\u0431\u0435\u0442\u043e\u0432\"],\n[\"\u0153\", null],\n[\"ux\", null],\n[\"\u2581de\", null],\n[\"\u2581fer\", \"\u0436\u0435\u043b\u0435\u0437\u043d\u044b\u0445\"],\n[\"\u2581qui\", null],\n[\"\u2581me\", \"\u043c\u0435\u043d\u044f\"],\n[\"\u2581sc\", \"\u043f\u0440\u0438\u043a\u043e\u0432\u044b\u0432\u0430\u044e\u0442\"],\n[\"ell\", null],\n[\"ent\", null],\n[\",\", null],\n[\"\u2581mis\", \"\u043d\u0435\u0441\u0447\u0430\u0441\u0442\u043d\u043e\u0433\u043e\"],\n[\"\u00e9r\", null],\n[\"able\", null],\n[\",\", null],\n[\"\u2581aux\", null],\n[\"\u2581fro\", \"\u0445\u043e\u043b\u043e\u0434\u043d\u044b\u043c\"],\n[\"ides\", null],\n[\"\u2581pier\", \"\u043a\u0430\u043c\u043d\u044f\u043c\"],\n[\"res\", null],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"aut\", \"\u0430\u043b\u0442\u0430\u0440\u044f\"],\n[\"el\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nEnfin on disait, horreur dans un enfant de seize ans, que ses d\u00e9bordements allaient souventes fois jusqu\u2019\u00e0 la rue de Glatigny.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581En', 'fin', '\u2581on', '\u2581dis', 'ait', ',', '\u2581hor', 're', 'ur', '\u2581dans', '\u2581un', '\u2581enf', 'ant', '\u2581de', '\u2581se', 'ize', '\u2581ans', ',', '\u2581que', '\u2581ses', '\u2581d\u00e9', 'b', 'ord', 'ements', '\u2581alla', 'ient', '\u2581souvent', 'es', '\u2581fois', '\u2581jusqu', '\u2019', '\u00e0', '\u2581la', '\u2581rue', '\u2581de', '\u2581Gl', 'at', 'ign', 'y', '.']"}], "[\n  [\"\u2581En\", \"Finally\"],\n  [\"fin\", null],\n  [\"\u2581on\", null],\n  [\"\u2581dis\", \"said\"],\n  [\"ait\", null],\n  [\",\", null],\n  [\"\u2581hor\", \"horror\"],\n  [\"re\", null],\n  [\"ur\", null],\n  [\"\u2581dans\", null],\n  [\"\u2581un\", null],\n  [\"\u2581enf\", \"child\"],\n  [\"ant\", null],\n  [\"\u2581de\", null],\n  [\"\u2581se\", \"sixteen\"],\n  [\"ize\", null],\n  [\"\u2581ans\", \"years\"],\n  [\",\", null],\n  [\"\u2581que\", null],\n  [\"\u2581ses\", null],\n  [\"\u2581d\u00e9\", \"excesses\"],\n  [\"b\", null],\n  [\"ord\", null],\n  [\"ements\", null],\n  [\"\u2581alla\", \"went\"],\n  [\"ient\", null],\n  [\"\u2581souvent\", \"often\"],\n  [\"es\", null],\n  [\"\u2581fois\", \"times\"],\n  [\"\u2581jusqu\", \"as far as\"],\n  [\"'\", null],\n  [\"\u00e0\", null],\n  [\"\u2581la\", null],\n  [\"\u2581rue\", \"street\"],\n  [\"\u2581de\", null],\n  [\"\u2581Gl\", \"Glatigny\"],\n  [\"at\", null],\n  [\"ign\", null],\n  [\"y\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nEnfin on disait, horreur dans un enfant de seize ans, que ses d\u00e9bordements allaient souventes fois jusqu\u2019\u00e0 la rue de Glatigny.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581En', 'fin', '\u2581on', '\u2581dis', 'ait', ',', '\u2581hor', 're', 'ur', '\u2581dans', '\u2581un', '\u2581enf', 'ant', '\u2581de', '\u2581se', 'ize', '\u2581ans', ',', '\u2581que', '\u2581ses', '\u2581d\u00e9', 'b', 'ord', 'ements', '\u2581alla', 'ient', '\u2581souvent', 'es', '\u2581fois', '\u2581jusqu', '\u2019', '\u00e0', '\u2581la', '\u2581rue', '\u2581de', '\u2581Gl', 'at', 'ign', 'y', '.']"}], "[\n[\"\u2581En\", \"\u041d\u0430\u043a\u043e\u043d\u0435\u0446\"],\n[\"fin\", null],\n[\"\u2581on\", null],\n[\"\u2581dis\", \"\u0433\u043e\u0432\u043e\u0440\u0438\u043b\u0438\"],\n[\"ait\", null],\n[\",\", null],\n[\"\u2581hor\", \"\u0443\u0436\u0430\u0441\"],\n[\"re\", null],\n[\"ur\", null],\n[\"\u2581dans\", null],\n[\"\u2581un\", null],\n[\"\u2581enf\", \"\u0440\u0435\u0431\u0435\u043d\u043e\u043a\"],\n[\"ant\", null],\n[\"\u2581de\", null],\n[\"\u2581se\", \"\u0448\u0435\u0441\u0442\u043d\u0430\u0434\u0446\u0430\u0442\u0438\"],\n[\"ize\", null],\n[\"\u2581ans\", \"\u043b\u0435\u0442\"],\n[\",\", null],\n[\"\u2581que\", null],\n[\"\u2581ses\", null],\n[\"\u2581d\u00e9\", \"\u0440\u0430\u0437\u0433\u0443\u043b\"],\n[\"b\", null],\n[\"ord\", null],\n[\"ements\", null],\n[\"\u2581alla\", \"\u0434\u043e\u0445\u043e\u0434\u0438\u043b\"],\n[\"ient\", null],\n[\"\u2581souvent\", \"\u0447\u0430\u0441\u0442\u043e\"],\n[\"es\", null],\n[\"\u2581fois\", null],\n[\"\u2581jusqu\", \"\u0434\u043e\"],\n[\"'\", null],\n[\"\u00e0\", null],\n[\"\u2581la\", null],\n[\"\u2581rue\", \"\u0443\u043b\u0438\u0446\u044b\"],\n[\"\u2581de\", null],\n[\"\u2581Gl\", \"\u0413\u043b\u0430\u0442\u0438\u043d\u044c\u0438\"],\n[\"at\", null],\n[\"ign\", null],\n[\"y\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nQui e\u00fbt pu voir en ce moment la figure du malheureux coll\u00e9e aux barreaux vermoulus e\u00fbt cru voir une face de tigre regardant du fond d\u2019une cage quelque chacal qui d\u00e9vore une gazelle.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Q', 'ui', '\u2581e', '\u00fbt', '\u2581pu', '\u2581vo', 'ir', '\u2581en', '\u2581ce', '\u2581moment', '\u2581la', '\u2581figure', '\u2581du', '\u2581mal', 'he', 'ure', 'ux', '\u2581coll', '\u00e9e', '\u2581aux', '\u2581bar', 're', 'aux', '\u2581ver', 'm', 'oul', 'us', '\u2581e', '\u00fbt', '\u2581cru', '\u2581vo', 'ir', '\u2581une', '\u2581face', '\u2581de', '\u2581t', 'ig', 're', '\u2581regard', 'ant', '\u2581du', '\u2581fond', '\u2581d', '\u2019', 'une', '\u2581cage', '\u2581quel', 'que', '\u2581ch', 'ac', 'al', '\u2581qui', '\u2581d\u00e9', 'v', 'ore', '\u2581une', '\u2581gaz', 'elle', '.']"}], "[\n[\"\u2581Q\", \"Who\"],\n[\"ui\", null],\n[\"\u2581e\", null],\n[\"\u00fbt\", null],\n[\"\u2581pu\", \"could\"],\n[\"\u2581vo\", \"see\"],\n[\"ir\", null],\n[\"\u2581en\", null],\n[\"\u2581ce\", \"this\"],\n[\"\u2581moment\", \"moment\"],\n[\"\u2581la\", null],\n[\"\u2581figure\", \"face\"],\n[\"\u2581du\", null],\n[\"\u2581mal\", \"unfortunate\"],\n[\"he\", null],\n[\"ure\", null],\n[\"ux\", null],\n[\"\u2581coll\", \"pressed\"],\n[\"\u00e9e\", null],\n[\"\u2581aux\", \"against\"],\n[\"\u2581bar\", \"bars\"],\n[\"re\", null],\n[\"aux\", null],\n[\"\u2581ver\", \"worm-eaten\"],\n[\"m\", null],\n[\"oul\", null],\n[\"us\", null],\n[\"\u2581e\", null],\n[\"\u00fbt\", null],\n[\"\u2581cru\", \"believed\"],\n[\"\u2581vo\", \"see\"],\n[\"ir\", null],\n[\"\u2581une\", null],\n[\"\u2581face\", \"face\"],\n[\"\u2581de\", null],\n[\"\u2581t\", \"tiger\"],\n[\"ig\", null],\n[\"re\", null],\n[\"\u2581regard\", \"looking\"],\n[\"ant\", null],\n[\"\u2581du\", \"from\"],\n[\"\u2581fond\", \"bottom\"],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"une\", null],\n[\"\u2581cage\", \"cage\"],\n[\"\u2581quel\", \"some\"],\n[\"que\", null],\n[\"\u2581ch\", \"jackal\"],\n[\"ac\", null],\n[\"al\", null],\n[\"\u2581qui\", \"that\"],\n[\"\u2581d\u00e9\", \"devours\"],\n[\"v\", null],\n[\"ore\", null],\n[\"\u2581une\", null],\n[\"\u2581gaz\", \"gazelle\"],\n[\"elle\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nQui e\u00fbt pu voir en ce moment la figure du malheureux coll\u00e9e aux barreaux vermoulus e\u00fbt cru voir une face de tigre regardant du fond d\u2019une cage quelque chacal qui d\u00e9vore une gazelle.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Q', 'ui', '\u2581e', '\u00fbt', '\u2581pu', '\u2581vo', 'ir', '\u2581en', '\u2581ce', '\u2581moment', '\u2581la', '\u2581figure', '\u2581du', '\u2581mal', 'he', 'ure', 'ux', '\u2581coll', '\u00e9e', '\u2581aux', '\u2581bar', 're', 'aux', '\u2581ver', 'm', 'oul', 'us', '\u2581e', '\u00fbt', '\u2581cru', '\u2581vo', 'ir', '\u2581une', '\u2581face', '\u2581de', '\u2581t', 'ig', 're', '\u2581regard', 'ant', '\u2581du', '\u2581fond', '\u2581d', '\u2019', 'une', '\u2581cage', '\u2581quel', 'que', '\u2581ch', 'ac', 'al', '\u2581qui', '\u2581d\u00e9', 'v', 'ore', '\u2581une', '\u2581gaz', 'elle', '.']"}], "[\n[\"\u2581Q\", \"\u041a\u0442\u043e\"],\n[\"ui\", null],\n[\"\u2581e\", null],\n[\"\u00fbt\", null],\n[\"\u2581pu\", \"\u043c\u043e\u0433\"],\n[\"\u2581vo\", \"\u0443\u0432\u0438\u0434\u0435\u0442\u044c\"],\n[\"ir\", null],\n[\"\u2581en\", null],\n[\"\u2581ce\", \"\u044d\u0442\u043e\u0442\"],\n[\"\u2581moment\", \"\u043c\u043e\u043c\u0435\u043d\u0442\"],\n[\"\u2581la\", null],\n[\"\u2581figure\", \"\u043b\u0438\u0446\u043e\"],\n[\"\u2581du\", null],\n[\"\u2581mal\", \"\u043d\u0435\u0441\u0447\u0430\u0441\u0442\u043d\u043e\u0433\u043e\"],\n[\"he\", null],\n[\"ure\", null],\n[\"ux\", null],\n[\"\u2581coll\", \"\u043f\u0440\u0438\u0436\u0430\u0442\u043e\u0435\"],\n[\"\u00e9e\", null],\n[\"\u2581aux\", \"\u043a\"],\n[\"\u2581bar\", \"\u043f\u0440\u0443\u0442\u044c\u044f\u043c\"],\n[\"re\", null],\n[\"aux\", null],\n[\"\u2581ver\", \"\u0438\u0437\u044a\u0435\u0434\u0435\u043d\u043d\u044b\u043c\"],\n[\"m\", null],\n[\"oul\", null],\n[\"us\", null],\n[\"\u2581e\", null],\n[\"\u00fbt\", null],\n[\"\u2581cru\", \"\u043f\u043e\u0434\u0443\u043c\u0430\u043b\"],\n[\"\u2581vo\", \"\u0443\u0432\u0438\u0434\u0435\u043b\"],\n[\"ir\", null],\n[\"\u2581une\", null],\n[\"\u2581face\", \"\u043c\u043e\u0440\u0434\u0443\"],\n[\"\u2581de\", null],\n[\"\u2581t\", \"\u0442\u0438\u0433\u0440\u0430\"],\n[\"ig\", null],\n[\"re\", null],\n[\"\u2581regard\", \"\u0441\u043c\u043e\u0442\u0440\u044f\u0449\u0435\u0433\u043e\"],\n[\"ant\", null],\n[\"\u2581du\", \"\u0438\u0437\"],\n[\"\u2581fond\", \"\u0433\u043b\u0443\u0431\u0438\u043d\u044b\"],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"une\", null],\n[\"\u2581cage\", \"\u043a\u043b\u0435\u0442\u043a\u0438\"],\n[\"\u2581quel\", \"\u043d\u0430\"],\n[\"que\", null],\n[\"\u2581ch\", \"\u0448\u0430\u043a\u0430\u043b\u0430\"],\n[\"ac\", null],\n[\"al\", null],\n[\"\u2581qui\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u0439\"],\n[\"\u2581d\u00e9\", \"\u043f\u043e\u0436\u0438\u0440\u0430\u0435\u0442\"],\n[\"v\", null],\n[\"ore\", null],\n[\"\u2581une\", null],\n[\"\u2581gaz\", \"\u0433\u0430\u0437\u0435\u043b\u044c\"],\n[\"elle\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nGlenarvan garda pour lui le secret de ses angoisses, et ni lady Helena, ni Mary Grant, ni Robert ne soup\u00e7onn\u00e8rent la gravit\u00e9 des p\u00e9rils auxquels ils venaient d\u2019\u00e9chapper.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Glen', 'ar', 'van', '\u2581gard', 'a', '\u2581pour', '\u2581lui', '\u2581le', '\u2581secret', '\u2581de', '\u2581ses', '\u2581an', 'go', 'iss', 'es', ',', '\u2581et', '\u2581ni', '\u2581lady', '\u2581Hel', 'ena', ',', '\u2581ni', '\u2581Mary', '\u2581Grant', ',', '\u2581ni', '\u2581Robert', '\u2581ne', '\u2581soup', '\u00e7', 'on', 'n', '\u00e8rent', '\u2581la', '\u2581grav', 'it\u00e9', '\u2581des', '\u2581p', '\u00e9r', 'ils', '\u2581aux', 'qu', 'els', '\u2581ils', '\u2581v', 'ena', 'ient', '\u2581d', '\u2019', '\u00e9', 'chap', 'per', '.']"}], "[\n  [\"\u2581Glen\", \"Glenarvan\"],\n  [\"ar\", null],\n  [\"van\", null],\n  [\"\u2581gard\", \"kept\"],\n  [\"a\", null],\n  [\"\u2581pour\", \"for\"],\n  [\"\u2581lui\", \"himself\"],\n  [\"\u2581le\", null],\n  [\"\u2581secret\", \"secret\"],\n  [\"\u2581de\", null],\n  [\"\u2581ses\", null],\n  [\"\u2581an\", \"anxieties\"],\n  [\"go\", null],\n  [\"iss\", null],\n  [\"es\", null],\n  [\",\", null],\n  [\"\u2581et\", null],\n  [\"\u2581ni\", \"neither\"],\n  [\"\u2581lady\", \"Lady\"],\n  [\"\u2581Hel\", \"Helena\"],\n  [\"ena\", null],\n  [\",\", null],\n  [\"\u2581ni\", \"nor\"],\n  [\"\u2581Mary\", \"Mary\"],\n  [\"\u2581Grant\", \"Grant\"],\n  [\",\", null],\n  [\"\u2581ni\", \"nor\"],\n  [\"\u2581Robert\", \"Robert\"],\n  [\"\u2581ne\", null],\n  [\"\u2581soup\", \"suspected\"],\n  [\"\u00e7\", null],\n  [\"on\", null],\n  [\"n\", null],\n  [\"\u00e8rent\", null],\n  [\"\u2581la\", null],\n  [\"\u2581grav\", \"seriousness\"],\n  [\"it\u00e9\", null],\n  [\"\u2581des\", null],\n  [\"\u2581p\", \"dangers\"],\n  [\"\u00e9r\", null],\n  [\"ils\", null],\n  [\"\u2581aux\", null],\n  [\"qu\", null],\n  [\"els\", null],\n  [\"\u2581ils\", \"they\"],\n  [\"\u2581v\", \"had\"],\n  [\"ena\", null],\n  [\"ient\", null],\n  [\"\u2581d\", null],\n  [\"'\", null],\n  [\"\u00e9\", \"escaped\"],\n  [\"chap\", null],\n  [\"per\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nGlenarvan garda pour lui le secret de ses angoisses, et ni lady Helena, ni Mary Grant, ni Robert ne soup\u00e7onn\u00e8rent la gravit\u00e9 des p\u00e9rils auxquels ils venaient d\u2019\u00e9chapper.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Glen', 'ar', 'van', '\u2581gard', 'a', '\u2581pour', '\u2581lui', '\u2581le', '\u2581secret', '\u2581de', '\u2581ses', '\u2581an', 'go', 'iss', 'es', ',', '\u2581et', '\u2581ni', '\u2581lady', '\u2581Hel', 'ena', ',', '\u2581ni', '\u2581Mary', '\u2581Grant', ',', '\u2581ni', '\u2581Robert', '\u2581ne', '\u2581soup', '\u00e7', 'on', 'n', '\u00e8rent', '\u2581la', '\u2581grav', 'it\u00e9', '\u2581des', '\u2581p', '\u00e9r', 'ils', '\u2581aux', 'qu', 'els', '\u2581ils', '\u2581v', 'ena', 'ient', '\u2581d', '\u2019', '\u00e9', 'chap', 'per', '.']"}], "[\n[\"\u2581Glen\", \"\u0413\u043b\u0435\u043d\u0430\u0440\u0432\u0430\u043d\"],\n[\"ar\", null],\n[\"van\", null],\n[\"\u2581gard\", \"\u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043b\"],\n[\"a\", null],\n[\"\u2581pour\", \"\u0434\u043b\u044f\"],\n[\"\u2581lui\", \"\u0441\u0435\u0431\u044f\"],\n[\"\u2581le\", null],\n[\"\u2581secret\", \"\u0441\u0435\u043a\u0440\u0435\u0442\"],\n[\"\u2581de\", null],\n[\"\u2581ses\", \"\u0441\u0432\u043e\u0438\u0445\"],\n[\"\u2581an\", \"\u0442\u0440\u0435\u0432\u043e\u0433\"],\n[\"go\", null],\n[\"iss\", null],\n[\"es\", null],\n[\",\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581ni\", \"\u043d\u0438\"],\n[\"\u2581lady\", \"\u043b\u0435\u0434\u0438\"],\n[\"\u2581Hel\", \"\u0415\u043b\u0435\u043d\u0430\"],\n[\"ena\", null],\n[\",\", null],\n[\"\u2581ni\", \"\u043d\u0438\"],\n[\"\u2581Mary\", \"\u041c\u044d\u0440\u0438\"],\n[\"\u2581Grant\", \"\u0413\u0440\u0430\u043d\u0442\"],\n[\",\", null],\n[\"\u2581ni\", \"\u043d\u0438\"],\n[\"\u2581Robert\", \"\u0420\u043e\u0431\u0435\u0440\u0442\"],\n[\"\u2581ne\", \"\u043d\u0435\"],\n[\"\u2581soup\", \"\u043f\u043e\u0434\u043e\u0437\u0440\u0435\u0432\u0430\u043b\u0438\"],\n[\"\u00e7\", null],\n[\"on\", null],\n[\"n\", null],\n[\"\u00e8rent\", null],\n[\"\u2581la\", null],\n[\"\u2581grav\", \"\u0441\u0435\u0440\u044c\u0435\u0437\u043d\u043e\u0441\u0442\u044c\"],\n[\"it\u00e9\", null],\n[\"\u2581des\", null],\n[\"\u2581p\", \"\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u0435\u0439\"],\n[\"\u00e9r\", null],\n[\"ils\", null],\n[\"\u2581aux\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u043c\"],\n[\"qu\", null],\n[\"els\", null],\n[\"\u2581ils\", \"\u043e\u043d\u0438\"],\n[\"\u2581v\", \"\u0442\u043e\u043b\u044c\u043a\u043e\"],\n[\"ena\", null],\n[\"ient\", null],\n[\"\u2581d\", \"\u0447\u0442\u043e\"],\n[\"'\", null],\n[\"\u00e9\", \"\u0438\u0437\u0431\u0435\u0436\u0430\u043b\u0438\"],\n[\"chap\", null],\n[\"per\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nJe commen\u00e7ai \u00e0 comprendre ce qu'elle devait \u00eatre dans un sermon que je l'entendis faire \u00e0 sa propre paroisse de Morton: il n'est pas en mon pouvoir de raconter ce sermon;\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Je', '\u2581comm', 'en', '\u00e7', 'ai', '\u2581\u00e0', '\u2581comprend', 're', '\u2581ce', '\u2581qu', \"'\", 'elle', '\u2581dev', 'ait', '\u2581\u00eatre', '\u2581dans', '\u2581un', '\u2581s', 'erm', 'on', '\u2581que', '\u2581je', '\u2581l', \"'\", 'ent', 'end', 'is', '\u2581faire', '\u2581\u00e0', '\u2581sa', '\u2581pro', 'pre', '\u2581par', 'ois', 'se', '\u2581de', '\u2581Mort', 'on', ':', '\u2581il', '\u2581n', \"'\", 'est', '\u2581pas', '\u2581en', '\u2581mon', '\u2581pou', 'voir', '\u2581de', '\u2581rac', 'on', 'ter', '\u2581ce', '\u2581s', 'erm', 'on', ';']"}], "[\n[\"\u2581Je\", \"I\"],\n[\"\u2581comm\", \"began\"],\n[\"en\", null],\n[\"\u00e7\", null],\n[\"ai\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581comprend\", \"understand\"],\n[\"re\", null],\n[\"\u2581ce\", null],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"elle\", \"she\"],\n[\"\u2581dev\", \"must\"],\n[\"ait\", null],\n[\"\u2581\u00eatre\", \"be\"],\n[\"\u2581dans\", \"in\"],\n[\"\u2581un\", null],\n[\"\u2581s\", \"sermon\"],\n[\"erm\", null],\n[\"on\", null],\n[\"\u2581que\", \"that\"],\n[\"\u2581je\", \"I\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"ent\", \"heard\"],\n[\"end\", null],\n[\"is\", null],\n[\"\u2581faire\", \"give\"],\n[\"\u2581\u00e0\", \"to\"],\n[\"\u2581sa\", \"her\"],\n[\"\u2581pro\", \"own\"],\n[\"pre\", null],\n[\"\u2581par\", \"parish\"],\n[\"ois\", null],\n[\"se\", null],\n[\"\u2581de\", \"of\"],\n[\"\u2581Mort\", \"Morton\"],\n[\"on\", null],\n[\":\", null],\n[\"\u2581il\", \"it\"],\n[\"\u2581n\", null],\n[\"'\", null],\n[\"est\", \"is\"],\n[\"\u2581pas\", \"not\"],\n[\"\u2581en\", \"in\"],\n[\"\u2581mon\", \"my\"],\n[\"\u2581pou\", \"power\"],\n[\"voir\", null],\n[\"\u2581de\", \"to\"],\n[\"\u2581rac\", \"recount\"],\n[\"on\", null],\n[\"ter\", null],\n[\"\u2581ce\", \"this\"],\n[\"\u2581s\", \"sermon\"],\n[\"erm\", null],\n[\"on\", null],\n[\";\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nJe commen\u00e7ai \u00e0 comprendre ce qu'elle devait \u00eatre dans un sermon que je l'entendis faire \u00e0 sa propre paroisse de Morton: il n'est pas en mon pouvoir de raconter ce sermon;\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Je', '\u2581comm', 'en', '\u00e7', 'ai', '\u2581\u00e0', '\u2581comprend', 're', '\u2581ce', '\u2581qu', \"'\", 'elle', '\u2581dev', 'ait', '\u2581\u00eatre', '\u2581dans', '\u2581un', '\u2581s', 'erm', 'on', '\u2581que', '\u2581je', '\u2581l', \"'\", 'ent', 'end', 'is', '\u2581faire', '\u2581\u00e0', '\u2581sa', '\u2581pro', 'pre', '\u2581par', 'ois', 'se', '\u2581de', '\u2581Mort', 'on', ':', '\u2581il', '\u2581n', \"'\", 'est', '\u2581pas', '\u2581en', '\u2581mon', '\u2581pou', 'voir', '\u2581de', '\u2581rac', 'on', 'ter', '\u2581ce', '\u2581s', 'erm', 'on', ';']"}], "[\n[\"\u2581Je\", \"\u042f\"],\n[\"\u2581comm\", \"\u043d\u0430\u0447\u0438\u043d\u0430\u0442\u044c\"],\n[\"en\", null],\n[\"\u00e7\", null],\n[\"ai\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581comprend\", \"\u043f\u043e\u043d\u0438\u043c\u0430\u0442\u044c\"],\n[\"re\", null],\n[\"\u2581ce\", null],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"elle\", \"\u043e\u043d\u0430\"],\n[\"\u2581dev\", \"\u0434\u043e\u043b\u0436\u043d\u0430\"],\n[\"ait\", null],\n[\"\u2581\u00eatre\", \"\u0431\u044b\u0442\u044c\"],\n[\"\u2581dans\", \"\u0432\"],\n[\"\u2581un\", null],\n[\"\u2581s\", \"\u043f\u0440\u043e\u043f\u043e\u0432\u0435\u0434\u044c\"],\n[\"erm\", null],\n[\"on\", null],\n[\"\u2581que\", \"\u043a\u043e\u0442\u043e\u0440\u0443\u044e\"],\n[\"\u2581je\", \"\u044f\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"ent\", \"\u0441\u043b\u044b\u0448\u0430\u043b\"],\n[\"end\", null],\n[\"is\", null],\n[\"\u2581faire\", \"\u043f\u0440\u043e\u0438\u0437\u043d\u043e\u0441\u0438\u0442\u044c\"],\n[\"\u2581\u00e0\", \"\u0432\"],\n[\"\u2581sa\", \"\u0441\u0432\u043e\u0435\u043c\"],\n[\"\u2581pro\", \"\u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u043c\"],\n[\"pre\", null],\n[\"\u2581par\", \"\u043f\u0440\u0438\u0445\u043e\u0434\u0435\"],\n[\"ois\", null],\n[\"se\", null],\n[\"\u2581de\", null],\n[\"\u2581Mort\", \"\u041c\u043e\u0440\u0442\u043e\u043d\"],\n[\"on\", null],\n[\":\", null],\n[\"\u2581il\", null],\n[\"\u2581n\", \"\u043d\u0435\"],\n[\"'\", null],\n[\"est\", null],\n[\"\u2581pas\", null],\n[\"\u2581en\", \"\u0432\"],\n[\"\u2581mon\", \"\u043c\u043e\u0438\u0445\"],\n[\"\u2581pou\", \"\u0441\u0438\u043b\u0430\u0445\"],\n[\"voir\", null],\n[\"\u2581de\", null],\n[\"\u2581rac\", \"\u0440\u0430\u0441\u0441\u043a\u0430\u0437\u0430\u0442\u044c\"],\n[\"on\", null],\n[\"ter\", null],\n[\"\u2581ce\", \"\u044d\u0442\u0443\"],\n[\"\u2581s\", \"\u043f\u0440\u043e\u043f\u043e\u0432\u0435\u0434\u044c\"],\n[\"erm\", null],\n[\"on\", null],\n[\";\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nHuit avirons \u00e0 l\u2019avant le faisaient voler \u00e0 la surface des eaux, pendant qu\u2019un homme, assis \u00e0 l\u2019arri\u00e8re, le dirigeait au moyen d\u2019une pagaie mobile.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581H', 'uit', '\u2581av', 'ir', 'ons', '\u2581\u00e0', '\u2581l', '\u2019', 'av', 'ant', '\u2581le', '\u2581fais', 'aient', '\u2581vol', 'er', '\u2581\u00e0', '\u2581la', '\u2581surface', '\u2581des', '\u2581e', 'aux', ',', '\u2581pendant', '\u2581qu', '\u2019', 'un', '\u2581homme', ',', '\u2581ass', 'is', '\u2581\u00e0', '\u2581l', '\u2019', 'ar', 'ri\u00e8re', ',', '\u2581le', '\u2581dir', 'ige', 'ait', '\u2581au', '\u2581moy', 'en', '\u2581d', '\u2019', 'une', '\u2581p', 'aga', 'ie', '\u2581mobile', '.']"}], "[\n  [\"\u2581H\", \"Eight\"],\n  [\"uit\", null],\n  [\"\u2581av\", \"oars\"],\n  [\"ir\", null],\n  [\"ons\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"av\", \"front\"],\n  [\"ant\", null],\n  [\"\u2581le\", null],\n  [\"\u2581fais\", \"made\"],\n  [\"aient\", null],\n  [\"\u2581vol\", \"fly\"],\n  [\"er\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581la\", null],\n  [\"\u2581surface\", \"surface\"],\n  [\"\u2581des\", null],\n  [\"\u2581e\", \"waters\"],\n  [\"aux\", null],\n  [\",\", null],\n  [\"\u2581pendant\", \"while\"],\n  [\"\u2581qu\", null],\n  [\"'\", null],\n  [\"un\", \"a\"],\n  [\"\u2581homme\", \"man\"],\n  [\",\", null],\n  [\"\u2581ass\", \"seated\"],\n  [\"is\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"ar\", \"rear\"],\n  [\"ri\u00e8re\", null],\n  [\",\", null],\n  [\"\u2581le\", null],\n  [\"\u2581dir\", \"steered\"],\n  [\"ige\", null],\n  [\"ait\", null],\n  [\"\u2581au\", null],\n  [\"\u2581moy\", \"means\"],\n  [\"en\", null],\n  [\"\u2581d\", null],\n  [\"'\", null],\n  [\"une\", \"a\"],\n  [\"\u2581p\", \"paddle\"],\n  [\"aga\", null],\n  [\"ie\", null],\n  [\"\u2581mobile\", \"movable\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nHuit avirons \u00e0 l\u2019avant le faisaient voler \u00e0 la surface des eaux, pendant qu\u2019un homme, assis \u00e0 l\u2019arri\u00e8re, le dirigeait au moyen d\u2019une pagaie mobile.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581H', 'uit', '\u2581av', 'ir', 'ons', '\u2581\u00e0', '\u2581l', '\u2019', 'av', 'ant', '\u2581le', '\u2581fais', 'aient', '\u2581vol', 'er', '\u2581\u00e0', '\u2581la', '\u2581surface', '\u2581des', '\u2581e', 'aux', ',', '\u2581pendant', '\u2581qu', '\u2019', 'un', '\u2581homme', ',', '\u2581ass', 'is', '\u2581\u00e0', '\u2581l', '\u2019', 'ar', 'ri\u00e8re', ',', '\u2581le', '\u2581dir', 'ige', 'ait', '\u2581au', '\u2581moy', 'en', '\u2581d', '\u2019', 'une', '\u2581p', 'aga', 'ie', '\u2581mobile', '.']"}], "[\n  [\"\u2581H\", \"\u0412\u043e\u0441\u0435\u043c\u044c\"],\n  [\"uit\", null],\n  [\"\u2581av\", \"\u0432\u0435\u0441\u0435\u043b\"],\n  [\"ir\", null],\n  [\"ons\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"av\", \"\u0441\u043f\u0435\u0440\u0435\u0434\u0438\"],\n  [\"ant\", null],\n  [\"\u2581le\", null],\n  [\"\u2581fais\", \"\u0437\u0430\u0441\u0442\u0430\u0432\u043b\u044f\u043b\u0438\"],\n  [\"aient\", null],\n  [\"\u2581vol\", \"\u043b\u0435\u0442\u0435\u0442\u044c\"],\n  [\"er\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581la\", null],\n  [\"\u2581surface\", \"\u043f\u043e\u0432\u0435\u0440\u0445\u043d\u043e\u0441\u0442\u0438\"],\n  [\"\u2581des\", null],\n  [\"\u2581e\", \"\u0432\u043e\u0434\u044b\"],\n  [\"aux\", null],\n  [\",\", null],\n  [\"\u2581pendant\", \"\u043f\u043e\u043a\u0430\"],\n  [\"\u2581qu\", null],\n  [\"'\", null],\n  [\"un\", \"\u0447\u0435\u043b\u043e\u0432\u0435\u043a\"],\n  [\"\u2581homme\", null],\n  [\",\", null],\n  [\"\u2581ass\", \"\u0441\u0438\u0434\u044f\u0449\u0438\u0439\"],\n  [\"is\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"ar\", \"\u0441\u0437\u0430\u0434\u0438\"],\n  [\"ri\u00e8re\", null],\n  [\",\", null],\n  [\"\u2581le\", null],\n  [\"\u2581dir\", \"\u0443\u043f\u0440\u0430\u0432\u043b\u044f\u043b\"],\n  [\"ige\", null],\n  [\"ait\", null],\n  [\"\u2581au\", null],\n  [\"\u2581moy\", \"\u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e\"],\n  [\"en\", null],\n  [\"\u2581d\", null],\n  [\"'\", null],\n  [\"une\", null],\n  [\"\u2581p\", \"\u043f\u043e\u0434\u0432\u0438\u0436\u043d\u043e\u0433\u043e\"],\n  [\"aga\", \"\u0432\u0435\u0441\u043b\u0430\"],\n  [\"ie\", null],\n  [\"\u2581mobile\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nLa r\u00e9ponse co\u00efncidait si peu avec la question, que le fou rire recommen\u00e7a \u00e0 circuler, et que messire Robert s\u2019\u00e9cria, rouge de col\u00e8re :\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581La', '\u2581r\u00e9', 'ponse', '\u2581co', '\u00ef', 'nc', 'id', 'ait', '\u2581si', '\u2581peu', '\u2581avec', '\u2581la', '\u2581question', ',', '\u2581que', '\u2581le', '\u2581fou', '\u2581ri', 're', '\u2581recomm', 'en', '\u00e7a', '\u2581\u00e0', '\u2581circul', 'er', ',', '\u2581et', '\u2581que', '\u2581mess', 'ire', '\u2581Robert', '\u2581s', '\u2019', '\u00e9', 'cri', 'a', ',', '\u2581rou', 'ge', '\u2581de', '\u2581col', '\u00e8re', '\u2581:']"}], "[\n[\"\u2581La\", \"The\"],\n[\"\u2581r\u00e9\", \"answer\"],\n[\"ponse\", null],\n[\"\u2581co\", \"coincided\"],\n[\"\u00ef\", null],\n[\"nc\", null],\n[\"id\", null],\n[\"ait\", null],\n[\"\u2581si\", \"so\"],\n[\"\u2581peu\", \"little\"],\n[\"\u2581avec\", \"with\"],\n[\"\u2581la\", \"the\"],\n[\"\u2581question\", \"question\"],\n[\",\", null],\n[\"\u2581que\", \"that\"],\n[\"\u2581le\", \"the\"],\n[\"\u2581fou\", \"mad\"],\n[\"\u2581ri\", \"laughter\"],\n[\"re\", null],\n[\"\u2581recomm\", \"began\"],\n[\"en\", null],\n[\"\u00e7a\", null],\n[\"\u2581\u00e0\", \"to\"],\n[\"\u2581circul\", \"circulate\"],\n[\"er\", null],\n[\",\", null],\n[\"\u2581et\", \"and\"],\n[\"\u2581que\", \"that\"],\n[\"\u2581mess\", \"sir\"],\n[\"ire\", null],\n[\"\u2581Robert\", \"Robert\"],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"\u00e9\", \"exclaimed\"],\n[\"cri\", null],\n[\"a\", null],\n[\",\", null],\n[\"\u2581rou\", \"red\"],\n[\"ge\", null],\n[\"\u2581de\", \"with\"],\n[\"\u2581col\", \"anger\"],\n[\"\u00e8re\", null],\n[\"\u2581:\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nLa r\u00e9ponse co\u00efncidait si peu avec la question, que le fou rire recommen\u00e7a \u00e0 circuler, et que messire Robert s\u2019\u00e9cria, rouge de col\u00e8re :\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581La', '\u2581r\u00e9', 'ponse', '\u2581co', '\u00ef', 'nc', 'id', 'ait', '\u2581si', '\u2581peu', '\u2581avec', '\u2581la', '\u2581question', ',', '\u2581que', '\u2581le', '\u2581fou', '\u2581ri', 're', '\u2581recomm', 'en', '\u00e7a', '\u2581\u00e0', '\u2581circul', 'er', ',', '\u2581et', '\u2581que', '\u2581mess', 'ire', '\u2581Robert', '\u2581s', '\u2019', '\u00e9', 'cri', 'a', ',', '\u2581rou', 'ge', '\u2581de', '\u2581col', '\u00e8re', '\u2581:']"}], "[\n[\"\u2581La\", \"\u041e\u0442\u0432\u0435\u0442\"],\n[\"\u2581r\u00e9\", null],\n[\"ponse\", null],\n[\"\u2581co\", null],\n[\"\u00ef\", null],\n[\"nc\", null],\n[\"id\", null],\n[\"ait\", null],\n[\"\u2581si\", \"\u0442\u0430\u043a\"],\n[\"\u2581peu\", \"\u043c\u0430\u043b\u043e\"],\n[\"\u2581avec\", \"\u0441\"],\n[\"\u2581la\", null],\n[\"\u2581question\", \"\u0432\u043e\u043f\u0440\u043e\u0441\u043e\u043c\"],\n[\",\", null],\n[\"\u2581que\", \"\u0447\u0442\u043e\"],\n[\"\u2581le\", null],\n[\"\u2581fou\", \"\u0431\u0435\u0437\u0443\u043c\u043d\u044b\u0439\"],\n[\"\u2581ri\", null],\n[\"re\", \"\u0441\u043c\u0435\u0445\"],\n[\"\u2581recomm\", null],\n[\"en\", null],\n[\"\u00e7a\", \"\u043d\u0430\u0447\u0430\u043b\u0441\u044f\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581circul\", null],\n[\"er\", \"\u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u044f\u0442\u044c\u0441\u044f\"],\n[\",\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581que\", \"\u0447\u0442\u043e\"],\n[\"\u2581mess\", null],\n[\"ire\", null],\n[\"\u2581Robert\", \"\u0420\u043e\u0431\u0435\u0440\u0442\"],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"\u00e9\", null],\n[\"cri\", null],\n[\"a\", \"\u0432\u043e\u0441\u043a\u043b\u0438\u043a\u043d\u0443\u043b\"],\n[\",\", null],\n[\"\u2581rou\", null],\n[\"ge\", \"\u043a\u0440\u0430\u0441\u043d\u044b\u0439\"],\n[\"\u2581de\", \"\u043e\u0442\"],\n[\"\u2581col\", null],\n[\"\u00e8re\", \"\u0433\u043d\u0435\u0432\u0430\"],\n[\"\u2581:\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nEn 1697, trois b\u00e2timents hollandais de la compagnie des Indes y rel\u00e2ch\u00e8rent, et en d\u00e9termin\u00e8rent les coordonn\u00e9es, laissant au grand astronome Halley le soin de revoir leurs calculs en l\u2019an 1700.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581En', '\u2581', '1', '6', '9', '7', ',', '\u2581trois', '\u2581b', '\u00e2', 'tim', 'ents', '\u2581h', 'oll', 'and', 'ais', '\u2581de', '\u2581la', '\u2581comp', 'agn', 'ie', '\u2581des', '\u2581Ind', 'es', '\u2581y', '\u2581rel', '\u00e2', 'ch', '\u00e8rent', ',', '\u2581et', '\u2581en', '\u2581d\u00e9', 'termin', '\u00e8rent', '\u2581les', '\u2581coord', 'on', 'n\u00e9es', ',', '\u2581la', 'issant', '\u2581au', '\u2581grand', '\u2581astr', 'on', 'ome', '\u2581Hal', 'ley', '\u2581le', '\u2581so', 'in', '\u2581de', '\u2581re', 'voir', '\u2581leurs', '\u2581calcul', 's', '\u2581en', '\u2581l', '\u2019', 'an', '\u2581', '1', '7', '0', '0', '.']"}], "[\n[\"\u2581En\", \"In\"],\n[\"\u2581\", null],\n[\"1\", null],\n[\"6\", null],\n[\"9\", null],\n[\"7\", null],\n[\",\", null],\n[\"\u2581trois\", \"three\"],\n[\"\u2581b\", \"ships\"],\n[\"\u00e2\", null],\n[\"tim\", null],\n[\"ents\", null],\n[\"\u2581h\", \"Dutch\"],\n[\"oll\", null],\n[\"and\", null],\n[\"ais\", null],\n[\"\u2581de\", \"of\"],\n[\"\u2581la\", \"the\"],\n[\"\u2581comp\", \"company\"],\n[\"agn\", null],\n[\"ie\", null],\n[\"\u2581des\", \"of\"],\n[\"\u2581Ind\", \"Indies\"],\n[\"es\", null],\n[\"\u2581y\", \"there\"],\n[\"\u2581rel\", \"stopped\"],\n[\"\u00e2\", null],\n[\"ch\", null],\n[\"\u00e8rent\", null],\n[\",\", null],\n[\"\u2581et\", \"and\"],\n[\"\u2581en\", null],\n[\"\u2581d\u00e9\", \"determined\"],\n[\"termin\", null],\n[\"\u00e8rent\", null],\n[\"\u2581les\", \"the\"],\n[\"\u2581coord\", \"coordinates\"],\n[\"on\", null],\n[\"n\u00e9es\", null],\n[\",\", null],\n[\"\u2581la\", \"leaving\"],\n[\"issant\", null],\n[\"\u2581au\", \"to\"],\n[\"\u2581grand\", \"great\"],\n[\"\u2581astr\", \"astronomer\"],\n[\"on\", null],\n[\"ome\", null],\n[\"\u2581Hal\", \"Halley\"],\n[\"ley\", null],\n[\"\u2581le\", \"the\"],\n[\"\u2581so\", \"task\"],\n[\"in\", null],\n[\"\u2581de\", \"of\"],\n[\"\u2581re\", \"reviewing\"],\n[\"voir\", null],\n[\"\u2581leurs\", \"their\"],\n[\"\u2581calcul\", \"calculations\"],\n[\"s\", null],\n[\"\u2581en\", \"in\"],\n[\"\u2581l\", \"the\"],\n[\"'\", null],\n[\"an\", \"year\"],\n[\"\u2581\", null],\n[\"1\", null],\n[\"7\", null],\n[\"0\", null],\n[\"0\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nEn 1697, trois b\u00e2timents hollandais de la compagnie des Indes y rel\u00e2ch\u00e8rent, et en d\u00e9termin\u00e8rent les coordonn\u00e9es, laissant au grand astronome Halley le soin de revoir leurs calculs en l\u2019an 1700.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581En', '\u2581', '1', '6', '9', '7', ',', '\u2581trois', '\u2581b', '\u00e2', 'tim', 'ents', '\u2581h', 'oll', 'and', 'ais', '\u2581de', '\u2581la', '\u2581comp', 'agn', 'ie', '\u2581des', '\u2581Ind', 'es', '\u2581y', '\u2581rel', '\u00e2', 'ch', '\u00e8rent', ',', '\u2581et', '\u2581en', '\u2581d\u00e9', 'termin', '\u00e8rent', '\u2581les', '\u2581coord', 'on', 'n\u00e9es', ',', '\u2581la', 'issant', '\u2581au', '\u2581grand', '\u2581astr', 'on', 'ome', '\u2581Hal', 'ley', '\u2581le', '\u2581so', 'in', '\u2581de', '\u2581re', 'voir', '\u2581leurs', '\u2581calcul', 's', '\u2581en', '\u2581l', '\u2019', 'an', '\u2581', '1', '7', '0', '0', '.']"}], "[\n[\"\u2581En\", \"\u0412\"],\n[\"\u2581\", null],\n[\"1\", null],\n[\"6\", null],\n[\"9\", null],\n[\"7\", null],\n[\",\", null],\n[\"\u2581trois\", \"\u0442\u0440\u0438\"],\n[\"\u2581b\", \"\u043a\u043e\u0440\u0430\u0431\u043b\u044f\"],\n[\"\u00e2\", null],\n[\"tim\", null],\n[\"ents\", null],\n[\"\u2581h\", \"\u0433\u043e\u043b\u043b\u0430\u043d\u0434\u0441\u043a\u043e\u0439\"],\n[\"oll\", null],\n[\"and\", null],\n[\"ais\", null],\n[\"\u2581de\", null],\n[\"\u2581la\", null],\n[\"\u2581comp\", \"\u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438\"],\n[\"agn\", null],\n[\"ie\", null],\n[\"\u2581des\", null],\n[\"\u2581Ind\", \"\u041e\u0441\u0442-\u0418\u043d\u0434\u0441\u043a\u043e\u0439\"],\n[\"es\", null],\n[\"\u2581y\", \"\u0442\u0430\u043c\"],\n[\"\u2581rel\", \"\u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043b\u0438\u0441\u044c\"],\n[\"\u00e2\", null],\n[\"ch\", null],\n[\"\u00e8rent\", null],\n[\",\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581en\", null],\n[\"\u2581d\u00e9\", \"\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043b\u0438\"],\n[\"termin\", null],\n[\"\u00e8rent\", null],\n[\"\u2581les\", null],\n[\"\u2581coord\", \"\u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u044b\"],\n[\"on\", null],\n[\"n\u00e9es\", null],\n[\",\", null],\n[\"\u2581la\", \"\u043e\u0441\u0442\u0430\u0432\u0438\u0432\"],\n[\"issant\", null],\n[\"\u2581au\", null],\n[\"\u2581grand\", \"\u0432\u0435\u043b\u0438\u043a\u043e\u043c\u0443\"],\n[\"\u2581astr\", \"\u0430\u0441\u0442\u0440\u043e\u043d\u043e\u043c\u0443\"],\n[\"on\", null],\n[\"ome\", null],\n[\"\u2581Hal\", \"\u0413\u0430\u043b\u043b\u0435\u044e\"],\n[\"ley\", null],\n[\"\u2581le\", null],\n[\"\u2581so\", \"\u0437\u0430\u0434\u0430\u0447\u0443\"],\n[\"in\", null],\n[\"\u2581de\", null],\n[\"\u2581re\", \"\u043f\u0435\u0440\u0435\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c\"],\n[\"voir\", null],\n[\"\u2581leurs\", \"\u0438\u0445\"],\n[\"\u2581calcul\", \"\u0440\u0430\u0441\u0447\u0435\u0442\u044b\"],\n[\"s\", null],\n[\"\u2581en\", \"\u0432\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"an\", \"\u0433\u043e\u0434\u0443\"],\n[\"\u2581\", null],\n[\"1\", null],\n[\"7\", null],\n[\"0\", null],\n[\"0\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nIl a l'intention, je crois, de la faire \u00e9lever dans le comt\u00e9 de...\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Il', '\u2581a', '\u2581l', \"'\", 'int', 'ention', ',', '\u2581je', '\u2581cro', 'is', ',', '\u2581de', '\u2581la', '\u2581faire', '\u2581\u00e9', 'le', 'ver', '\u2581dans', '\u2581le', '\u2581com', 't\u00e9', '\u2581de', '...']"}], "[\n  [\"\u2581Il\", \"He\"],\n  [\"\u2581a\", null],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"int\", \"intention\"],\n  [\"ention\", null],\n  [\",\", null],\n  [\"\u2581je\", \"I\"],\n  [\"\u2581cro\", \"believe\"],\n  [\"is\", null],\n  [\",\", null],\n  [\"\u2581de\", null],\n  [\"\u2581la\", null],\n  [\"\u2581faire\", \"have\"],\n  [\"\u2581\u00e9\", \"raised\"],\n  [\"le\", null],\n  [\"ver\", null],\n  [\"\u2581dans\", \"in\"],\n  [\"\u2581le\", null],\n  [\"\u2581com\", \"county\"],\n  [\"t\u00e9\", null],\n  [\"\u2581de\", \"of\"],\n  [\"...\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nIl a l'intention, je crois, de la faire \u00e9lever dans le comt\u00e9 de...\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Il', '\u2581a', '\u2581l', \"'\", 'int', 'ention', ',', '\u2581je', '\u2581cro', 'is', ',', '\u2581de', '\u2581la', '\u2581faire', '\u2581\u00e9', 'le', 'ver', '\u2581dans', '\u2581le', '\u2581com', 't\u00e9', '\u2581de', '...']"}], "[\n  [\"\u2581Il\", \"\u041e\u043d\"],\n  [\"\u2581a\", null],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"int\", \"\u043d\u0430\u043c\u0435\u0440\u0435\u043d\u0438\u0435\"],\n  [\"ention\", null],\n  [\",\", null],\n  [\"\u2581je\", \"\u044f\"],\n  [\"\u2581cro\", \"\u0434\u0443\u043c\u0430\u044e\"],\n  [\"is\", null],\n  [\",\", null],\n  [\"\u2581de\", null],\n  [\"\u2581la\", null],\n  [\"\u2581faire\", \"\u0441\u0434\u0435\u043b\u0430\u0442\u044c\"],\n  [\"\u2581\u00e9\", \"\u0432\u043e\u0441\u043f\u0438\u0442\u0430\u0442\u044c\"],\n  [\"le\", null],\n  [\"ver\", null],\n  [\"\u2581dans\", \"\u0432\"],\n  [\"\u2581le\", null],\n  [\"\u2581com\", \"\u0433\u0440\u0430\u0444\u0441\u0442\u0432\u0435\"],\n  [\"t\u00e9\", null],\n  [\"\u2581de\", null],\n  [\"...\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nIl la pla\u00e7a dans l'esp\u00e8ce de charrette qui devait nous conduire; je montai ensuite, et, avant qu'il referm\u00e2t la porti\u00e8re, je lui demandai \u00e0 quelle distance nous \u00e9tions de Thornfield.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Il', '\u2581la', '\u2581pl', 'a', '\u00e7a', '\u2581dans', '\u2581l', \"'\", 'esp', '\u00e8ce', '\u2581de', '\u2581char', 'ret', 'te', '\u2581qui', '\u2581dev', 'ait', '\u2581nous', '\u2581condu', 'ire', ';', '\u2581je', '\u2581mont', 'ai', '\u2581ensuite', ',', '\u2581et', ',', '\u2581avant', '\u2581qu', \"'\", 'il', '\u2581refer', 'm', '\u00e2t', '\u2581la', '\u2581port', 'i\u00e8re', ',', '\u2581je', '\u2581lui', '\u2581demand', 'ai', '\u2581\u00e0', '\u2581qu', 'elle', '\u2581distance', '\u2581nous', '\u2581\u00e9t', 'ions', '\u2581de', '\u2581Th', 'orn', 'field', '.']"}], "[\n[\"\u2581Il\", \"He\"],\n[\"\u2581la\", null],\n[\"\u2581pl\", \"placed\"],\n[\"a\", null],\n[\"\u00e7a\", null],\n[\"\u2581dans\", \"in\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"esp\", \"kind\"],\n[\"\u00e8ce\", null],\n[\"\u2581de\", \"of\"],\n[\"\u2581char\", \"cart\"],\n[\"ret\", null],\n[\"te\", null],\n[\"\u2581qui\", \"that\"],\n[\"\u2581dev\", \"was\"],\n[\"ait\", null],\n[\"\u2581nous\", \"to\"],\n[\"\u2581condu\", \"take\"],\n[\"ire\", null],\n[\";\", null],\n[\"\u2581je\", \"I\"],\n[\"\u2581mont\", \"got\"],\n[\"ai\", null],\n[\"\u2581ensuite\", \"then\"],\n[\",\", null],\n[\"\u2581et\", \"and\"],\n[\",\", null],\n[\"\u2581avant\", \"before\"],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"il\", \"he\"],\n[\"\u2581refer\", \"closed\"],\n[\"m\", null],\n[\"\u00e2t\", null],\n[\"\u2581la\", \"the\"],\n[\"\u2581port\", \"door\"],\n[\"i\u00e8re\", null],\n[\",\", null],\n[\"\u2581je\", \"I\"],\n[\"\u2581lui\", \"him\"],\n[\"\u2581demand\", \"asked\"],\n[\"ai\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581qu\", \"what\"],\n[\"elle\", null],\n[\"\u2581distance\", \"distance\"],\n[\"\u2581nous\", \"we\"],\n[\"\u2581\u00e9t\", \"were\"],\n[\"ions\", null],\n[\"\u2581de\", \"from\"],\n[\"\u2581Th\", \"Thornfield\"],\n[\"orn\", null],\n[\"field\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nIl la pla\u00e7a dans l'esp\u00e8ce de charrette qui devait nous conduire; je montai ensuite, et, avant qu'il referm\u00e2t la porti\u00e8re, je lui demandai \u00e0 quelle distance nous \u00e9tions de Thornfield.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Il', '\u2581la', '\u2581pl', 'a', '\u00e7a', '\u2581dans', '\u2581l', \"'\", 'esp', '\u00e8ce', '\u2581de', '\u2581char', 'ret', 'te', '\u2581qui', '\u2581dev', 'ait', '\u2581nous', '\u2581condu', 'ire', ';', '\u2581je', '\u2581mont', 'ai', '\u2581ensuite', ',', '\u2581et', ',', '\u2581avant', '\u2581qu', \"'\", 'il', '\u2581refer', 'm', '\u00e2t', '\u2581la', '\u2581port', 'i\u00e8re', ',', '\u2581je', '\u2581lui', '\u2581demand', 'ai', '\u2581\u00e0', '\u2581qu', 'elle', '\u2581distance', '\u2581nous', '\u2581\u00e9t', 'ions', '\u2581de', '\u2581Th', 'orn', 'field', '.']"}], "[\n[\"\u2581Il\", \"\u041e\u043d\"],\n[\"\u2581la\", null],\n[\"\u2581pl\", \"\u043f\u043e\u043c\u0435\u0441\u0442\u0438\u043b\"],\n[\"a\", null],\n[\"\u00e7a\", null],\n[\"\u2581dans\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"esp\", \"\u043f\u043e\u0432\u043e\u0437\u043a\u0443\"],\n[\"\u00e8ce\", null],\n[\"\u2581de\", null],\n[\"\u2581char\", null],\n[\"ret\", null],\n[\"te\", null],\n[\"\u2581qui\", \"\u043a\u043e\u0442\u043e\u0440\u0430\u044f\"],\n[\"\u2581dev\", \"\u0434\u043e\u043b\u0436\u043d\u0430\"],\n[\"ait\", null],\n[\"\u2581nous\", \"\u043d\u0430\u0441\"],\n[\"\u2581condu\", \"\u043e\u0442\u0432\u0435\u0437\u0442\u0438\"],\n[\"ire\", null],\n[\";\", null],\n[\"\u2581je\", \"\u044f\"],\n[\"\u2581mont\", \"\u0441\u0435\u043b\u0430\"],\n[\"ai\", null],\n[\"\u2581ensuite\", \"\u0437\u0430\u0442\u0435\u043c\"],\n[\",\", null],\n[\"\u2581et\", \"\u0438\"],\n[\",\", null],\n[\"\u2581avant\", \"\u043f\u0440\u0435\u0436\u0434\u0435\"],\n[\"\u2581qu\", \"\u0447\u0435\u043c\"],\n[\"'\", null],\n[\"il\", \"\u043e\u043d\"],\n[\"\u2581refer\", \"\u0437\u0430\u043a\u0440\u044b\u043b\"],\n[\"m\", null],\n[\"\u00e2t\", null],\n[\"\u2581la\", null],\n[\"\u2581port\", \"\u0434\u0432\u0435\u0440\u0446\u0443\"],\n[\"i\u00e8re\", null],\n[\",\", null],\n[\"\u2581je\", \"\u044f\"],\n[\"\u2581lui\", \"\u0435\u0433\u043e\"],\n[\"\u2581demand\", \"\u0441\u043f\u0440\u043e\u0441\u0438\u043b\u0430\"],\n[\"ai\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581qu\", \"\u043a\u0430\u043a\u043e\u0435\"],\n[\"elle\", null],\n[\"\u2581distance\", \"\u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0435\"],\n[\"\u2581nous\", \"\u043c\u044b\"],\n[\"\u2581\u00e9t\", \"\u043d\u0430\u0445\u043e\u0434\u0438\u043c\u0441\u044f\"],\n[\"ions\", null],\n[\"\u2581de\", \"\u043e\u0442\"],\n[\"\u2581Th\", \"\u0422\u043e\u0440\u043d\u0444\u0438\u043b\u0434\u0430\"],\n[\"orn\", null],\n[\"field\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nTous ces animaux, achet\u00e9s maigres dans les plaines des montagnes Bleues, allaient s\u2019engraisser au milieu des p\u00e2turages salutaires de l\u2019Australie m\u00e9ridionale, o\u00f9 ils sont revendus avec grand b\u00e9n\u00e9fice. Ainsi, Sam Machell, gagnant deux livres par boeuf et une demi-livre par mouton, devait r\u00e9aliser un b\u00e9n\u00e9fice de cinquante mille francs.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581T', 'ous', '\u2581ces', '\u2581anim', 'aux', ',', '\u2581a', 'chet', '\u00e9s', '\u2581ma', 'ig', 'res', '\u2581dans', '\u2581les', '\u2581plain', 'es', '\u2581des', '\u2581mont', 'agn', 'es', '\u2581B', 'le', 'ues', ',', '\u2581alla', 'ient', '\u2581s', '\u2019', 'eng', 'rais', 'ser', '\u2581au', '\u2581mil', 'ieu', '\u2581des', '\u2581p', '\u00e2t', 'ur', 'ages', '\u2581sal', 'ut', 'aires', '\u2581de', '\u2581l', '\u2019', 'Austral', 'ie', '\u2581m', '\u00e9r', 'id', 'ionale', ',', '\u2581o\u00f9', '\u2581ils', '\u2581sont', '\u2581rev', 'end', 'us', '\u2581avec', '\u2581grand', '\u2581b', '\u00e9n', '\u00e9', 'f', 'ice', '.', '\u2581A', 'insi', ',', '\u2581Sam', '\u2581M', 'ache', 'll', ',', '\u2581g', 'agn', 'ant', '\u2581deux', '\u2581liv', 'res', '\u2581par', '\u2581bo', 'e', 'uf', '\u2581et', '\u2581une', '\u2581dem', 'i', '-', 'liv', 're', '\u2581par', '\u2581m', 'out', 'on', ',', '\u2581dev', 'ait', '\u2581r\u00e9al', 'iser', '\u2581un', '\u2581b', '\u00e9n', '\u00e9', 'f', 'ice', '\u2581de', '\u2581cin', 'qu', 'ante', '\u2581mil', 'le', '\u2581franc', 's', '.']"}], "[\n[\"\u2581T\", \"All\"],\n[\"ous\", null],\n[\"\u2581ces\", \"these\"],\n[\"\u2581anim\", \"animals\"],\n[\"aux\", null],\n[\",\", null],\n[\"\u2581a\", null],\n[\"chet\", \"bought\"],\n[\"\u00e9s\", null],\n[\"\u2581ma\", \"lean\"],\n[\"ig\", null],\n[\"res\", null],\n[\"\u2581dans\", \"in\"],\n[\"\u2581les\", \"the\"],\n[\"\u2581plain\", \"plains\"],\n[\"es\", null],\n[\"\u2581des\", \"of\"],\n[\"\u2581mont\", \"Blue\"],\n[\"agn\", null],\n[\"es\", null],\n[\"\u2581B\", null],\n[\"le\", null],\n[\"ues\", null],\n[\",\", null],\n[\"\u2581alla\", \"were\"],\n[\"ient\", null],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"eng\", \"fattening\"],\n[\"rais\", null],\n[\"ser\", null],\n[\"\u2581au\", \"in\"],\n[\"\u2581mil\", \"middle\"],\n[\"ieu\", null],\n[\"\u2581des\", \"of\"],\n[\"\u2581p\", \"pastures\"],\n[\"\u00e2t\", null],\n[\"ur\", null],\n[\"ages\", null],\n[\"\u2581sal\", \"healthy\"],\n[\"ut\", null],\n[\"aires\", null],\n[\"\u2581de\", \"of\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"Austral\", \"Southern\"],\n[\"ie\", null],\n[\"\u2581m\", null],\n[\"\u00e9r\", null],\n[\"id\", null],\n[\"ionale\", null],\n[\",\", null],\n[\"\u2581o\u00f9\", \"where\"],\n[\"\u2581ils\", \"they\"],\n[\"\u2581sont\", \"are\"],\n[\"\u2581rev\", \"resold\"],\n[\"end\", null],\n[\"us\", null],\n[\"\u2581avec\", \"with\"],\n[\"\u2581grand\", \"great\"],\n[\"\u2581b\", \"profit\"],\n[\"\u00e9n\", null],\n[\"\u00e9\", null],\n[\"f\", null],\n[\"ice\", null],\n[\".\", null],\n[\"\u2581A\", \"Thus\"],\n[\"insi\", null],\n[\",\", null],\n[\"\u2581Sam\", \"Sam\"],\n[\"\u2581M\", \"Machell\"],\n[\"ache\", null],\n[\"ll\", null],\n[\",\", null],\n[\"\u2581g\", \"earning\"],\n[\"agn\", null],\n[\"ant\", null],\n[\"\u2581deux\", \"two\"],\n[\"\u2581liv\", \"pounds\"],\n[\"res\", null],\n[\"\u2581par\", \"per\"],\n[\"\u2581bo\", \"ox\"],\n[\"e\", null],\n[\"uf\", null],\n[\"\u2581et\", \"and\"],\n[\"\u2581une\", \"half\"],\n[\"\u2581dem\", null],\n[\"i\", null],\n[\"-\", null],\n[\"liv\", \"pound\"],\n[\"re\", null],\n[\"\u2581par\", \"per\"],\n[\"\u2581m\", \"sheep\"],\n[\"out\", null],\n[\"on\", null],\n[\",\", null],\n[\"\u2581dev\", \"was\"],\n[\"ait\", null],\n[\"\u2581r\u00e9al\", \"realize\"],\n[\"iser\", null],\n[\"\u2581un\", \"a\"],\n[\"\u2581b\", \"profit\"],\n[\"\u00e9n\", null],\n[\"\u00e9\", null],\n[\"f\", null],\n[\"ice\", null],\n[\"\u2581de\", \"of\"],\n[\"\u2581cin\", \"fifty\"],\n[\"qu\", null],\n[\"ante\", null],\n[\"\u2581mil\", \"thousand\"],\n[\"le\", null],\n[\"\u2581franc\", \"francs\"],\n[\"s\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nTous ces animaux, achet\u00e9s maigres dans les plaines des montagnes Bleues, allaient s\u2019engraisser au milieu des p\u00e2turages salutaires de l\u2019Australie m\u00e9ridionale, o\u00f9 ils sont revendus avec grand b\u00e9n\u00e9fice. Ainsi, Sam Machell, gagnant deux livres par boeuf et une demi-livre par mouton, devait r\u00e9aliser un b\u00e9n\u00e9fice de cinquante mille francs.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581T', 'ous', '\u2581ces', '\u2581anim', 'aux', ',', '\u2581a', 'chet', '\u00e9s', '\u2581ma', 'ig', 'res', '\u2581dans', '\u2581les', '\u2581plain', 'es', '\u2581des', '\u2581mont', 'agn', 'es', '\u2581B', 'le', 'ues', ',', '\u2581alla', 'ient', '\u2581s', '\u2019', 'eng', 'rais', 'ser', '\u2581au', '\u2581mil', 'ieu', '\u2581des', '\u2581p', '\u00e2t', 'ur', 'ages', '\u2581sal', 'ut', 'aires', '\u2581de', '\u2581l', '\u2019', 'Austral', 'ie', '\u2581m', '\u00e9r', 'id', 'ionale', ',', '\u2581o\u00f9', '\u2581ils', '\u2581sont', '\u2581rev', 'end', 'us', '\u2581avec', '\u2581grand', '\u2581b', '\u00e9n', '\u00e9', 'f', 'ice', '.', '\u2581A', 'insi', ',', '\u2581Sam', '\u2581M', 'ache', 'll', ',', '\u2581g', 'agn', 'ant', '\u2581deux', '\u2581liv', 'res', '\u2581par', '\u2581bo', 'e', 'uf', '\u2581et', '\u2581une', '\u2581dem', 'i', '-', 'liv', 're', '\u2581par', '\u2581m', 'out', 'on', ',', '\u2581dev', 'ait', '\u2581r\u00e9al', 'iser', '\u2581un', '\u2581b', '\u00e9n', '\u00e9', 'f', 'ice', '\u2581de', '\u2581cin', 'qu', 'ante', '\u2581mil', 'le', '\u2581franc', 's', '.']"}], "[\n[\"\u2581T\", \"\u0412\u0441\u0435\"],\n[\"ous\", null],\n[\"\u2581ces\", \"\u044d\u0442\u0438\"],\n[\"\u2581anim\", \"\u0436\u0438\u0432\u043e\u0442\u043d\u044b\u0435\"],\n[\"aux\", null],\n[\",\", null],\n[\"\u2581a\", null],\n[\"chet\", null],\n[\"\u00e9s\", null],\n[\"\u2581ma\", \"\u0445\u0443\u0434\u044b\u0435\"],\n[\"ig\", null],\n[\"res\", null],\n[\"\u2581dans\", \"\u0432\"],\n[\"\u2581les\", null],\n[\"\u2581plain\", \"\u0440\u0430\u0432\u043d\u0438\u043d\u0430\u0445\"],\n[\"es\", null],\n[\"\u2581des\", null],\n[\"\u2581mont\", \"\u0413\u043e\u043b\u0443\u0431\u044b\u0445\"],\n[\"agn\", null],\n[\"es\", null],\n[\"\u2581B\", null],\n[\"le\", null],\n[\"ues\", null],\n[\",\", null],\n[\"\u2581alla\", \"\u043e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u043b\u0438\u0441\u044c\"],\n[\"ient\", null],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"eng\", \"\u043e\u0442\u043a\u0430\u0440\u043c\u043b\u0438\u0432\u0430\u0442\u044c\u0441\u044f\"],\n[\"rais\", null],\n[\"ser\", null],\n[\"\u2581au\", \"\u0432\"],\n[\"\u2581mil\", \"\u0441\u0435\u0440\u0435\u0434\u0438\u043d\u0435\"],\n[\"ieu\", null],\n[\"\u2581des\", null],\n[\"\u2581p\", \"\u0446\u0435\u043b\u0435\u0431\u043d\u044b\u0445\"],\n[\"\u00e2t\", null],\n[\"ur\", null],\n[\"ages\", null],\n[\"\u2581sal\", null],\n[\"ut\", null],\n[\"aires\", null],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"Austral\", \"\u042e\u0436\u043d\u043e\u0439\"],\n[\"ie\", null],\n[\"\u2581m\", null],\n[\"\u00e9r\", null],\n[\"id\", null],\n[\"ionale\", null],\n[\",\", null],\n[\"\u2581o\u00f9\", \"\u0433\u0434\u0435\"],\n[\"\u2581ils\", \"\u0438\u0445\"],\n[\"\u2581sont\", null],\n[\"\u2581rev\", \"\u043f\u0435\u0440\u0435\u043f\u0440\u043e\u0434\u0430\u0432\u0430\u043b\u0438\"],\n[\"end\", null],\n[\"us\", null],\n[\"\u2581avec\", \"\u0441\"],\n[\"\u2581grand\", \"\u0431\u043e\u043b\u044c\u0448\u043e\u0439\"],\n[\"\u2581b\", \"\u0432\u044b\u0433\u043e\u0434\u043e\u0439\"],\n[\"\u00e9n\", null],\n[\"\u00e9\", null],\n[\"f\", null],\n[\"ice\", null],\n[\".\", null],\n[\"\u2581A\", \"\u0422\u0430\u043a\u0438\u043c\"],\n[\"insi\", null],\n[\",\", null],\n[\"\u2581Sam\", \"\u0421\u044d\u043c\"],\n[\"\u2581M\", \"\u041c\u0430\u0447\u0435\u043b\u043b\"],\n[\"ache\", null],\n[\"ll\", null],\n[\",\", null],\n[\"\u2581g\", \"\u0437\u0430\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u044f\"],\n[\"agn\", null],\n[\"ant\", null],\n[\"\u2581deux\", \"\u0434\u0432\u0430\"],\n[\"\u2581liv\", \"\u0444\u0443\u043d\u0442\u0430\"],\n[\"res\", null],\n[\"\u2581par\", \"\u043d\u0430\"],\n[\"\u2581bo\", \"\u0431\u044b\u043a\u0435\"],\n[\"e\", null],\n[\"uf\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581une\", \"\u043f\u043e\u043b\u0444\u0443\u043d\u0442\u0430\"],\n[\"\u2581dem\", null],\n[\"i\", null],\n[\"-\", null],\n[\"liv\", null],\n[\"re\", null],\n[\"\u2581par\", \"\u043d\u0430\"],\n[\"\u2581m\", \"\u043e\u0432\u0446\u0435\"],\n[\"out\", null],\n[\"on\", null],\n[\",\", null],\n[\"\u2581dev\", \"\u0434\u043e\u043b\u0436\u0435\u043d\"],\n[\"ait\", null],\n[\"\u2581r\u00e9al\", \"\u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c\"],\n[\"iser\", null],\n[\"\u2581un\", null],\n[\"\u2581b\", \"\u043f\u0440\u0438\u0431\u044b\u043b\u044c\"],\n[\"\u00e9n\", null],\n[\"\u00e9\", null],\n[\"f\", null],\n[\"ice\", null],\n[\"\u2581de\", \"\u0432\"],\n[\"\u2581cin\", \"\u043f\u044f\u0442\u044c\u0434\u0435\u0441\u044f\u0442\"],\n[\"qu\", null],\n[\"ante\", null],\n[\"\u2581mil\", \"\u0442\u044b\u0441\u044f\u0447\"],\n[\"le\", null],\n[\"\u2581franc\", \"\u0444\u0440\u0430\u043d\u043a\u043e\u0432\"],\n[\"s\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nGr\u00e8ve, aboye, Gr\u00e8ve, grouille !File, file, ma quenouille,File sa corde au bourreau Qui siffle dans le pr\u00e9au.Gr\u00e8ve, aboye, Gr\u00e8ve, grouille !\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Gr', '\u00e8ve', ',', '\u2581ab', 'oy', 'e', ',', '\u2581Gr', '\u00e8ve', ',', '\u2581g', 'rou', 'ille', '\u2581!', 'File', ',', '\u2581file', ',', '\u2581ma', '\u2581qu', 'en', 'ou', 'ille', ',', 'File', '\u2581sa', '\u2581cor', 'de', '\u2581au', '\u2581bour', 're', 'au', '\u2581Q', 'ui', '\u2581s', 'iff', 'le', '\u2581dans', '\u2581le', '\u2581pr\u00e9', 'au', '.', 'Gr', '\u00e8ve', ',', '\u2581ab', 'oy', 'e', ',', '\u2581Gr', '\u00e8ve', ',', '\u2581g', 'rou', 'ille', '\u2581!']"}], "[\n  [\"\u2581Gr\", \"Strike\"],\n  [\"\u00e8ve\", null],\n  [\",\", null],\n  [\"\u2581ab\", \"bark\"],\n  [\"oy\", null],\n  [\"e\", null],\n  [\",\", null],\n  [\"\u2581Gr\", \"Strike\"],\n  [\"\u00e8ve\", null],\n  [\",\", null],\n  [\"\u2581g\", \"swarm\"],\n  [\"rou\", null],\n  [\"ille\", null],\n  [\"\u2581!\", null],\n  [\"File\", \"Spin\"],\n  [\",\", null],\n  [\"\u2581file\", \"spin\"],\n  [\",\", null],\n  [\"\u2581ma\", \"my\"],\n  [\"\u2581qu\", \"distaff\"],\n  [\"en\", null],\n  [\"ou\", null],\n  [\"ille\", null],\n  [\",\", null],\n  [\"File\", \"Spin\"],\n  [\"\u2581sa\", \"his\"],\n  [\"\u2581cor\", \"rope\"],\n  [\"de\", null],\n  [\"\u2581au\", \"for\"],\n  [\"\u2581bour\", \"executioner\"],\n  [\"re\", null],\n  [\"au\", null],\n  [\"\u2581Q\", \"Who\"],\n  [\"ui\", null],\n  [\"\u2581s\", \"whistles\"],\n  [\"iff\", null],\n  [\"le\", null],\n  [\"\u2581dans\", \"in\"],\n  [\"\u2581le\", \"the\"],\n  [\"\u2581pr\u00e9\", \"courtyard\"],\n  [\"au\", null],\n  [\".\", null],\n  [\"Gr\", \"Strike\"],\n  [\"\u00e8ve\", null],\n  [\",\", null],\n  [\"\u2581ab\", \"bark\"],\n  [\"oy\", null],\n  [\"e\", null],\n  [\",\", null],\n  [\"\u2581Gr\", \"Strike\"],\n  [\"\u00e8ve\", null],\n  [\",\", null],\n  [\"\u2581g\", \"swarm\"],\n  [\"rou\", null],\n  [\"ille\", null],\n  [\"\u2581!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nGr\u00e8ve, aboye, Gr\u00e8ve, grouille !File, file, ma quenouille,File sa corde au bourreau Qui siffle dans le pr\u00e9au.Gr\u00e8ve, aboye, Gr\u00e8ve, grouille !\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Gr', '\u00e8ve', ',', '\u2581ab', 'oy', 'e', ',', '\u2581Gr', '\u00e8ve', ',', '\u2581g', 'rou', 'ille', '\u2581!', 'File', ',', '\u2581file', ',', '\u2581ma', '\u2581qu', 'en', 'ou', 'ille', ',', 'File', '\u2581sa', '\u2581cor', 'de', '\u2581au', '\u2581bour', 're', 'au', '\u2581Q', 'ui', '\u2581s', 'iff', 'le', '\u2581dans', '\u2581le', '\u2581pr\u00e9', 'au', '.', 'Gr', '\u00e8ve', ',', '\u2581ab', 'oy', 'e', ',', '\u2581Gr', '\u00e8ve', ',', '\u2581g', 'rou', 'ille', '\u2581!']"}], "[\n  [\"\u2581Gr\", \"\u0417\u0430\u0431\u0430\u0441\u0442\u043e\u0432\u043a\u0430\"],\n  [\"\u00e8ve\", null],\n  [\",\", null],\n  [\"\u2581ab\", \"\u043b\u0430\u0439\"],\n  [\"oy\", null],\n  [\"e\", null],\n  [\",\", null],\n  [\"\u2581Gr\", \"\u0417\u0430\u0431\u0430\u0441\u0442\u043e\u0432\u043a\u0430\"],\n  [\"\u00e8ve\", null],\n  [\",\", null],\n  [\"\u2581g\", \"\u043a\u043e\u043f\u043e\u0448\u0438\u0441\u044c\"],\n  [\"rou\", null],\n  [\"ille\", null],\n  [\"\u2581!\", null],\n  [\"File\", \"\u041f\u0440\u044f\u0434\u0438\u0441\u044c\"],\n  [\",\", null],\n  [\"\u2581file\", \"\u043f\u0440\u044f\u0434\u0438\u0441\u044c\"],\n  [\",\", null],\n  [\"\u2581ma\", \"\u043c\u043e\u044f\"],\n  [\"\u2581qu\", \"\u043f\u0440\u044f\u043b\u043a\u0430\"],\n  [\"en\", null],\n  [\"ou\", null],\n  [\"ille\", null],\n  [\",\", null],\n  [\"File\", \"\u041f\u0440\u044f\u0434\u0438\"],\n  [\"\u2581sa\", \"\u0435\u0433\u043e\"],\n  [\"\u2581cor\", \"\u0432\u0435\u0440\u0435\u0432\u043a\u0443\"],\n  [\"de\", null],\n  [\"\u2581au\", null],\n  [\"\u2581bour\", \"\u043f\u0430\u043b\u0430\u0447\u0443\"],\n  [\"re\", null],\n  [\"au\", null],\n  [\"\u2581Q\", \"\u041a\u043e\u0442\u043e\u0440\u044b\u0439\"],\n  [\"ui\", null],\n  [\"\u2581s\", \"\u0441\u0432\u0438\u0441\u0442\u0438\u0442\"],\n  [\"iff\", null],\n  [\"le\", null],\n  [\"\u2581dans\", \"\u0432\u043e\"],\n  [\"\u2581le\", null],\n  [\"\u2581pr\u00e9\", \"\u0434\u0432\u043e\u0440\u0435\"],\n  [\"au\", null],\n  [\".\", null],\n  [\"Gr\", \"\u0417\u0430\u0431\u0430\u0441\u0442\u043e\u0432\u043a\u0430\"],\n  [\"\u00e8ve\", null],\n  [\",\", null],\n  [\"\u2581ab\", \"\u043b\u0430\u0439\"],\n  [\"oy\", null],\n  [\"e\", null],\n  [\",\", null],\n  [\"\u2581Gr\", \"\u0417\u0430\u0431\u0430\u0441\u0442\u043e\u0432\u043a\u0430\"],\n  [\"\u00e8ve\", null],\n  [\",\", null],\n  [\"\u2581g\", \"\u043a\u043e\u043f\u043e\u0448\u0438\u0441\u044c\"],\n  [\"rou\", null],\n  [\"ille\", null],\n  [\"\u2581!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nQuand elle eut un enfant, il le fallut mettre en nourrice. Rentr\u00e9 chez eux, le marmot fut g\u00e2t\u00e9 comme un prince.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Qu', 'and', '\u2581elle', '\u2581e', 'ut', '\u2581un', '\u2581enf', 'ant', ',', '\u2581il', '\u2581le', '\u2581fall', 'ut', '\u2581met', 'tre', '\u2581en', '\u2581n', 'our', 'rice', '.', '\u2581R', 'ent', 'r\u00e9', '\u2581chez', '\u2581e', 'ux', ',', '\u2581le', '\u2581m', 'arm', 'ot', '\u2581fut', '\u2581g', '\u00e2', 't\u00e9', '\u2581comme', '\u2581un', '\u2581prince', '.']"}], "[\n    [\"\u2581Qu\", \"When\"],\n    [\"and\", null],\n    [\"\u2581elle\", \"she\"],\n    [\"\u2581e\", null],\n    [\"ut\", null],\n    [\"\u2581un\", \"a\"],\n    [\"\u2581enf\", \"child\"],\n    [\"ant\", null],\n    [\",\", null],\n    [\"\u2581il\", \"it\"],\n    [\"\u2581le\", null],\n    [\"\u2581fall\", \"was necessary\"],\n    [\"ut\", null],\n    [\"\u2581met\", \"put\"],\n    [\"tre\", null],\n    [\"\u2581en\", \"with\"],\n    [\"\u2581n\", \"wet nurse\"],\n    [\"our\", null],\n    [\"rice\", null],\n    [\".\", null],\n    [\"\u2581R\", \"Returned\"],\n    [\"ent\", null],\n    [\"r\u00e9\", null],\n    [\"\u2581chez\", \"home\"],\n    [\"\u2581e\", null],\n    [\"ux\", null],\n    [\",\", null],\n    [\"\u2581le\", \"the\"],\n    [\"\u2581m\", \"baby\"],\n    [\"arm\", null],\n    [\"ot\", null],\n    [\"\u2581fut\", \"was\"],\n    [\"\u2581g\", \"spoiled\"],\n    [\"\u00e2\", null],\n    [\"t\u00e9\", null],\n    [\"\u2581comme\", \"like\"],\n    [\"\u2581un\", \"a\"],\n    [\"\u2581prince\", \"prince\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nQuand elle eut un enfant, il le fallut mettre en nourrice. Rentr\u00e9 chez eux, le marmot fut g\u00e2t\u00e9 comme un prince.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Qu', 'and', '\u2581elle', '\u2581e', 'ut', '\u2581un', '\u2581enf', 'ant', ',', '\u2581il', '\u2581le', '\u2581fall', 'ut', '\u2581met', 'tre', '\u2581en', '\u2581n', 'our', 'rice', '.', '\u2581R', 'ent', 'r\u00e9', '\u2581chez', '\u2581e', 'ux', ',', '\u2581le', '\u2581m', 'arm', 'ot', '\u2581fut', '\u2581g', '\u00e2', 't\u00e9', '\u2581comme', '\u2581un', '\u2581prince', '.']"}], "[\n[\"\u2581Qu\", \"\u041a\u043e\u0433\u0434\u0430\"],\n[\"and\", null],\n[\"\u2581elle\", \"\u043e\u043d\u0430\"],\n[\"\u2581e\", null],\n[\"ut\", null],\n[\"\u2581un\", null],\n[\"\u2581enf\", \"\u0440\u0435\u0431\u0435\u043d\u043e\u043a\"],\n[\"ant\", null],\n[\",\", null],\n[\"\u2581il\", null],\n[\"\u2581le\", null],\n[\"\u2581fall\", \"\u043f\u0440\u0438\u0448\u043b\u043e\u0441\u044c\"],\n[\"ut\", null],\n[\"\u2581met\", \"\u043e\u0442\u0434\u0430\u0442\u044c\"],\n[\"tre\", null],\n[\"\u2581en\", null],\n[\"\u2581n\", \"\u043a\u043e\u0440\u043c\u0438\u043b\u0438\u0446\u0430\"],\n[\"our\", null],\n[\"rice\", null],\n[\".\", null],\n[\"\u2581R\", \"\u0412\u0435\u0440\u043d\u0443\u0432\u0448\u0438\u0441\u044c\"],\n[\"ent\", null],\n[\"r\u00e9\", null],\n[\"\u2581chez\", \"\u0434\u043e\u043c\u043e\u0439\"],\n[\"\u2581e\", null],\n[\"ux\", null],\n[\",\", null],\n[\"\u2581le\", null],\n[\"\u2581m\", \"\u043c\u0430\u043b\u044b\u0448\"],\n[\"arm\", null],\n[\"ot\", null],\n[\"\u2581fut\", null],\n[\"\u2581g\", \"\u0438\u0437\u0431\u0430\u043b\u043e\u0432\u0430\u043b\u0438\"],\n[\"\u00e2\", null],\n[\"t\u00e9\", null],\n[\"\u2581comme\", \"\u043a\u0430\u043a\"],\n[\"\u2581un\", null],\n[\"\u2581prince\", \"\u043f\u0440\u0438\u043d\u0446\u0430\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n\u00ab Voil\u00e0 une histoire bien vraisemblable ! \u00bb dit le Pigeon d\u2019un air de profond m\u00e9pris. \u00ab J\u2019ai vu bien des petites filles dans mon temps, mais je n\u2019en ai jamais vu avec un cou comme cela.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00ab', '\u2581Vo', 'il', '\u00e0', '\u2581une', '\u2581hist', 'oire', '\u2581bien', '\u2581v', 'rais', 'em', 'bl', 'able', '\u2581!', '\u2581\u00bb', '\u2581dit', '\u2581le', '\u2581P', 'ige', 'on', '\u2581d', '\u2019', 'un', '\u2581air', '\u2581de', '\u2581prof', 'ond', '\u2581m\u00e9', 'pr', 'is', '.', '\u2581\u00ab', '\u2581J', '\u2019', 'ai', '\u2581vu', '\u2581bien', '\u2581des', '\u2581pet', 'ites', '\u2581fill', 'es', '\u2581dans', '\u2581mon', '\u2581temps', ',', '\u2581mais', '\u2581je', '\u2581n', '\u2019', 'en', '\u2581ai', '\u2581jamais', '\u2581vu', '\u2581avec', '\u2581un', '\u2581cou', '\u2581comme', '\u2581c', 'ela', '.']"}], "[\n[\"\u2581\u00ab\", null],\n[\"\u2581Vo\", \"There\"],\n[\"il\", null],\n[\"\u00e0\", null],\n[\"\u2581une\", \"a\"],\n[\"\u2581hist\", \"story\"],\n[\"oire\", null],\n[\"\u2581bien\", \"very\"],\n[\"\u2581v\", \"likely\"],\n[\"rais\", null],\n[\"em\", null],\n[\"bl\", null],\n[\"able\", null],\n[\"\u2581!\", null],\n[\"\u2581\u00bb\", null],\n[\"\u2581dit\", \"said\"],\n[\"\u2581le\", \"the\"],\n[\"\u2581P\", \"Pigeon\"],\n[\"ige\", null],\n[\"on\", null],\n[\"\u2581d\", \"with\"],\n[\"'\", null],\n[\"un\", \"a\"],\n[\"\u2581air\", \"air\"],\n[\"\u2581de\", \"of\"],\n[\"\u2581prof\", \"deep\"],\n[\"ond\", null],\n[\"\u2581m\u00e9\", \"contempt\"],\n[\"pr\", null],\n[\"is\", null],\n[\".\", null],\n[\"\u2581\u00ab\", null],\n[\"\u2581J\", \"I\"],\n[\"'\", null],\n[\"ai\", \"have\"],\n[\"\u2581vu\", \"seen\"],\n[\"\u2581bien\", \"many\"],\n[\"\u2581des\", null],\n[\"\u2581pet\", \"little\"],\n[\"ites\", null],\n[\"\u2581fill\", \"girls\"],\n[\"es\", null],\n[\"\u2581dans\", \"in\"],\n[\"\u2581mon\", \"my\"],\n[\"\u2581temps\", \"time\"],\n[\",\", null],\n[\"\u2581mais\", \"but\"],\n[\"\u2581je\", \"I\"],\n[\"\u2581n\", \"have\"],\n[\"'\", null],\n[\"en\", null],\n[\"\u2581ai\", null],\n[\"\u2581jamais\", \"never\"],\n[\"\u2581vu\", \"seen\"],\n[\"\u2581avec\", \"with\"],\n[\"\u2581un\", \"a\"],\n[\"\u2581cou\", \"neck\"],\n[\"\u2581comme\", \"like\"],\n[\"\u2581c\", \"that\"],\n[\"ela\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n\u00ab Voil\u00e0 une histoire bien vraisemblable ! \u00bb dit le Pigeon d\u2019un air de profond m\u00e9pris. \u00ab J\u2019ai vu bien des petites filles dans mon temps, mais je n\u2019en ai jamais vu avec un cou comme cela.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00ab', '\u2581Vo', 'il', '\u00e0', '\u2581une', '\u2581hist', 'oire', '\u2581bien', '\u2581v', 'rais', 'em', 'bl', 'able', '\u2581!', '\u2581\u00bb', '\u2581dit', '\u2581le', '\u2581P', 'ige', 'on', '\u2581d', '\u2019', 'un', '\u2581air', '\u2581de', '\u2581prof', 'ond', '\u2581m\u00e9', 'pr', 'is', '.', '\u2581\u00ab', '\u2581J', '\u2019', 'ai', '\u2581vu', '\u2581bien', '\u2581des', '\u2581pet', 'ites', '\u2581fill', 'es', '\u2581dans', '\u2581mon', '\u2581temps', ',', '\u2581mais', '\u2581je', '\u2581n', '\u2019', 'en', '\u2581ai', '\u2581jamais', '\u2581vu', '\u2581avec', '\u2581un', '\u2581cou', '\u2581comme', '\u2581c', 'ela', '.']"}], "[\n[\"\u2581\u00ab\", null],\n[\"\u2581Vo\", \"\u0412\u043e\u0442\"],\n[\"il\", null],\n[\"\u00e0\", null],\n[\"\u2581une\", null],\n[\"\u2581hist\", \"\u0438\u0441\u0442\u043e\u0440\u0438\u044f\"],\n[\"oire\", null],\n[\"\u2581bien\", null],\n[\"\u2581v\", \"\u043f\u0440\u0430\u0432\u0434\u043e\u043f\u043e\u0434\u043e\u0431\u043d\u0430\u044f\"],\n[\"rais\", null],\n[\"em\", null],\n[\"bl\", null],\n[\"able\", null],\n[\"\u2581!\", null],\n[\"\u2581\u00bb\", null],\n[\"\u2581dit\", \"\u0441\u043a\u0430\u0437\u0430\u043b\"],\n[\"\u2581le\", null],\n[\"\u2581P\", \"\u0413\u043e\u043b\u0443\u0431\u044c\"],\n[\"ige\", null],\n[\"on\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"un\", null],\n[\"\u2581air\", \"\u0441 \u0432\u0438\u0434\u043e\u043c\"],\n[\"\u2581de\", null],\n[\"\u2581prof\", \"\u0433\u043b\u0443\u0431\u043e\u043a\u043e\u0433\u043e\"],\n[\"ond\", null],\n[\"\u2581m\u00e9\", \"\u043f\u0440\u0435\u0437\u0440\u0435\u043d\u0438\u044f\"],\n[\"pr\", null],\n[\"is\", null],\n[\".\", null],\n[\"\u2581\u00ab\", null],\n[\"\u2581J\", \"\u042f\"],\n[\"'\", null],\n[\"ai\", null],\n[\"\u2581vu\", \"\u0432\u0438\u0434\u0435\u043b\"],\n[\"\u2581bien\", null],\n[\"\u2581des\", null],\n[\"\u2581pet\", \"\u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0438\u0445\"],\n[\"ites\", null],\n[\"\u2581fill\", \"\u0434\u0435\u0432\u043e\u0447\u0435\u043a\"],\n[\"es\", null],\n[\"\u2581dans\", \"\u0437\u0430\"],\n[\"\u2581mon\", \"\u0441\u0432\u043e\u0435\"],\n[\"\u2581temps\", \"\u0432\u0440\u0435\u043c\u044f\"],\n[\",\", null],\n[\"\u2581mais\", \"\u043d\u043e\"],\n[\"\u2581je\", \"\u044f\"],\n[\"\u2581n\", \"\u043d\u0438\u043a\u043e\u0433\u0434\u0430\"],\n[\"'\", null],\n[\"en\", null],\n[\"\u2581ai\", null],\n[\"\u2581jamais\", null],\n[\"\u2581vu\", \"\u043d\u0435 \u0432\u0438\u0434\u0435\u043b\"],\n[\"\u2581avec\", \"\u0441\"],\n[\"\u2581un\", null],\n[\"\u2581cou\", \"\u0448\u0435\u0435\u0439\"],\n[\"\u2581comme\", \"\u043a\u0430\u043a\"],\n[\"\u2581c\", \"\u044d\u0442\u0430\"],\n[\"ela\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nLe salon \u00e9tait une petite pi\u00e8ce simplement meubl\u00e9e, mais propre et confortable.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Le', '\u2581sal', 'on', '\u2581\u00e9tait', '\u2581une', '\u2581pet', 'ite', '\u2581pi', '\u00e8ce', '\u2581simple', 'ment', '\u2581me', 'ubl', '\u00e9e', ',', '\u2581mais', '\u2581pro', 'pre', '\u2581et', '\u2581conf', 'ort', 'able', '.']"}], "[\n  [\"\u2581Le\", \"The\"],\n  [\"\u2581sal\", \"living room\"],\n  [\"on\", null],\n  [\"\u2581\u00e9tait\", \"was\"],\n  [\"\u2581une\", \"a\"],\n  [\"\u2581pet\", \"small\"],\n  [\"ite\", null],\n  [\"\u2581pi\", \"room\"],\n  [\"\u00e8ce\", null],\n  [\"\u2581simple\", \"simply\"],\n  [\"ment\", null],\n  [\"\u2581me\", \"furnished\"],\n  [\"ubl\", null],\n  [\"\u00e9e\", null],\n  [\",\", null],\n  [\"\u2581mais\", \"but\"],\n  [\"\u2581pro\", \"clean\"],\n  [\"pre\", null],\n  [\"\u2581et\", \"and\"],\n  [\"\u2581conf\", \"comfortable\"],\n  [\"ort\", null],\n  [\"able\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nLe salon \u00e9tait une petite pi\u00e8ce simplement meubl\u00e9e, mais propre et confortable.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Le', '\u2581sal', 'on', '\u2581\u00e9tait', '\u2581une', '\u2581pet', 'ite', '\u2581pi', '\u00e8ce', '\u2581simple', 'ment', '\u2581me', 'ubl', '\u00e9e', ',', '\u2581mais', '\u2581pro', 'pre', '\u2581et', '\u2581conf', 'ort', 'able', '.']"}], "[\n[\"\u2581Le\", null],\n[\"\u2581sal\", \"\u0433\u043e\u0441\u0442\u0438\u043d\u0430\u044f\"],\n[\"on\", null],\n[\"\u2581\u00e9tait\", \"\u0431\u044b\u043b\u0430\"],\n[\"\u2581une\", null],\n[\"\u2581pet\", \"\u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0430\u044f\"],\n[\"ite\", null],\n[\"\u2581pi\", \"\u043a\u043e\u043c\u043d\u0430\u0442\u0430\"],\n[\"\u00e8ce\", null],\n[\"\u2581simple\", \"\u043f\u0440\u043e\u0441\u0442\u043e\"],\n[\"ment\", null],\n[\"\u2581me\", \"\u043c\u0435\u0431\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u0430\u044f\"],\n[\"ubl\", null],\n[\"\u00e9e\", null],\n[\",\", null],\n[\"\u2581mais\", null],\n[\"\u2581pro\", \"\u0447\u0438\u0441\u0442\u0430\u044f\"],\n[\"pre\", null],\n[\"\u2581et\", null],\n[\"\u2581conf\", \"\u0443\u044e\u0442\u043d\u0430\u044f\"],\n[\"ort\", null],\n[\"able\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nSi vous saviez que je n\u2019ai qu\u2019elle, que je suis vieille, que c\u2019est une b\u00e9n\u00e9diction que la sainte Vierge m\u2019envoie.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Si', '\u2581vous', '\u2581sav', 'ie', 'z', '\u2581que', '\u2581je', '\u2581n', '\u2019', 'ai', '\u2581qu', '\u2019', 'elle', ',', '\u2581que', '\u2581je', '\u2581su', 'is', '\u2581vie', 'ille', ',', '\u2581que', '\u2581c', '\u2019', 'est', '\u2581une', '\u2581b', '\u00e9n', '\u00e9d', 'iction', '\u2581que', '\u2581la', '\u2581s', 'ainte', '\u2581V', 'ier', 'ge', '\u2581m', '\u2019', 'en', 'vo', 'ie', '.']"}], "[\n[\"\u2581Si\", \"If\"],\n[\"\u2581vous\", null],\n[\"\u2581sav\", \"knew\"],\n[\"ie\", null],\n[\"z\", null],\n[\"\u2581que\", null],\n[\"\u2581je\", null],\n[\"\u2581n\", null],\n[\"'\", null],\n[\"ai\", null],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"elle\", \"her\"],\n[\",\", null],\n[\"\u2581que\", null],\n[\"\u2581je\", null],\n[\"\u2581su\", \"am\"],\n[\"is\", null],\n[\"\u2581vie\", \"old\"],\n[\"ille\", null],\n[\",\", null],\n[\"\u2581que\", null],\n[\"\u2581c\", null],\n[\"'\", null],\n[\"est\", null],\n[\"\u2581une\", null],\n[\"\u2581b\", \"blessing\"],\n[\"\u00e9n\", null],\n[\"\u00e9d\", null],\n[\"iction\", null],\n[\"\u2581que\", null],\n[\"\u2581la\", null],\n[\"\u2581s\", \"Holy\"],\n[\"ainte\", null],\n[\"\u2581V\", \"Virgin\"],\n[\"ier\", null],\n[\"ge\", null],\n[\"\u2581m\", null],\n[\"'\", null],\n[\"en\", \"sends\"],\n[\"vo\", null],\n[\"ie\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nSi vous saviez que je n\u2019ai qu\u2019elle, que je suis vieille, que c\u2019est une b\u00e9n\u00e9diction que la sainte Vierge m\u2019envoie.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Si', '\u2581vous', '\u2581sav', 'ie', 'z', '\u2581que', '\u2581je', '\u2581n', '\u2019', 'ai', '\u2581qu', '\u2019', 'elle', ',', '\u2581que', '\u2581je', '\u2581su', 'is', '\u2581vie', 'ille', ',', '\u2581que', '\u2581c', '\u2019', 'est', '\u2581une', '\u2581b', '\u00e9n', '\u00e9d', 'iction', '\u2581que', '\u2581la', '\u2581s', 'ainte', '\u2581V', 'ier', 'ge', '\u2581m', '\u2019', 'en', 'vo', 'ie', '.']"}], "[\n[\"\u2581Si\", \"\u0415\u0441\u043b\u0438\"],\n[\"\u2581vous\", null],\n[\"\u2581sav\", \"\u0437\u043d\u0430\u043b\u0438\"],\n[\"ie\", null],\n[\"z\", null],\n[\"\u2581que\", null],\n[\"\u2581je\", null],\n[\"\u2581n\", null],\n[\"'\", null],\n[\"ai\", null],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"elle\", \"\u043e\u043d\u0430\"],\n[\",\", null],\n[\"\u2581que\", null],\n[\"\u2581je\", null],\n[\"\u2581su\", \"\u044f\u0432\u043b\u044f\u044e\u0441\u044c\"],\n[\"is\", null],\n[\"\u2581vie\", \"\u0441\u0442\u0430\u0440\u043e\u0439\"],\n[\"ille\", null],\n[\",\", null],\n[\"\u2581que\", null],\n[\"\u2581c\", null],\n[\"'\", null],\n[\"est\", null],\n[\"\u2581une\", null],\n[\"\u2581b\", \"\u0431\u043b\u0430\u0433\u043e\u0441\u043b\u043e\u0432\u0435\u043d\u0438\u0435\"],\n[\"\u00e9n\", null],\n[\"\u00e9d\", null],\n[\"iction\", null],\n[\"\u2581que\", null],\n[\"\u2581la\", null],\n[\"\u2581s\", \"\u0441\u0432\u044f\u0442\u0430\u044f\"],\n[\"ainte\", null],\n[\"\u2581V\", \"\u0414\u0435\u0432\u0430\"],\n[\"ier\", null],\n[\"ge\", null],\n[\"\u2581m\", null],\n[\"'\", null],\n[\"en\", \"\u043f\u043e\u0441\u044b\u043b\u0430\u0435\u0442\"],\n[\"vo\", null],\n[\"ie\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nAu del\u00e0 verdoyaient les pr\u00e9s, au del\u00e0 s\u2019enfuyaient les routes, le long desquelles tra\u00eenaient encore quelques maisons de faubourg, d\u2019autant plus rares qu\u2019elles s\u2019\u00e9loignaient plus. Quelques-uns de ces faubourgs avaient de l\u2019importance.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Au', '\u2581del', '\u00e0', '\u2581verd', 'oy', 'aient', '\u2581les', '\u2581pr\u00e9s', ',', '\u2581au', '\u2581del', '\u00e0', '\u2581s', '\u2019', 'en', 'f', 'uy', 'aient', '\u2581les', '\u2581routes', ',', '\u2581le', '\u2581long', '\u2581des', 'qu', 'elles', '\u2581tra', '\u00ee', 'na', 'ient', '\u2581encore', '\u2581quelques', '\u2581mais', 'ons', '\u2581de', '\u2581fa', 'ub', 'our', 'g', ',', '\u2581d', '\u2019', 'aut', 'ant', '\u2581plus', '\u2581r', 'ares', '\u2581qu', '\u2019', 'elles', '\u2581s', '\u2019', '\u00e9', 'lo', 'ign', 'aient', '\u2581plus', '.', '\u2581Qu', 'el', 'ques', '-', 'uns', '\u2581de', '\u2581ces', '\u2581fa', 'ub', 'our', 'gs', '\u2581av', 'aient', '\u2581de', '\u2581l', '\u2019', 'import', 'ance', '.']"}], "[\n[\"\u2581Au\", \"Beyond\"],\n[\"\u2581del\", null],\n[\"\u00e0\", null],\n[\"\u2581verd\", \"green\"],\n[\"oy\", null],\n[\"aient\", null],\n[\"\u2581les\", null],\n[\"\u2581pr\u00e9s\", \"meadows\"],\n[\",\", null],\n[\"\u2581au\", null],\n[\"\u2581del\", null],\n[\"\u00e0\", null],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"en\", null],\n[\"f\", \"stretched\"],\n[\"uy\", null],\n[\"aient\", null],\n[\"\u2581les\", null],\n[\"\u2581routes\", \"roads\"],\n[\",\", null],\n[\"\u2581le\", null],\n[\"\u2581long\", \"along\"],\n[\"\u2581des\", null],\n[\"qu\", null],\n[\"elles\", null],\n[\"\u2581tra\", \"lingered\"],\n[\"\u00ee\", null],\n[\"na\", null],\n[\"ient\", null],\n[\"\u2581encore\", \"still\"],\n[\"\u2581quelques\", \"some\"],\n[\"\u2581mais\", \"houses\"],\n[\"ons\", null],\n[\"\u2581de\", null],\n[\"\u2581fa\", \"suburb\"],\n[\"ub\", null],\n[\"our\", null],\n[\"g\", null],\n[\",\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"aut\", \"increasingly\"],\n[\"ant\", null],\n[\"\u2581plus\", null],\n[\"\u2581r\", \"rare\"],\n[\"ares\", null],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"elles\", null],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"\u00e9\", \"moved\"],\n[\"lo\", null],\n[\"ign\", null],\n[\"aient\", null],\n[\"\u2581plus\", \"further\"],\n[\".\", null],\n[\"\u2581Qu\", \"Some\"],\n[\"el\", null],\n[\"ques\", null],\n[\"-\", null],\n[\"uns\", null],\n[\"\u2581de\", null],\n[\"\u2581ces\", null],\n[\"\u2581fa\", \"suburbs\"],\n[\"ub\", null],\n[\"our\", null],\n[\"gs\", null],\n[\"\u2581av\", \"had\"],\n[\"aient\", null],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"import\", \"importance\"],\n[\"ance\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nAu del\u00e0 verdoyaient les pr\u00e9s, au del\u00e0 s\u2019enfuyaient les routes, le long desquelles tra\u00eenaient encore quelques maisons de faubourg, d\u2019autant plus rares qu\u2019elles s\u2019\u00e9loignaient plus. Quelques-uns de ces faubourgs avaient de l\u2019importance.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Au', '\u2581del', '\u00e0', '\u2581verd', 'oy', 'aient', '\u2581les', '\u2581pr\u00e9s', ',', '\u2581au', '\u2581del', '\u00e0', '\u2581s', '\u2019', 'en', 'f', 'uy', 'aient', '\u2581les', '\u2581routes', ',', '\u2581le', '\u2581long', '\u2581des', 'qu', 'elles', '\u2581tra', '\u00ee', 'na', 'ient', '\u2581encore', '\u2581quelques', '\u2581mais', 'ons', '\u2581de', '\u2581fa', 'ub', 'our', 'g', ',', '\u2581d', '\u2019', 'aut', 'ant', '\u2581plus', '\u2581r', 'ares', '\u2581qu', '\u2019', 'elles', '\u2581s', '\u2019', '\u00e9', 'lo', 'ign', 'aient', '\u2581plus', '.', '\u2581Qu', 'el', 'ques', '-', 'uns', '\u2581de', '\u2581ces', '\u2581fa', 'ub', 'our', 'gs', '\u2581av', 'aient', '\u2581de', '\u2581l', '\u2019', 'import', 'ance', '.']"}], "[\n[\"\u2581Au\", \"\u0417\u0430\"],\n[\"\u2581del\", null],\n[\"\u00e0\", null],\n[\"\u2581verd\", \"\u0437\u0435\u043b\u0435\u043d\u0435\u043b\u0438\"],\n[\"oy\", null],\n[\"aient\", null],\n[\"\u2581les\", null],\n[\"\u2581pr\u00e9s\", \"\u043b\u0443\u0433\u0430\"],\n[\",\", null],\n[\"\u2581au\", null],\n[\"\u2581del\", null],\n[\"\u00e0\", null],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"en\", null],\n[\"f\", \"\u0443\u0431\u0435\u0433\u0430\u043b\u0438\"],\n[\"uy\", null],\n[\"aient\", null],\n[\"\u2581les\", null],\n[\"\u2581routes\", \"\u0434\u043e\u0440\u043e\u0433\u0438\"],\n[\",\", null],\n[\"\u2581le\", null],\n[\"\u2581long\", \"\u0432\u0434\u043e\u043b\u044c\"],\n[\"\u2581des\", null],\n[\"qu\", null],\n[\"elles\", null],\n[\"\u2581tra\", \"\u0442\u044f\u043d\u0443\u043b\u0438\u0441\u044c\"],\n[\"\u00ee\", null],\n[\"na\", null],\n[\"ient\", null],\n[\"\u2581encore\", \"\u0435\u0449\u0435\"],\n[\"\u2581quelques\", \"\u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e\"],\n[\"\u2581mais\", \"\u0434\u043e\u043c\u043e\u0432\"],\n[\"ons\", null],\n[\"\u2581de\", null],\n[\"\u2581fa\", \"\u043f\u0440\u0438\u0433\u043e\u0440\u043e\u0434\u0430\"],\n[\"ub\", null],\n[\"our\", null],\n[\"g\", null],\n[\",\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"aut\", \"\u0442\u0435\u043c\"],\n[\"ant\", null],\n[\"\u2581plus\", \"\u0431\u043e\u043b\u0435\u0435\"],\n[\"\u2581r\", \"\u0440\u0435\u0434\u043a\u0438\u0435\"],\n[\"ares\", null],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"elles\", null],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"\u00e9\", \"\u0443\u0434\u0430\u043b\u044f\u043b\u0438\u0441\u044c\"],\n[\"lo\", null],\n[\"ign\", null],\n[\"aient\", null],\n[\"\u2581plus\", \"\u0434\u0430\u043b\u044c\u0448\u0435\"],\n[\".\", null],\n[\"\u2581Qu\", \"\u041d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435\"],\n[\"el\", null],\n[\"ques\", null],\n[\"-\", null],\n[\"uns\", null],\n[\"\u2581de\", null],\n[\"\u2581ces\", \"\u0438\u0437 \u044d\u0442\u0438\u0445\"],\n[\"\u2581fa\", \"\u043f\u0440\u0438\u0433\u043e\u0440\u043e\u0434\u043e\u0432\"],\n[\"ub\", null],\n[\"our\", null],\n[\"gs\", null],\n[\"\u2581av\", \"\u0438\u043c\u0435\u043b\u0438\"],\n[\"aient\", null],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"import\", \"\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\"],\n[\"ance\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n\u00c0 la Wimerra, il a communiqu\u00e9 avec l\u2019un des siens, le forgeron de Black-Point, et a laiss\u00e9 des traces reconnaissables de notre passage.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00c0', '\u2581la', '\u2581W', 'imer', 'ra', ',', '\u2581il', '\u2581a', '\u2581commun', 'iqu', '\u00e9', '\u2581avec', '\u2581l', '\u2019', 'un', '\u2581des', '\u2581si', 'ens', ',', '\u2581le', '\u2581for', 'ger', 'on', '\u2581de', '\u2581Black', '-', 'Point', ',', '\u2581et', '\u2581a', '\u2581la', 'iss', '\u00e9', '\u2581des', '\u2581traces', '\u2581recon', 'na', 'iss', 'ables', '\u2581de', '\u2581not', 're', '\u2581passage', '.']"}], "[\n[\"\u2581\u00c0\", \"At\"],\n[\"\u2581la\", null],\n[\"\u2581W\", \"Wimerra\"],\n[\"imer\", null],\n[\"ra\", null],\n[\",\", null],\n[\"\u2581il\", null],\n[\"\u2581a\", null],\n[\"\u2581commun\", \"communicated\"],\n[\"iqu\", null],\n[\"\u00e9\", null],\n[\"\u2581avec\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"un\", null],\n[\"\u2581des\", null],\n[\"\u2581si\", \"his\"],\n[\"ens\", null],\n[\",\", null],\n[\"\u2581le\", null],\n[\"\u2581for\", \"blacksmith\"],\n[\"ger\", null],\n[\"on\", null],\n[\"\u2581de\", null],\n[\"\u2581Black\", \"Black\"],\n[\"-\", null],\n[\"Point\", \"Point\"],\n[\",\", null],\n[\"\u2581et\", null],\n[\"\u2581a\", null],\n[\"\u2581la\", \"left\"],\n[\"iss\", null],\n[\"\u00e9\", null],\n[\"\u2581des\", null],\n[\"\u2581traces\", \"traces\"],\n[\"\u2581recon\", \"recognizable\"],\n[\"na\", null],\n[\"iss\", null],\n[\"ables\", null],\n[\"\u2581de\", null],\n[\"\u2581not\", \"our\"],\n[\"re\", null],\n[\"\u2581passage\", \"passage\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n\u00c0 la Wimerra, il a communiqu\u00e9 avec l\u2019un des siens, le forgeron de Black-Point, et a laiss\u00e9 des traces reconnaissables de notre passage.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00c0', '\u2581la', '\u2581W', 'imer', 'ra', ',', '\u2581il', '\u2581a', '\u2581commun', 'iqu', '\u00e9', '\u2581avec', '\u2581l', '\u2019', 'un', '\u2581des', '\u2581si', 'ens', ',', '\u2581le', '\u2581for', 'ger', 'on', '\u2581de', '\u2581Black', '-', 'Point', ',', '\u2581et', '\u2581a', '\u2581la', 'iss', '\u00e9', '\u2581des', '\u2581traces', '\u2581recon', 'na', 'iss', 'ables', '\u2581de', '\u2581not', 're', '\u2581passage', '.']"}], "[\n  [\"\u2581\u00c0\", \"\u0412\"],\n  [\"\u2581la\", null],\n  [\"\u2581W\", \"\u0412\u0438\u043c\u0435\u0440\u0440\u0435\"],\n  [\"imer\", null],\n  [\"ra\", null],\n  [\",\", null],\n  [\"\u2581il\", \"\u043e\u043d\"],\n  [\"\u2581a\", null],\n  [\"\u2581commun\", \"\u043e\u0431\u0449\u0430\u043b\u0441\u044f\"],\n  [\"iqu\", null],\n  [\"\u00e9\", null],\n  [\"\u2581avec\", \"\u0441\"],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"un\", \"\u043e\u0434\u043d\u0438\u043c\"],\n  [\"\u2581des\", \"\u0438\u0437\"],\n  [\"\u2581si\", \"\u0441\u0432\u043e\u0438\u0445\"],\n  [\"ens\", null],\n  [\",\", null],\n  [\"\u2581le\", null],\n  [\"\u2581for\", \"\u043a\u0443\u0437\u043d\u0435\u0446\u043e\u043c\"],\n  [\"ger\", null],\n  [\"on\", null],\n  [\"\u2581de\", \"\u0438\u0437\"],\n  [\"\u2581Black\", \"\u0411\u043b\u044d\u043a\"],\n  [\"-\", null],\n  [\"Point\", \"\u041f\u043e\u0439\u043d\u0442\u0430\"],\n  [\",\", null],\n  [\"\u2581et\", \"\u0438\"],\n  [\"\u2581a\", null],\n  [\"\u2581la\", \"\u043e\u0441\u0442\u0430\u0432\u0438\u043b\"],\n  [\"iss\", null],\n  [\"\u00e9\", null],\n  [\"\u2581des\", null],\n  [\"\u2581traces\", \"\u0441\u043b\u0435\u0434\u044b\"],\n  [\"\u2581recon\", \"\u0443\u0437\u043d\u0430\u0432\u0430\u0435\u043c\u044b\u0435\"],\n  [\"na\", null],\n  [\"iss\", null],\n  [\"ables\", null],\n  [\"\u2581de\", null],\n  [\"\u2581not\", \"\u043d\u0430\u0448\u0435\u0433\u043e\"],\n  [\"re\", null],\n  [\"\u2581passage\", \"\u043f\u0440\u043e\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nMonsieur l\u2019archev\u00eaque s\u2019est int\u00e9ress\u00e9 \u00e0 l\u2019enfant d\u2019\u00c9gypte, l\u2019a exorcis\u00e9, l\u2019a b\u00e9ni, lui a \u00f4t\u00e9 bien soigneusement le diable du corps, et l\u2019a envoy\u00e9 \u00e0 Paris pour \u00eatre expos\u00e9 sur le lit de bois, \u00e0 Notre-Dame, comme enfant trouv\u00e9.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Mons', 'ieur', '\u2581l', '\u2019', 'ar', 'che', 'v', '\u00ea', 'que', '\u2581s', '\u2019', 'est', '\u2581int', '\u00e9', 'ress', '\u00e9', '\u2581\u00e0', '\u2581l', '\u2019', 'en', 'f', 'ant', '\u2581d', '\u2019', '\u00c9', 'g', 'yp', 'te', ',', '\u2581l', '\u2019', 'a', '\u2581ex', 'or', 'c', 'is\u00e9', ',', '\u2581l', '\u2019', 'a', '\u2581b', '\u00e9n', 'i', ',', '\u2581lui', '\u2581a', '\u2581', '\u00f4', 't\u00e9', '\u2581bien', '\u2581so', 'igne', 'usement', '\u2581le', '\u2581di', 'able', '\u2581du', '\u2581corps', ',', '\u2581et', '\u2581l', '\u2019', 'a', '\u2581env', 'oy', '\u00e9', '\u2581\u00e0', '\u2581Paris', '\u2581pour', '\u2581\u00eatre', '\u2581expos', '\u00e9', '\u2581sur', '\u2581le', '\u2581lit', '\u2581de', '\u2581bo', 'is', ',', '\u2581\u00e0', '\u2581Not', 're', '-', 'D', 'ame', ',', '\u2581comme', '\u2581enf', 'ant', '\u2581trou', 'v\u00e9', '.']"}], "[\n[\"\u2581Mons\", \"Mister\"],\n[\"ieur\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"ar\", null],\n[\"che\", null],\n[\"v\", null],\n[\"\u00ea\", null],\n[\"que\", null],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"est\", null],\n[\"\u2581int\", null],\n[\"\u00e9\", null],\n[\"ress\", null],\n[\"\u00e9\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"en\", null],\n[\"f\", null],\n[\"ant\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"\u00c9\", null],\n[\"g\", null],\n[\"yp\", null],\n[\"te\", null],\n[\",\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"a\", null],\n[\"\u2581ex\", null],\n[\"or\", null],\n[\"c\", null],\n[\"is\u00e9\", null],\n[\",\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"a\", null],\n[\"\u2581b\", null],\n[\"\u00e9n\", null],\n[\"i\", null],\n[\",\", null],\n[\"\u2581lui\", \"him\"],\n[\"\u2581a\", null],\n[\"\u2581\", null],\n[\"\u00f4\", null],\n[\"t\u00e9\", null],\n[\"\u2581bien\", \"very\"],\n[\"\u2581so\", null],\n[\"igne\", null],\n[\"usement\", null],\n[\"\u2581le\", null],\n[\"\u2581di\", null],\n[\"able\", null],\n[\"\u2581du\", null],\n[\"\u2581corps\", \"body\"],\n[\",\", null],\n[\"\u2581et\", \"and\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"a\", null],\n[\"\u2581env\", null],\n[\"oy\", null],\n[\"\u00e9\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581Paris\", \"Paris\"],\n[\"\u2581pour\", \"to\"],\n[\"\u2581\u00eatre\", \"be\"],\n[\"\u2581expos\", null],\n[\"\u00e9\", null],\n[\"\u2581sur\", \"on\"],\n[\"\u2581le\", null],\n[\"\u2581lit\", \"bed\"],\n[\"\u2581de\", null],\n[\"\u2581bo\", null],\n[\"is\", null],\n[\",\", null],\n[\"\u2581\u00e0\", \"at\"],\n[\"\u2581Not\", null],\n[\"re\", null],\n[\"-\", null],\n[\"D\", null],\n[\"ame\", null],\n[\",\", null],\n[\"\u2581comme\", \"as\"],\n[\"\u2581enf\", null],\n[\"ant\", null],\n[\"\u2581trou\", null],\n[\"v\u00e9\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nMonsieur l\u2019archev\u00eaque s\u2019est int\u00e9ress\u00e9 \u00e0 l\u2019enfant d\u2019\u00c9gypte, l\u2019a exorcis\u00e9, l\u2019a b\u00e9ni, lui a \u00f4t\u00e9 bien soigneusement le diable du corps, et l\u2019a envoy\u00e9 \u00e0 Paris pour \u00eatre expos\u00e9 sur le lit de bois, \u00e0 Notre-Dame, comme enfant trouv\u00e9.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Mons', 'ieur', '\u2581l', '\u2019', 'ar', 'che', 'v', '\u00ea', 'que', '\u2581s', '\u2019', 'est', '\u2581int', '\u00e9', 'ress', '\u00e9', '\u2581\u00e0', '\u2581l', '\u2019', 'en', 'f', 'ant', '\u2581d', '\u2019', '\u00c9', 'g', 'yp', 'te', ',', '\u2581l', '\u2019', 'a', '\u2581ex', 'or', 'c', 'is\u00e9', ',', '\u2581l', '\u2019', 'a', '\u2581b', '\u00e9n', 'i', ',', '\u2581lui', '\u2581a', '\u2581', '\u00f4', 't\u00e9', '\u2581bien', '\u2581so', 'igne', 'usement', '\u2581le', '\u2581di', 'able', '\u2581du', '\u2581corps', ',', '\u2581et', '\u2581l', '\u2019', 'a', '\u2581env', 'oy', '\u00e9', '\u2581\u00e0', '\u2581Paris', '\u2581pour', '\u2581\u00eatre', '\u2581expos', '\u00e9', '\u2581sur', '\u2581le', '\u2581lit', '\u2581de', '\u2581bo', 'is', ',', '\u2581\u00e0', '\u2581Not', 're', '-', 'D', 'ame', ',', '\u2581comme', '\u2581enf', 'ant', '\u2581trou', 'v\u00e9', '.']"}], "[\n  [\"\u2581Mons\", \"\u0413\u043e\u0441\u043f\u043e\u0434\u0438\u043d\"],\n  [\"ieur\", null],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"ar\", null],\n  [\"che\", null],\n  [\"v\", null],\n  [\"\u00ea\", null],\n  [\"que\", null],\n  [\"\u2581s\", null],\n  [\"'\", null],\n  [\"est\", null],\n  [\"\u2581int\", null],\n  [\"\u00e9\", null],\n  [\"ress\", null],\n  [\"\u00e9\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"en\", null],\n  [\"f\", null],\n  [\"ant\", null],\n  [\"\u2581d\", null],\n  [\"'\", null],\n  [\"\u00c9\", null],\n  [\"g\", null],\n  [\"yp\", null],\n  [\"te\", \"\u0415\u0433\u0438\u043f\u0442\u0430\"],\n  [\",\", null],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"a\", null],\n  [\"\u2581ex\", null],\n  [\"or\", null],\n  [\"c\", null],\n  [\"is\u00e9\", \"\u0438\u0437\u0433\u043d\u0430\u043b\"],\n  [\",\", null],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"a\", null],\n  [\"\u2581b\", null],\n  [\"\u00e9n\", null],\n  [\"i\", \"\u0431\u043b\u0430\u0433\u043e\u0441\u043b\u043e\u0432\u0438\u043b\"],\n  [\",\", null],\n  [\"\u2581lui\", \"\u0435\u0433\u043e\"],\n  [\"\u2581a\", null],\n  [\"\u2581\", null],\n  [\"\u00f4\", null],\n  [\"t\u00e9\", \"\u0438\u0437\u0431\u0430\u0432\u0438\u043b\"],\n  [\"\u2581bien\", \"\u0442\u0449\u0430\u0442\u0435\u043b\u044c\u043d\u043e\"],\n  [\"\u2581so\", null],\n  [\"igne\", null],\n  [\"usement\", null],\n  [\"\u2581le\", null],\n  [\"\u2581di\", null],\n  [\"able\", \"\u0434\u044c\u044f\u0432\u043e\u043b\u0430\"],\n  [\"\u2581du\", null],\n  [\"\u2581corps\", \"\u0442\u0435\u043b\u0430\"],\n  [\",\", null],\n  [\"\u2581et\", \"\u0438\"],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"a\", null],\n  [\"\u2581env\", null],\n  [\"oy\", null],\n  [\"\u00e9\", \"\u043e\u0442\u043f\u0440\u0430\u0432\u0438\u043b\"],\n  [\"\u2581\u00e0\", \"\u0432\"],\n  [\"\u2581Paris\", \"\u041f\u0430\u0440\u0438\u0436\"],\n  [\"\u2581pour\", \"\u0447\u0442\u043e\u0431\u044b\"],\n  [\"\u2581\u00eatre\", null],\n  [\"\u2581expos\", null],\n  [\"\u00e9\", \"\u0432\u044b\u0441\u0442\u0430\u0432\u0438\u0442\u044c\"],\n  [\"\u2581sur\", \"\u043d\u0430\"],\n  [\"\u2581le\", null],\n  [\"\u2581lit\", \"\u043a\u0440\u043e\u0432\u0430\u0442\u0438\"],\n  [\"\u2581de\", null],\n  [\"\u2581bo\", null],\n  [\"is\", \"\u0434\u0435\u0440\u0435\u0432\u044f\u043d\u043d\u043e\u0439\"],\n  [\",\", null],\n  [\"\u2581\u00e0\", \"\u0432\"],\n  [\"\u2581Not\", null],\n  [\"re\", null],\n  [\"-\", null],\n  [\"D\", null],\n  [\"ame\", \"\u041d\u043e\u0442\u0440-\u0414\u0430\u043c\"],\n  [\",\", null],\n  [\"\u2581comme\", \"\u043a\u0430\u043a\"],\n  [\"\u2581enf\", null],\n  [\"ant\", \"\u0440\u0435\u0431\u0435\u043d\u043a\u0430\"],\n  [\"\u2581trou\", null],\n  [\"v\u00e9\", \"\u043f\u043e\u0434\u043a\u0438\u0434\u044b\u0448\u0430\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nDes cygnes \u00e0 t\u00eate noire s\u2019y \u00e9battaient capricieusement et disputaient l\u2019empire des eaux \u00e0 de nombreuses autruches qui gambadaient \u00e0 travers les ilanos.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Des', '\u2581cy', 'gn', 'es', '\u2581\u00e0', '\u2581t\u00eate', '\u2581no', 'ire', '\u2581s', '\u2019', 'y', '\u2581\u00e9', 'b', 'atta', 'ient', '\u2581cap', 'ric', 'ie', 'usement', '\u2581et', '\u2581disput', 'aient', '\u2581l', '\u2019', 'emp', 'ire', '\u2581des', '\u2581e', 'aux', '\u2581\u00e0', '\u2581de', '\u2581nombre', 'uses', '\u2581aut', 'ru', 'ches', '\u2581qui', '\u2581g', 'amb', 'ada', 'ient', '\u2581\u00e0', '\u2581travers', '\u2581les', '\u2581il', 'anos', '.']"}], "[\n  [\"\u2581Des\", null],\n  [\"\u2581cy\", \"Black\"],\n  [\"gn\", null],\n  [\"es\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581t\u00eate\", \"headed\"],\n  [\"\u2581no\", null],\n  [\"ire\", null],\n  [\"\u2581s\", null],\n  [\"'\", null],\n  [\"y\", null],\n  [\"\u2581\u00e9\", \"frolicked\"],\n  [\"b\", null],\n  [\"atta\", null],\n  [\"ient\", null],\n  [\"\u2581cap\", \"capriciously\"],\n  [\"ric\", null],\n  [\"ie\", null],\n  [\"usement\", null],\n  [\"\u2581et\", null],\n  [\"\u2581disput\", \"vied\"],\n  [\"aient\", null],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"emp\", \"control\"],\n  [\"ire\", null],\n  [\"\u2581des\", null],\n  [\"\u2581e\", \"waters\"],\n  [\"aux\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581de\", null],\n  [\"\u2581nombre\", \"numerous\"],\n  [\"uses\", null],\n  [\"\u2581aut\", \"ostriches\"],\n  [\"ru\", null],\n  [\"ches\", null],\n  [\"\u2581qui\", null],\n  [\"\u2581g\", \"frolicked\"],\n  [\"amb\", null],\n  [\"ada\", null],\n  [\"ient\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581travers\", \"across\"],\n  [\"\u2581les\", null],\n  [\"\u2581il\", \"llanos\"],\n  [\"anos\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nDes cygnes \u00e0 t\u00eate noire s\u2019y \u00e9battaient capricieusement et disputaient l\u2019empire des eaux \u00e0 de nombreuses autruches qui gambadaient \u00e0 travers les ilanos.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Des', '\u2581cy', 'gn', 'es', '\u2581\u00e0', '\u2581t\u00eate', '\u2581no', 'ire', '\u2581s', '\u2019', 'y', '\u2581\u00e9', 'b', 'atta', 'ient', '\u2581cap', 'ric', 'ie', 'usement', '\u2581et', '\u2581disput', 'aient', '\u2581l', '\u2019', 'emp', 'ire', '\u2581des', '\u2581e', 'aux', '\u2581\u00e0', '\u2581de', '\u2581nombre', 'uses', '\u2581aut', 'ru', 'ches', '\u2581qui', '\u2581g', 'amb', 'ada', 'ient', '\u2581\u00e0', '\u2581travers', '\u2581les', '\u2581il', 'anos', '.']"}], "[\n[\"\u2581Des\", null],\n[\"\u2581cy\", \"\u0427\u0435\u0440\u043d\u043e\u0448\u0435\u0438\u0435\"],\n[\"gn\", null],\n[\"es\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581t\u00eate\", null],\n[\"\u2581no\", null],\n[\"ire\", null],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"y\", null],\n[\"\u2581\u00e9\", \"\u0440\u0435\u0437\u0432\u0438\u043b\u0438\u0441\u044c\"],\n[\"b\", null],\n[\"atta\", null],\n[\"ient\", null],\n[\"\u2581cap\", null],\n[\"ric\", null],\n[\"ie\", null],\n[\"usement\", null],\n[\"\u2581et\", null],\n[\"\u2581disput\", \"\u043e\u0441\u043f\u0430\u0440\u0438\u0432\u0430\u043b\u0438\"],\n[\"aient\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"emp\", \"\u0433\u043e\u0441\u043f\u043e\u0434\u0441\u0442\u0432\u043e\"],\n[\"ire\", null],\n[\"\u2581des\", null],\n[\"\u2581e\", \"\u0432\u043e\u0434\"],\n[\"aux\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581de\", null],\n[\"\u2581nombre\", \"\u043c\u043d\u043e\u0433\u043e\u0447\u0438\u0441\u043b\u0435\u043d\u043d\u044b\u0445\"],\n[\"uses\", null],\n[\"\u2581aut\", \"\u0441\u0442\u0440\u0430\u0443\u0441\u043e\u0432\"],\n[\"ru\", null],\n[\"ches\", null],\n[\"\u2581qui\", null],\n[\"\u2581g\", \"\u0440\u0435\u0437\u0432\u0438\u043b\u0438\u0441\u044c\"],\n[\"amb\", null],\n[\"ada\", null],\n[\"ient\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581travers\", \"\u043f\u043e\"],\n[\"\u2581les\", null],\n[\"\u2581il\", \"\u043b\u044c\u044f\u043d\u043e\u0441\u0430\u043c\"],\n[\"anos\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nAlors, emport\u00e9e dans ses souvenirs comme dans un torrent qui bouillonne, elle arriva bient\u00f4t \u00e0 se rappeler la journ\u00e9e de la veille.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Al', 'ors', ',', '\u2581em', 'port', '\u00e9e', '\u2581dans', '\u2581ses', '\u2581sou', 'ven', 'irs', '\u2581comme', '\u2581dans', '\u2581un', '\u2581tor', 'rent', '\u2581qui', '\u2581bou', 'illon', 'ne', ',', '\u2581elle', '\u2581arriv', 'a', '\u2581b', 'ient', '\u00f4t', '\u2581\u00e0', '\u2581se', '\u2581rapp', 'eler', '\u2581la', '\u2581jour', 'n\u00e9e', '\u2581de', '\u2581la', '\u2581ve', 'ille', '.']"}], "[\n  [\"\u2581Al\", \"Then\"],\n  [\"ors\", null],\n  [\",\", null],\n  [\"\u2581em\", \"carried\"],\n  [\"port\", null],\n  [\"\u00e9e\", null],\n  [\"\u2581dans\", \"in\"],\n  [\"\u2581ses\", \"her\"],\n  [\"\u2581sou\", \"memories\"],\n  [\"ven\", null],\n  [\"irs\", null],\n  [\"\u2581comme\", \"as\"],\n  [\"\u2581dans\", \"in\"],\n  [\"\u2581un\", \"a\"],\n  [\"\u2581tor\", \"torrent\"],\n  [\"rent\", null],\n  [\"\u2581qui\", \"that\"],\n  [\"\u2581bou\", \"bubbles\"],\n  [\"illon\", null],\n  [\"ne\", null],\n  [\",\", null],\n  [\"\u2581elle\", \"she\"],\n  [\"\u2581arriv\", \"came\"],\n  [\"a\", null],\n  [\"\u2581b\", \"soon\"],\n  [\"ient\", null],\n  [\"\u00f4t\", null],\n  [\"\u2581\u00e0\", \"to\"],\n  [\"\u2581se\", null],\n  [\"\u2581rapp\", \"remember\"],\n  [\"eler\", null],\n  [\"\u2581la\", \"the\"],\n  [\"\u2581jour\", \"day\"],\n  [\"n\u00e9e\", null],\n  [\"\u2581de\", \"of\"],\n  [\"\u2581la\", \"the\"],\n  [\"\u2581ve\", \"previous\"],\n  [\"ille\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nAlors, emport\u00e9e dans ses souvenirs comme dans un torrent qui bouillonne, elle arriva bient\u00f4t \u00e0 se rappeler la journ\u00e9e de la veille.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Al', 'ors', ',', '\u2581em', 'port', '\u00e9e', '\u2581dans', '\u2581ses', '\u2581sou', 'ven', 'irs', '\u2581comme', '\u2581dans', '\u2581un', '\u2581tor', 'rent', '\u2581qui', '\u2581bou', 'illon', 'ne', ',', '\u2581elle', '\u2581arriv', 'a', '\u2581b', 'ient', '\u00f4t', '\u2581\u00e0', '\u2581se', '\u2581rapp', 'eler', '\u2581la', '\u2581jour', 'n\u00e9e', '\u2581de', '\u2581la', '\u2581ve', 'ille', '.']"}], "[\n  [\"\u2581Al\", \"\u0418\u0442\u0430\u043a\"],\n  [\"ors\", null],\n  [\",\", null],\n  [\"\u2581em\", \"\u0443\u043d\u0435\u0441\u0435\u043d\u043d\u0430\u044f\"],\n  [\"port\", null],\n  [\"\u00e9e\", null],\n  [\"\u2581dans\", null],\n  [\"\u2581ses\", \"\u0441\u0432\u043e\u0438\u043c\u0438\"],\n  [\"\u2581sou\", \"\u0432\u043e\u0441\u043f\u043e\u043c\u0438\u043d\u0430\u043d\u0438\u044f\u043c\u0438\"],\n  [\"ven\", null],\n  [\"irs\", null],\n  [\"\u2581comme\", \"\u043a\u0430\u043a\"],\n  [\"\u2581dans\", null],\n  [\"\u2581un\", null],\n  [\"\u2581tor\", \"\u0431\u0443\u0440\u043b\u044f\u0449\u0438\u043c\"],\n  [\"rent\", null],\n  [\"\u2581qui\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u0439\"],\n  [\"\u2581bou\", \"\u0431\u0443\u0440\u043b\u0438\u0442\"],\n  [\"illon\", null],\n  [\"ne\", null],\n  [\",\", null],\n  [\"\u2581elle\", \"\u043e\u043d\u0430\"],\n  [\"\u2581arriv\", \"\u043f\u0440\u0438\u0448\u043b\u0430\"],\n  [\"a\", null],\n  [\"\u2581b\", \"\u0432\u0441\u043a\u043e\u0440\u0435\"],\n  [\"ient\", null],\n  [\"\u00f4t\", null],\n  [\"\u2581\u00e0\", \"\u043a\"],\n  [\"\u2581se\", null],\n  [\"\u2581rapp\", \"\u0432\u043e\u0441\u043f\u043e\u043c\u0438\u043d\u0430\u043d\u0438\u044e\"],\n  [\"eler\", null],\n  [\"\u2581la\", null],\n  [\"\u2581jour\", \"\u0434\u043d\u044f\"],\n  [\"n\u00e9e\", null],\n  [\"\u2581de\", null],\n  [\"\u2581la\", null],\n  [\"\u2581ve\", \"\u043d\u0430\u043a\u0430\u043d\u0443\u043d\u0435\"],\n  [\"ille\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nL\u00e0-bas, c\u2019est l\u2019abbaye Saint-Martin, chanteuse aigre et f\u00eal\u00e9e ; ici, la voix sinistre et bourrue de la Bastille ; \u00e0 l\u2019autre bout, la grosse Tour du Louvre, avec sa basse-taille.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581L', '\u00e0', '-', 'bas', ',', '\u2581c', '\u2019', 'est', '\u2581l', '\u2019', 'abb', 'aye', '\u2581Saint', '-', 'Mart', 'in', ',', '\u2581ch', 'ante', 'use', '\u2581a', 'ig', 're', '\u2581et', '\u2581f', '\u00ea', 'l', '\u00e9e', '\u2581;', '\u2581', 'ici', ',', '\u2581la', '\u2581vo', 'ix', '\u2581sin', 'ist', 're', '\u2581et', '\u2581bour', 'r', 'ue', '\u2581de', '\u2581la', '\u2581Bast', 'ille', '\u2581;', '\u2581\u00e0', '\u2581l', '\u2019', 'autre', '\u2581bout', ',', '\u2581la', '\u2581g', 'ros', 'se', '\u2581Tour', '\u2581du', '\u2581Lou', 'vre', ',', '\u2581avec', '\u2581sa', '\u2581bas', 'se', '-', 'tail', 'le', '.']"}], "[\n[\"\u2581L\", \"Over\"],\n[\"\u00e0\", null],\n[\"-\", null],\n[\"bas\", null],\n[\",\", null],\n[\"\u2581c\", \"it\"],\n[\"'\", null],\n[\"est\", null],\n[\"\u2581l\", \"the\"],\n[\"'\", null],\n[\"abb\", \"abbey\"],\n[\"aye\", null],\n[\"\u2581Saint\", \"Saint\"],\n[\"-\", null],\n[\"Mart\", \"Martin\"],\n[\"in\", null],\n[\",\", null],\n[\"\u2581ch\", \"singer\"],\n[\"ante\", null],\n[\"use\", null],\n[\"\u2581a\", \"bitter\"],\n[\"ig\", null],\n[\"re\", null],\n[\"\u2581et\", \"and\"],\n[\"\u2581f\", \"cracked\"],\n[\"\u00ea\", null],\n[\"l\", null],\n[\"\u00e9e\", null],\n[\"\u2581;\", null],\n[\"\u2581\", null],\n[\"ici\", \"here\"],\n[\",\", null],\n[\"\u2581la\", \"the\"],\n[\"\u2581vo\", \"voice\"],\n[\"ix\", null],\n[\"\u2581sin\", \"sinister\"],\n[\"ist\", null],\n[\"re\", null],\n[\"\u2581et\", \"and\"],\n[\"\u2581bour\", \"gruff\"],\n[\"r\", null],\n[\"ue\", null],\n[\"\u2581de\", \"of\"],\n[\"\u2581la\", \"the\"],\n[\"\u2581Bast\", \"Bastille\"],\n[\"ille\", null],\n[\"\u2581;\", null],\n[\"\u2581\u00e0\", \"at\"],\n[\"\u2581l\", \"the\"],\n[\"'\", null],\n[\"autre\", \"other\"],\n[\"\u2581bout\", \"end\"],\n[\",\", null],\n[\"\u2581la\", \"the\"],\n[\"\u2581g\", \"big\"],\n[\"ros\", null],\n[\"se\", null],\n[\"\u2581Tour\", \"Tower\"],\n[\"\u2581du\", \"of\"],\n[\"\u2581Lou\", \"Louvre\"],\n[\"vre\", null],\n[\",\", null],\n[\"\u2581avec\", \"with\"],\n[\"\u2581sa\", \"its\"],\n[\"\u2581bas\", \"bass\"],\n[\"se\", null],\n[\"-\", null],\n[\"tail\", \"tone\"],\n[\"le\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nL\u00e0-bas, c\u2019est l\u2019abbaye Saint-Martin, chanteuse aigre et f\u00eal\u00e9e ; ici, la voix sinistre et bourrue de la Bastille ; \u00e0 l\u2019autre bout, la grosse Tour du Louvre, avec sa basse-taille.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581L', '\u00e0', '-', 'bas', ',', '\u2581c', '\u2019', 'est', '\u2581l', '\u2019', 'abb', 'aye', '\u2581Saint', '-', 'Mart', 'in', ',', '\u2581ch', 'ante', 'use', '\u2581a', 'ig', 're', '\u2581et', '\u2581f', '\u00ea', 'l', '\u00e9e', '\u2581;', '\u2581', 'ici', ',', '\u2581la', '\u2581vo', 'ix', '\u2581sin', 'ist', 're', '\u2581et', '\u2581bour', 'r', 'ue', '\u2581de', '\u2581la', '\u2581Bast', 'ille', '\u2581;', '\u2581\u00e0', '\u2581l', '\u2019', 'autre', '\u2581bout', ',', '\u2581la', '\u2581g', 'ros', 'se', '\u2581Tour', '\u2581du', '\u2581Lou', 'vre', ',', '\u2581avec', '\u2581sa', '\u2581bas', 'se', '-', 'tail', 'le', '.']"}], "[\n[\"\u2581L\", \"\u0422\u0430\u043c\"],\n[\"\u00e0\", null],\n[\"-\", null],\n[\"bas\", null],\n[\",\", null],\n[\"\u2581c\", null],\n[\"'\", null],\n[\"est\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"abb\", null],\n[\"aye\", \"\u0430\u0431\u0431\u0430\u0442\u0441\u0442\u0432\u043e\"],\n[\"\u2581Saint\", \"\u0421\u0435\u043d\"],\n[\"-\", null],\n[\"Mart\", null],\n[\"in\", \"\u041c\u0430\u0440\u0442\u0435\u043d\"],\n[\",\", null],\n[\"\u2581ch\", null],\n[\"ante\", null],\n[\"use\", \"\u043f\u0435\u0432\u0438\u0446\u0430\"],\n[\"\u2581a\", null],\n[\"ig\", null],\n[\"re\", \"\u0440\u0435\u0437\u043a\u0430\u044f\"],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581f\", null],\n[\"\u00ea\", null],\n[\"l\", null],\n[\"\u00e9e\", \"\u043d\u0430\u0434\u0442\u0440\u0435\u0441\u043d\u0443\u0442\u0430\u044f\"],\n[\"\u2581;\", null],\n[\"\u2581\", null],\n[\"ici\", \"\u0437\u0434\u0435\u0441\u044c\"],\n[\",\", null],\n[\"\u2581la\", null],\n[\"\u2581vo\", null],\n[\"ix\", \"\u0433\u043e\u043b\u043e\u0441\"],\n[\"\u2581sin\", null],\n[\"ist\", null],\n[\"re\", \"\u0437\u043b\u043e\u0432\u0435\u0449\u0438\u0439\"],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581bour\", null],\n[\"r\", null],\n[\"ue\", \"\u0433\u0440\u0443\u0431\u044b\u0439\"],\n[\"\u2581de\", null],\n[\"\u2581la\", null],\n[\"\u2581Bast\", null],\n[\"ille\", \"\u0411\u0430\u0441\u0442\u0438\u043b\u0438\u0438\"],\n[\"\u2581;\", null],\n[\"\u2581\u00e0\", \"\u043d\u0430\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"autre\", \"\u0434\u0440\u0443\u0433\u043e\u043c\"],\n[\"\u2581bout\", \"\u043a\u043e\u043d\u0446\u0435\"],\n[\",\", null],\n[\"\u2581la\", null],\n[\"\u2581g\", null],\n[\"ros\", null],\n[\"se\", \"\u0431\u043e\u043b\u044c\u0448\u0430\u044f\"],\n[\"\u2581Tour\", \"\u0411\u0430\u0448\u043d\u044f\"],\n[\"\u2581du\", null],\n[\"\u2581Lou\", null],\n[\"vre\", \"\u041b\u0443\u0432\u0440\u0430\"],\n[\",\", null],\n[\"\u2581avec\", \"\u0441\"],\n[\"\u2581sa\", \"\u0435\u0451\"],\n[\"\u2581bas\", null],\n[\"se\", null],\n[\"-\", null],\n[\"tail\", null],\n[\"le\", \"\u043d\u0438\u0437\u043a\u0438\u043c \u0431\u0430\u0441\u043e\u043c\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nJe n'avais pour tout ornement qu'une petite perle, dernier souvenir de Mlle Temple. Je la mis et nous descend\u00eemes.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Je', '\u2581n', \"'\", 'av', 'ais', '\u2581pour', '\u2581tout', '\u2581or', 'nement', '\u2581qu', \"'\", 'une', '\u2581pet', 'ite', '\u2581per', 'le', ',', '\u2581dernier', '\u2581sou', 'ven', 'ir', '\u2581de', '\u2581M', 'lle', '\u2581Temple', '.', '\u2581Je', '\u2581la', '\u2581mis', '\u2581et', '\u2581nous', '\u2581descend', '\u00ee', 'mes', '.']"}], "[\n  [\"\u2581Je\", \"I\"],\n  [\"\u2581n\", null],\n  [\"'\", null],\n  [\"av\", null],\n  [\"ais\", null],\n  [\"\u2581pour\", \"for\"],\n  [\"\u2581tout\", \"only\"],\n  [\"\u2581or\", null],\n  [\"nement\", null],\n  [\"\u2581qu\", null],\n  [\"'\", null],\n  [\"une\", \"a\"],\n  [\"\u2581pet\", null],\n  [\"ite\", null],\n  [\"\u2581per\", null],\n  [\"le\", null],\n  [\",\", null],\n  [\"\u2581dernier\", \"last\"],\n  [\"\u2581sou\", null],\n  [\"ven\", null],\n  [\"ir\", null],\n  [\"\u2581de\", \"of\"],\n  [\"\u2581M\", null],\n  [\"lle\", null],\n  [\"\u2581Temple\", \"Temple\"],\n  [\".\", null],\n  [\"\u2581Je\", \"I\"],\n  [\"\u2581la\", \"it\"],\n  [\"\u2581mis\", \"put on\"],\n  [\"\u2581et\", \"and\"],\n  [\"\u2581nous\", \"we\"],\n  [\"\u2581descend\", null],\n  [\"\u00ee\", null],\n  [\"mes\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nJe n'avais pour tout ornement qu'une petite perle, dernier souvenir de Mlle Temple. Je la mis et nous descend\u00eemes.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Je', '\u2581n', \"'\", 'av', 'ais', '\u2581pour', '\u2581tout', '\u2581or', 'nement', '\u2581qu', \"'\", 'une', '\u2581pet', 'ite', '\u2581per', 'le', ',', '\u2581dernier', '\u2581sou', 'ven', 'ir', '\u2581de', '\u2581M', 'lle', '\u2581Temple', '.', '\u2581Je', '\u2581la', '\u2581mis', '\u2581et', '\u2581nous', '\u2581descend', '\u00ee', 'mes', '.']"}], "[\n[\"\u2581Je\", \"\u042f\"],\n[\"\u2581n\", null],\n[\"'\", null],\n[\"av\", null],\n[\"ais\", null],\n[\"\u2581pour\", null],\n[\"\u2581tout\", \"\u0435\u0434\u0438\u043d\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u043c\"],\n[\"\u2581or\", null],\n[\"nement\", null],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"une\", null],\n[\"\u2581pet\", null],\n[\"ite\", null],\n[\"\u2581per\", null],\n[\"le\", \"\u0436\u0435\u043c\u0447\u0443\u0436\u0438\u043d\u0430\"],\n[\",\", null],\n[\"\u2581dernier\", \"\u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435\"],\n[\"\u2581sou\", null],\n[\"ven\", null],\n[\"ir\", \"\u0432\u043e\u0441\u043f\u043e\u043c\u0438\u043d\u0430\u043d\u0438\u0435\"],\n[\"\u2581de\", null],\n[\"\u2581M\", null],\n[\"lle\", \"\u043c\u0438\u0441\u0441\"],\n[\"\u2581Temple\", \"\u0422\u0435\u043c\u043f\u043b\"],\n[\".\", null],\n[\"\u2581Je\", \"\u042f\"],\n[\"\u2581la\", null],\n[\"\u2581mis\", \"\u043d\u0430\u0434\u0435\u043b\u0430\"],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581nous\", \"\u043c\u044b\"],\n[\"\u2581descend\", null],\n[\"\u00ee\", null],\n[\"mes\", \"\u0441\u043f\u0443\u0441\u0442\u0438\u043b\u0438\u0441\u044c\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n-- Des documents! des documents! s\u2019\u00e9cria lady Helena. -- Seulement, r\u00e9pondit Glenarvan, ils paraissent \u00eatre rong\u00e9s par l\u2019humidit\u00e9, et il est impossible de les retirer, car ils adh\u00e8rent aux parois de la bouteille.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581--', '\u2581Des', '\u2581documents', '!', '\u2581des', '\u2581documents', '!', '\u2581s', '\u2019', '\u00e9', 'cri', 'a', '\u2581lady', '\u2581Hel', 'ena', '.', '\u2581--', '\u2581Se', 'u', 'lement', ',', '\u2581r\u00e9', 'pon', 'dit', '\u2581Glen', 'ar', 'van', ',', '\u2581ils', '\u2581para', 'issent', '\u2581\u00eatre', '\u2581r', 'ong', '\u00e9s', '\u2581par', '\u2581l', '\u2019', 'hum', 'id', 'it\u00e9', ',', '\u2581et', '\u2581il', '\u2581est', '\u2581impossible', '\u2581de', '\u2581les', '\u2581ret', 'ir', 'er', ',', '\u2581car', '\u2581ils', '\u2581ad', 'h', '\u00e8rent', '\u2581aux', '\u2581par', 'ois', '\u2581de', '\u2581la', '\u2581b', 'oute', 'ille', '.']"}], "[\n[\"\u2581--\", null],\n[\"\u2581Des\", \"Documents\"],\n[\"\u2581documents\", null],\n[\"!\", null],\n[\"\u2581des\", \"documents\"],\n[\"\u2581documents\", null],\n[\"!\", null],\n[\"\u2581s\", \"exclaimed\"],\n[\"'\", null],\n[\"\u00e9\", null],\n[\"cri\", null],\n[\"a\", null],\n[\"\u2581lady\", \"Lady\"],\n[\"\u2581Hel\", \"Helena\"],\n[\"ena\", null],\n[\".\", null],\n[\"\u2581--\", null],\n[\"\u2581Se\", \"Only\"],\n[\"u\", null],\n[\"lement\", null],\n[\",\", null],\n[\"\u2581r\u00e9\", \"replied\"],\n[\"pon\", null],\n[\"dit\", null],\n[\"\u2581Glen\", \"Glenarvan\"],\n[\"ar\", null],\n[\"van\", null],\n[\",\", null],\n[\"\u2581ils\", \"they\"],\n[\"\u2581para\", \"seem\"],\n[\"issent\", null],\n[\"\u2581\u00eatre\", \"to be\"],\n[\"\u2581r\", \"eaten\"],\n[\"ong\", null],\n[\"\u00e9s\", null],\n[\"\u2581par\", \"by\"],\n[\"\u2581l\", \"the\"],\n[\"'\", null],\n[\"hum\", \"moisture\"],\n[\"id\", null],\n[\"it\u00e9\", null],\n[\",\", null],\n[\"\u2581et\", \"and\"],\n[\"\u2581il\", \"it\"],\n[\"\u2581est\", \"is\"],\n[\"\u2581impossible\", \"impossible\"],\n[\"\u2581de\", \"to\"],\n[\"\u2581les\", \"them\"],\n[\"\u2581ret\", \"remove\"],\n[\"ir\", null],\n[\"er\", null],\n[\",\", null],\n[\"\u2581car\", \"because\"],\n[\"\u2581ils\", \"they\"],\n[\"\u2581ad\", \"adhere\"],\n[\"h\", null],\n[\"\u00e8rent\", null],\n[\"\u2581aux\", \"to the\"],\n[\"\u2581par\", \"walls\"],\n[\"ois\", null],\n[\"\u2581de\", \"of\"],\n[\"\u2581la\", \"the\"],\n[\"\u2581b\", \"bottle\"],\n[\"oute\", null],\n[\"ille\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n-- Des documents! des documents! s\u2019\u00e9cria lady Helena. -- Seulement, r\u00e9pondit Glenarvan, ils paraissent \u00eatre rong\u00e9s par l\u2019humidit\u00e9, et il est impossible de les retirer, car ils adh\u00e8rent aux parois de la bouteille.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581--', '\u2581Des', '\u2581documents', '!', '\u2581des', '\u2581documents', '!', '\u2581s', '\u2019', '\u00e9', 'cri', 'a', '\u2581lady', '\u2581Hel', 'ena', '.', '\u2581--', '\u2581Se', 'u', 'lement', ',', '\u2581r\u00e9', 'pon', 'dit', '\u2581Glen', 'ar', 'van', ',', '\u2581ils', '\u2581para', 'issent', '\u2581\u00eatre', '\u2581r', 'ong', '\u00e9s', '\u2581par', '\u2581l', '\u2019', 'hum', 'id', 'it\u00e9', ',', '\u2581et', '\u2581il', '\u2581est', '\u2581impossible', '\u2581de', '\u2581les', '\u2581ret', 'ir', 'er', ',', '\u2581car', '\u2581ils', '\u2581ad', 'h', '\u00e8rent', '\u2581aux', '\u2581par', 'ois', '\u2581de', '\u2581la', '\u2581b', 'oute', 'ille', '.']"}], "[\n[\"\u2581--\", null],\n[\"\u2581Des\", \"\u0414\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u044b\"],\n[\"\u2581documents\", null],\n[\"!\", null],\n[\"\u2581des\", null],\n[\"\u2581documents\", \"\u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u044b\"],\n[\"!\", null],\n[\"\u2581s\", \"\u0432\u043e\u0441\u043a\u043b\u0438\u043a\u043d\u0443\u043b\u0430\"],\n[\"'\", null],\n[\"\u00e9\", null],\n[\"cri\", null],\n[\"a\", null],\n[\"\u2581lady\", \"\u043b\u0435\u0434\u0438\"],\n[\"\u2581Hel\", \"\u0415\u043b\u0435\u043d\u0430\"],\n[\"ena\", null],\n[\".\", null],\n[\"\u2581--\", null],\n[\"\u2581Se\", \"\u0422\u043e\u043b\u044c\u043a\u043e\"],\n[\"u\", null],\n[\"lement\", null],\n[\",\", null],\n[\"\u2581r\u00e9\", \"\u043e\u0442\u0432\u0435\u0442\u0438\u043b\"],\n[\"pon\", null],\n[\"dit\", null],\n[\"\u2581Glen\", \"\u0413\u043b\u0435\u043d\u0430\u0440\u0432\u0430\u043d\"],\n[\"ar\", null],\n[\"van\", null],\n[\",\", null],\n[\"\u2581ils\", \"\u043e\u043d\u0438\"],\n[\"\u2581para\", \"\u043a\u0430\u0436\u0443\u0442\u0441\u044f\"],\n[\"issent\", null],\n[\"\u2581\u00eatre\", null],\n[\"\u2581r\", \"\u0438\u0437\u044a\u0435\u0434\u0435\u043d\u043d\u044b\u043c\u0438\"],\n[\"ong\", null],\n[\"\u00e9s\", null],\n[\"\u2581par\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"hum\", \"\u0432\u043b\u0430\u0433\u043e\u0439\"],\n[\"id\", null],\n[\"it\u00e9\", null],\n[\",\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581il\", null],\n[\"\u2581est\", \"\u043d\u0435\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\"],\n[\"\u2581impossible\", null],\n[\"\u2581de\", null],\n[\"\u2581les\", \"\u0438\u0445\"],\n[\"\u2581ret\", \"\u0438\u0437\u0432\u043b\u0435\u0447\u044c\"],\n[\"ir\", null],\n[\"er\", null],\n[\",\", null],\n[\"\u2581car\", \"\u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e\"],\n[\"\u2581ils\", \"\u043e\u043d\u0438\"],\n[\"\u2581ad\", \"\u043f\u0440\u0438\u043b\u0438\u043f\u043b\u0438\"],\n[\"h\", null],\n[\"\u00e8rent\", null],\n[\"\u2581aux\", \"\u043a\"],\n[\"\u2581par\", \"\u0441\u0442\u0435\u043d\u043a\u0430\u043c\"],\n[\"ois\", null],\n[\"\u2581de\", null],\n[\"\u2581la\", null],\n[\"\u2581b\", \"\u0431\u0443\u0442\u044b\u043b\u043a\u0438\"],\n[\"oute\", null],\n[\"ille\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nJ'\u00e9tais dans une petite chambre. Pr\u00e8s du feu se trouvait une table ronde; sur un fauteuil \u00e0 dos \u00e9lev\u00e9 et de forme antique \u00e9tait assise la plus propre et la plus mignonne petite dame qu'on puisse imaginer.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581J', \"'\", '\u00e9ta', 'is', '\u2581dans', '\u2581une', '\u2581pet', 'ite', '\u2581ch', 'amb', 're', '.', '\u2581Pr', '\u00e8s', '\u2581du', '\u2581fe', 'u', '\u2581se', '\u2581trou', 'v', 'ait', '\u2581une', '\u2581table', '\u2581r', 'onde', ';', '\u2581sur', '\u2581un', '\u2581f', 'aut', 'eu', 'il', '\u2581\u00e0', '\u2581dos', '\u2581\u00e9', 'lev', '\u00e9', '\u2581et', '\u2581de', '\u2581forme', '\u2581ant', 'ique', '\u2581\u00e9tait', '\u2581ass', 'ise', '\u2581la', '\u2581plus', '\u2581pro', 'pre', '\u2581et', '\u2581la', '\u2581plus', '\u2581m', 'ign', 'onne', '\u2581pet', 'ite', '\u2581d', 'ame', '\u2581qu', \"'\", 'on', '\u2581pu', 'isse', '\u2581imag', 'iner', '.']"}], "[\n  [\"\u2581J\", \"I\"],\n  [\"'\", null],\n  [\"\u00e9ta\", null],\n  [\"is\", null],\n  [\"\u2581dans\", \"in\"],\n  [\"\u2581une\", \"a\"],\n  [\"\u2581pet\", null],\n  [\"ite\", \"small\"],\n  [\"\u2581ch\", null],\n  [\"amb\", null],\n  [\"re\", \"room\"],\n  [\".\", null],\n  [\"\u2581Pr\", null],\n  [\"\u00e8s\", \"Near\"],\n  [\"\u2581du\", \"the\"],\n  [\"\u2581fe\", null],\n  [\"u\", \"fire\"],\n  [\"\u2581se\", null],\n  [\"\u2581trou\", null],\n  [\"v\", null],\n  [\"ait\", \"was\"],\n  [\"\u2581une\", \"a\"],\n  [\"\u2581table\", \"table\"],\n  [\"\u2581r\", null],\n  [\"onde\", \"round\"],\n  [\";\", null],\n  [\"\u2581sur\", \"on\"],\n  [\"\u2581un\", \"a\"],\n  [\"\u2581f\", null],\n  [\"aut\", null],\n  [\"eu\", null],\n  [\"il\", \"chair\"],\n  [\"\u2581\u00e0\", \"with\"],\n  [\"\u2581dos\", \"back\"],\n  [\"\u2581\u00e9\", null],\n  [\"lev\", null],\n  [\"\u00e9\", \"high\"],\n  [\"\u2581et\", \"and\"],\n  [\"\u2581de\", \"of\"],\n  [\"\u2581forme\", \"shape\"],\n  [\"\u2581ant\", null],\n  [\"ique\", \"antique\"],\n  [\"\u2581\u00e9tait\", \"was\"],\n  [\"\u2581ass\", null],\n  [\"ise\", \"seated\"],\n  [\"\u2581la\", \"the\"],\n  [\"\u2581plus\", \"most\"],\n  [\"\u2581pro\", null],\n  [\"pre\", \"neat\"],\n  [\"\u2581et\", \"and\"],\n  [\"\u2581la\", \"the\"],\n  [\"\u2581plus\", \"most\"],\n  [\"\u2581m\", null],\n  [\"ign\", null],\n  [\"onne\", \"dainty\"],\n  [\"\u2581pet\", null],\n  [\"ite\", \"little\"],\n  [\"\u2581d\", null],\n  [\"ame\", \"lady\"],\n  [\"\u2581qu\", \"that\"],\n  [\"'\", null],\n  [\"on\", \"one\"],\n  [\"\u2581pu\", null],\n  [\"isse\", \"could\"],\n  [\"\u2581imag\", null],\n  [\"iner\", \"imagine\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nJ'\u00e9tais dans une petite chambre. Pr\u00e8s du feu se trouvait une table ronde; sur un fauteuil \u00e0 dos \u00e9lev\u00e9 et de forme antique \u00e9tait assise la plus propre et la plus mignonne petite dame qu'on puisse imaginer.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581J', \"'\", '\u00e9ta', 'is', '\u2581dans', '\u2581une', '\u2581pet', 'ite', '\u2581ch', 'amb', 're', '.', '\u2581Pr', '\u00e8s', '\u2581du', '\u2581fe', 'u', '\u2581se', '\u2581trou', 'v', 'ait', '\u2581une', '\u2581table', '\u2581r', 'onde', ';', '\u2581sur', '\u2581un', '\u2581f', 'aut', 'eu', 'il', '\u2581\u00e0', '\u2581dos', '\u2581\u00e9', 'lev', '\u00e9', '\u2581et', '\u2581de', '\u2581forme', '\u2581ant', 'ique', '\u2581\u00e9tait', '\u2581ass', 'ise', '\u2581la', '\u2581plus', '\u2581pro', 'pre', '\u2581et', '\u2581la', '\u2581plus', '\u2581m', 'ign', 'onne', '\u2581pet', 'ite', '\u2581d', 'ame', '\u2581qu', \"'\", 'on', '\u2581pu', 'isse', '\u2581imag', 'iner', '.']"}], "[\n[\"\u2581J\", \"\u042f\"],\n[\"'\", null],\n[\"\u00e9ta\", \"\u0431\u044b\u043b\"],\n[\"is\", null],\n[\"\u2581dans\", null],\n[\"\u2581une\", null],\n[\"\u2581pet\", \"\u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0430\u044f\"],\n[\"ite\", null],\n[\"\u2581ch\", \"\u043a\u043e\u043c\u043d\u0430\u0442\u0430\"],\n[\"amb\", null],\n[\"re\", null],\n[\".\", null],\n[\"\u2581Pr\", \"\u041e\u043a\u043e\u043b\u043e\"],\n[\"\u00e8s\", null],\n[\"\u2581du\", null],\n[\"\u2581fe\", \"\u043e\u0433\u043e\u043d\u044c\"],\n[\"u\", null],\n[\"\u2581se\", null],\n[\"\u2581trou\", \"\u043d\u0430\u0445\u043e\u0434\u0438\u0442\u044c\u0441\u044f\"],\n[\"v\", null],\n[\"ait\", null],\n[\"\u2581une\", null],\n[\"\u2581table\", \"\u0441\u0442\u043e\u043b\"],\n[\"\u2581r\", \"\u043a\u0440\u0443\u0433\u043b\u044b\u0439\"],\n[\"onde\", null],\n[\";\", null],\n[\"\u2581sur\", null],\n[\"\u2581un\", null],\n[\"\u2581f\", \"\u043a\u0440\u0435\u0441\u043b\u043e\"],\n[\"aut\", null],\n[\"eu\", null],\n[\"il\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581dos\", \"\u0441\u043f\u0438\u043d\u043a\u0430\"],\n[\"\u2581\u00e9\", \"\u0432\u044b\u0441\u043e\u043a\u0438\u0439\"],\n[\"lev\", null],\n[\"\u00e9\", null],\n[\"\u2581et\", null],\n[\"\u2581de\", null],\n[\"\u2581forme\", \"\u0444\u043e\u0440\u043c\u0430\"],\n[\"\u2581ant\", \"\u0441\u0442\u0430\u0440\u0438\u043d\u043d\u044b\u0439\"],\n[\"ique\", null],\n[\"\u2581\u00e9tait\", null],\n[\"\u2581ass\", \"\u0441\u0438\u0434\u0435\u0442\u044c\"],\n[\"ise\", null],\n[\"\u2581la\", null],\n[\"\u2581plus\", null],\n[\"\u2581pro\", \"\u043e\u043f\u0440\u044f\u0442\u043d\u044b\u0439\"],\n[\"pre\", null],\n[\"\u2581et\", null],\n[\"\u2581la\", null],\n[\"\u2581plus\", null],\n[\"\u2581m\", \"\u043c\u0438\u043b\u043e\u0432\u0438\u0434\u043d\u044b\u0439\"],\n[\"ign\", null],\n[\"onne\", null],\n[\"\u2581pet\", \"\u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0438\u0439\"],\n[\"ite\", null],\n[\"\u2581d\", \"\u0434\u0430\u043c\u0430\"],\n[\"ame\", null],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"on\", null],\n[\"\u2581pu\", \"\u043c\u043e\u0436\u043d\u043e\"],\n[\"isse\", null],\n[\"\u2581imag\", \"\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u044c\"],\n[\"iner\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nJohn n'avait beaucoup d'affection ni pour sa m\u00e8re ni pour ses soeurs.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581John', '\u2581n', \"'\", 'av', 'ait', '\u2581be', 'auc', 'oup', '\u2581d', \"'\", 'aff', 'ection', '\u2581ni', '\u2581pour', '\u2581sa', '\u2581m', '\u00e8re', '\u2581ni', '\u2581pour', '\u2581ses', '\u2581so', 'eurs', '.']"}], "[\n  [\"\u2581John\", \"John\"],\n  [\"\u2581n\", null],\n  [\"'\", null],\n  [\"av\", null],\n  [\"ait\", null],\n  [\"\u2581be\", null],\n  [\"auc\", null],\n  [\"oup\", \"much\"],\n  [\"\u2581d\", null],\n  [\"'\", null],\n  [\"aff\", null],\n  [\"ection\", \"affection\"],\n  [\"\u2581ni\", null],\n  [\"\u2581pour\", null],\n  [\"\u2581sa\", null],\n  [\"\u2581m\", null],\n  [\"\u00e8re\", \"mother\"],\n  [\"\u2581ni\", null],\n  [\"\u2581pour\", null],\n  [\"\u2581ses\", null],\n  [\"\u2581so\", null],\n  [\"eurs\", \"sisters\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nJohn n'avait beaucoup d'affection ni pour sa m\u00e8re ni pour ses soeurs.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581John', '\u2581n', \"'\", 'av', 'ait', '\u2581be', 'auc', 'oup', '\u2581d', \"'\", 'aff', 'ection', '\u2581ni', '\u2581pour', '\u2581sa', '\u2581m', '\u00e8re', '\u2581ni', '\u2581pour', '\u2581ses', '\u2581so', 'eurs', '.']"}], "[\n  [\"\u2581John\", \"\u0414\u0436\u043e\u043d\"],\n  [\"\u2581n\", null],\n  [\"'\", null],\n  [\"av\", null],\n  [\"ait\", null],\n  [\"\u2581be\", null],\n  [\"auc\", null],\n  [\"oup\", \"\u043c\u043d\u043e\u0433\u043e\"],\n  [\"\u2581d\", null],\n  [\"'\", null],\n  [\"aff\", null],\n  [\"ection\", \"\u043f\u0440\u0438\u0432\u044f\u0437\u0430\u043d\u043d\u043e\u0441\u0442\u0438\"],\n  [\"\u2581ni\", null],\n  [\"\u2581pour\", null],\n  [\"\u2581sa\", null],\n  [\"\u2581m\", null],\n  [\"\u00e8re\", \"\u043c\u0430\u0442\u044c\"],\n  [\"\u2581ni\", null],\n  [\"\u2581pour\", null],\n  [\"\u2581ses\", null],\n  [\"\u2581so\", null],\n  [\"eurs\", \"\u0441\u0435\u0441\u0442\u0440\u044b\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n\u00ab Mais non, je rampe encore ; je m\u2019\u00e9corche la face et les genoux aux cailloux de la voie souterraine.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00ab', '\u2581Mais', '\u2581non', ',', '\u2581je', '\u2581ram', 'pe', '\u2581encore', '\u2581;', '\u2581je', '\u2581m', '\u2019', '\u00e9', 'cor', 'che', '\u2581la', '\u2581face', '\u2581et', '\u2581les', '\u2581gen', 'oux', '\u2581aux', '\u2581ca', 'ill', 'oux', '\u2581de', '\u2581la', '\u2581vo', 'ie', '\u2581sou', 'ter', 'r', 'aine', '.']"}], "[\n[\"\u2581\u00ab\", null],\n[\"\u2581Mais\", \"But\"],\n[\"\u2581non\", null],\n[\",\", null],\n[\"\u2581je\", null],\n[\"\u2581ram\", \"crawl\"],\n[\"pe\", null],\n[\"\u2581encore\", \"still\"],\n[\"\u2581;\", null],\n[\"\u2581je\", null],\n[\"\u2581m\", null],\n[\"'\", null],\n[\"\u00e9\", null],\n[\"cor\", \"scratch\"],\n[\"che\", null],\n[\"\u2581la\", null],\n[\"\u2581face\", \"face\"],\n[\"\u2581et\", null],\n[\"\u2581les\", null],\n[\"\u2581gen\", \"knees\"],\n[\"oux\", null],\n[\"\u2581aux\", null],\n[\"\u2581ca\", \"pebbles\"],\n[\"ill\", null],\n[\"oux\", null],\n[\"\u2581de\", null],\n[\"\u2581la\", null],\n[\"\u2581vo\", \"underground\"],\n[\"ie\", null],\n[\"\u2581sou\", null],\n[\"ter\", null],\n[\"r\", null],\n[\"aine\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n\u00ab Mais non, je rampe encore ; je m\u2019\u00e9corche la face et les genoux aux cailloux de la voie souterraine.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00ab', '\u2581Mais', '\u2581non', ',', '\u2581je', '\u2581ram', 'pe', '\u2581encore', '\u2581;', '\u2581je', '\u2581m', '\u2019', '\u00e9', 'cor', 'che', '\u2581la', '\u2581face', '\u2581et', '\u2581les', '\u2581gen', 'oux', '\u2581aux', '\u2581ca', 'ill', 'oux', '\u2581de', '\u2581la', '\u2581vo', 'ie', '\u2581sou', 'ter', 'r', 'aine', '.']"}], "[\n  [\"\u2581\u00ab\", null],\n  [\"\u2581Mais\", \"\u041d\u043e\"],\n  [\"\u2581non\", null],\n  [\",\", null],\n  [\"\u2581je\", null],\n  [\"\u2581ram\", \"\u043f\u043e\u043b\u0437\u0443\"],\n  [\"pe\", null],\n  [\"\u2581encore\", \"\u0432\u0441\u0435 \u0435\u0449\u0435\"],\n  [\"\u2581;\", null],\n  [\"\u2581je\", null],\n  [\"\u2581m\", \"\u044f\"],\n  [\"'\", null],\n  [\"\u00e9\", null],\n  [\"cor\", \"\u0446\u0430\u0440\u0430\u043f\u0430\u044e\"],\n  [\"che\", null],\n  [\"\u2581la\", null],\n  [\"\u2581face\", \"\u043b\u0438\u0446\u043e\"],\n  [\"\u2581et\", \"\u0438\"],\n  [\"\u2581les\", null],\n  [\"\u2581gen\", \"\u043a\u043e\u043b\u0435\u043d\u0438\"],\n  [\"oux\", null],\n  [\"\u2581aux\", \"\u043e\"],\n  [\"\u2581ca\", \"\u043a\u0430\u043c\u043d\u0438\"],\n  [\"ill\", null],\n  [\"oux\", null],\n  [\"\u2581de\", null],\n  [\"\u2581la\", null],\n  [\"\u2581vo\", \"\u043f\u043e\u0434\u0437\u0435\u043c\u043d\u043e\u0433\u043e\"],\n  [\"ie\", null],\n  [\"\u2581sou\", \"\u043f\u0443\u0442\u0438\"],\n  [\"ter\", null],\n  [\"r\", null],\n  [\"aine\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nTout salut semblait impossible, quand la voix du major se fit entendre.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581T', 'out', '\u2581sal', 'ut', '\u2581sem', 'bl', 'ait', '\u2581impossible', ',', '\u2581quand', '\u2581la', '\u2581vo', 'ix', '\u2581du', '\u2581major', '\u2581se', '\u2581fit', '\u2581ent', 'end', 're', '.']"}], "[\n  [\"\u2581T\", \"All\"],\n  [\"out\", null],\n  [\"\u2581sal\", \"salvation\"],\n  [\"ut\", null],\n  [\"\u2581sem\", \"seemed\"],\n  [\"bl\", null],\n  [\"ait\", null],\n  [\"\u2581impossible\", \"impossible\"],\n  [\",\", null],\n  [\"\u2581quand\", \"when\"],\n  [\"\u2581la\", null],\n  [\"\u2581vo\", \"voice\"],\n  [\"ix\", null],\n  [\"\u2581du\", null],\n  [\"\u2581major\", \"major\"],\n  [\"\u2581se\", null],\n  [\"\u2581fit\", \"was\"],\n  [\"\u2581ent\", \"heard\"],\n  [\"end\", null],\n  [\"re\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nTout salut semblait impossible, quand la voix du major se fit entendre.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581T', 'out', '\u2581sal', 'ut', '\u2581sem', 'bl', 'ait', '\u2581impossible', ',', '\u2581quand', '\u2581la', '\u2581vo', 'ix', '\u2581du', '\u2581major', '\u2581se', '\u2581fit', '\u2581ent', 'end', 're', '.']"}], "[\n[\"\u2581T\", \"\u0412\u0441\u044f\u043a\u043e\u0435\"],\n[\"out\", null],\n[\"\u2581sal\", \"\u0441\u043f\u0430\u0441\u0435\u043d\u0438\u0435\"],\n[\"ut\", null],\n[\"\u2581sem\", \"\u043a\u0430\u0437\u0430\u043b\u043e\u0441\u044c\"],\n[\"bl\", null],\n[\"ait\", null],\n[\"\u2581impossible\", \"\u043d\u0435\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u043c\"],\n[\",\", null],\n[\"\u2581quand\", \"\u043a\u043e\u0433\u0434\u0430\"],\n[\"\u2581la\", null],\n[\"\u2581vo\", \"\u0433\u043e\u043b\u043e\u0441\"],\n[\"ix\", null],\n[\"\u2581du\", null],\n[\"\u2581major\", \"\u043c\u0430\u0439\u043e\u0440\u0430\"],\n[\"\u2581se\", null],\n[\"\u2581fit\", \"\u0440\u0430\u0437\u0434\u0430\u043b\u0441\u044f\"],\n[\"\u2581ent\", null],\n[\"end\", null],\n[\"re\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nMais Paganel lui apprit que, sous cette latitude, les sauvages fr\u00e9quentaient principalement les plaines du Murray, situ\u00e9es \u00e0 cent milles dans l\u2019est.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Mais', '\u2581P', 'ag', 'anel', '\u2581lui', '\u2581app', 'rit', '\u2581que', ',', '\u2581sous', '\u2581cette', '\u2581lat', 'itude', ',', '\u2581les', '\u2581sau', 'v', 'ages', '\u2581fr', '\u00e9qu', 'enta', 'ient', '\u2581principal', 'ement', '\u2581les', '\u2581plain', 'es', '\u2581du', '\u2581Murray', ',', '\u2581situ', '\u00e9es', '\u2581\u00e0', '\u2581cent', '\u2581mill', 'es', '\u2581dans', '\u2581l', '\u2019', 'est', '.']"}], "[\n  [\"\u2581Mais\", \"But\"],\n  [\"\u2581P\", \"Paganel\"],\n  [\"ag\", null],\n  [\"anel\", null],\n  [\"\u2581lui\", null],\n  [\"\u2581app\", \"informed\"],\n  [\"rit\", null],\n  [\"\u2581que\", null],\n  [\",\", null],\n  [\"\u2581sous\", \"under\"],\n  [\"\u2581cette\", null],\n  [\"\u2581lat\", \"latitude\"],\n  [\"itude\", null],\n  [\",\", null],\n  [\"\u2581les\", null],\n  [\"\u2581sau\", \"savages\"],\n  [\"v\", null],\n  [\"ages\", null],\n  [\"\u2581fr\", \"frequented\"],\n  [\"\u00e9qu\", null],\n  [\"enta\", null],\n  [\"ient\", null],\n  [\"\u2581principal\", \"mainly\"],\n  [\"ement\", null],\n  [\"\u2581les\", null],\n  [\"\u2581plain\", \"plains\"],\n  [\"es\", null],\n  [\"\u2581du\", null],\n  [\"\u2581Murray\", \"Murray\"],\n  [\",\", null],\n  [\"\u2581situ\", \"located\"],\n  [\"\u00e9es\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581cent\", \"hundred\"],\n  [\"\u2581mill\", \"miles\"],\n  [\"es\", null],\n  [\"\u2581dans\", \"in\"],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"est\", \"east\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nMais Paganel lui apprit que, sous cette latitude, les sauvages fr\u00e9quentaient principalement les plaines du Murray, situ\u00e9es \u00e0 cent milles dans l\u2019est.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Mais', '\u2581P', 'ag', 'anel', '\u2581lui', '\u2581app', 'rit', '\u2581que', ',', '\u2581sous', '\u2581cette', '\u2581lat', 'itude', ',', '\u2581les', '\u2581sau', 'v', 'ages', '\u2581fr', '\u00e9qu', 'enta', 'ient', '\u2581principal', 'ement', '\u2581les', '\u2581plain', 'es', '\u2581du', '\u2581Murray', ',', '\u2581situ', '\u00e9es', '\u2581\u00e0', '\u2581cent', '\u2581mill', 'es', '\u2581dans', '\u2581l', '\u2019', 'est', '.']"}], "[\n    [\"\u2581Mais\", \"\u041d\u043e\"],\n    [\"\u2581P\", \"\u041f\u0430\u0433\u0430\u043d\u0435\u043b\u044c\"],\n    [\"ag\", null],\n    [\"anel\", null],\n    [\"\u2581lui\", null],\n    [\"\u2581app\", \"\u0441\u043e\u043e\u0431\u0449\u0438\u043b\"],\n    [\"rit\", null],\n    [\"\u2581que\", null],\n    [\",\", null],\n    [\"\u2581sous\", \"\u043f\u043e\u0434\"],\n    [\"\u2581cette\", \"\u044d\u0442\u043e\u0439\"],\n    [\"\u2581lat\", \"\u0448\u0438\u0440\u043e\u0442\u043e\u0439\"],\n    [\"itude\", null],\n    [\",\", null],\n    [\"\u2581les\", null],\n    [\"\u2581sau\", \"\u0434\u0438\u043a\u0430\u0440\u0438\"],\n    [\"v\", null],\n    [\"ages\", null],\n    [\"\u2581fr\", \"\u0447\u0430\u0441\u0442\u043e\"],\n    [\"\u00e9qu\", null],\n    [\"enta\", null],\n    [\"ient\", null],\n    [\"\u2581principal\", \"\u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c\"],\n    [\"ement\", null],\n    [\"\u2581les\", null],\n    [\"\u2581plain\", \"\u0440\u0430\u0432\u043d\u0438\u043d\u044b\"],\n    [\"es\", null],\n    [\"\u2581du\", null],\n    [\"\u2581Murray\", \"\u041c\u0443\u0440\u0440\u0435\u044f\"],\n    [\",\", null],\n    [\"\u2581situ\", \"\u0440\u0430\u0441\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u043d\u044b\u0435\"],\n    [\"\u00e9es\", null],\n    [\"\u2581\u00e0\", \"\u0432\"],\n    [\"\u2581cent\", \"\u0441\u0442\u0430\"],\n    [\"\u2581mill\", \"\u043c\u0438\u043b\u044f\u0445\"],\n    [\"es\", null],\n    [\"\u2581dans\", \"\u043d\u0430\"],\n    [\"\u2581l\", null],\n    [\"'\", null],\n    [\"est\", \"\u0432\u043e\u0441\u0442\u043e\u043a\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n\u2013 Est-ce que vous ne trouvez pas, r\u00e9pondit l\u2019archidiacre d\u2019un air de profonde r\u00e9flexion, que l\u2019habit de ces cavaliers que nous venons de voir est plus beau que le v\u00f4tre et que le mien ? \u00bb Gringoire hocha la t\u00eate.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u2013', '\u2581Est', '-', 'ce', '\u2581que', '\u2581vous', '\u2581ne', '\u2581trou', 'vez', '\u2581pas', ',', '\u2581r\u00e9', 'pon', 'dit', '\u2581l', '\u2019', 'arch', 'idi', 'acre', '\u2581d', '\u2019', 'un', '\u2581air', '\u2581de', '\u2581prof', 'onde', '\u2581r\u00e9', 'flex', 'ion', ',', '\u2581que', '\u2581l', '\u2019', 'hab', 'it', '\u2581de', '\u2581ces', '\u2581caval', 'iers', '\u2581que', '\u2581nous', '\u2581ven', 'ons', '\u2581de', '\u2581vo', 'ir', '\u2581est', '\u2581plus', '\u2581be', 'au', '\u2581que', '\u2581le', '\u2581v', '\u00f4', 'tre', '\u2581et', '\u2581que', '\u2581le', '\u2581m', 'ien', '\u2581?', '\u2581\u00bb', '\u2581G', 'ring', 'oire', '\u2581h', 'och', 'a', '\u2581la', '\u2581t\u00eate', '.']"}], "[\n[\"\u2581\u2013\", null],\n[\"\u2581Est\", \"Is\"],\n[\"-\", null],\n[\"ce\", null],\n[\"\u2581que\", null],\n[\"\u2581vous\", \"you\"],\n[\"\u2581ne\", null],\n[\"\u2581trou\", \"find\"],\n[\"vez\", null],\n[\"\u2581pas\", null],\n[\",\", null],\n[\"\u2581r\u00e9\", \"replied\"],\n[\"pon\", null],\n[\"dit\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"arch\", \"archdeacon\"],\n[\"idi\", null],\n[\"acre\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"un\", null],\n[\"\u2581air\", \"air\"],\n[\"\u2581de\", null],\n[\"\u2581prof\", \"deep\"],\n[\"onde\", null],\n[\"\u2581r\u00e9\", \"reflection\"],\n[\"flex\", null],\n[\"ion\", null],\n[\",\", null],\n[\"\u2581que\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"hab\", \"clothing\"],\n[\"it\", null],\n[\"\u2581de\", null],\n[\"\u2581ces\", \"these\"],\n[\"\u2581caval\", \"horsemen\"],\n[\"iers\", null],\n[\"\u2581que\", null],\n[\"\u2581nous\", \"we\"],\n[\"\u2581ven\", \"have\"],\n[\"ons\", null],\n[\"\u2581de\", null],\n[\"\u2581vo\", \"seen\"],\n[\"ir\", null],\n[\"\u2581est\", \"is\"],\n[\"\u2581plus\", \"more\"],\n[\"\u2581be\", \"beautiful\"],\n[\"au\", null],\n[\"\u2581que\", \"than\"],\n[\"\u2581le\", null],\n[\"\u2581v\", \"yours\"],\n[\"\u00f4\", null],\n[\"tre\", null],\n[\"\u2581et\", \"and\"],\n[\"\u2581que\", null],\n[\"\u2581le\", null],\n[\"\u2581m\", \"mine\"],\n[\"ien\", null],\n[\"\u2581?\", null],\n[\"\u2581\u00bb\", null],\n[\"\u2581G\", \"Gringoire\"],\n[\"ring\", null],\n[\"oire\", null],\n[\"\u2581h\", \"nodded\"],\n[\"och\", null],\n[\"a\", null],\n[\"\u2581la\", null],\n[\"\u2581t\u00eate\", \"head\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n\u2013 Est-ce que vous ne trouvez pas, r\u00e9pondit l\u2019archidiacre d\u2019un air de profonde r\u00e9flexion, que l\u2019habit de ces cavaliers que nous venons de voir est plus beau que le v\u00f4tre et que le mien ? \u00bb Gringoire hocha la t\u00eate.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u2013', '\u2581Est', '-', 'ce', '\u2581que', '\u2581vous', '\u2581ne', '\u2581trou', 'vez', '\u2581pas', ',', '\u2581r\u00e9', 'pon', 'dit', '\u2581l', '\u2019', 'arch', 'idi', 'acre', '\u2581d', '\u2019', 'un', '\u2581air', '\u2581de', '\u2581prof', 'onde', '\u2581r\u00e9', 'flex', 'ion', ',', '\u2581que', '\u2581l', '\u2019', 'hab', 'it', '\u2581de', '\u2581ces', '\u2581caval', 'iers', '\u2581que', '\u2581nous', '\u2581ven', 'ons', '\u2581de', '\u2581vo', 'ir', '\u2581est', '\u2581plus', '\u2581be', 'au', '\u2581que', '\u2581le', '\u2581v', '\u00f4', 'tre', '\u2581et', '\u2581que', '\u2581le', '\u2581m', 'ien', '\u2581?', '\u2581\u00bb', '\u2581G', 'ring', 'oire', '\u2581h', 'och', 'a', '\u2581la', '\u2581t\u00eate', '.']"}], "[\n[\"\u2581\u2013\", null],\n[\"\u2581Est\", \"\u0420\u0430\u0437\u0432\u0435\"],\n[\"-\", null],\n[\"ce\", null],\n[\"\u2581que\", null],\n[\"\u2581vous\", null],\n[\"\u2581ne\", null],\n[\"\u2581trou\", \"\u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0435\"],\n[\"vez\", null],\n[\"\u2581pas\", null],\n[\",\", null],\n[\"\u2581r\u00e9\", \"\u043e\u0442\u0432\u0435\u0442\u0438\u043b\"],\n[\"pon\", null],\n[\"dit\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"arch\", \"\u0430\u0440\u0445\u0438\u0434\u044c\u044f\u043a\u043e\u043d\"],\n[\"idi\", null],\n[\"acre\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"un\", null],\n[\"\u2581air\", \"\u0441 \u0432\u0438\u0434\u043e\u043c\"],\n[\"\u2581de\", null],\n[\"\u2581prof\", \"\u0433\u043b\u0443\u0431\u043e\u043a\u043e\u0439\"],\n[\"onde\", null],\n[\"\u2581r\u00e9\", \"\u0437\u0430\u0434\u0443\u043c\u0447\u0438\u0432\u043e\u0441\u0442\u0438\"],\n[\"flex\", null],\n[\"ion\", null],\n[\",\", null],\n[\"\u2581que\", \"\u0447\u0442\u043e\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"hab\", \"\u043e\u0434\u0435\u0436\u0434\u0430\"],\n[\"it\", null],\n[\"\u2581de\", null],\n[\"\u2581ces\", \"\u044d\u0442\u0438\u0445\"],\n[\"\u2581caval\", \"\u0432\u0441\u0430\u0434\u043d\u0438\u043a\u043e\u0432\"],\n[\"iers\", null],\n[\"\u2581que\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u0445\"],\n[\"\u2581nous\", null],\n[\"\u2581ven\", \"\u0442\u043e\u043b\u044c\u043a\u043e \u0447\u0442\u043e\"],\n[\"ons\", null],\n[\"\u2581de\", null],\n[\"\u2581vo\", \"\u0432\u0438\u0434\u0435\u043b\u0438\"],\n[\"ir\", null],\n[\"\u2581est\", null],\n[\"\u2581plus\", \"\u043a\u0440\u0430\u0441\u0438\u0432\u0435\u0435\"],\n[\"\u2581be\", null],\n[\"au\", null],\n[\"\u2581que\", \"\u0447\u0435\u043c\"],\n[\"\u2581le\", null],\n[\"\u2581v\", \"\u0432\u0430\u0448\u0430\"],\n[\"\u00f4\", null],\n[\"tre\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581que\", null],\n[\"\u2581le\", null],\n[\"\u2581m\", \"\u043c\u043e\u044f\"],\n[\"ien\", null],\n[\"\u2581?\", null],\n[\"\u2581\u00bb\", null],\n[\"\u2581G\", \"\u0413\u0440\u0435\u043d\u0433\u0443\u0430\u0440\"],\n[\"ring\", null],\n[\"oire\", null],\n[\"\u2581h\", \"\u043f\u043e\u043a\u0430\u0447\u0430\u043b\"],\n[\"och\", null],\n[\"a\", null],\n[\"\u2581la\", null],\n[\"\u2581t\u00eate\", \"\u0433\u043e\u043b\u043e\u0432\u043e\u0439\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nEn moins de cinq minutes, ils atteignirent le sommet du mont; l\u00e0, ils se retourn\u00e8rent afin de juger la situation et de prendre une direction qui p\u00fbt d\u00e9pister les maoris.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581En', '\u2581moins', '\u2581de', '\u2581cinq', '\u2581minutes', ',', '\u2581ils', '\u2581at', 'te', 'ign', 'ire', 'nt', '\u2581le', '\u2581som', 'met', '\u2581du', '\u2581mont', ';', '\u2581l\u00e0', ',', '\u2581ils', '\u2581se', '\u2581retour', 'n', '\u00e8rent', '\u2581afin', '\u2581de', '\u2581jug', 'er', '\u2581la', '\u2581situation', '\u2581et', '\u2581de', '\u2581prend', 're', '\u2581une', '\u2581direction', '\u2581qui', '\u2581p', '\u00fbt', '\u2581d\u00e9', 'p', 'ister', '\u2581les', '\u2581ma', 'or', 'is', '.']"}], "[\n[\"\u2581En\", \"In\"],\n[\"\u2581moins\", \"less\"],\n[\"\u2581de\", null],\n[\"\u2581cinq\", \"five\"],\n[\"\u2581minutes\", \"minutes\"],\n[\",\", null],\n[\"\u2581ils\", \"they\"],\n[\"\u2581at\", \"reached\"],\n[\"te\", null],\n[\"ign\", null],\n[\"ire\", null],\n[\"nt\", null],\n[\"\u2581le\", null],\n[\"\u2581som\", \"summit\"],\n[\"met\", null],\n[\"\u2581du\", null],\n[\"\u2581mont\", \"mountain\"],\n[\";\", null],\n[\"\u2581l\u00e0\", \"there\"],\n[\",\", null],\n[\"\u2581ils\", \"they\"],\n[\"\u2581se\", null],\n[\"\u2581retour\", \"turned\"],\n[\"n\", null],\n[\"\u00e8rent\", null],\n[\"\u2581afin\", \"in order\"],\n[\"\u2581de\", null],\n[\"\u2581jug\", \"assess\"],\n[\"er\", null],\n[\"\u2581la\", null],\n[\"\u2581situation\", \"situation\"],\n[\"\u2581et\", \"and\"],\n[\"\u2581de\", null],\n[\"\u2581prend\", \"choose\"],\n[\"re\", null],\n[\"\u2581une\", null],\n[\"\u2581direction\", \"direction\"],\n[\"\u2581qui\", \"that\"],\n[\"\u2581p\", \"could\"],\n[\"\u00fbt\", null],\n[\"\u2581d\u00e9\", \"elude\"],\n[\"p\", null],\n[\"ister\", null],\n[\"\u2581les\", null],\n[\"\u2581ma\", \"Maoris\"],\n[\"or\", null],\n[\"is\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nEn moins de cinq minutes, ils atteignirent le sommet du mont; l\u00e0, ils se retourn\u00e8rent afin de juger la situation et de prendre une direction qui p\u00fbt d\u00e9pister les maoris.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581En', '\u2581moins', '\u2581de', '\u2581cinq', '\u2581minutes', ',', '\u2581ils', '\u2581at', 'te', 'ign', 'ire', 'nt', '\u2581le', '\u2581som', 'met', '\u2581du', '\u2581mont', ';', '\u2581l\u00e0', ',', '\u2581ils', '\u2581se', '\u2581retour', 'n', '\u00e8rent', '\u2581afin', '\u2581de', '\u2581jug', 'er', '\u2581la', '\u2581situation', '\u2581et', '\u2581de', '\u2581prend', 're', '\u2581une', '\u2581direction', '\u2581qui', '\u2581p', '\u00fbt', '\u2581d\u00e9', 'p', 'ister', '\u2581les', '\u2581ma', 'or', 'is', '.']"}], "[\n  [\"\u2581En\", null],\n  [\"\u2581moins\", \"\u041c\u0435\u043d\u0435\u0435\"],\n  [\"\u2581de\", null],\n  [\"\u2581cinq\", \"\u043f\u044f\u0442\u0438\"],\n  [\"\u2581minutes\", \"\u043c\u0438\u043d\u0443\u0442\"],\n  [\",\", null],\n  [\"\u2581ils\", \"\u043e\u043d\u0438\"],\n  [\"\u2581at\", \"\u0434\u043e\u0441\u0442\u0438\u0433\u043b\u0438\"],\n  [\"te\", null],\n  [\"ign\", null],\n  [\"ire\", null],\n  [\"nt\", null],\n  [\"\u2581le\", null],\n  [\"\u2581som\", \"\u0432\u0435\u0440\u0448\u0438\u043d\u044b\"],\n  [\"met\", null],\n  [\"\u2581du\", null],\n  [\"\u2581mont\", \"\u0433\u043e\u0440\u044b\"],\n  [\";\", null],\n  [\"\u2581l\u00e0\", \"\u0442\u0430\u043c\"],\n  [\",\", null],\n  [\"\u2581ils\", \"\u043e\u043d\u0438\"],\n  [\"\u2581se\", null],\n  [\"\u2581retour\", \"\u043f\u043e\u0432\u0435\u0440\u043d\u0443\u043b\u0438\u0441\u044c\"],\n  [\"n\", null],\n  [\"\u00e8rent\", null],\n  [\"\u2581afin\", \"\u0447\u0442\u043e\u0431\u044b\"],\n  [\"\u2581de\", null],\n  [\"\u2581jug\", \"\u043e\u0446\u0435\u043d\u0438\u0442\u044c\"],\n  [\"er\", null],\n  [\"\u2581la\", null],\n  [\"\u2581situation\", \"\u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044e\"],\n  [\"\u2581et\", \"\u0438\"],\n  [\"\u2581de\", null],\n  [\"\u2581prend\", \"\u0432\u044b\u0431\u0440\u0430\u0442\u044c\"],\n  [\"re\", null],\n  [\"\u2581une\", null],\n  [\"\u2581direction\", \"\u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435\"],\n  [\"\u2581qui\", \"\u043a\u043e\u0442\u043e\u0440\u043e\u0435\"],\n  [\"\u2581p\", \"\u043c\u043e\u0433\u043b\u043e\"],\n  [\"\u00fbt\", null],\n  [\"\u2581d\u00e9\", \"\u0441\u0431\u0438\u0442\u044c\"],\n  [\"p\", null],\n  [\"ister\", null],\n  [\"\u2581les\", null],\n  [\"\u2581ma\", \"\u043c\u0430\u043e\u0440\u0438\"],\n  [\"or\", null],\n  [\"is\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\nPardon, -- \u043f\u0440\u0438\u0431\u0430\u0432\u0438\u043b\u0430 \u043e\u043d\u0430, \u0433\u043b\u044f\u0434\u044f \u043d\u0430 \u043e\u043f\u044f\u0442\u044c \u0432\u043e\u0448\u0435\u0434\u0448\u0435\u0433\u043e \u0441 \u0434\u0440\u0443\u0433\u043e\u0439 \u0437\u0430\u043f\u0438\u0441\u043a\u043e\u0439 \u043b\u0430\u043a\u0435\u044f.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581P', 'ardon', ',', '\u2581--', '\u2581\u043f\u0440\u0438', '\u0431\u0430', '\u0432\u0438', '\u043b\u0430', '\u2581\u043e\u043d\u0430', ',', '\u2581\u0433', '\u043b\u044f', '\u0434\u044f', '\u2581\u043d\u0430', '\u2581\u043e\u043f', '\u044f', '\u0442\u044c', '\u2581\u0432\u043e', '\u0448\u0435', '\u0434', '\u0448\u0435\u0433\u043e', '\u2581\u0441', '\u2581\u0434\u0440\u0443', '\u0433\u043e', '\u0439', '\u2581\u0437\u0430', '\u043f\u0438', '\u0441\u043a\u043e\u0439', '\u2581\u043b\u0430', '\u043a\u0435', '\u044f', '.']"}], "[\n  [\"\u2581P\", \"Pardon\"],\n  [\"ardon\", null],\n  [\",\", null],\n  [\"\u2581--\", null],\n  [\"\u2581\u043f\u0440\u0438\", \"added\"],\n  [\"\u0431\u0430\", null],\n  [\"\u0432\u0438\", null],\n  [\"\u043b\u0430\", null],\n  [\"\u2581\u043e\u043d\u0430\", \"she\"],\n  [\",\", null],\n  [\"\u2581\u0433\", \"looking\"],\n  [\"\u043b\u044f\", null],\n  [\"\u0434\u044f\", null],\n  [\"\u2581\u043d\u0430\", \"at\"],\n  [\"\u2581\u043e\u043f\", \"again\"],\n  [\"\u044f\", null],\n  [\"\u0442\u044c\", null],\n  [\"\u2581\u0432\u043e\", \"entered\"],\n  [\"\u0448\u0435\", null],\n  [\"\u0434\", null],\n  [\"\u0448\u0435\u0433\u043e\", null],\n  [\"\u2581\u0441\", \"with\"],\n  [\"\u2581\u0434\u0440\u0443\", \"another\"],\n  [\"\u0433\u043e\", null],\n  [\"\u0439\", null],\n  [\"\u2581\u0437\u0430\", \"note\"],\n  [\"\u043f\u0438\", null],\n  [\"\u0441\u043a\u043e\u0439\", null],\n  [\"\u2581\u043b\u0430\", \"footman\"],\n  [\"\u043a\u0435\", null],\n  [\"\u044f\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\nPardon, -- \u043f\u0440\u0438\u0431\u0430\u0432\u0438\u043b\u0430 \u043e\u043d\u0430, \u0433\u043b\u044f\u0434\u044f \u043d\u0430 \u043e\u043f\u044f\u0442\u044c \u0432\u043e\u0448\u0435\u0434\u0448\u0435\u0433\u043e \u0441 \u0434\u0440\u0443\u0433\u043e\u0439 \u0437\u0430\u043f\u0438\u0441\u043a\u043e\u0439 \u043b\u0430\u043a\u0435\u044f.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581P', 'ardon', ',', '\u2581--', '\u2581\u043f\u0440\u0438', '\u0431\u0430', '\u0432\u0438', '\u043b\u0430', '\u2581\u043e\u043d\u0430', ',', '\u2581\u0433', '\u043b\u044f', '\u0434\u044f', '\u2581\u043d\u0430', '\u2581\u043e\u043f', '\u044f', '\u0442\u044c', '\u2581\u0432\u043e', '\u0448\u0435', '\u0434', '\u0448\u0435\u0433\u043e', '\u2581\u0441', '\u2581\u0434\u0440\u0443', '\u0433\u043e', '\u0439', '\u2581\u0437\u0430', '\u043f\u0438', '\u0441\u043a\u043e\u0439', '\u2581\u043b\u0430', '\u043a\u0435', '\u044f', '.']"}], "[\n[\"\u2581P\", \"Pardon\"],\n[\"ardon\", null],\n[\",\", null],\n[\"\u2581--\", null],\n[\"\u2581\u043f\u0440\u0438\", \"ajouta\"],\n[\"\u0431\u0430\", null],\n[\"\u0432\u0438\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\u043e\u043d\u0430\", \"elle\"],\n[\",\", null],\n[\"\u2581\u0433\", \"regardant\"],\n[\"\u043b\u044f\", null],\n[\"\u0434\u044f\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u043e\u043f\", \"nouveau\"],\n[\"\u044f\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0432\u043e\", \"entr\u00e9\"],\n[\"\u0448\u0435\", null],\n[\"\u0434\", null],\n[\"\u0448\u0435\u0433\u043e\", null],\n[\"\u2581\u0441\", \"avec\"],\n[\"\u2581\u0434\u0440\u0443\", \"autre\"],\n[\"\u0433\u043e\", null],\n[\"\u0439\", null],\n[\"\u2581\u0437\u0430\", \"note\"],\n[\"\u043f\u0438\", null],\n[\"\u0441\u043a\u043e\u0439\", null],\n[\"\u2581\u043b\u0430\", \"valet\"],\n[\"\u043a\u0435\", null],\n[\"\u044f\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0422\u0443\u0442 \u0431\u044b\u043b\u0430 \u0438 \u043f\u0440\u043e\u0441\u044c\u0431\u0430 \u043e \u043f\u0440\u043e\u0449\u0435\u043d\u0438\u0438, \u0438 \u0434\u043e\u0432\u0435\u0440\u0438\u0435 \u043a \u043d\u0435\u043c\u0443, \u0438 \u043b\u0430\u0441\u043a\u0430, \u043d\u0435\u0436\u043d\u0430\u044f, \u0440\u043e\u0431\u043a\u0430\u044f \u043b\u0430\u0441\u043a\u0430, \u0438 \u043e\u0431\u0435\u0449\u0430\u043d\u0438\u0435, \u0438 \u043d\u0430\u0434\u0435\u0436\u0434\u0430, \u0438 \u043b\u044e\u0431\u043e\u0432\u044c \u043a \u043d\u0435\u043c\u0443, \u0432 \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043e\u043d \u043d\u0435 \u043c\u043e\u0433 \u043d\u0435 \u0432\u0435\u0440\u0438\u0442\u044c \u0438 \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0434\u0443\u0448\u0438\u043b\u0430 \u0435\u0433\u043e \u0441\u0447\u0430\u0441\u0442\u044c\u0435\u043c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0422', '\u0443\u0442', '\u2581\u0431\u044b\u043b\u0430', '\u2581\u0438', '\u2581\u043f\u0440\u043e', '\u0441\u044c', '\u0431\u0430', '\u2581\u043e', '\u2581\u043f\u0440\u043e', '\u0449\u0435', '\u043d\u0438\u0438', ',', '\u2581\u0438', '\u2581\u0434\u043e', '\u0432\u0435', '\u0440\u0438', '\u0435', '\u2581\u043a', '\u2581\u043d\u0435', '\u043c\u0443', ',', '\u2581\u0438', '\u2581\u043b\u0430', '\u0441\u043a\u0430', ',', '\u2581\u043d\u0435', '\u0436', '\u043d\u0430\u044f', ',', '\u2581\u0440\u043e', '\u0431', '\u043a\u0430', '\u044f', '\u2581\u043b\u0430', '\u0441\u043a\u0430', ',', '\u2581\u0438', '\u2581\u043e\u0431\u0435', '\u0449\u0430', '\u043d\u0438\u0435', ',', '\u2581\u0438', '\u2581\u043d\u0430', '\u0434\u0435', '\u0436\u0434\u0430', ',', '\u2581\u0438', '\u2581\u043b\u044e', '\u0431\u043e', '\u0432', '\u044c', '\u2581\u043a', '\u2581\u043d\u0435', '\u043c\u0443', ',', '\u2581\u0432', '\u2581\u043a\u043e\u0442\u043e', '\u0440\u0443', '\u044e', '\u2581\u043e\u043d', '\u2581\u043d\u0435', '\u2581\u043c\u043e', '\u0433', '\u2581\u043d\u0435', '\u2581\u0432\u0435', '\u0440\u0438', '\u0442\u044c', '\u2581\u0438', '\u2581\u043a\u043e\u0442\u043e', '\u0440\u0430\u044f', '\u2581\u0434\u0443', '\u0448\u0438', '\u043b\u0430', '\u2581\u0435\u0433\u043e', '\u2581\u0441', '\u0447\u0430', '\u0441\u0442\u044c', '\u0435\u043c', '.']"}], "[\n[\"\u2581\u0422\", \"Here\"],\n[\"\u0443\u0442\", null],\n[\"\u2581\u0431\u044b\u043b\u0430\", \"was\"],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043f\u0440\u043e\", \"request\"],\n[\"\u0441\u044c\", null],\n[\"\u0431\u0430\", null],\n[\"\u2581\u043e\", null],\n[\"\u2581\u043f\u0440\u043e\", \"forgiveness\"],\n[\"\u0449\u0435\", null],\n[\"\u043d\u0438\u0438\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0434\u043e\", \"trust\"],\n[\"\u0432\u0435\", null],\n[\"\u0440\u0438\", null],\n[\"\u0435\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u043d\u0435\", \"him\"],\n[\"\u043c\u0443\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043b\u0430\", \"tenderness\"],\n[\"\u0441\u043a\u0430\", null],\n[\",\", null],\n[\"\u2581\u043d\u0435\", \"gentle\"],\n[\"\u0436\", null],\n[\"\u043d\u0430\u044f\", null],\n[\",\", null],\n[\"\u2581\u0440\u043e\", \"timid\"],\n[\"\u0431\", null],\n[\"\u043a\u0430\", null],\n[\"\u044f\", null],\n[\"\u2581\u043b\u0430\", \"tenderness\"],\n[\"\u0441\u043a\u0430\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043e\u0431\u0435\", \"promise\"],\n[\"\u0449\u0430\", null],\n[\"\u043d\u0438\u0435\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043d\u0430\", \"hope\"],\n[\"\u0434\u0435\", null],\n[\"\u0436\u0434\u0430\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043b\u044e\", \"love\"],\n[\"\u0431\u043e\", null],\n[\"\u0432\", null],\n[\"\u044c\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u043d\u0435\", \"him\"],\n[\"\u043c\u0443\", null],\n[\",\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\", \"which\"],\n[\"\u0440\u0443\", null],\n[\"\u044e\", null],\n[\"\u2581\u043e\u043d\", \"he\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u043c\u043e\", \"could\"],\n[\"\u0433\", null],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0432\u0435\", \"believe\"],\n[\"\u0440\u0438\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\", \"which\"],\n[\"\u0440\u0430\u044f\", null],\n[\"\u2581\u0434\u0443\", \"overwhelmed\"],\n[\"\u0448\u0438\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\u0435\u0433\u043e\", \"him\"],\n[\"\u2581\u0441\", \"with\"],\n[\"\u0447\u0430\", \"happiness\"],\n[\"\u0441\u0442\u044c\", null],\n[\"\u0435\u043c\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u0422\u0443\u0442 \u0431\u044b\u043b\u0430 \u0438 \u043f\u0440\u043e\u0441\u044c\u0431\u0430 \u043e \u043f\u0440\u043e\u0449\u0435\u043d\u0438\u0438, \u0438 \u0434\u043e\u0432\u0435\u0440\u0438\u0435 \u043a \u043d\u0435\u043c\u0443, \u0438 \u043b\u0430\u0441\u043a\u0430, \u043d\u0435\u0436\u043d\u0430\u044f, \u0440\u043e\u0431\u043a\u0430\u044f \u043b\u0430\u0441\u043a\u0430, \u0438 \u043e\u0431\u0435\u0449\u0430\u043d\u0438\u0435, \u0438 \u043d\u0430\u0434\u0435\u0436\u0434\u0430, \u0438 \u043b\u044e\u0431\u043e\u0432\u044c \u043a \u043d\u0435\u043c\u0443, \u0432 \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043e\u043d \u043d\u0435 \u043c\u043e\u0433 \u043d\u0435 \u0432\u0435\u0440\u0438\u0442\u044c \u0438 \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0434\u0443\u0448\u0438\u043b\u0430 \u0435\u0433\u043e \u0441\u0447\u0430\u0441\u0442\u044c\u0435\u043c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0422', '\u0443\u0442', '\u2581\u0431\u044b\u043b\u0430', '\u2581\u0438', '\u2581\u043f\u0440\u043e', '\u0441\u044c', '\u0431\u0430', '\u2581\u043e', '\u2581\u043f\u0440\u043e', '\u0449\u0435', '\u043d\u0438\u0438', ',', '\u2581\u0438', '\u2581\u0434\u043e', '\u0432\u0435', '\u0440\u0438', '\u0435', '\u2581\u043a', '\u2581\u043d\u0435', '\u043c\u0443', ',', '\u2581\u0438', '\u2581\u043b\u0430', '\u0441\u043a\u0430', ',', '\u2581\u043d\u0435', '\u0436', '\u043d\u0430\u044f', ',', '\u2581\u0440\u043e', '\u0431', '\u043a\u0430', '\u044f', '\u2581\u043b\u0430', '\u0441\u043a\u0430', ',', '\u2581\u0438', '\u2581\u043e\u0431\u0435', '\u0449\u0430', '\u043d\u0438\u0435', ',', '\u2581\u0438', '\u2581\u043d\u0430', '\u0434\u0435', '\u0436\u0434\u0430', ',', '\u2581\u0438', '\u2581\u043b\u044e', '\u0431\u043e', '\u0432', '\u044c', '\u2581\u043a', '\u2581\u043d\u0435', '\u043c\u0443', ',', '\u2581\u0432', '\u2581\u043a\u043e\u0442\u043e', '\u0440\u0443', '\u044e', '\u2581\u043e\u043d', '\u2581\u043d\u0435', '\u2581\u043c\u043e', '\u0433', '\u2581\u043d\u0435', '\u2581\u0432\u0435', '\u0440\u0438', '\u0442\u044c', '\u2581\u0438', '\u2581\u043a\u043e\u0442\u043e', '\u0440\u0430\u044f', '\u2581\u0434\u0443', '\u0448\u0438', '\u043b\u0430', '\u2581\u0435\u0433\u043e', '\u2581\u0441', '\u0447\u0430', '\u0441\u0442\u044c', '\u0435\u043c', '.']"}], "[\n[\"\u2581\u0422\", \"Ici\"],\n[\"\u0443\u0442\", null],\n[\"\u2581\u0431\u044b\u043b\u0430\", \"\u00e9tait\"],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043f\u0440\u043e\", \"demande\"],\n[\"\u0441\u044c\", null],\n[\"\u0431\u0430\", null],\n[\"\u2581\u043e\", null],\n[\"\u2581\u043f\u0440\u043e\", \"pardon\"],\n[\"\u0449\u0435\", null],\n[\"\u043d\u0438\u0438\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0434\u043e\", \"confiance\"],\n[\"\u0432\u0435\", null],\n[\"\u0440\u0438\", null],\n[\"\u0435\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u043d\u0435\", \"lui\"],\n[\"\u043c\u0443\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043b\u0430\", \"caresse\"],\n[\"\u0441\u043a\u0430\", null],\n[\",\", null],\n[\"\u2581\u043d\u0435\", \"tendre\"],\n[\"\u0436\", null],\n[\"\u043d\u0430\u044f\", null],\n[\",\", null],\n[\"\u2581\u0440\u043e\", \"timide\"],\n[\"\u0431\", null],\n[\"\u043a\u0430\", null],\n[\"\u044f\", null],\n[\"\u2581\u043b\u0430\", \"caresse\"],\n[\"\u0441\u043a\u0430\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043e\u0431\u0435\", \"promesse\"],\n[\"\u0449\u0430\", null],\n[\"\u043d\u0438\u0435\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043d\u0430\", \"espoir\"],\n[\"\u0434\u0435\", null],\n[\"\u0436\u0434\u0430\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043b\u044e\", \"amour\"],\n[\"\u0431\u043e\", null],\n[\"\u0432\", null],\n[\"\u044c\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u043d\u0435\", \"lui\"],\n[\"\u043c\u0443\", null],\n[\",\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\", \"laquelle\"],\n[\"\u0440\u0443\", null],\n[\"\u044e\", null],\n[\"\u2581\u043e\u043d\", \"il\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u043c\u043e\", \"pouvait\"],\n[\"\u0433\", null],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0432\u0435\", \"croire\"],\n[\"\u0440\u0438\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\", \"qui\"],\n[\"\u0440\u0430\u044f\", null],\n[\"\u2581\u0434\u0443\", \"\u00e9touffait\"],\n[\"\u0448\u0438\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\u0435\u0433\u043e\", \"le\"],\n[\"\u2581\u0441\", \"bonheur\"],\n[\"\u0447\u0430\", null],\n[\"\u0441\u0442\u044c\", null],\n[\"\u0435\u043c\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0421\u0443\u0445\u043e\u0441\u0442\u044c \u044d\u0442\u0430 \u043e\u0433\u043e\u0440\u0447\u0438\u043b\u0430 \u041a\u0438\u0442\u0438, \u0438 \u043e\u043d\u0430 \u043d\u0435 \u043c\u043e\u0433\u043b\u0430 \u0443\u0434\u0435\u0440\u0436\u0430\u0442\u044c\u0441\u044f \u043e\u0442 \u0436\u0435\u043b\u0430\u043d\u0438\u044f \u0437\u0430\u0433\u043b\u0430\u0434\u0438\u0442\u044c \u0445\u043e\u043b\u043e\u0434\u043d\u043e\u0441\u0442\u044c \u043c\u0430\u0442\u0435\u0440\u0438.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0421\u0443', '\u0445\u043e', '\u0441\u0442\u044c', '\u2581\u044d', '\u0442\u0430', '\u2581\u043e', '\u0433\u043e\u0440', '\u0447\u0438', '\u043b\u0430', '\u2581\u041a\u0438', '\u0442\u0438', ',', '\u2581\u0438', '\u2581\u043e\u043d\u0430', '\u2581\u043d\u0435', '\u2581\u043c\u043e', '\u0433\u043b\u0430', '\u2581\u0443', '\u0434\u0435\u0440', '\u0436\u0430', '\u0442\u044c\u0441\u044f', '\u2581\u043e\u0442', '\u2581\u0436\u0435', '\u043b\u0430', '\u043d\u0438\u044f', '\u2581\u0437\u0430', '\u0433\u043b\u0430', '\u0434\u0438', '\u0442\u044c', '\u2581\u0445\u043e', '\u043b\u043e', '\u0434', '\u043d\u043e\u0441\u0442\u044c', '\u2581\u043c\u0430', '\u0442\u0435\u0440\u0438', '.']"}], "[\n  [\"\u2581\u0421\u0443\", \"Dryness\"],\n  [\"\u0445\u043e\", null],\n  [\"\u0441\u0442\u044c\", null],\n  [\"\u2581\u044d\", \"this\"],\n  [\"\u0442\u0430\", null],\n  [\"\u2581\u043e\", \"upset\"],\n  [\"\u0433\u043e\u0440\", null],\n  [\"\u0447\u0438\", null],\n  [\"\u043b\u0430\", null],\n  [\"\u2581\u041a\u0438\", \"Kitty\"],\n  [\"\u0442\u0438\", null],\n  [\",\", null],\n  [\"\u2581\u0438\", \"and\"],\n  [\"\u2581\u043e\u043d\u0430\", \"she\"],\n  [\"\u2581\u043d\u0435\", \"could\"],\n  [\"\u2581\u043c\u043e\", null],\n  [\"\u0433\u043b\u0430\", null],\n  [\"\u2581\u0443\", \"not\"],\n  [\"\u0434\u0435\u0440\", \"resist\"],\n  [\"\u0436\u0430\", null],\n  [\"\u0442\u044c\u0441\u044f\", null],\n  [\"\u2581\u043e\u0442\", null],\n  [\"\u2581\u0436\u0435\", \"desire\"],\n  [\"\u043b\u0430\", null],\n  [\"\u043d\u0438\u044f\", null],\n  [\"\u2581\u0437\u0430\", \"smooth\"],\n  [\"\u0433\u043b\u0430\", null],\n  [\"\u0434\u0438\", null],\n  [\"\u0442\u044c\", null],\n  [\"\u2581\u0445\u043e\", \"coldness\"],\n  [\"\u043b\u043e\", null],\n  [\"\u0434\", null],\n  [\"\u043d\u043e\u0441\u0442\u044c\", null],\n  [\"\u2581\u043c\u0430\", \"mother\"],\n  [\"\u0442\u0435\u0440\u0438\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u0421\u0443\u0445\u043e\u0441\u0442\u044c \u044d\u0442\u0430 \u043e\u0433\u043e\u0440\u0447\u0438\u043b\u0430 \u041a\u0438\u0442\u0438, \u0438 \u043e\u043d\u0430 \u043d\u0435 \u043c\u043e\u0433\u043b\u0430 \u0443\u0434\u0435\u0440\u0436\u0430\u0442\u044c\u0441\u044f \u043e\u0442 \u0436\u0435\u043b\u0430\u043d\u0438\u044f \u0437\u0430\u0433\u043b\u0430\u0434\u0438\u0442\u044c \u0445\u043e\u043b\u043e\u0434\u043d\u043e\u0441\u0442\u044c \u043c\u0430\u0442\u0435\u0440\u0438.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0421\u0443', '\u0445\u043e', '\u0441\u0442\u044c', '\u2581\u044d', '\u0442\u0430', '\u2581\u043e', '\u0433\u043e\u0440', '\u0447\u0438', '\u043b\u0430', '\u2581\u041a\u0438', '\u0442\u0438', ',', '\u2581\u0438', '\u2581\u043e\u043d\u0430', '\u2581\u043d\u0435', '\u2581\u043c\u043e', '\u0433\u043b\u0430', '\u2581\u0443', '\u0434\u0435\u0440', '\u0436\u0430', '\u0442\u044c\u0441\u044f', '\u2581\u043e\u0442', '\u2581\u0436\u0435', '\u043b\u0430', '\u043d\u0438\u044f', '\u2581\u0437\u0430', '\u0433\u043b\u0430', '\u0434\u0438', '\u0442\u044c', '\u2581\u0445\u043e', '\u043b\u043e', '\u0434', '\u043d\u043e\u0441\u0442\u044c', '\u2581\u043c\u0430', '\u0442\u0435\u0440\u0438', '.']"}], "[\n  [\"\u2581\u0421\u0443\", \"Cette\"],\n  [\"\u0445\u043e\", null],\n  [\"\u0441\u0442\u044c\", null],\n  [\"\u2581\u044d\", null],\n  [\"\u0442\u0430\", null],\n  [\"\u2581\u043e\", null],\n  [\"\u0433\u043e\u0440\", \"attrista\"],\n  [\"\u0447\u0438\", null],\n  [\"\u043b\u0430\", null],\n  [\"\u2581\u041a\u0438\", \"Kitty\"],\n  [\"\u0442\u0438\", null],\n  [\",\", null],\n  [\"\u2581\u0438\", \"et\"],\n  [\"\u2581\u043e\u043d\u0430\", \"elle\"],\n  [\"\u2581\u043d\u0435\", \"ne\"],\n  [\"\u2581\u043c\u043e\", \"put\"],\n  [\"\u0433\u043b\u0430\", null],\n  [\"\u2581\u0443\", null],\n  [\"\u0434\u0435\u0440\", \"s'emp\u00eacher\"],\n  [\"\u0436\u0430\", null],\n  [\"\u0442\u044c\u0441\u044f\", null],\n  [\"\u2581\u043e\u0442\", null],\n  [\"\u2581\u0436\u0435\", \"d\u00e9sir\"],\n  [\"\u043b\u0430\", null],\n  [\"\u043d\u0438\u044f\", null],\n  [\"\u2581\u0437\u0430\", \"adoucir\"],\n  [\"\u0433\u043b\u0430\", null],\n  [\"\u0434\u0438\", null],\n  [\"\u0442\u044c\", null],\n  [\"\u2581\u0445\u043e\", \"froideur\"],\n  [\"\u043b\u043e\", null],\n  [\"\u0434\", null],\n  [\"\u043d\u043e\u0441\u0442\u044c\", null],\n  [\"\u2581\u043c\u0430\", \"m\u00e8re\"],\n  [\"\u0442\u0435\u0440\u0438\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0410\u043d\u043d\u0430 \u0437\u0430\u043c\u0435\u0442\u0438\u043b\u0430 \u044d\u0442\u043e\u0442 \u0432\u0437\u0434\u043e\u0445, \u0432\u044b\u043a\u0430\u0437\u044b\u0432\u0430\u0432\u0448\u0438\u0439 \u043d\u0435\u0441\u043e\u0433\u043b\u0430\u0441\u0438\u0435, \u0438 \u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0430\u043b\u0430.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0410\u043d', '\u043d\u0430', '\u2581\u0437\u0430', '\u043c\u0435', '\u0442\u0438', '\u043b\u0430', '\u2581\u044d\u0442\u043e', '\u0442', '\u2581\u0432', '\u0437', '\u0434\u043e', '\u0445', ',', '\u2581\u0432\u044b', '\u043a\u0430', '\u0437\u044b\u0432\u0430', '\u0432\u0448\u0438\u0439', '\u2581\u043d\u0435', '\u0441\u043e', '\u0433\u043b\u0430', '\u0441\u0438', '\u0435', ',', '\u2581\u0438', '\u2581\u043f\u0440\u043e', '\u0434\u043e\u043b', '\u0436\u0430', '\u043b\u0430', '.']"}], "[\n  [\"\u2581\u0410\u043d\", \"Anna\"],\n  [\"\u043d\u0430\", null],\n  [\"\u2581\u0437\u0430\", \"noticed\"],\n  [\"\u043c\u0435\", null],\n  [\"\u0442\u0438\", null],\n  [\"\u043b\u0430\", null],\n  [\"\u2581\u044d\u0442\u043e\", \"this\"],\n  [\"\u0442\", null],\n  [\"\u2581\u0432\", \"sigh\"],\n  [\"\u0437\", null],\n  [\"\u0434\u043e\", null],\n  [\"\u0445\", null],\n  [\",\", null],\n  [\"\u2581\u0432\u044b\", \"expressing\"],\n  [\"\u043a\u0430\", null],\n  [\"\u0437\u044b\u0432\u0430\", null],\n  [\"\u0432\u0448\u0438\u0439\", null],\n  [\"\u2581\u043d\u0435\", \"disagreement\"],\n  [\"\u0441\u043e\", null],\n  [\"\u0433\u043b\u0430\", null],\n  [\"\u0441\u0438\", null],\n  [\"\u0435\", null],\n  [\",\", null],\n  [\"\u2581\u0438\", \"and\"],\n  [\"\u2581\u043f\u0440\u043e\", \"continued\"],\n  [\"\u0434\u043e\u043b\", null],\n  [\"\u0436\u0430\", null],\n  [\"\u043b\u0430\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u0410\u043d\u043d\u0430 \u0437\u0430\u043c\u0435\u0442\u0438\u043b\u0430 \u044d\u0442\u043e\u0442 \u0432\u0437\u0434\u043e\u0445, \u0432\u044b\u043a\u0430\u0437\u044b\u0432\u0430\u0432\u0448\u0438\u0439 \u043d\u0435\u0441\u043e\u0433\u043b\u0430\u0441\u0438\u0435, \u0438 \u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0430\u043b\u0430.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0410\u043d', '\u043d\u0430', '\u2581\u0437\u0430', '\u043c\u0435', '\u0442\u0438', '\u043b\u0430', '\u2581\u044d\u0442\u043e', '\u0442', '\u2581\u0432', '\u0437', '\u0434\u043e', '\u0445', ',', '\u2581\u0432\u044b', '\u043a\u0430', '\u0437\u044b\u0432\u0430', '\u0432\u0448\u0438\u0439', '\u2581\u043d\u0435', '\u0441\u043e', '\u0433\u043b\u0430', '\u0441\u0438', '\u0435', ',', '\u2581\u0438', '\u2581\u043f\u0440\u043e', '\u0434\u043e\u043b', '\u0436\u0430', '\u043b\u0430', '.']"}], "[\n  [\"\u2581\u0410\u043d\", \"Anna\"],\n  [\"\u043d\u0430\", null],\n  [\"\u2581\u0437\u0430\", \"remarqua\"],\n  [\"\u043c\u0435\", null],\n  [\"\u0442\u0438\", null],\n  [\"\u043b\u0430\", null],\n  [\"\u2581\u044d\u0442\u043e\", \"ce\"],\n  [\"\u0442\", null],\n  [\"\u2581\u0432\", \"soupir\"],\n  [\"\u0437\", null],\n  [\"\u0434\u043e\", null],\n  [\"\u0445\", null],\n  [\",\", null],\n  [\"\u2581\u0432\u044b\", \"exprimant\"],\n  [\"\u043a\u0430\", null],\n  [\"\u0437\u044b\u0432\u0430\", null],\n  [\"\u0432\u0448\u0438\u0439\", null],\n  [\"\u2581\u043d\u0435\", \"d\u00e9saccord\"],\n  [\"\u0441\u043e\", null],\n  [\"\u0433\u043b\u0430\", null],\n  [\"\u0441\u0438\", null],\n  [\"\u0435\", null],\n  [\",\", null],\n  [\"\u2581\u0438\", \"et\"],\n  [\"\u2581\u043f\u0440\u043e\", \"continua\"],\n  [\"\u0434\u043e\u043b\", null],\n  [\"\u0436\u0430\", null],\n  [\"\u043b\u0430\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n-- \u0414\u0430, \u044f \u043f\u043e\u0442\u0435\u0440\u044f\u043b \u0434\u0430\u0436\u0435 \u043b\u044e\u0431\u043e\u0432\u044c \u043a \u0441\u044b\u043d\u0443, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0441 \u043d\u0438\u043c \u0441\u0432\u044f\u0437\u0430\u043d\u043e \u043c\u043e\u0435 \u043e\u0442\u0432\u0440\u0430\u0449\u0435\u043d\u0438\u0435 \u043a \u0432\u0430\u043c.\u041d\u043e \u044f \u0432\u0441\u0435-\u0442\u0430\u043a\u0438 \u0432\u043e\u0437\u044c\u043c\u0443 \u0435\u0433\u043e.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581--', '\u2581\u0414\u0430', ',', '\u2581\u044f', '\u2581\u043f\u043e', '\u0442\u0435', '\u0440\u044f', '\u043b', '\u2581\u0434\u0430', '\u0436\u0435', '\u2581\u043b\u044e', '\u0431\u043e', '\u0432', '\u044c', '\u2581\u043a', '\u2581\u0441\u044b', '\u043d\u0443', ',', '\u2581\u043f\u043e', '\u0442\u043e', '\u043c\u0443', '\u2581\u0447\u0442\u043e', '\u2581\u0441', '\u2581', '\u043d\u0438\u043c', '\u2581\u0441\u0432\u044f', '\u0437\u0430', '\u043d\u043e', '\u2581\u043c\u043e', '\u0435', '\u2581\u043e\u0442', '\u0432\u0440\u0430', '\u0449\u0435', '\u043d\u0438\u0435', '\u2581\u043a', '\u2581\u0432\u0430', '\u043c', '.', '\u041d', '\u043e', '\u2581\u044f', '\u2581\u0432\u0441\u0435', '-', '\u0442\u0430', '\u043a\u0438', '\u2581\u0432\u043e\u0437', '\u044c', '\u043c\u0443', '\u2581\u0435\u0433\u043e', '.']"}], "[\n[\"\u2581--\", null],\n[\"\u2581\u0414\u0430\", \"Yes\"],\n[\",\", null],\n[\"\u2581\u044f\", \"I\"],\n[\"\u2581\u043f\u043e\", \"lost\"],\n[\"\u0442\u0435\", null],\n[\"\u0440\u044f\", null],\n[\"\u043b\", null],\n[\"\u2581\u0434\u0430\", \"even\"],\n[\"\u0436\u0435\", null],\n[\"\u2581\u043b\u044e\", \"love\"],\n[\"\u0431\u043e\", null],\n[\"\u0432\", null],\n[\"\u044c\", null],\n[\"\u2581\u043a\", \"for\"],\n[\"\u2581\u0441\u044b\", \"son\"],\n[\"\u043d\u0443\", null],\n[\",\", null],\n[\"\u2581\u043f\u043e\", \"because\"],\n[\"\u0442\u043e\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0441\", \"with\"],\n[\"\u2581\", null],\n[\"\u043d\u0438\u043c\", \"him\"],\n[\"\u2581\u0441\u0432\u044f\", \"connected\"],\n[\"\u0437\u0430\", null],\n[\"\u043d\u043e\", null],\n[\"\u2581\u043c\u043e\", \"my\"],\n[\"\u0435\", null],\n[\"\u2581\u043e\u0442\", \"disgust\"],\n[\"\u0432\u0440\u0430\", null],\n[\"\u0449\u0435\", null],\n[\"\u043d\u0438\u0435\", null],\n[\"\u2581\u043a\", \"towards\"],\n[\"\u2581\u0432\u0430\", \"you\"],\n[\"\u043c\", null],\n[\".\", null],\n[\"\u041d\", \"But\"],\n[\"\u043e\", null],\n[\"\u2581\u044f\", \"I\"],\n[\"\u2581\u0432\u0441\u0435\", \"still\"],\n[\"-\", null],\n[\"\u0442\u0430\", null],\n[\"\u043a\u0438\", null],\n[\"\u2581\u0432\u043e\u0437\", \"will take\"],\n[\"\u044c\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u0435\u0433\u043e\", \"him\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n-- \u0414\u0430, \u044f \u043f\u043e\u0442\u0435\u0440\u044f\u043b \u0434\u0430\u0436\u0435 \u043b\u044e\u0431\u043e\u0432\u044c \u043a \u0441\u044b\u043d\u0443, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0441 \u043d\u0438\u043c \u0441\u0432\u044f\u0437\u0430\u043d\u043e \u043c\u043e\u0435 \u043e\u0442\u0432\u0440\u0430\u0449\u0435\u043d\u0438\u0435 \u043a \u0432\u0430\u043c.\u041d\u043e \u044f \u0432\u0441\u0435-\u0442\u0430\u043a\u0438 \u0432\u043e\u0437\u044c\u043c\u0443 \u0435\u0433\u043e.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581--', '\u2581\u0414\u0430', ',', '\u2581\u044f', '\u2581\u043f\u043e', '\u0442\u0435', '\u0440\u044f', '\u043b', '\u2581\u0434\u0430', '\u0436\u0435', '\u2581\u043b\u044e', '\u0431\u043e', '\u0432', '\u044c', '\u2581\u043a', '\u2581\u0441\u044b', '\u043d\u0443', ',', '\u2581\u043f\u043e', '\u0442\u043e', '\u043c\u0443', '\u2581\u0447\u0442\u043e', '\u2581\u0441', '\u2581', '\u043d\u0438\u043c', '\u2581\u0441\u0432\u044f', '\u0437\u0430', '\u043d\u043e', '\u2581\u043c\u043e', '\u0435', '\u2581\u043e\u0442', '\u0432\u0440\u0430', '\u0449\u0435', '\u043d\u0438\u0435', '\u2581\u043a', '\u2581\u0432\u0430', '\u043c', '.', '\u041d', '\u043e', '\u2581\u044f', '\u2581\u0432\u0441\u0435', '-', '\u0442\u0430', '\u043a\u0438', '\u2581\u0432\u043e\u0437', '\u044c', '\u043c\u0443', '\u2581\u0435\u0433\u043e', '.']"}], "[\n[\"\u2581--\", null],\n[\"\u2581\u0414\u0430\", \"Oui\"],\n[\",\", null],\n[\"\u2581\u044f\", \"je\"],\n[\"\u2581\u043f\u043e\", \"perdu\"],\n[\"\u0442\u0435\", null],\n[\"\u0440\u044f\", null],\n[\"\u043b\", null],\n[\"\u2581\u0434\u0430\", \"m\u00eame\"],\n[\"\u0436\u0435\", null],\n[\"\u2581\u043b\u044e\", \"amour\"],\n[\"\u0431\u043e\", null],\n[\"\u0432\", null],\n[\"\u044c\", null],\n[\"\u2581\u043a\", \"pour\"],\n[\"\u2581\u0441\u044b\", \"fils\"],\n[\"\u043d\u0443\", null],\n[\",\", null],\n[\"\u2581\u043f\u043e\", \"parce\"],\n[\"\u0442\u043e\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u0447\u0442\u043e\", \"que\"],\n[\"\u2581\u0441\", \"avec\"],\n[\"\u2581\", null],\n[\"\u043d\u0438\u043c\", \"lui\"],\n[\"\u2581\u0441\u0432\u044f\", \"li\u00e9\"],\n[\"\u0437\u0430\", null],\n[\"\u043d\u043e\", null],\n[\"\u2581\u043c\u043e\", \"mon\"],\n[\"\u0435\", null],\n[\"\u2581\u043e\u0442\", \"d\u00e9go\u00fbt\"],\n[\"\u0432\u0440\u0430\", null],\n[\"\u0449\u0435\", null],\n[\"\u043d\u0438\u0435\", null],\n[\"\u2581\u043a\", \"envers\"],\n[\"\u2581\u0432\u0430\", \"vous\"],\n[\"\u043c\", null],\n[\".\", null],\n[\"\u041d\", \"Mais\"],\n[\"\u043e\", null],\n[\"\u2581\u044f\", \"je\"],\n[\"\u2581\u0432\u0441\u0435\", \"quand\"],\n[\"-\", null],\n[\"\u0442\u0430\", \"m\u00eame\"],\n[\"\u043a\u0438\", null],\n[\"\u2581\u0432\u043e\u0437\", \"prendrai\"],\n[\"\u044c\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u0435\u0433\u043e\", \"le\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041f\u0440\u0430\u0432\u0434\u0430, \u0447\u0442\u043e \u043d\u0430 \u0441\u043a\u043e\u0442\u043d\u043e\u043c \u0434\u0432\u043e\u0440\u0435 \u0434\u0435\u043b\u043e \u0448\u043b\u043e \u0434\u043e \u0441\u0438\u0445 \u043f\u043e\u0440 \u043d\u0435 \u043b\u0443\u0447\u0448\u0435, \u0447\u0435\u043c \u043f\u0440\u0435\u0436\u0434\u0435, \u0438 \u0418\u0432\u0430\u043d \u0441\u0438\u043b\u044c\u043d\u043e \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u043e\u0432\u0430\u043b \u0442\u0435\u043f\u043b\u043e\u043c\u0443 \u043f\u043e\u043c\u0435\u0449\u0435\u043d\u0438\u044e \u043a\u043e\u0440\u043e\u0432 \u0438 \u0441\u043b\u0438\u0432\u043e\u0447\u043d\u043e\u043c\u0443 \u043c\u0430\u0441\u043b\u0443, \u0443\u0442\u0432\u0435\u0440\u0436\u0434\u0430\u044f, \u0447\u0442\u043e \u043a\u043e\u0440\u043e\u0432\u0435 \u043d\u0430 \u0445\u043e\u043b\u043e\u0434\u0443 \u043f\u043e\u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u043c\u0435\u043d\u044c\u0448\u0435 \u043a\u043e\u0440\u043c\u0443 \u0438 \u0447\u0442\u043e \u0441\u043c\u0435\u0442\u0430\u043d\u043d\u043e\u0435 \u043c\u0430\u0441\u043b\u043e \u0441\u043f\u043e\u0440\u0435\u0435, \u0438 \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043b \u0436\u0430\u043b\u043e\u0432\u0430\u043d\u044c\u044f, \u043a\u0430\u043a \u0438 \u0432 \u0441\u0442\u0430\u0440\u0438\u043d\u0443, \u0438 \u043d\u0438\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043d\u0435 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043e\u0432\u0430\u043b\u0441\u044f \u0442\u0435\u043c, \u0447\u0442\u043e \u0434\u0435\u043d\u044c\u0433\u0438, \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c\u044b\u0435 \u0438\u043c, \u0431\u044b\u043b\u0438 \u043d\u0435 \u0436\u0430\u043b\u043e\u0432\u0430\u043d\u044c\u0435, \u0430 \u0432\u044b\u0434\u0430\u0447\u0430 \u0432\u043f\u0435\u0440\u0435\u0434 \u0434\u043e\u043b\u0438 \u0431\u0430\u0440\u044b\u0448\u0430.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041f', '\u0440\u0430\u0432', '\u0434\u0430', ',', '\u2581\u0447\u0442\u043e', '\u2581\u043d\u0430', '\u2581\u0441\u043a\u043e', '\u0442', '\u043d\u043e\u043c', '\u2581\u0434\u0432\u043e', '\u0440\u0435', '\u2581\u0434\u0435', '\u043b\u043e', '\u2581\u0448', '\u043b\u043e', '\u2581\u0434\u043e', '\u2581\u0441\u0438', '\u0445', '\u2581\u043f\u043e', '\u0440', '\u2581\u043d\u0435', '\u2581\u043b\u0443', '\u0447', '\u0448\u0435', ',', '\u2581\u0447\u0435\u043c', '\u2581\u043f\u0440\u0435', '\u0436\u0434\u0435', ',', '\u2581\u0438', '\u2581\u0418', '\u0432\u0430\u043d', '\u2581\u0441\u0438', '\u043b\u044c\u043d\u043e', '\u2581\u043f\u0440\u043e\u0442\u0438', '\u0432\u043e', '\u0434\u0435\u0439', '\u0441\u0442\u0432\u043e', '\u0432\u0430\u043b', '\u2581\u0442\u0435', '\u043f\u043b\u043e', '\u043c\u0443', '\u2581\u043f\u043e', '\u043c\u0435', '\u0449\u0435', '\u043d\u0438\u044e', '\u2581\u043a\u043e', '\u0440\u043e\u0432', '\u2581\u0438', '\u2581\u0441', '\u043b\u0438', '\u0432\u043e', '\u0447\u043d\u043e', '\u043c\u0443', '\u2581\u043c\u0430', '\u0441\u043b\u0443', ',', '\u2581\u0443', '\u0442\u0432\u0435\u0440', '\u0436\u0434\u0430', '\u044f', ',', '\u2581\u0447\u0442\u043e', '\u2581\u043a\u043e\u0440\u043e', '\u0432\u0435', '\u2581\u043d\u0430', '\u2581\u0445\u043e', '\u043b\u043e', '\u0434\u0443', '\u2581\u043f\u043e', '\u0442\u0440\u0435', '\u0431\u0443', '\u0435\u0442\u0441\u044f', '\u2581\u043c', '\u0435\u043d\u044c', '\u0448\u0435', '\u2581\u043a\u043e\u0440', '\u043c\u0443', '\u2581\u0438', '\u2581\u0447\u0442\u043e', '\u2581\u0441', '\u043c\u0435', '\u0442\u0430\u043d', '\u043d\u043e\u0435', '\u2581\u043c\u0430', '\u0441\u043b\u043e', '\u2581\u0441\u043f\u043e', '\u0440\u0435', '\u0435', ',', '\u2581\u0438', '\u2581\u0442\u0440\u0435', '\u0431\u043e', '\u0432\u0430\u043b', '\u2581', '\u0436\u0430', '\u043b\u043e', '\u0432\u0430\u043d', '\u044c\u044f', ',', '\u2581\u043a\u0430\u043a', '\u2581\u0438', '\u2581\u0432', '\u2581\u0441\u0442\u0430', '\u0440\u0438', '\u043d\u0443', ',', '\u2581\u0438', '\u2581\u043d\u0438', '\u0441\u043a\u043e', '\u043b\u044c\u043a\u043e', '\u2581\u043d\u0435', '\u2581\u0438\u043d', '\u0442\u0435', '\u0440\u0435', '\u0441\u043e', '\u0432\u0430', '\u043b\u0441\u044f', '\u2581\u0442\u0435\u043c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0434', '\u0435\u043d\u044c', '\u0433\u0438', ',', '\u2581\u043f\u043e\u043b\u0443', '\u0447\u0430', '\u0435\u043c\u044b', '\u0435', '\u2581\u0438\u043c', ',', '\u2581\u0431\u044b\u043b\u0438', '\u2581\u043d\u0435', '\u2581', '\u0436\u0430', '\u043b\u043e', '\u0432\u0430\u043d', '\u044c\u0435', ',', '\u2581\u0430', '\u2581\u0432\u044b', '\u0434\u0430', '\u0447\u0430', '\u2581\u0432', '\u043f', '\u0435\u0440\u0435\u0434', '\u2581\u0434\u043e', '\u043b\u0438', '\u2581\u0431', '\u0430\u0440', '\u044b', '\u0448\u0430', '.']"}], "[\n[\"\u2581\u041f\", \"Truth\"],\n[\"\u0440\u0430\u0432\", null],\n[\"\u0434\u0430\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u0441\u043a\u043e\", \"cattle\"],\n[\"\u0442\", null],\n[\"\u043d\u043e\u043c\", null],\n[\"\u2581\u0434\u0432\u043e\", \"yard\"],\n[\"\u0440\u0435\", null],\n[\"\u2581\u0434\u0435\", \"matter\"],\n[\"\u043b\u043e\", null],\n[\"\u2581\u0448\", \"went\"],\n[\"\u043b\u043e\", null],\n[\"\u2581\u0434\u043e\", null],\n[\"\u2581\u0441\u0438\", \"these\"],\n[\"\u0445\", null],\n[\"\u2581\u043f\u043e\", null],\n[\"\u0440\", null],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u043b\u0443\", \"better\"],\n[\"\u0447\", null],\n[\"\u0448\u0435\", null],\n[\",\", null],\n[\"\u2581\u0447\u0435\u043c\", null],\n[\"\u2581\u043f\u0440\u0435\", \"before\"],\n[\"\u0436\u0434\u0435\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0418\", \"Ivan\"],\n[\"\u0432\u0430\u043d\", null],\n[\"\u2581\u0441\u0438\", \"strongly\"],\n[\"\u043b\u044c\u043d\u043e\", null],\n[\"\u2581\u043f\u0440\u043e\u0442\u0438\", \"opposed\"],\n[\"\u0432\u043e\", null],\n[\"\u0434\u0435\u0439\", null],\n[\"\u0441\u0442\u0432\u043e\", null],\n[\"\u0432\u0430\u043b\", null],\n[\"\u2581\u0442\u0435\", \"warm\"],\n[\"\u043f\u043b\u043e\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u043f\u043e\", \"housing\"],\n[\"\u043c\u0435\", null],\n[\"\u0449\u0435\", null],\n[\"\u043d\u0438\u044e\", null],\n[\"\u2581\u043a\u043e\", \"cows\"],\n[\"\u0440\u043e\u0432\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0441\", \"cream\"],\n[\"\u043b\u0438\", null],\n[\"\u0432\u043e\", null],\n[\"\u0447\u043d\u043e\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u043c\u0430\", \"butter\"],\n[\"\u0441\u043b\u0443\", null],\n[\",\", null],\n[\"\u2581\u0443\", \"claiming\"],\n[\"\u0442\u0432\u0435\u0440\", null],\n[\"\u0436\u0434\u0430\", null],\n[\"\u044f\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u043a\u043e\u0440\u043e\", \"cow\"],\n[\"\u0432\u0435\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u0445\u043e\", \"cold\"],\n[\"\u043b\u043e\", null],\n[\"\u0434\u0443\", null],\n[\"\u2581\u043f\u043e\", \"require\"],\n[\"\u0442\u0440\u0435\", null],\n[\"\u0431\u0443\", null],\n[\"\u0435\u0442\u0441\u044f\", null],\n[\"\u2581\u043c\", \"less\"],\n[\"\u0435\u043d\u044c\", null],\n[\"\u0448\u0435\", null],\n[\"\u2581\u043a\u043e\u0440\", \"feed\"],\n[\"\u043c\u0443\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0441\", \"sour\"],\n[\"\u043c\u0435\", null],\n[\"\u0442\u0430\u043d\", null],\n[\"\u043d\u043e\u0435\", null],\n[\"\u2581\u043c\u0430\", \"cream\"],\n[\"\u0441\u043b\u043e\", null],\n[\"\u2581\u0441\u043f\u043e\", \"quicker\"],\n[\"\u0440\u0435\", null],\n[\"\u0435\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0442\u0440\u0435\", \"demanded\"],\n[\"\u0431\u043e\", null],\n[\"\u0432\u0430\u043b\", null],\n[\"\u2581\", null],\n[\"\u0436\u0430\", \"salary\"],\n[\"\u043b\u043e\", null],\n[\"\u0432\u0430\u043d\", null],\n[\"\u044c\u044f\", null],\n[\",\", null],\n[\"\u2581\u043a\u0430\u043a\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0441\u0442\u0430\", \"old\"],\n[\"\u0440\u0438\", null],\n[\"\u043d\u0443\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043d\u0438\", \"not\"],\n[\"\u0441\u043a\u043e\", null],\n[\"\u043b\u044c\u043a\u043e\", null],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0438\u043d\", \"interested\"],\n[\"\u0442\u0435\", null],\n[\"\u0440\u0435\", null],\n[\"\u0441\u043e\", null],\n[\"\u0432\u0430\", null],\n[\"\u043b\u0441\u044f\", null],\n[\"\u2581\u0442\u0435\u043c\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0434\", \"money\"],\n[\"\u0435\u043d\u044c\", null],\n[\"\u0433\u0438\", null],\n[\",\", null],\n[\"\u2581\u043f\u043e\u043b\u0443\", \"received\"],\n[\"\u0447\u0430\", null],\n[\"\u0435\u043c\u044b\", null],\n[\"\u0435\", null],\n[\"\u2581\u0438\u043c\", null],\n[\",\", null],\n[\"\u2581\u0431\u044b\u043b\u0438\", null],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\", null],\n[\"\u0436\u0430\", \"salary\"],\n[\"\u043b\u043e\", null],\n[\"\u0432\u0430\u043d\", null],\n[\"\u044c\u0435\", null],\n[\",\", null],\n[\"\u2581\u0430\", null],\n[\"\u2581\u0432\u044b\", \"advance\"],\n[\"\u0434\u0430\", null],\n[\"\u0447\u0430\", null],\n[\"\u2581\u0432\", null],\n[\"\u043f\", null],\n[\"\u0435\u0440\u0435\u0434\", null],\n[\"\u2581\u0434\u043e\", \"share\"],\n[\"\u043b\u0438\", null],\n[\"\u2581\u0431\", \"profit\"],\n[\"\u0430\u0440\", null],\n[\"\u044b\", null],\n[\"\u0448\u0430\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u041f\u0440\u0430\u0432\u0434\u0430, \u0447\u0442\u043e \u043d\u0430 \u0441\u043a\u043e\u0442\u043d\u043e\u043c \u0434\u0432\u043e\u0440\u0435 \u0434\u0435\u043b\u043e \u0448\u043b\u043e \u0434\u043e \u0441\u0438\u0445 \u043f\u043e\u0440 \u043d\u0435 \u043b\u0443\u0447\u0448\u0435, \u0447\u0435\u043c \u043f\u0440\u0435\u0436\u0434\u0435, \u0438 \u0418\u0432\u0430\u043d \u0441\u0438\u043b\u044c\u043d\u043e \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u043e\u0432\u0430\u043b \u0442\u0435\u043f\u043b\u043e\u043c\u0443 \u043f\u043e\u043c\u0435\u0449\u0435\u043d\u0438\u044e \u043a\u043e\u0440\u043e\u0432 \u0438 \u0441\u043b\u0438\u0432\u043e\u0447\u043d\u043e\u043c\u0443 \u043c\u0430\u0441\u043b\u0443, \u0443\u0442\u0432\u0435\u0440\u0436\u0434\u0430\u044f, \u0447\u0442\u043e \u043a\u043e\u0440\u043e\u0432\u0435 \u043d\u0430 \u0445\u043e\u043b\u043e\u0434\u0443 \u043f\u043e\u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u043c\u0435\u043d\u044c\u0448\u0435 \u043a\u043e\u0440\u043c\u0443 \u0438 \u0447\u0442\u043e \u0441\u043c\u0435\u0442\u0430\u043d\u043d\u043e\u0435 \u043c\u0430\u0441\u043b\u043e \u0441\u043f\u043e\u0440\u0435\u0435, \u0438 \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043b \u0436\u0430\u043b\u043e\u0432\u0430\u043d\u044c\u044f, \u043a\u0430\u043a \u0438 \u0432 \u0441\u0442\u0430\u0440\u0438\u043d\u0443, \u0438 \u043d\u0438\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043d\u0435 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043e\u0432\u0430\u043b\u0441\u044f \u0442\u0435\u043c, \u0447\u0442\u043e \u0434\u0435\u043d\u044c\u0433\u0438, \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c\u044b\u0435 \u0438\u043c, \u0431\u044b\u043b\u0438 \u043d\u0435 \u0436\u0430\u043b\u043e\u0432\u0430\u043d\u044c\u0435, \u0430 \u0432\u044b\u0434\u0430\u0447\u0430 \u0432\u043f\u0435\u0440\u0435\u0434 \u0434\u043e\u043b\u0438 \u0431\u0430\u0440\u044b\u0448\u0430.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041f', '\u0440\u0430\u0432', '\u0434\u0430', ',', '\u2581\u0447\u0442\u043e', '\u2581\u043d\u0430', '\u2581\u0441\u043a\u043e', '\u0442', '\u043d\u043e\u043c', '\u2581\u0434\u0432\u043e', '\u0440\u0435', '\u2581\u0434\u0435', '\u043b\u043e', '\u2581\u0448', '\u043b\u043e', '\u2581\u0434\u043e', '\u2581\u0441\u0438', '\u0445', '\u2581\u043f\u043e', '\u0440', '\u2581\u043d\u0435', '\u2581\u043b\u0443', '\u0447', '\u0448\u0435', ',', '\u2581\u0447\u0435\u043c', '\u2581\u043f\u0440\u0435', '\u0436\u0434\u0435', ',', '\u2581\u0438', '\u2581\u0418', '\u0432\u0430\u043d', '\u2581\u0441\u0438', '\u043b\u044c\u043d\u043e', '\u2581\u043f\u0440\u043e\u0442\u0438', '\u0432\u043e', '\u0434\u0435\u0439', '\u0441\u0442\u0432\u043e', '\u0432\u0430\u043b', '\u2581\u0442\u0435', '\u043f\u043b\u043e', '\u043c\u0443', '\u2581\u043f\u043e', '\u043c\u0435', '\u0449\u0435', '\u043d\u0438\u044e', '\u2581\u043a\u043e', '\u0440\u043e\u0432', '\u2581\u0438', '\u2581\u0441', '\u043b\u0438', '\u0432\u043e', '\u0447\u043d\u043e', '\u043c\u0443', '\u2581\u043c\u0430', '\u0441\u043b\u0443', ',', '\u2581\u0443', '\u0442\u0432\u0435\u0440', '\u0436\u0434\u0430', '\u044f', ',', '\u2581\u0447\u0442\u043e', '\u2581\u043a\u043e\u0440\u043e', '\u0432\u0435', '\u2581\u043d\u0430', '\u2581\u0445\u043e', '\u043b\u043e', '\u0434\u0443', '\u2581\u043f\u043e', '\u0442\u0440\u0435', '\u0431\u0443', '\u0435\u0442\u0441\u044f', '\u2581\u043c', '\u0435\u043d\u044c', '\u0448\u0435', '\u2581\u043a\u043e\u0440', '\u043c\u0443', '\u2581\u0438', '\u2581\u0447\u0442\u043e', '\u2581\u0441', '\u043c\u0435', '\u0442\u0430\u043d', '\u043d\u043e\u0435', '\u2581\u043c\u0430', '\u0441\u043b\u043e', '\u2581\u0441\u043f\u043e', '\u0440\u0435', '\u0435', ',', '\u2581\u0438', '\u2581\u0442\u0440\u0435', '\u0431\u043e', '\u0432\u0430\u043b', '\u2581', '\u0436\u0430', '\u043b\u043e', '\u0432\u0430\u043d', '\u044c\u044f', ',', '\u2581\u043a\u0430\u043a', '\u2581\u0438', '\u2581\u0432', '\u2581\u0441\u0442\u0430', '\u0440\u0438', '\u043d\u0443', ',', '\u2581\u0438', '\u2581\u043d\u0438', '\u0441\u043a\u043e', '\u043b\u044c\u043a\u043e', '\u2581\u043d\u0435', '\u2581\u0438\u043d', '\u0442\u0435', '\u0440\u0435', '\u0441\u043e', '\u0432\u0430', '\u043b\u0441\u044f', '\u2581\u0442\u0435\u043c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0434', '\u0435\u043d\u044c', '\u0433\u0438', ',', '\u2581\u043f\u043e\u043b\u0443', '\u0447\u0430', '\u0435\u043c\u044b', '\u0435', '\u2581\u0438\u043c', ',', '\u2581\u0431\u044b\u043b\u0438', '\u2581\u043d\u0435', '\u2581', '\u0436\u0430', '\u043b\u043e', '\u0432\u0430\u043d', '\u044c\u0435', ',', '\u2581\u0430', '\u2581\u0432\u044b', '\u0434\u0430', '\u0447\u0430', '\u2581\u0432', '\u043f', '\u0435\u0440\u0435\u0434', '\u2581\u0434\u043e', '\u043b\u0438', '\u2581\u0431', '\u0430\u0440', '\u044b', '\u0448\u0430', '.']"}], "[\n[\"\u2581\u041f\", \"V\u00e9rit\u00e9\"],\n[\"\u0440\u0430\u0432\", null],\n[\"\u0434\u0430\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u0441\u043a\u043e\", \"ferme\"],\n[\"\u0442\", null],\n[\"\u043d\u043e\u043c\", null],\n[\"\u2581\u0434\u0432\u043e\", null],\n[\"\u0440\u0435\", null],\n[\"\u2581\u0434\u0435\", \"affaires\"],\n[\"\u043b\u043e\", null],\n[\"\u2581\u0448\", \"allaient\"],\n[\"\u043b\u043e\", null],\n[\"\u2581\u0434\u043e\", null],\n[\"\u2581\u0441\u0438\", null],\n[\"\u0445\", null],\n[\"\u2581\u043f\u043e\", null],\n[\"\u0440\", null],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u043b\u0443\", \"mieux\"],\n[\"\u0447\", null],\n[\"\u0448\u0435\", null],\n[\",\", null],\n[\"\u2581\u0447\u0435\u043c\", null],\n[\"\u2581\u043f\u0440\u0435\", \"avant\"],\n[\"\u0436\u0434\u0435\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0418\", \"Ivan\"],\n[\"\u0432\u0430\u043d\", null],\n[\"\u2581\u0441\u0438\", \"fortement\"],\n[\"\u043b\u044c\u043d\u043e\", null],\n[\"\u2581\u043f\u0440\u043e\u0442\u0438\", \"s'opposait\"],\n[\"\u0432\u043e\", null],\n[\"\u0434\u0435\u0439\", null],\n[\"\u0441\u0442\u0432\u043e\", null],\n[\"\u0432\u0430\u043b\", null],\n[\"\u2581\u0442\u0435\", \"chauff\u00e9\"],\n[\"\u043f\u043b\u043e\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u043f\u043e\", \"logement\"],\n[\"\u043c\u0435\", null],\n[\"\u0449\u0435\", null],\n[\"\u043d\u0438\u044e\", null],\n[\"\u2581\u043a\u043e\", \"vaches\"],\n[\"\u0440\u043e\u0432\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0441\", \"beurre\"],\n[\"\u043b\u0438\", null],\n[\"\u0432\u043e\", null],\n[\"\u0447\u043d\u043e\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u043c\u0430\", null],\n[\"\u0441\u043b\u0443\", null],\n[\",\", null],\n[\"\u2581\u0443\", \"affirmant\"],\n[\"\u0442\u0432\u0435\u0440\", null],\n[\"\u0436\u0434\u0430\", null],\n[\"\u044f\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u043a\u043e\u0440\u043e\", \"vache\"],\n[\"\u0432\u0435\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u0445\u043e\", \"froid\"],\n[\"\u043b\u043e\", null],\n[\"\u0434\u0443\", null],\n[\"\u2581\u043f\u043e\", \"n\u00e9cessiterait\"],\n[\"\u0442\u0440\u0435\", null],\n[\"\u0431\u0443\", null],\n[\"\u0435\u0442\u0441\u044f\", null],\n[\"\u2581\u043c\", \"moins\"],\n[\"\u0435\u043d\u044c\", null],\n[\"\u0448\u0435\", null],\n[\"\u2581\u043a\u043e\u0440\", \"nourriture\"],\n[\"\u043c\u0443\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0441\", \"beurre\"],\n[\"\u043c\u0435\", null],\n[\"\u0442\u0430\u043d\", null],\n[\"\u043d\u043e\u0435\", null],\n[\"\u2581\u043c\u0430\", null],\n[\"\u0441\u043b\u043e\", null],\n[\"\u2581\u0441\u043f\u043e\", \"rentable\"],\n[\"\u0440\u0435\", null],\n[\"\u0435\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0442\u0440\u0435\", \"exigeait\"],\n[\"\u0431\u043e\", null],\n[\"\u0432\u0430\u043b\", null],\n[\"\u2581\", null],\n[\"\u0436\u0430\", \"salaire\"],\n[\"\u043b\u043e\", null],\n[\"\u0432\u0430\u043d\", null],\n[\"\u044c\u044f\", null],\n[\",\", null],\n[\"\u2581\u043a\u0430\u043a\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0441\u0442\u0430\", \"autrefois\"],\n[\"\u0440\u0438\", null],\n[\"\u043d\u0443\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043d\u0438\", \"aucunement\"],\n[\"\u0441\u043a\u043e\", null],\n[\"\u043b\u044c\u043a\u043e\", null],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0438\u043d\", \"int\u00e9ressait\"],\n[\"\u0442\u0435\", null],\n[\"\u0440\u0435\", null],\n[\"\u0441\u043e\", null],\n[\"\u0432\u0430\", null],\n[\"\u043b\u0441\u044f\", null],\n[\"\u2581\u0442\u0435\u043c\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0434\", \"argent\"],\n[\"\u0435\u043d\u044c\", null],\n[\"\u0433\u0438\", null],\n[\",\", null],\n[\"\u2581\u043f\u043e\u043b\u0443\", \"re\u00e7u\"],\n[\"\u0447\u0430\", null],\n[\"\u0435\u043c\u044b\", null],\n[\"\u0435\", null],\n[\"\u2581\u0438\u043c\", null],\n[\",\", null],\n[\"\u2581\u0431\u044b\u043b\u0438\", null],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\", null],\n[\"\u0436\u0430\", \"salaire\"],\n[\"\u043b\u043e\", null],\n[\"\u0432\u0430\u043d\", null],\n[\"\u044c\u0435\", null],\n[\",\", null],\n[\"\u2581\u0430\", null],\n[\"\u2581\u0432\u044b\", \"avance\"],\n[\"\u0434\u0430\", null],\n[\"\u0447\u0430\", null],\n[\"\u2581\u0432\", null],\n[\"\u043f\", null],\n[\"\u0435\u0440\u0435\u0434\", null],\n[\"\u2581\u0434\u043e\", \"part\"],\n[\"\u043b\u0438\", null],\n[\"\u2581\u0431\", \"profit\"],\n[\"\u0430\u0440\", null],\n[\"\u044b\", null],\n[\"\u0448\u0430\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041a\u043d\u044f\u0433\u0438\u043d\u044f, \u0443\u0441\u043b\u044b\u0445\u0430\u0432 \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u0412\u0430\u0440\u0435\u043d\u044c\u043a\u0430 \u0445\u043e\u0440\u043e\u0448\u043e \u043f\u043e\u0435\u0442, \u0432\u043e\u043f\u0440\u043e\u0441\u0438\u043b\u0430 \u0435\u0435 \u043f\u0440\u0438\u0439\u0442\u0438 \u043a \u043d\u0438\u043c \u043f\u0435\u0442\u044c \u0432\u0435\u0447\u0435\u0440\u043e\u043c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041a', '\u043d\u044f', '\u0433\u0438', '\u043d\u044f', ',', '\u2581\u0443', '\u0441', '\u043b\u044b', '\u0445\u0430', '\u0432', '\u2581\u043e', '\u2581\u0442\u043e\u043c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0412\u0430', '\u0440\u0435', '\u043d\u044c', '\u043a\u0430', '\u2581\u0445\u043e', '\u0440\u043e', '\u0448\u043e', '\u2581\u043f\u043e', '\u0435\u0442', ',', '\u2581\u0432\u043e', '\u043f\u0440\u043e', '\u0441\u0438', '\u043b\u0430', '\u2581\u0435', '\u0435', '\u2581\u043f\u0440\u0438', '\u0439', '\u0442\u0438', '\u2581\u043a', '\u2581', '\u043d\u0438\u043c', '\u2581\u043f\u0435', '\u0442\u044c', '\u2581\u0432\u0435', '\u0447\u0435', '\u0440\u043e\u043c', '.']"}], "[\n  [\"\u2581\u041a\", \"Princess\"],\n  [\"\u043d\u044f\", null],\n  [\"\u0433\u0438\", null],\n  [\"\u043d\u044f\", null],\n  [\",\", null],\n  [\"\u2581\u0443\", \"having\"],\n  [\"\u0441\", null],\n  [\"\u043b\u044b\", null],\n  [\"\u0445\u0430\", null],\n  [\"\u0432\", null],\n  [\"\u2581\u043e\", null],\n  [\"\u2581\u0442\u043e\u043c\", \"that\"],\n  [\",\", null],\n  [\"\u2581\u0447\u0442\u043e\", \"that\"],\n  [\"\u2581\u0412\u0430\", \"Varenka\"],\n  [\"\u0440\u0435\", null],\n  [\"\u043d\u044c\", null],\n  [\"\u043a\u0430\", null],\n  [\"\u2581\u0445\u043e\", \"well\"],\n  [\"\u0440\u043e\", null],\n  [\"\u0448\u043e\", null],\n  [\"\u2581\u043f\u043e\", \"sings\"],\n  [\"\u0435\u0442\", null],\n  [\",\", null],\n  [\"\u2581\u0432\u043e\", \"asked\"],\n  [\"\u043f\u0440\u043e\", null],\n  [\"\u0441\u0438\", null],\n  [\"\u043b\u0430\", null],\n  [\"\u2581\u0435\", \"her\"],\n  [\"\u0435\", null],\n  [\"\u2581\u043f\u0440\u0438\", \"to come\"],\n  [\"\u0439\", null],\n  [\"\u0442\u0438\", null],\n  [\"\u2581\u043a\", \"to\"],\n  [\"\u2581\", null],\n  [\"\u043d\u0438\u043c\", \"them\"],\n  [\"\u2581\u043f\u0435\", \"sing\"],\n  [\"\u0442\u044c\", null],\n  [\"\u2581\u0432\u0435\", \"in the evening\"],\n  [\"\u0447\u0435\", null],\n  [\"\u0440\u043e\u043c\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u041a\u043d\u044f\u0433\u0438\u043d\u044f, \u0443\u0441\u043b\u044b\u0445\u0430\u0432 \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u0412\u0430\u0440\u0435\u043d\u044c\u043a\u0430 \u0445\u043e\u0440\u043e\u0448\u043e \u043f\u043e\u0435\u0442, \u0432\u043e\u043f\u0440\u043e\u0441\u0438\u043b\u0430 \u0435\u0435 \u043f\u0440\u0438\u0439\u0442\u0438 \u043a \u043d\u0438\u043c \u043f\u0435\u0442\u044c \u0432\u0435\u0447\u0435\u0440\u043e\u043c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041a', '\u043d\u044f', '\u0433\u0438', '\u043d\u044f', ',', '\u2581\u0443', '\u0441', '\u043b\u044b', '\u0445\u0430', '\u0432', '\u2581\u043e', '\u2581\u0442\u043e\u043c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0412\u0430', '\u0440\u0435', '\u043d\u044c', '\u043a\u0430', '\u2581\u0445\u043e', '\u0440\u043e', '\u0448\u043e', '\u2581\u043f\u043e', '\u0435\u0442', ',', '\u2581\u0432\u043e', '\u043f\u0440\u043e', '\u0441\u0438', '\u043b\u0430', '\u2581\u0435', '\u0435', '\u2581\u043f\u0440\u0438', '\u0439', '\u0442\u0438', '\u2581\u043a', '\u2581', '\u043d\u0438\u043c', '\u2581\u043f\u0435', '\u0442\u044c', '\u2581\u0432\u0435', '\u0447\u0435', '\u0440\u043e\u043c', '.']"}], "[\n[\"\u2581\u041a\", \"Princesse\"],\n[\"\u043d\u044f\", null],\n[\"\u0433\u0438\", null],\n[\"\u043d\u044f\", null],\n[\",\", null],\n[\"\u2581\u0443\", \"ayant\"],\n[\"\u0441\", null],\n[\"\u043b\u044b\", null],\n[\"\u0445\u0430\", null],\n[\"\u0432\", null],\n[\"\u2581\u043e\", null],\n[\"\u2581\u0442\u043e\u043c\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0412\u0430\", \"Varenka\"],\n[\"\u0440\u0435\", null],\n[\"\u043d\u044c\", null],\n[\"\u043a\u0430\", null],\n[\"\u2581\u0445\u043e\", \"bien\"],\n[\"\u0440\u043e\", null],\n[\"\u0448\u043e\", null],\n[\"\u2581\u043f\u043e\", \"chantait\"],\n[\"\u0435\u0442\", null],\n[\",\", null],\n[\"\u2581\u0432\u043e\", \"demanda\"],\n[\"\u043f\u0440\u043e\", null],\n[\"\u0441\u0438\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\u0435\", \"lui\"],\n[\"\u0435\", null],\n[\"\u2581\u043f\u0440\u0438\", \"venir\"],\n[\"\u0439\", null],\n[\"\u0442\u0438\", null],\n[\"\u2581\u043a\", \"chez\"],\n[\"\u2581\", null],\n[\"\u043d\u0438\u043c\", \"eux\"],\n[\"\u2581\u043f\u0435\", \"chanter\"],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0432\u0435\", \"le\"],\n[\"\u0447\u0435\", \"soir\"],\n[\"\u0440\u043e\u043c\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041f\u043e \u043c\u043e\u0435\u043c\u0443 \u043c\u043d\u0435\u043d\u0438\u044e, \u0434\u0435\u0439\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c \u043d\u0430 \u0434\u0440\u0443\u0433\u043e\u0439 \u043d\u0430\u0440\u043e\u0434 \u043c\u043e\u0436\u0435\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u043e\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0438\u043c\u0435\u0435\u0442 \u0432\u044b\u0441\u0448\u0435\u0435 \u0440\u0430\u0437\u0432\u0438\u0442\u0438\u0435, \u043a\u043e\u0442\u043e\u0440\u044b\u0439...\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041f\u043e', '\u2581\u043c\u043e', '\u0435', '\u043c\u0443', '\u2581\u043c', '\u043d\u0435', '\u043d\u0438\u044e', ',', '\u2581\u0434\u0435\u0439', '\u0441\u0442\u0432\u043e', '\u0432\u0430\u0442\u044c', '\u2581\u043d\u0430', '\u2581\u0434\u0440\u0443', '\u0433\u043e', '\u0439', '\u2581\u043d\u0430\u0440\u043e\u0434', '\u2581\u043c\u043e\u0436\u0435', '\u0442', '\u2581\u0442\u043e\u043b\u044c\u043a\u043e', '\u2581\u0442\u043e', '\u0442', ',', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439', '\u2581\u0438\u043c\u0435', '\u0435\u0442', '\u2581\u0432\u044b', '\u0441', '\u0448\u0435', '\u0435', '\u2581\u0440\u0430\u0437\u0432\u0438', '\u0442\u0438\u0435', ',', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439', '...']"}], "[\n[\"\u2581\u041f\u043e\", \"In\"],\n[\"\u2581\u043c\u043e\", null],\n[\"\u0435\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u043c\", null],\n[\"\u043d\u0435\", null],\n[\"\u043d\u0438\u044e\", null],\n[\",\", null],\n[\"\u2581\u0434\u0435\u0439\", \"influence\"],\n[\"\u0441\u0442\u0432\u043e\", null],\n[\"\u0432\u0430\u0442\u044c\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u0434\u0440\u0443\", \"another\"],\n[\"\u0433\u043e\", null],\n[\"\u0439\", null],\n[\"\u2581\u043d\u0430\u0440\u043e\u0434\", \"people\"],\n[\"\u2581\u043c\u043e\u0436\u0435\", \"can\"],\n[\"\u0442\", null],\n[\"\u2581\u0442\u043e\u043b\u044c\u043a\u043e\", \"only\"],\n[\"\u2581\u0442\u043e\", \"the one\"],\n[\"\u0442\", null],\n[\",\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439\", \"which\"],\n[\"\u2581\u0438\u043c\u0435\", \"has\"],\n[\"\u0435\u0442\", null],\n[\"\u2581\u0432\u044b\", \"higher\"],\n[\"\u0441\", null],\n[\"\u0448\u0435\", null],\n[\"\u0435\", null],\n[\"\u2581\u0440\u0430\u0437\u0432\u0438\", \"development\"],\n[\"\u0442\u0438\u0435\", null],\n[\",\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439\", \"which\"],\n[\"...\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u041f\u043e \u043c\u043e\u0435\u043c\u0443 \u043c\u043d\u0435\u043d\u0438\u044e, \u0434\u0435\u0439\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c \u043d\u0430 \u0434\u0440\u0443\u0433\u043e\u0439 \u043d\u0430\u0440\u043e\u0434 \u043c\u043e\u0436\u0435\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u043e\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0438\u043c\u0435\u0435\u0442 \u0432\u044b\u0441\u0448\u0435\u0435 \u0440\u0430\u0437\u0432\u0438\u0442\u0438\u0435, \u043a\u043e\u0442\u043e\u0440\u044b\u0439...\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041f\u043e', '\u2581\u043c\u043e', '\u0435', '\u043c\u0443', '\u2581\u043c', '\u043d\u0435', '\u043d\u0438\u044e', ',', '\u2581\u0434\u0435\u0439', '\u0441\u0442\u0432\u043e', '\u0432\u0430\u0442\u044c', '\u2581\u043d\u0430', '\u2581\u0434\u0440\u0443', '\u0433\u043e', '\u0439', '\u2581\u043d\u0430\u0440\u043e\u0434', '\u2581\u043c\u043e\u0436\u0435', '\u0442', '\u2581\u0442\u043e\u043b\u044c\u043a\u043e', '\u2581\u0442\u043e', '\u0442', ',', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439', '\u2581\u0438\u043c\u0435', '\u0435\u0442', '\u2581\u0432\u044b', '\u0441', '\u0448\u0435', '\u0435', '\u2581\u0440\u0430\u0437\u0432\u0438', '\u0442\u0438\u0435', ',', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439', '...']"}], "[\n[\"\u2581\u041f\u043e\", \"Selon\"],\n[\"\u2581\u043c\u043e\", null],\n[\"\u0435\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u043c\", null],\n[\"\u043d\u0435\", null],\n[\"\u043d\u0438\u044e\", null],\n[\",\", null],\n[\"\u2581\u0434\u0435\u0439\", \"agir\"],\n[\"\u0441\u0442\u0432\u043e\", null],\n[\"\u0432\u0430\u0442\u044c\", null],\n[\"\u2581\u043d\u0430\", \"sur\"],\n[\"\u2581\u0434\u0440\u0443\", \"autre\"],\n[\"\u0433\u043e\", null],\n[\"\u0439\", null],\n[\"\u2581\u043d\u0430\u0440\u043e\u0434\", \"peuple\"],\n[\"\u2581\u043c\u043e\u0436\u0435\", \"peut\"],\n[\"\u0442\", null],\n[\"\u2581\u0442\u043e\u043b\u044c\u043a\u043e\", \"seulement\"],\n[\"\u2581\u0442\u043e\", \"celui\"],\n[\"\u0442\", null],\n[\",\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439\", \"qui\"],\n[\"\u2581\u0438\u043c\u0435\", \"poss\u00e8de\"],\n[\"\u0435\u0442\", null],\n[\"\u2581\u0432\u044b\", \"sup\u00e9rieur\"],\n[\"\u0441\", null],\n[\"\u0448\u0435\", null],\n[\"\u0435\", null],\n[\"\u2581\u0440\u0430\u0437\u0432\u0438\", \"d\u00e9veloppement\"],\n[\"\u0442\u0438\u0435\", null],\n[\",\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439\", \"qui\"],\n[\"...\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0411\u044b\u043b \u043e\u0434\u0438\u043d \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442\u0441\u043a\u0438\u0439 \u0442\u043e\u0432\u0430\u0440\u0438\u0449, \u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043e\u043d \u0441\u0431\u043b\u0438\u0437\u0438\u043b\u0441\u044f \u043f\u043e\u0441\u043b\u0435 \u0438 \u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043e\u043d \u043c\u043e\u0433 \u0431\u044b \u043f\u043e\u0433\u043e\u0432\u043e\u0440\u0438\u0442\u044c \u043e \u043b\u0438\u0447\u043d\u043e\u043c \u0433\u043e\u0440\u0435; \u043d\u043e \u0442\u043e\u0432\u0430\u0440\u0438\u0449 \u044d\u0442\u043e\u0442 \u0431\u044b\u043b \u043f\u043e\u043f\u0435\u0447\u0438\u0442\u0435\u043b\u0435\u043c \u0432 \u0434\u0430\u043b\u044c\u043d\u0435\u043c \u0443\u0447\u0435\u0431\u043d\u043e\u043c \u043e\u043a\u0440\u0443\u0433\u0435.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0411', '\u044b', '\u043b', '\u2581\u043e\u0434\u0438\u043d', '\u2581\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435', '\u0442', '\u0441\u043a\u0438\u0439', '\u2581\u0442\u043e\u0432\u0430', '\u0440\u0438', '\u0449', ',', '\u2581\u0441', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b', '\u043c', '\u2581\u043e\u043d', '\u2581\u0441', '\u0431\u043b\u0438', '\u0437\u0438', '\u043b\u0441\u044f', '\u2581\u043f\u043e\u0441\u043b\u0435', '\u2581\u0438', '\u2581\u0441', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b', '\u043c', '\u2581\u043e\u043d', '\u2581\u043c\u043e', '\u0433', '\u2581\u0431\u044b', '\u2581\u043f\u043e', '\u0433\u043e\u0432\u043e', '\u0440\u0438', '\u0442\u044c', '\u2581\u043e', '\u2581\u043b\u0438', '\u0447', '\u043d\u043e\u043c', '\u2581\u0433\u043e', '\u0440\u0435', ';', '\u2581\u043d\u043e', '\u2581\u0442\u043e\u0432\u0430', '\u0440\u0438', '\u0449', '\u2581\u044d\u0442\u043e', '\u0442', '\u2581\u0431\u044b\u043b', '\u2581\u043f\u043e', '\u043f\u0435', '\u0447\u0438', '\u0442\u0435\u043b\u0435\u043c', '\u2581\u0432', '\u2581\u0434\u0430', '\u043b\u044c', '\u043d\u0435\u043c', '\u2581\u0443\u0447\u0435', '\u0431', '\u043d\u043e\u043c', '\u2581\u043e\u043a\u0440\u0443', '\u0433\u0435', '.']"}], "[\n[\"\u2581\u0411\", \"There\"],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\"\u2581\u043e\u0434\u0438\u043d\", \"was\"],\n[\"\u2581\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\", \"university\"],\n[\"\u0442\", null],\n[\"\u0441\u043a\u0438\u0439\", null],\n[\"\u2581\u0442\u043e\u0432\u0430\", \"friend\"],\n[\"\u0440\u0438\", null],\n[\"\u0449\", null],\n[\",\", null],\n[\"\u2581\u0441\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\", \"whom\"],\n[\"\u043c\", null],\n[\"\u2581\u043e\u043d\", \"he\"],\n[\"\u2581\u0441\", null],\n[\"\u0431\u043b\u0438\", \"became\"],\n[\"\u0437\u0438\", null],\n[\"\u043b\u0441\u044f\", null],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\", \"close\"],\n[\"\u2581\u0438\", \"and\"],\n[\"\u2581\u0441\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\", \"whom\"],\n[\"\u043c\", null],\n[\"\u2581\u043e\u043d\", \"he\"],\n[\"\u2581\u043c\u043e\", \"could\"],\n[\"\u0433\", null],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u043f\u043e\", \"talk\"],\n[\"\u0433\u043e\u0432\u043e\", null],\n[\"\u0440\u0438\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043e\", \"about\"],\n[\"\u2581\u043b\u0438\", \"personal\"],\n[\"\u0447\", null],\n[\"\u043d\u043e\u043c\", null],\n[\"\u2581\u0433\u043e\", \"grief\"],\n[\"\u0440\u0435\", null],\n[\";\", null],\n[\"\u2581\u043d\u043e\", \"but\"],\n[\"\u2581\u0442\u043e\u0432\u0430\", \"friend\"],\n[\"\u0440\u0438\", null],\n[\"\u0449\", null],\n[\"\u2581\u044d\u0442\u043e\", \"this\"],\n[\"\u0442\", null],\n[\"\u2581\u0431\u044b\u043b\", \"was\"],\n[\"\u2581\u043f\u043e\", \"supervisor\"],\n[\"\u043f\u0435\", null],\n[\"\u0447\u0438\", null],\n[\"\u0442\u0435\u043b\u0435\u043c\", null],\n[\"\u2581\u0432\", \"in\"],\n[\"\u2581\u0434\u0430\", \"distant\"],\n[\"\u043b\u044c\", null],\n[\"\u043d\u0435\u043c\", null],\n[\"\u2581\u0443\u0447\u0435\", \"educational\"],\n[\"\u0431\", null],\n[\"\u043d\u043e\u043c\", null],\n[\"\u2581\u043e\u043a\u0440\u0443\", \"district\"],\n[\"\u0433\u0435\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u0411\u044b\u043b \u043e\u0434\u0438\u043d \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442\u0441\u043a\u0438\u0439 \u0442\u043e\u0432\u0430\u0440\u0438\u0449, \u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043e\u043d \u0441\u0431\u043b\u0438\u0437\u0438\u043b\u0441\u044f \u043f\u043e\u0441\u043b\u0435 \u0438 \u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043e\u043d \u043c\u043e\u0433 \u0431\u044b \u043f\u043e\u0433\u043e\u0432\u043e\u0440\u0438\u0442\u044c \u043e \u043b\u0438\u0447\u043d\u043e\u043c \u0433\u043e\u0440\u0435; \u043d\u043e \u0442\u043e\u0432\u0430\u0440\u0438\u0449 \u044d\u0442\u043e\u0442 \u0431\u044b\u043b \u043f\u043e\u043f\u0435\u0447\u0438\u0442\u0435\u043b\u0435\u043c \u0432 \u0434\u0430\u043b\u044c\u043d\u0435\u043c \u0443\u0447\u0435\u0431\u043d\u043e\u043c \u043e\u043a\u0440\u0443\u0433\u0435.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0411', '\u044b', '\u043b', '\u2581\u043e\u0434\u0438\u043d', '\u2581\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435', '\u0442', '\u0441\u043a\u0438\u0439', '\u2581\u0442\u043e\u0432\u0430', '\u0440\u0438', '\u0449', ',', '\u2581\u0441', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b', '\u043c', '\u2581\u043e\u043d', '\u2581\u0441', '\u0431\u043b\u0438', '\u0437\u0438', '\u043b\u0441\u044f', '\u2581\u043f\u043e\u0441\u043b\u0435', '\u2581\u0438', '\u2581\u0441', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b', '\u043c', '\u2581\u043e\u043d', '\u2581\u043c\u043e', '\u0433', '\u2581\u0431\u044b', '\u2581\u043f\u043e', '\u0433\u043e\u0432\u043e', '\u0440\u0438', '\u0442\u044c', '\u2581\u043e', '\u2581\u043b\u0438', '\u0447', '\u043d\u043e\u043c', '\u2581\u0433\u043e', '\u0440\u0435', ';', '\u2581\u043d\u043e', '\u2581\u0442\u043e\u0432\u0430', '\u0440\u0438', '\u0449', '\u2581\u044d\u0442\u043e', '\u0442', '\u2581\u0431\u044b\u043b', '\u2581\u043f\u043e', '\u043f\u0435', '\u0447\u0438', '\u0442\u0435\u043b\u0435\u043c', '\u2581\u0432', '\u2581\u0434\u0430', '\u043b\u044c', '\u043d\u0435\u043c', '\u2581\u0443\u0447\u0435', '\u0431', '\u043d\u043e\u043c', '\u2581\u043e\u043a\u0440\u0443', '\u0433\u0435', '.']"}], "[\n[\"\u2581\u0411\", \"Il\"],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\"\u2581\u043e\u0434\u0438\u043d\", \"un\"],\n[\"\u2581\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\", \"camarade\"],\n[\"\u0442\", null],\n[\"\u0441\u043a\u0438\u0439\", null],\n[\"\u2581\u0442\u043e\u0432\u0430\", null],\n[\"\u0440\u0438\", null],\n[\"\u0449\", null],\n[\",\", null],\n[\"\u2581\u0441\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\", \"avec\"],\n[\"\u043c\", null],\n[\"\u2581\u043e\u043d\", \"il\"],\n[\"\u2581\u0441\", null],\n[\"\u0431\u043b\u0438\", \"s'\u00e9tait\"],\n[\"\u0437\u0438\", null],\n[\"\u043b\u0441\u044f\", null],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\", \"li\u00e9\"],\n[\"\u2581\u0438\", \"et\"],\n[\"\u2581\u0441\", \"avec\"],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\", \"lequel\"],\n[\"\u043c\", null],\n[\"\u2581\u043e\u043d\", \"il\"],\n[\"\u2581\u043c\u043e\", \"aurait\"],\n[\"\u0433\", null],\n[\"\u2581\u0431\u044b\", \"pu\"],\n[\"\u2581\u043f\u043e\", \"parler\"],\n[\"\u0433\u043e\u0432\u043e\", null],\n[\"\u0440\u0438\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043e\", \"de\"],\n[\"\u2581\u043b\u0438\", \"son\"],\n[\"\u0447\", null],\n[\"\u043d\u043e\u043c\", null],\n[\"\u2581\u0433\u043e\", \"chagrin\"],\n[\"\u0440\u0435\", null],\n[\";\", null],\n[\"\u2581\u043d\u043e\", \"mais\"],\n[\"\u2581\u0442\u043e\u0432\u0430\", \"ce\"],\n[\"\u0440\u0438\", null],\n[\"\u0449\", null],\n[\"\u2581\u044d\u0442\u043e\", \"camarade\"],\n[\"\u0442\", null],\n[\"\u2581\u0431\u044b\u043b\", \"\u00e9tait\"],\n[\"\u2581\u043f\u043e\", \"curateur\"],\n[\"\u043f\u0435\", null],\n[\"\u0447\u0438\", null],\n[\"\u0442\u0435\u043b\u0435\u043c\", null],\n[\"\u2581\u0432\", \"dans\"],\n[\"\u2581\u0434\u0430\", \"un\"],\n[\"\u043b\u044c\", null],\n[\"\u043d\u0435\u043c\", null],\n[\"\u2581\u0443\u0447\u0435\", \"district\"],\n[\"\u0431\", null],\n[\"\u043d\u043e\u043c\", null],\n[\"\u2581\u043e\u043a\u0440\u0443\", \"scolaire\"],\n[\"\u0433\u0435\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041d\u0430 \u0442\u0440\u0435\u0442\u0438\u0439 \u0434\u0435\u043d\u044c \u041d\u0438\u043a\u043e\u043b\u0430\u0439 \u0432\u044b\u0437\u0432\u0430\u043b \u0431\u0440\u0430\u0442\u0430 \u0432\u044b\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u043e\u043f\u044f\u0442\u044c \u0435\u043c\u0443 \u0441\u0432\u043e\u0439 \u043f\u043b\u0430\u043d \u0438 \u0441\u0442\u0430\u043b \u043d\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0441\u0443\u0436\u0434\u0430\u0442\u044c \u0435\u0433\u043e, \u043d\u043e \u0441\u0442\u0430\u043b \u0443\u043c\u044b\u0448\u043b\u0435\u043d\u043d\u043e \u0441\u043c\u0435\u0448\u0438\u0432\u0430\u0442\u044c \u0435\u0433\u043e \u0441 \u043a\u043e\u043c\u043c\u0443\u043d\u0438\u0437\u043c\u043e\u043c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041d\u0430', '\u2581\u0442\u0440\u0435', '\u0442\u0438\u0439', '\u2581\u0434', '\u0435\u043d\u044c', '\u2581\u041d\u0438\u043a\u043e\u043b\u0430', '\u0439', '\u2581\u0432\u044b', '\u0437\u0432\u0430', '\u043b', '\u2581\u0431\u0440\u0430', '\u0442\u0430', '\u2581\u0432\u044b', '\u0441\u043a\u0430', '\u0437\u0430', '\u0442\u044c', '\u2581\u043e\u043f', '\u044f', '\u0442\u044c', '\u2581\u0435', '\u043c\u0443', '\u2581\u0441\u0432\u043e', '\u0439', '\u2581\u043f', '\u043b\u0430\u043d', '\u2581\u0438', '\u2581\u0441\u0442\u0430\u043b', '\u2581\u043d\u0435', '\u2581\u0442\u043e\u043b\u044c\u043a\u043e', '\u2581\u043e', '\u0441\u0443', '\u0436\u0434\u0430', '\u0442\u044c', '\u2581\u0435\u0433\u043e', ',', '\u2581\u043d\u043e', '\u2581\u0441\u0442\u0430\u043b', '\u2581\u0443', '\u043c\u044b', '\u0448', '\u043b\u0435\u043d', '\u043d\u043e', '\u2581\u0441', '\u043c\u0435', '\u0448\u0438', '\u0432\u0430\u0442\u044c', '\u2581\u0435\u0433\u043e', '\u2581\u0441', '\u2581\u043a\u043e\u043c', '\u043c\u0443', '\u043d\u0438', '\u0437', '\u043c\u043e\u043c', '.']"}], "[\n[\"\u2581\u041d\u0430\", \"On\"],\n[\"\u2581\u0442\u0440\u0435\", null],\n[\"\u0442\u0438\u0439\", null],\n[\"\u2581\u0434\", null],\n[\"\u0435\u043d\u044c\", null],\n[\"\u2581\u041d\u0438\u043a\u043e\u043b\u0430\", null],\n[\"\u0439\", \"Nikolai\"],\n[\"\u2581\u0432\u044b\", null],\n[\"\u0437\u0432\u0430\", null],\n[\"\u043b\", null],\n[\"\u2581\u0431\u0440\u0430\", null],\n[\"\u0442\u0430\", \"brother\"],\n[\"\u2581\u0432\u044b\", null],\n[\"\u0441\u043a\u0430\", null],\n[\"\u0437\u0430\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043e\u043f\", null],\n[\"\u044f\", null],\n[\"\u0442\u044c\", \"again\"],\n[\"\u2581\u0435\", null],\n[\"\u043c\u0443\", \"him\"],\n[\"\u2581\u0441\u0432\u043e\", null],\n[\"\u0439\", \"his\"],\n[\"\u2581\u043f\", null],\n[\"\u043b\u0430\u043d\", \"plan\"],\n[\"\u2581\u0438\", \"and\"],\n[\"\u2581\u0441\u0442\u0430\u043b\", \"began\"],\n[\"\u2581\u043d\u0435\", \"not\"],\n[\"\u2581\u0442\u043e\u043b\u044c\u043a\u043e\", \"only\"],\n[\"\u2581\u043e\", null],\n[\"\u0441\u0443\", null],\n[\"\u0436\u0434\u0430\", null],\n[\"\u0442\u044c\", \"criticize\"],\n[\"\u2581\u0435\u0433\u043e\", \"it\"],\n[\",\", null],\n[\"\u2581\u043d\u043e\", \"but\"],\n[\"\u2581\u0441\u0442\u0430\u043b\", \"began\"],\n[\"\u2581\u0443\", null],\n[\"\u043c\u044b\", null],\n[\"\u0448\", null],\n[\"\u043b\u0435\u043d\", null],\n[\"\u043d\u043e\", \"deliberately\"],\n[\"\u2581\u0441\", null],\n[\"\u043c\u0435\", null],\n[\"\u0448\u0438\", null],\n[\"\u0432\u0430\u0442\u044c\", \"confuse\"],\n[\"\u2581\u0435\u0433\u043e\", \"it\"],\n[\"\u2581\u0441\", \"with\"],\n[\"\u2581\u043a\u043e\u043c\", null],\n[\"\u043c\u0443\", null],\n[\"\u043d\u0438\", null],\n[\"\u0437\", null],\n[\"\u043c\u043e\u043c\", \"communism\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u041d\u0430 \u0442\u0440\u0435\u0442\u0438\u0439 \u0434\u0435\u043d\u044c \u041d\u0438\u043a\u043e\u043b\u0430\u0439 \u0432\u044b\u0437\u0432\u0430\u043b \u0431\u0440\u0430\u0442\u0430 \u0432\u044b\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u043e\u043f\u044f\u0442\u044c \u0435\u043c\u0443 \u0441\u0432\u043e\u0439 \u043f\u043b\u0430\u043d \u0438 \u0441\u0442\u0430\u043b \u043d\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0441\u0443\u0436\u0434\u0430\u0442\u044c \u0435\u0433\u043e, \u043d\u043e \u0441\u0442\u0430\u043b \u0443\u043c\u044b\u0448\u043b\u0435\u043d\u043d\u043e \u0441\u043c\u0435\u0448\u0438\u0432\u0430\u0442\u044c \u0435\u0433\u043e \u0441 \u043a\u043e\u043c\u043c\u0443\u043d\u0438\u0437\u043c\u043e\u043c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041d\u0430', '\u2581\u0442\u0440\u0435', '\u0442\u0438\u0439', '\u2581\u0434', '\u0435\u043d\u044c', '\u2581\u041d\u0438\u043a\u043e\u043b\u0430', '\u0439', '\u2581\u0432\u044b', '\u0437\u0432\u0430', '\u043b', '\u2581\u0431\u0440\u0430', '\u0442\u0430', '\u2581\u0432\u044b', '\u0441\u043a\u0430', '\u0437\u0430', '\u0442\u044c', '\u2581\u043e\u043f', '\u044f', '\u0442\u044c', '\u2581\u0435', '\u043c\u0443', '\u2581\u0441\u0432\u043e', '\u0439', '\u2581\u043f', '\u043b\u0430\u043d', '\u2581\u0438', '\u2581\u0441\u0442\u0430\u043b', '\u2581\u043d\u0435', '\u2581\u0442\u043e\u043b\u044c\u043a\u043e', '\u2581\u043e', '\u0441\u0443', '\u0436\u0434\u0430', '\u0442\u044c', '\u2581\u0435\u0433\u043e', ',', '\u2581\u043d\u043e', '\u2581\u0441\u0442\u0430\u043b', '\u2581\u0443', '\u043c\u044b', '\u0448', '\u043b\u0435\u043d', '\u043d\u043e', '\u2581\u0441', '\u043c\u0435', '\u0448\u0438', '\u0432\u0430\u0442\u044c', '\u2581\u0435\u0433\u043e', '\u2581\u0441', '\u2581\u043a\u043e\u043c', '\u043c\u0443', '\u043d\u0438', '\u0437', '\u043c\u043e\u043c', '.']"}], "[\n[\"\u2581\u041d\u0430\", \"Le\"],\n[\"\u2581\u0442\u0440\u0435\", \"troisi\u00e8me\"],\n[\"\u0442\u0438\u0439\", null],\n[\"\u2581\u0434\", \"jour\"],\n[\"\u0435\u043d\u044c\", null],\n[\"\u2581\u041d\u0438\u043a\u043e\u043b\u0430\", \"Nicolas\"],\n[\"\u0439\", null],\n[\"\u2581\u0432\u044b\", null],\n[\"\u0437\u0432\u0430\", null],\n[\"\u043b\", null],\n[\"\u2581\u0431\u0440\u0430\", \"fr\u00e8re\"],\n[\"\u0442\u0430\", null],\n[\"\u2581\u0432\u044b\", null],\n[\"\u0441\u043a\u0430\", null],\n[\"\u0437\u0430\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043e\u043f\", \"encore\"],\n[\"\u044f\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0435\", \"lui\"],\n[\"\u043c\u0443\", null],\n[\"\u2581\u0441\u0432\u043e\", \"son\"],\n[\"\u0439\", null],\n[\"\u2581\u043f\", \"plan\"],\n[\"\u043b\u0430\u043d\", null],\n[\"\u2581\u0438\", \"et\"],\n[\"\u2581\u0441\u0442\u0430\u043b\", \"commen\u00e7a\"],\n[\"\u2581\u043d\u0435\", \"non\"],\n[\"\u2581\u0442\u043e\u043b\u044c\u043a\u043e\", \"seulement\"],\n[\"\u2581\u043e\", null],\n[\"\u0441\u0443\", null],\n[\"\u0436\u0434\u0430\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0435\u0433\u043e\", \"le\"],\n[\",\", null],\n[\"\u2581\u043d\u043e\", \"mais\"],\n[\"\u2581\u0441\u0442\u0430\u043b\", \"commen\u00e7a\"],\n[\"\u2581\u0443\", null],\n[\"\u043c\u044b\", null],\n[\"\u0448\", null],\n[\"\u043b\u0435\u043d\", null],\n[\"\u043d\u043e\", null],\n[\"\u2581\u0441\", null],\n[\"\u043c\u0435\", null],\n[\"\u0448\u0438\", null],\n[\"\u0432\u0430\u0442\u044c\", null],\n[\"\u2581\u0435\u0433\u043e\", \"le\"],\n[\"\u2581\u0441\", \"avec\"],\n[\"\u2581\u043a\u043e\u043c\", \"communisme\"],\n[\"\u043c\u0443\", null],\n[\"\u043d\u0438\", null],\n[\"\u0437\", null],\n[\"\u043c\u043e\u043c\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041e\u043d \u0441\u0435\u043b, \u043e\u0431\u0432\u0435\u0440\u043d\u0443\u043b \u0441\u0435\u0431\u0435 \u043d\u043e\u0433\u0438 \u0442\u0438\u0433\u0440\u043e\u0432\u044b\u043c \u043f\u043b\u0435\u0434\u043e\u043c \u0438 \u0437\u0430\u043a\u0443\u0440\u0438\u043b \u0441\u0438\u0433\u0430\u0440\u0443. -- \u041a\u0430\u043a \u044d\u0442\u043e \u0442\u044b \u043d\u0435 \u043a\u0443\u0440\u0438\u0448\u044c!\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041e\u043d', '\u2581\u0441\u0435', '\u043b', ',', '\u2581\u043e\u0431', '\u0432\u0435\u0440', '\u043d\u0443', '\u043b', '\u2581\u0441\u0435', '\u0431\u0435', '\u2581\u043d\u043e', '\u0433\u0438', '\u2581\u0442\u0438', '\u0433', '\u0440\u043e', '\u0432\u044b\u043c', '\u2581\u043f', '\u043b\u0435', '\u0434\u043e\u043c', '\u2581\u0438', '\u2581\u0437\u0430', '\u043a\u0443', '\u0440\u0438', '\u043b', '\u2581\u0441\u0438', '\u0433\u0430', '\u0440\u0443', '.', '\u2581--', '\u2581\u041a\u0430', '\u043a', '\u2581\u044d\u0442\u043e', '\u2581\u0442', '\u044b', '\u2581\u043d\u0435', '\u2581\u043a\u0443', '\u0440\u0438', '\u0448', '\u044c', '!']"}], "[\n[\"\u2581\u041e\u043d\", \"He\"],\n[\"\u2581\u0441\u0435\", \"sat\"],\n[\"\u043b\", null],\n[\",\", null],\n[\"\u2581\u043e\u0431\", \"wrapped\"],\n[\"\u0432\u0435\u0440\", null],\n[\"\u043d\u0443\", null],\n[\"\u043b\", null],\n[\"\u2581\u0441\u0435\", null],\n[\"\u0431\u0435\", null],\n[\"\u2581\u043d\u043e\", \"legs\"],\n[\"\u0433\u0438\", null],\n[\"\u2581\u0442\u0438\", \"tiger\"],\n[\"\u0433\", null],\n[\"\u0440\u043e\", null],\n[\"\u0432\u044b\u043c\", null],\n[\"\u2581\u043f\", \"blanket\"],\n[\"\u043b\u0435\", null],\n[\"\u0434\u043e\u043c\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0437\u0430\", \"lit\"],\n[\"\u043a\u0443\", null],\n[\"\u0440\u0438\", null],\n[\"\u043b\", null],\n[\"\u2581\u0441\u0438\", \"cigar\"],\n[\"\u0433\u0430\", null],\n[\"\u0440\u0443\", null],\n[\".\", null],\n[\"\u2581--\", null],\n[\"\u2581\u041a\u0430\", \"How\"],\n[\"\u043a\", null],\n[\"\u2581\u044d\u0442\u043e\", null],\n[\"\u2581\u0442\", \"you\"],\n[\"\u044b\", null],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u043a\u0443\", \"smoke\"],\n[\"\u0440\u0438\", null],\n[\"\u0448\", null],\n[\"\u044c\", null],\n[\"!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u041e\u043d \u0441\u0435\u043b, \u043e\u0431\u0432\u0435\u0440\u043d\u0443\u043b \u0441\u0435\u0431\u0435 \u043d\u043e\u0433\u0438 \u0442\u0438\u0433\u0440\u043e\u0432\u044b\u043c \u043f\u043b\u0435\u0434\u043e\u043c \u0438 \u0437\u0430\u043a\u0443\u0440\u0438\u043b \u0441\u0438\u0433\u0430\u0440\u0443. -- \u041a\u0430\u043a \u044d\u0442\u043e \u0442\u044b \u043d\u0435 \u043a\u0443\u0440\u0438\u0448\u044c!\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041e\u043d', '\u2581\u0441\u0435', '\u043b', ',', '\u2581\u043e\u0431', '\u0432\u0435\u0440', '\u043d\u0443', '\u043b', '\u2581\u0441\u0435', '\u0431\u0435', '\u2581\u043d\u043e', '\u0433\u0438', '\u2581\u0442\u0438', '\u0433', '\u0440\u043e', '\u0432\u044b\u043c', '\u2581\u043f', '\u043b\u0435', '\u0434\u043e\u043c', '\u2581\u0438', '\u2581\u0437\u0430', '\u043a\u0443', '\u0440\u0438', '\u043b', '\u2581\u0441\u0438', '\u0433\u0430', '\u0440\u0443', '.', '\u2581--', '\u2581\u041a\u0430', '\u043a', '\u2581\u044d\u0442\u043e', '\u2581\u0442', '\u044b', '\u2581\u043d\u0435', '\u2581\u043a\u0443', '\u0440\u0438', '\u0448', '\u044c', '!']"}], "[\n  [\"\u2581\u041e\u043d\", \"Il\"],\n  [\"\u2581\u0441\u0435\", \"s'est\"],\n  [\"\u043b\", null],\n  [\",\", null],\n  [\"\u2581\u043e\u0431\", \"envelopp\u00e9\"],\n  [\"\u0432\u0435\u0440\", null],\n  [\"\u043d\u0443\", null],\n  [\"\u043b\", null],\n  [\"\u2581\u0441\u0435\", null],\n  [\"\u0431\u0435\", null],\n  [\"\u2581\u043d\u043e\", \"jambes\"],\n  [\"\u0433\u0438\", null],\n  [\"\u2581\u0442\u0438\", \"plaid\"],\n  [\"\u0433\", null],\n  [\"\u0440\u043e\", null],\n  [\"\u0432\u044b\u043c\", null],\n  [\"\u2581\u043f\", null],\n  [\"\u043b\u0435\", null],\n  [\"\u0434\u043e\u043c\", null],\n  [\"\u2581\u0438\", \"et\"],\n  [\"\u2581\u0437\u0430\", \"allum\u00e9\"],\n  [\"\u043a\u0443\", null],\n  [\"\u0440\u0438\", null],\n  [\"\u043b\", null],\n  [\"\u2581\u0441\u0438\", \"cigare\"],\n  [\"\u0433\u0430\", null],\n  [\"\u0440\u0443\", null],\n  [\".\", null],\n  [\"\u2581--\", null],\n  [\"\u2581\u041a\u0430\", \"Comment\"],\n  [\"\u043a\", null],\n  [\"\u2581\u044d\u0442\u043e\", null],\n  [\"\u2581\u0442\", \"tu\"],\n  [\"\u044b\", null],\n  [\"\u2581\u043d\u0435\", \"ne\"],\n  [\"\u2581\u043a\u0443\", \"fumes\"],\n  [\"\u0440\u0438\", null],\n  [\"\u0448\", null],\n  [\"\u044c\", null],\n  [\"!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041a\u043d\u044f\u0433\u0438\u043d\u044f \u041c\u044f\u0433\u043a\u0430\u044f \u0441\u0438\u0434\u0435\u043b\u0430 \u043f\u043e\u0441\u0435\u0440\u0435\u0434\u0438\u043d\u0435 \u043c\u0435\u0436\u0434\u0443 \u043e\u0431\u043e\u0438\u043c\u0438 \u043a\u0440\u0443\u0436\u043a\u0430\u043c\u0438 \u0438, \u043f\u0440\u0438\u0441\u043b\u0443\u0448\u0438\u0432\u0430\u044f\u0441\u044c, \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u043b\u0430 \u0443\u0447\u0430\u0441\u0442\u0438\u0435 \u0442\u043e \u0432 \u0442\u043e\u043c, \u0442\u043e \u0432 \u0434\u0440\u0443\u0433\u043e\u043c. -- \u041c\u043d\u0435 \u043d\u044b\u043d\u0447\u0435 \u0442\u0440\u0438 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430 \u0441\u043a\u0430\u0437\u0430\u043b\u0438 \u044d\u0442\u0443 \u0441\u0430\u043c\u0443\u044e \u0444\u0440\u0430\u0437\u0443 \u043f\u0440\u043e \u041a\u0430\u0443\u043b\u044c\u0431\u0430\u0445\u0430, \u0442\u043e\u0447\u043d\u043e \u0441\u0433\u043e\u0432\u043e\u0440\u0438\u043b\u0438\u0441\u044c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041a', '\u043d\u044f', '\u0433\u0438', '\u043d\u044f', '\u2581\u041c', '\u044f', '\u0433', '\u043a\u0430', '\u044f', '\u2581\u0441\u0438', '\u0434\u0435\u043b\u0430', '\u2581\u043f\u043e\u0441\u0435', '\u0440\u0435\u0434\u0438', '\u043d\u0435', '\u2581\u043c\u0435\u0436\u0434\u0443', '\u2581\u043e\u0431\u043e', '\u0438', '\u043c\u0438', '\u2581\u043a', '\u0440\u0443', '\u0436', '\u043a\u0430\u043c\u0438', '\u2581\u0438', ',', '\u2581\u043f\u0440\u0438', '\u0441\u043b\u0443', '\u0448\u0438', '\u0432\u0430\u044f', '\u0441\u044c', ',', '\u2581\u043f\u0440\u0438', '\u043d\u0438\u043c\u0430', '\u043b\u0430', '\u2581\u0443\u0447\u0430\u0441\u0442\u0438', '\u0435', '\u2581\u0442\u043e', '\u2581\u0432', '\u2581\u0442\u043e\u043c', ',', '\u2581\u0442\u043e', '\u2581\u0432', '\u2581\u0434\u0440\u0443', '\u0433\u043e\u043c', '.', '\u2581--', '\u2581\u041c', '\u043d\u0435', '\u2581', '\u043d\u044b', '\u043d', '\u0447\u0435', '\u2581\u0442\u0440\u0438', '\u2581\u0447\u0435\u043b\u043e\u0432\u0435', '\u043a\u0430', '\u2581\u0441', '\u043a\u0430\u0437\u0430', '\u043b\u0438', '\u2581\u044d', '\u0442\u0443', '\u2581\u0441\u0430', '\u043c\u0443', '\u044e', '\u2581\u0444', '\u0440\u0430', '\u0437\u0443', '\u2581\u043f\u0440\u043e', '\u2581\u041a\u0430', '\u0443', '\u043b\u044c', '\u0431\u0430', '\u0445\u0430', ',', '\u2581\u0442\u043e', '\u0447\u043d\u043e', '\u2581\u0441', '\u0433\u043e\u0432\u043e', '\u0440\u0438', '\u043b\u0438\u0441\u044c', '.']"}], "[\n[\"\u2581\u041a\", \"Princess\"],\n[\"\u043d\u044f\", null],\n[\"\u0433\u0438\", null],\n[\"\u043d\u044f\", null],\n[\"\u2581\u041c\", \"Myagkaya\"],\n[\"\u044f\", null],\n[\"\u0433\", null],\n[\"\u043a\u0430\", null],\n[\"\u044f\", null],\n[\"\u2581\u0441\u0438\", \"sat\"],\n[\"\u0434\u0435\u043b\u0430\", null],\n[\"\u2581\u043f\u043e\u0441\u0435\", \"in\"],\n[\"\u0440\u0435\u0434\u0438\", null],\n[\"\u043d\u0435\", null],\n[\"\u2581\u043c\u0435\u0436\u0434\u0443\", \"between\"],\n[\"\u2581\u043e\u0431\u043e\", \"both\"],\n[\"\u0438\", null],\n[\"\u043c\u0438\", null],\n[\"\u2581\u043a\", \"circles\"],\n[\"\u0440\u0443\", null],\n[\"\u0436\", null],\n[\"\u043a\u0430\u043c\u0438\", null],\n[\"\u2581\u0438\", \"and\"],\n[\",\", null],\n[\"\u2581\u043f\u0440\u0438\", \"listening\"],\n[\"\u0441\u043b\u0443\", null],\n[\"\u0448\u0438\", null],\n[\"\u0432\u0430\u044f\", null],\n[\"\u0441\u044c\", null],\n[\",\", null],\n[\"\u2581\u043f\u0440\u0438\", \"participated\"],\n[\"\u043d\u0438\u043c\u0430\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\u0443\u0447\u0430\u0441\u0442\u0438\", null],\n[\"\u0435\", null],\n[\"\u2581\u0442\u043e\", \"now\"],\n[\"\u2581\u0432\", \"in\"],\n[\"\u2581\u0442\u043e\u043c\", \"this\"],\n[\",\", null],\n[\"\u2581\u0442\u043e\", \"now\"],\n[\"\u2581\u0432\", \"in\"],\n[\"\u2581\u0434\u0440\u0443\", \"that\"],\n[\"\u0433\u043e\u043c\", null],\n[\".\", null],\n[\"\u2581--\", null],\n[\"\u2581\u041c\", \"Three\"],\n[\"\u043d\u0435\", null],\n[\"\u2581\", null],\n[\"\u043d\u044b\", null],\n[\"\u043d\", null],\n[\"\u0447\u0435\", null],\n[\"\u2581\u0442\u0440\u0438\", null],\n[\"\u2581\u0447\u0435\u043b\u043e\u0432\u0435\", \"people\"],\n[\"\u043a\u0430\", null],\n[\"\u2581\u0441\", \"told\"],\n[\"\u043a\u0430\u0437\u0430\", null],\n[\"\u043b\u0438\", null],\n[\"\u2581\u044d\", \"this\"],\n[\"\u0442\u0443\", null],\n[\"\u2581\u0441\u0430\", \"very\"],\n[\"\u043c\u0443\", null],\n[\"\u044e\", null],\n[\"\u2581\u0444\", \"phrase\"],\n[\"\u0440\u0430\", null],\n[\"\u0437\u0443\", null],\n[\"\u2581\u043f\u0440\u043e\", \"about\"],\n[\"\u2581\u041a\u0430\", \"Kaulbach\"],\n[\"\u0443\", null],\n[\"\u043b\u044c\", null],\n[\"\u0431\u0430\", null],\n[\"\u0445\u0430\", null],\n[\",\", null],\n[\"\u2581\u0442\u043e\", \"as if\"],\n[\"\u0447\u043d\u043e\", null],\n[\"\u2581\u0441\", \"they\"],\n[\"\u0433\u043e\u0432\u043e\", \"had\"],\n[\"\u0440\u0438\", null],\n[\"\u043b\u0438\u0441\u044c\", \"agreed\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u041a\u043d\u044f\u0433\u0438\u043d\u044f \u041c\u044f\u0433\u043a\u0430\u044f \u0441\u0438\u0434\u0435\u043b\u0430 \u043f\u043e\u0441\u0435\u0440\u0435\u0434\u0438\u043d\u0435 \u043c\u0435\u0436\u0434\u0443 \u043e\u0431\u043e\u0438\u043c\u0438 \u043a\u0440\u0443\u0436\u043a\u0430\u043c\u0438 \u0438, \u043f\u0440\u0438\u0441\u043b\u0443\u0448\u0438\u0432\u0430\u044f\u0441\u044c, \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u043b\u0430 \u0443\u0447\u0430\u0441\u0442\u0438\u0435 \u0442\u043e \u0432 \u0442\u043e\u043c, \u0442\u043e \u0432 \u0434\u0440\u0443\u0433\u043e\u043c. -- \u041c\u043d\u0435 \u043d\u044b\u043d\u0447\u0435 \u0442\u0440\u0438 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430 \u0441\u043a\u0430\u0437\u0430\u043b\u0438 \u044d\u0442\u0443 \u0441\u0430\u043c\u0443\u044e \u0444\u0440\u0430\u0437\u0443 \u043f\u0440\u043e \u041a\u0430\u0443\u043b\u044c\u0431\u0430\u0445\u0430, \u0442\u043e\u0447\u043d\u043e \u0441\u0433\u043e\u0432\u043e\u0440\u0438\u043b\u0438\u0441\u044c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041a', '\u043d\u044f', '\u0433\u0438', '\u043d\u044f', '\u2581\u041c', '\u044f', '\u0433', '\u043a\u0430', '\u044f', '\u2581\u0441\u0438', '\u0434\u0435\u043b\u0430', '\u2581\u043f\u043e\u0441\u0435', '\u0440\u0435\u0434\u0438', '\u043d\u0435', '\u2581\u043c\u0435\u0436\u0434\u0443', '\u2581\u043e\u0431\u043e', '\u0438', '\u043c\u0438', '\u2581\u043a', '\u0440\u0443', '\u0436', '\u043a\u0430\u043c\u0438', '\u2581\u0438', ',', '\u2581\u043f\u0440\u0438', '\u0441\u043b\u0443', '\u0448\u0438', '\u0432\u0430\u044f', '\u0441\u044c', ',', '\u2581\u043f\u0440\u0438', '\u043d\u0438\u043c\u0430', '\u043b\u0430', '\u2581\u0443\u0447\u0430\u0441\u0442\u0438', '\u0435', '\u2581\u0442\u043e', '\u2581\u0432', '\u2581\u0442\u043e\u043c', ',', '\u2581\u0442\u043e', '\u2581\u0432', '\u2581\u0434\u0440\u0443', '\u0433\u043e\u043c', '.', '\u2581--', '\u2581\u041c', '\u043d\u0435', '\u2581', '\u043d\u044b', '\u043d', '\u0447\u0435', '\u2581\u0442\u0440\u0438', '\u2581\u0447\u0435\u043b\u043e\u0432\u0435', '\u043a\u0430', '\u2581\u0441', '\u043a\u0430\u0437\u0430', '\u043b\u0438', '\u2581\u044d', '\u0442\u0443', '\u2581\u0441\u0430', '\u043c\u0443', '\u044e', '\u2581\u0444', '\u0440\u0430', '\u0437\u0443', '\u2581\u043f\u0440\u043e', '\u2581\u041a\u0430', '\u0443', '\u043b\u044c', '\u0431\u0430', '\u0445\u0430', ',', '\u2581\u0442\u043e', '\u0447\u043d\u043e', '\u2581\u0441', '\u0433\u043e\u0432\u043e', '\u0440\u0438', '\u043b\u0438\u0441\u044c', '.']"}], "[\n[\"\u2581\u041a\", \"La\"],\n[\"\u043d\u044f\", null],\n[\"\u0433\u0438\", null],\n[\"\u043d\u044f\", null],\n[\"\u2581\u041c\", null],\n[\"\u044f\", null],\n[\"\u0433\", null],\n[\"\u043a\u0430\", null],\n[\"\u044f\", null],\n[\"\u2581\u0441\u0438\", \"\u00e9tait assise\"],\n[\"\u0434\u0435\u043b\u0430\", null],\n[\"\u2581\u043f\u043e\u0441\u0435\", \"au milieu\"],\n[\"\u0440\u0435\u0434\u0438\", null],\n[\"\u043d\u0435\", null],\n[\"\u2581\u043c\u0435\u0436\u0434\u0443\", \"entre\"],\n[\"\u2581\u043e\u0431\u043e\", \"les deux\"],\n[\"\u0438\", null],\n[\"\u043c\u0438\", null],\n[\"\u2581\u043a\", \"cercles\"],\n[\"\u0440\u0443\", null],\n[\"\u0436\", null],\n[\"\u043a\u0430\u043c\u0438\", null],\n[\"\u2581\u0438\", \"et\"],\n[\",\", null],\n[\"\u2581\u043f\u0440\u0438\", \"\u00e9coutant\"],\n[\"\u0441\u043b\u0443\", null],\n[\"\u0448\u0438\", null],\n[\"\u0432\u0430\u044f\", null],\n[\"\u0441\u044c\", null],\n[\",\", null],\n[\"\u2581\u043f\u0440\u0438\", \"participait\"],\n[\"\u043d\u0438\u043c\u0430\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\u0443\u0447\u0430\u0441\u0442\u0438\", \"alternativement\"],\n[\"\u0435\", null],\n[\"\u2581\u0442\u043e\", \"tant\u00f4t\"],\n[\"\u2581\u0432\", \"\u00e0\"],\n[\"\u2581\u0442\u043e\u043c\", \"l'un\"],\n[\",\", null],\n[\"\u2581\u0442\u043e\", \"tant\u00f4t\"],\n[\"\u2581\u0432\", \"\u00e0\"],\n[\"\u2581\u0434\u0440\u0443\", \"l'autre\"],\n[\"\u0433\u043e\u043c\", null],\n[\".\", null],\n[\"\u2581--\", null],\n[\"\u2581\u041c\", \"Aujourd'hui\"],\n[\"\u043d\u0435\", null],\n[\"\u2581\", null],\n[\"\u043d\u044b\", null],\n[\"\u043d\", null],\n[\"\u0447\u0435\", null],\n[\"\u2581\u0442\u0440\u0438\", \"trois\"],\n[\"\u2581\u0447\u0435\u043b\u043e\u0432\u0435\", \"personnes\"],\n[\"\u043a\u0430\", null],\n[\"\u2581\u0441\", \"m'ont dit\"],\n[\"\u043a\u0430\u0437\u0430\", null],\n[\"\u043b\u0438\", null],\n[\"\u2581\u044d\", \"cette\"],\n[\"\u0442\u0443\", null],\n[\"\u2581\u0441\u0430\", \"m\u00eame\"],\n[\"\u043c\u0443\", null],\n[\"\u044e\", null],\n[\"\u2581\u0444\", \"phrase\"],\n[\"\u0440\u0430\", null],\n[\"\u0437\u0443\", null],\n[\"\u2581\u043f\u0440\u043e\", \"sur\"],\n[\"\u2581\u041a\u0430\", \"Kaulbach\"],\n[\"\u0443\", null],\n[\"\u043b\u044c\", null],\n[\"\u0431\u0430\", null],\n[\"\u0445\u0430\", null],\n[\",\", null],\n[\"\u2581\u0442\u043e\", \"comme si\"],\n[\"\u0447\u043d\u043e\", null],\n[\"\u2581\u0441\", \"elles s'\u00e9taient\"],\n[\"\u0433\u043e\u0432\u043e\", \"concert\u00e9es\"],\n[\"\u0440\u0438\", null],\n[\"\u043b\u0438\u0441\u044c\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0422\u043e\u0447\u043d\u043e \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043a\u0430\u043a\u043e\u0435-\u043d\u0438\u0431\u0443\u0434\u044c \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0438 \u0431\u043e\u0440\u044c\u0431\u0430 \u0432 \u0431\u0435\u0441\u043a\u043e\u043d\u0435\u0447\u043d\u043e\u043c!\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0422\u043e', '\u0447\u043d\u043e', '\u2581\u043c\u043e\u0436\u0435', '\u0442', '\u2581\u0431\u044b', '\u0442\u044c', '\u2581\u043a\u0430', '\u043a\u043e', '\u0435', '-', '\u043d\u0438', '\u0431\u0443', '\u0434\u044c', '\u2581\u043d\u0430', '\u043f\u0440\u0430\u0432', '\u043b\u0435\u043d\u0438\u0435', '\u2581\u0438', '\u2581\u0431', '\u043e\u0440', '\u044c', '\u0431\u0430', '\u2581\u0432', '\u2581\u0431\u0435', '\u0441\u043a\u043e', '\u043d\u0435', '\u0447', '\u043d\u043e\u043c', '!']"}], "[\n[\"\u2581\u0422\u043e\", \"Surely\"],\n[\"\u0447\u043d\u043e\", null],\n[\"\u2581\u043c\u043e\u0436\u0435\", \"can\"],\n[\"\u0442\", null],\n[\"\u2581\u0431\u044b\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043a\u0430\", \"some\"],\n[\"\u043a\u043e\", null],\n[\"\u0435\", null],\n[\"-\", null],\n[\"\u043d\u0438\", null],\n[\"\u0431\u0443\", null],\n[\"\u0434\u044c\", null],\n[\"\u2581\u043d\u0430\", \"direction\"],\n[\"\u043f\u0440\u0430\u0432\", null],\n[\"\u043b\u0435\u043d\u0438\u0435\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0431\", \"struggle\"],\n[\"\u043e\u0440\", null],\n[\"\u044c\", null],\n[\"\u0431\u0430\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0431\u0435\", \"infinite\"],\n[\"\u0441\u043a\u043e\", null],\n[\"\u043d\u0435\", null],\n[\"\u0447\", null],\n[\"\u043d\u043e\u043c\", null],\n[\"!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u0422\u043e\u0447\u043d\u043e \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043a\u0430\u043a\u043e\u0435-\u043d\u0438\u0431\u0443\u0434\u044c \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0438 \u0431\u043e\u0440\u044c\u0431\u0430 \u0432 \u0431\u0435\u0441\u043a\u043e\u043d\u0435\u0447\u043d\u043e\u043c!\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0422\u043e', '\u0447\u043d\u043e', '\u2581\u043c\u043e\u0436\u0435', '\u0442', '\u2581\u0431\u044b', '\u0442\u044c', '\u2581\u043a\u0430', '\u043a\u043e', '\u0435', '-', '\u043d\u0438', '\u0431\u0443', '\u0434\u044c', '\u2581\u043d\u0430', '\u043f\u0440\u0430\u0432', '\u043b\u0435\u043d\u0438\u0435', '\u2581\u0438', '\u2581\u0431', '\u043e\u0440', '\u044c', '\u0431\u0430', '\u2581\u0432', '\u2581\u0431\u0435', '\u0441\u043a\u043e', '\u043d\u0435', '\u0447', '\u043d\u043e\u043c', '!']"}], "[\n  [\"\u2581\u0422\u043e\", \"Certainement\"],\n  [\"\u0447\u043d\u043e\", null],\n  [\"\u2581\u043c\u043e\u0436\u0435\", \"peut\"],\n  [\"\u0442\", null],\n  [\"\u2581\u0431\u044b\", null],\n  [\"\u0442\u044c\", null],\n  [\"\u2581\u043a\u0430\", \"quelque\"],\n  [\"\u043a\u043e\", null],\n  [\"\u0435\", null],\n  [\"-\", null],\n  [\"\u043d\u0438\", null],\n  [\"\u0431\u0443\", null],\n  [\"\u0434\u044c\", null],\n  [\"\u2581\u043d\u0430\", \"direction\"],\n  [\"\u043f\u0440\u0430\u0432\", null],\n  [\"\u043b\u0435\u043d\u0438\u0435\", null],\n  [\"\u2581\u0438\", null],\n  [\"\u2581\u0431\", \"lutte\"],\n  [\"\u043e\u0440\", null],\n  [\"\u044c\", null],\n  [\"\u0431\u0430\", null],\n  [\"\u2581\u0432\", \"dans\"],\n  [\"\u2581\u0431\u0435\", \"l'infini\"],\n  [\"\u0441\u043a\u043e\", null],\n  [\"\u043d\u0435\", null],\n  [\"\u0447\", null],\n  [\"\u043d\u043e\u043c\", null],\n  [\"!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n-- \u041a\u043e\u0441\u0442\u044f, \u0442\u044b \u0437\u043d\u0430\u0435\u0448\u044c, \u0447\u0442\u043e \u0443 \u043c\u0435\u043d\u044f \u0443\u0436 \u043e\u0441\u0442\u0430\u0435\u0442\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u044f\u0442\u044c\u0434\u0435\u0441\u044f\u0442 \u0440\u0443\u0431\u043b\u0435\u0439.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581--', '\u2581\u041a\u043e', '\u0441\u0442', '\u044f', ',', '\u2581\u0442', '\u044b', '\u2581\u0437\u043d\u0430', '\u0435', '\u0448', '\u044c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0443', '\u2581\u043c', '\u0435\u043d', '\u044f', '\u2581\u0443', '\u0436', '\u2581\u043e\u0441\u0442\u0430', '\u0435\u0442\u0441\u044f', '\u2581\u0442\u043e\u043b\u044c\u043a\u043e', '\u2581\u043f', '\u044f', '\u0442\u044c', '\u0434\u0435', '\u0441\u044f', '\u0442', '\u2581\u0440\u0443', '\u0431', '\u043b\u0435\u0439', '.']"}], "[\n  [\"\u2581--\", null],\n  [\"\u2581\u041a\u043e\", \"Kostya\"],\n  [\"\u0441\u0442\", null],\n  [\"\u044f\", null],\n  [\",\", null],\n  [\"\u2581\u0442\", \"you\"],\n  [\"\u044b\", null],\n  [\"\u2581\u0437\u043d\u0430\", \"know\"],\n  [\"\u0435\", null],\n  [\"\u0448\", null],\n  [\"\u044c\", null],\n  [\",\", null],\n  [\"\u2581\u0447\u0442\u043e\", null],\n  [\"\u2581\u0443\", null],\n  [\"\u2581\u043c\", \"I\"],\n  [\"\u0435\u043d\", null],\n  [\"\u044f\", null],\n  [\"\u2581\u0443\", null],\n  [\"\u0436\", null],\n  [\"\u2581\u043e\u0441\u0442\u0430\", \"have\"],\n  [\"\u0435\u0442\u0441\u044f\", null],\n  [\"\u2581\u0442\u043e\u043b\u044c\u043a\u043e\", \"only\"],\n  [\"\u2581\u043f\", \"fifty\"],\n  [\"\u044f\", null],\n  [\"\u0442\u044c\", null],\n  [\"\u0434\u0435\", null],\n  [\"\u0441\u044f\", null],\n  [\"\u0442\", null],\n  [\"\u2581\u0440\u0443\", \"rubles\"],\n  [\"\u0431\", null],\n  [\"\u043b\u0435\u0439\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n-- \u041a\u043e\u0441\u0442\u044f, \u0442\u044b \u0437\u043d\u0430\u0435\u0448\u044c, \u0447\u0442\u043e \u0443 \u043c\u0435\u043d\u044f \u0443\u0436 \u043e\u0441\u0442\u0430\u0435\u0442\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u044f\u0442\u044c\u0434\u0435\u0441\u044f\u0442 \u0440\u0443\u0431\u043b\u0435\u0439.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581--', '\u2581\u041a\u043e', '\u0441\u0442', '\u044f', ',', '\u2581\u0442', '\u044b', '\u2581\u0437\u043d\u0430', '\u0435', '\u0448', '\u044c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0443', '\u2581\u043c', '\u0435\u043d', '\u044f', '\u2581\u0443', '\u0436', '\u2581\u043e\u0441\u0442\u0430', '\u0435\u0442\u0441\u044f', '\u2581\u0442\u043e\u043b\u044c\u043a\u043e', '\u2581\u043f', '\u044f', '\u0442\u044c', '\u0434\u0435', '\u0441\u044f', '\u0442', '\u2581\u0440\u0443', '\u0431', '\u043b\u0435\u0439', '.']"}], "[\n  [\"\u2581--\", null],\n  [\"\u2581\u041a\u043e\", \"Kostia\"],\n  [\"\u0441\u0442\", null],\n  [\"\u044f\", null],\n  [\",\", null],\n  [\"\u2581\u0442\", \"tu\"],\n  [\"\u044b\", null],\n  [\"\u2581\u0437\u043d\u0430\", \"sais\"],\n  [\"\u0435\", null],\n  [\"\u0448\", null],\n  [\"\u044c\", null],\n  [\",\", null],\n  [\"\u2581\u0447\u0442\u043e\", \"que\"],\n  [\"\u2581\u0443\", null],\n  [\"\u2581\u043c\", \"je\"],\n  [\"\u0435\u043d\", null],\n  [\"\u044f\", null],\n  [\"\u2581\u0443\", null],\n  [\"\u0436\", null],\n  [\"\u2581\u043e\u0441\u0442\u0430\", \"reste\"],\n  [\"\u0435\u0442\u0441\u044f\", null],\n  [\"\u2581\u0442\u043e\u043b\u044c\u043a\u043e\", \"seulement\"],\n  [\"\u2581\u043f\", \"cinquante\"],\n  [\"\u044f\", null],\n  [\"\u0442\u044c\", null],\n  [\"\u0434\u0435\", null],\n  [\"\u0441\u044f\", null],\n  [\"\u0442\", null],\n  [\"\u2581\u0440\u0443\", \"roubles\"],\n  [\"\u0431\", null],\n  [\"\u043b\u0435\u0439\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u042d\u0442\u0438 \u0448\u0435\u0441\u0442\u044c \u043d\u0435\u0434\u0435\u043b\u044c \u0431\u044b\u043b\u0438 \u0441\u0430\u043c\u043e\u0435 \u0431\u043b\u0430\u0436\u0435\u043d\u043d\u043e\u0435 \u0438 \u0441\u0430\u043c\u043e\u0435 \u043c\u0443\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435 \u0434\u043b\u044f \u043d\u0435\u0435 \u0432\u0440\u0435\u043c\u044f.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u042d', '\u0442\u0438', '\u2581', '\u0448\u0435', '\u0441\u0442\u044c', '\u2581\u043d\u0435', '\u0434\u0435\u043b\u044c', '\u2581\u0431\u044b\u043b\u0438', '\u2581\u0441\u0430\u043c\u043e', '\u0435', '\u2581\u0431', '\u043b\u0430', '\u0436\u0435\u043d', '\u043d\u043e\u0435', '\u2581\u0438', '\u2581\u0441\u0430\u043c\u043e', '\u0435', '\u2581\u043c\u0443', '\u0447\u0438', '\u0442\u0435\u043b\u044c', '\u043d\u043e\u0435', '\u2581\u0434\u043b\u044f', '\u2581\u043d\u0435', '\u0435', '\u2581\u0432\u0440\u0435\u043c\u044f', '.']"}], "[\n  [\"\u2581\u042d\", \"These\"],\n  [\"\u0442\u0438\", null],\n  [\"\u2581\", null],\n  [\"\u0448\u0435\", \"six\"],\n  [\"\u0441\u0442\u044c\", null],\n  [\"\u2581\u043d\u0435\", \"weeks\"],\n  [\"\u0434\u0435\u043b\u044c\", null],\n  [\"\u2581\u0431\u044b\u043b\u0438\", \"were\"],\n  [\"\u2581\u0441\u0430\u043c\u043e\", \"the most\"],\n  [\"\u0435\", null],\n  [\"\u2581\u0431\", \"blissful\"],\n  [\"\u043b\u0430\", null],\n  [\"\u0436\u0435\u043d\", null],\n  [\"\u043d\u043e\u0435\", null],\n  [\"\u2581\u0438\", null],\n  [\"\u2581\u0441\u0430\u043c\u043e\", \"the most\"],\n  [\"\u0435\", null],\n  [\"\u2581\u043c\u0443\", \"torturous\"],\n  [\"\u0447\u0438\", null],\n  [\"\u0442\u0435\u043b\u044c\", null],\n  [\"\u043d\u043e\u0435\", null],\n  [\"\u2581\u0434\u043b\u044f\", \"for\"],\n  [\"\u2581\u043d\u0435\", \"her\"],\n  [\"\u0435\", null],\n  [\"\u2581\u0432\u0440\u0435\u043c\u044f\", \"time\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u042d\u0442\u0438 \u0448\u0435\u0441\u0442\u044c \u043d\u0435\u0434\u0435\u043b\u044c \u0431\u044b\u043b\u0438 \u0441\u0430\u043c\u043e\u0435 \u0431\u043b\u0430\u0436\u0435\u043d\u043d\u043e\u0435 \u0438 \u0441\u0430\u043c\u043e\u0435 \u043c\u0443\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435 \u0434\u043b\u044f \u043d\u0435\u0435 \u0432\u0440\u0435\u043c\u044f.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u042d', '\u0442\u0438', '\u2581', '\u0448\u0435', '\u0441\u0442\u044c', '\u2581\u043d\u0435', '\u0434\u0435\u043b\u044c', '\u2581\u0431\u044b\u043b\u0438', '\u2581\u0441\u0430\u043c\u043e', '\u0435', '\u2581\u0431', '\u043b\u0430', '\u0436\u0435\u043d', '\u043d\u043e\u0435', '\u2581\u0438', '\u2581\u0441\u0430\u043c\u043e', '\u0435', '\u2581\u043c\u0443', '\u0447\u0438', '\u0442\u0435\u043b\u044c', '\u043d\u043e\u0435', '\u2581\u0434\u043b\u044f', '\u2581\u043d\u0435', '\u0435', '\u2581\u0432\u0440\u0435\u043c\u044f', '.']"}], "[\n    [\"\u2581\u042d\", \"Ces\"],\n    [\"\u0442\u0438\", null],\n    [\"\u2581\", null],\n    [\"\u0448\u0435\", \"six\"],\n    [\"\u0441\u0442\u044c\", null],\n    [\"\u2581\u043d\u0435\", \"semaines\"],\n    [\"\u0434\u0435\u043b\u044c\", null],\n    [\"\u2581\u0431\u044b\u043b\u0438\", \"\u00e9taient\"],\n    [\"\u2581\u0441\u0430\u043c\u043e\", \"la\"],\n    [\"\u0435\", null],\n    [\"\u2581\u0431\", \"plus\"],\n    [\"\u043b\u0430\", null],\n    [\"\u0436\u0435\u043d\", null],\n    [\"\u043d\u043e\u0435\", null],\n    [\"\u2581\u0438\", null],\n    [\"\u2581\u0441\u0430\u043c\u043e\", \"plus\"],\n    [\"\u0435\", null],\n    [\"\u2581\u043c\u0443\", \"douloureuse\"],\n    [\"\u0447\u0438\", null],\n    [\"\u0442\u0435\u043b\u044c\", null],\n    [\"\u043d\u043e\u0435\", null],\n    [\"\u2581\u0434\u043b\u044f\", \"pour\"],\n    [\"\u2581\u043d\u0435\", \"elle\"],\n    [\"\u0435\", null],\n    [\"\u2581\u0432\u0440\u0435\u043c\u044f\", \"p\u00e9riode\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nBut this is the way we manage it; we collect a crowd like this one here, then each personin turn passes his head through a hole, and makes a grimace at the rest; time one who makes the ugliest, is elected pope by general acclamation; that's the way it is.\n\n\n---=== TOKENIZED TEXT ===---\n\n['But', '\u2581this', '\u2581is', '\u2581the', '\u2581way', '\u2581we', '\u2581manage', '\u2581it', ';', '\u2581we', '\u2581collect', '\u2581a', '\u2581crowd', '\u2581like', '\u2581this', '\u2581one', '\u2581here', ',', '\u2581then', '\u2581each', '\u2581person', 'in', '\u2581turn', '\u2581passes', '\u2581his', '\u2581head', '\u2581through', '\u2581a', '\u2581hole', ',', '\u2581and', '\u2581makes', '\u2581a', '\u2581gri', 'mace', '\u2581at', '\u2581the', '\u2581rest', ';', '\u2581time', '\u2581one', '\u2581who', '\u2581makes', '\u2581the', '\u2581ugli', 'est', ',', '\u2581is', '\u2581elected', '\u2581pope', '\u2581by', '\u2581general', '\u2581acc', 'lamation', ';', '\u2581that', \"'\", 's', '\u2581the', '\u2581way', '\u2581it', '\u2581is', '.']"}], "[\n[\"But\", \"Mais\"],\n[\"\u2581this\", null],\n[\"\u2581is\", null],\n[\"\u2581the\", null],\n[\"\u2581way\", \"fa\u00e7on\"],\n[\"\u2581we\", null],\n[\"\u2581manage\", \"g\u00e9rons\"],\n[\"\u2581it\", null],\n[\";\", null],\n[\"\u2581we\", null],\n[\"\u2581collect\", \"rassemblons\"],\n[\"\u2581a\", null],\n[\"\u2581crowd\", \"foule\"],\n[\"\u2581like\", null],\n[\"\u2581this\", null],\n[\"\u2581one\", null],\n[\"\u2581here\", \"ici\"],\n[\",\", null],\n[\"\u2581then\", \"puis\"],\n[\"\u2581each\", \"chaque\"],\n[\"\u2581person\", \"personne\"],\n[\"in\", null],\n[\"\u2581turn\", \"tour\"],\n[\"\u2581passes\", \"passe\"],\n[\"\u2581his\", null],\n[\"\u2581head\", \"t\u00eate\"],\n[\"\u2581through\", \"\u00e0 travers\"],\n[\"\u2581a\", null],\n[\"\u2581hole\", \"trou\"],\n[\",\", null],\n[\"\u2581and\", null],\n[\"\u2581makes\", \"fait\"],\n[\"\u2581a\", null],\n[\"\u2581gri\", \"grimace\"],\n[\"mace\", null],\n[\"\u2581at\", null],\n[\"\u2581the\", null],\n[\"\u2581rest\", \"reste\"],\n[\";\", null],\n[\"\u2581time\", \"celui\"],\n[\"\u2581one\", null],\n[\"\u2581who\", null],\n[\"\u2581makes\", \"fait\"],\n[\"\u2581the\", null],\n[\"\u2581ugli\", \"plus laide\"],\n[\"est\", null],\n[\",\", null],\n[\"\u2581is\", null],\n[\"\u2581elected\", \"\u00e9lu\"],\n[\"\u2581pope\", \"pape\"],\n[\"\u2581by\", null],\n[\"\u2581general\", \"g\u00e9n\u00e9rale\"],\n[\"\u2581acc\", \"acclamation\"],\n[\"lamation\", null],\n[\";\", null],\n[\"\u2581that\", null],\n[\"'\", null],\n[\"s\", null],\n[\"\u2581the\", null],\n[\"\u2581way\", \"fa\u00e7on\"],\n[\"\u2581it\", null],\n[\"\u2581is\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nBut this is the way we manage it; we collect a crowd like this one here, then each personin turn passes his head through a hole, and makes a grimace at the rest; time one who makes the ugliest, is elected pope by general acclamation; that's the way it is.\n\n\n---=== TOKENIZED TEXT ===---\n\n['But', '\u2581this', '\u2581is', '\u2581the', '\u2581way', '\u2581we', '\u2581manage', '\u2581it', ';', '\u2581we', '\u2581collect', '\u2581a', '\u2581crowd', '\u2581like', '\u2581this', '\u2581one', '\u2581here', ',', '\u2581then', '\u2581each', '\u2581person', 'in', '\u2581turn', '\u2581passes', '\u2581his', '\u2581head', '\u2581through', '\u2581a', '\u2581hole', ',', '\u2581and', '\u2581makes', '\u2581a', '\u2581gri', 'mace', '\u2581at', '\u2581the', '\u2581rest', ';', '\u2581time', '\u2581one', '\u2581who', '\u2581makes', '\u2581the', '\u2581ugli', 'est', ',', '\u2581is', '\u2581elected', '\u2581pope', '\u2581by', '\u2581general', '\u2581acc', 'lamation', ';', '\u2581that', \"'\", 's', '\u2581the', '\u2581way', '\u2581it', '\u2581is', '.']"}], "[\n[\"But\", \"\u041d\u043e\"],\n[\"\u2581this\", \"\u044d\u0442\u043e\"],\n[\"\u2581is\", null],\n[\"\u2581the\", null],\n[\"\u2581way\", \"\u0441\u043f\u043e\u0441\u043e\u0431\"],\n[\"\u2581we\", \"\u043c\u044b\"],\n[\"\u2581manage\", \"\u0443\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u043c\"],\n[\"\u2581it\", \"\u044d\u0442\u0438\u043c\"],\n[\";\", null],\n[\"\u2581we\", \"\u043c\u044b\"],\n[\"\u2581collect\", \"\u0441\u043e\u0431\u0438\u0440\u0430\u0435\u043c\"],\n[\"\u2581a\", null],\n[\"\u2581crowd\", \"\u0442\u043e\u043b\u043f\u0443\"],\n[\"\u2581like\", \"\u043f\u043e\u0434\u043e\u0431\u043d\u0443\u044e\"],\n[\"\u2581this\", \"\u044d\u0442\u043e\u0439\"],\n[\"\u2581one\", null],\n[\"\u2581here\", \"\u0437\u0434\u0435\u0441\u044c\"],\n[\",\", null],\n[\"\u2581then\", \"\u0437\u0430\u0442\u0435\u043c\"],\n[\"\u2581each\", \"\u043a\u0430\u0436\u0434\u044b\u0439\"],\n[\"\u2581person\", \"\u0447\u0435\u043b\u043e\u0432\u0435\u043a\"],\n[\"in\", null],\n[\"\u2581turn\", \"\u043f\u043e \u043e\u0447\u0435\u0440\u0435\u0434\u0438\"],\n[\"\u2581passes\", \"\u043f\u0440\u043e\u0441\u043e\u0432\u044b\u0432\u0430\u0435\u0442\"],\n[\"\u2581his\", \"\u0441\u0432\u043e\u044e\"],\n[\"\u2581head\", \"\u0433\u043e\u043b\u043e\u0432\u0443\"],\n[\"\u2581through\", \"\u0447\u0435\u0440\u0435\u0437\"],\n[\"\u2581a\", null],\n[\"\u2581hole\", \"\u0434\u044b\u0440\u0443\"],\n[\",\", null],\n[\"\u2581and\", \"\u0438\"],\n[\"\u2581makes\", \"\u043a\u043e\u0440\u0447\u0438\u0442\"],\n[\"\u2581a\", null],\n[\"\u2581gri\", \"\u0433\u0440\u0438\u043c\u0430\u0441\u0443\"],\n[\"mace\", null],\n[\"\u2581at\", null],\n[\"\u2581the\", null],\n[\"\u2581rest\", \"\u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u043c\"],\n[\";\", null],\n[\"\u2581time\", null],\n[\"\u2581one\", \"\u0442\u043e\u0442\"],\n[\"\u2581who\", \"\u043a\u0442\u043e\"],\n[\"\u2581makes\", \"\u0441\u0434\u0435\u043b\u0430\u0435\u0442\"],\n[\"\u2581the\", null],\n[\"\u2581ugli\", \"\u0441\u0430\u043c\u0443\u044e \u0443\u0440\u043e\u0434\u043b\u0438\u0432\u0443\u044e\"],\n[\"est\", null],\n[\",\", null],\n[\"\u2581is\", null],\n[\"\u2581elected\", \"\u0438\u0437\u0431\u0438\u0440\u0430\u0435\u0442\u0441\u044f\"],\n[\"\u2581pope\", \"\u043f\u0430\u043f\u043e\u0439\"],\n[\"\u2581by\", null],\n[\"\u2581general\", \"\u0432\u0441\u0435\u043e\u0431\u0449\u0435\u0439\"],\n[\"\u2581acc\", \"\u0430\u043a\u043a\u043b\u0430\u043c\u0430\u0446\u0438\u0435\u0439\"],\n[\"lamation\", null],\n[\";\", null],\n[\"\u2581that\", \"\u0432\u043e\u0442\"],\n[\"'\", null],\n[\"s\", null],\n[\"\u2581the\", null],\n[\"\u2581way\", \"\u0442\u0430\u043a\"],\n[\"\u2581it\", null],\n[\"\u2581is\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nHe too was silent for a while and then again began in a less squeaky voice, coldly emphasizing certain chance words that had no special importance.\n\n\n---=== TOKENIZED TEXT ===---\n\n['He', '\u2581too', '\u2581was', '\u2581silent', '\u2581for', '\u2581a', '\u2581while', '\u2581and', '\u2581then', '\u2581again', '\u2581began', '\u2581in', '\u2581a', '\u2581less', '\u2581squ', 'eaky', '\u2581voice', ',', '\u2581coldly', '\u2581emphasizing', '\u2581certain', '\u2581chance', '\u2581words', '\u2581that', '\u2581had', '\u2581no', '\u2581special', '\u2581importance', '.']"}], "[\n  [\"He\", \"Il\"],\n  [\"\u2581too\", null],\n  [\"\u2581was\", null],\n  [\"\u2581silent\", \"silencieux\"],\n  [\"\u2581for\", null],\n  [\"\u2581a\", null],\n  [\"\u2581while\", \"moment\"],\n  [\"\u2581and\", null],\n  [\"\u2581then\", null],\n  [\"\u2581again\", \"encore\"],\n  [\"\u2581began\", \"commen\u00e7a\"],\n  [\"\u2581in\", null],\n  [\"\u2581a\", null],\n  [\"\u2581less\", \"moins\"],\n  [\"\u2581squ\", \"grin\u00e7ante\"],\n  [\"eaky\", null],\n  [\"\u2581voice\", \"voix\"],\n  [\",\", null],\n  [\"\u2581coldly\", \"froidement\"],\n  [\"\u2581emphasizing\", \"soulignant\"],\n  [\"\u2581certain\", \"certains\"],\n  [\"\u2581chance\", \"hasard\"],\n  [\"\u2581words\", \"mots\"],\n  [\"\u2581that\", null],\n  [\"\u2581had\", null],\n  [\"\u2581no\", null],\n  [\"\u2581special\", \"particuli\u00e8re\"],\n  [\"\u2581importance\", \"importance\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nHe too was silent for a while and then again began in a less squeaky voice, coldly emphasizing certain chance words that had no special importance.\n\n\n---=== TOKENIZED TEXT ===---\n\n['He', '\u2581too', '\u2581was', '\u2581silent', '\u2581for', '\u2581a', '\u2581while', '\u2581and', '\u2581then', '\u2581again', '\u2581began', '\u2581in', '\u2581a', '\u2581less', '\u2581squ', 'eaky', '\u2581voice', ',', '\u2581coldly', '\u2581emphasizing', '\u2581certain', '\u2581chance', '\u2581words', '\u2581that', '\u2581had', '\u2581no', '\u2581special', '\u2581importance', '.']"}], "[\n  [\"He\", \"\u041e\u043d\"],\n  [\"\u2581too\", null],\n  [\"\u2581was\", null],\n  [\"\u2581silent\", \"\u043c\u043e\u043b\u0447\u0430\u043b\"],\n  [\"\u2581for\", null],\n  [\"\u2581a\", null],\n  [\"\u2581while\", \"\u043d\u0435\u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u0432\u0440\u0435\u043c\u044f\"],\n  [\"\u2581and\", null],\n  [\"\u2581then\", null],\n  [\"\u2581again\", \"\u0441\u043d\u043e\u0432\u0430\"],\n  [\"\u2581began\", \"\u043d\u0430\u0447\u0430\u043b\"],\n  [\"\u2581in\", null],\n  [\"\u2581a\", null],\n  [\"\u2581less\", \"\u043c\u0435\u043d\u0435\u0435\"],\n  [\"\u2581squ\", \"\u043f\u0438\u0441\u043a\u043b\u0438\u0432\u044b\u043c\"],\n  [\"eaky\", null],\n  [\"\u2581voice\", \"\u0433\u043e\u043b\u043e\u0441\u043e\u043c\"],\n  [\",\", null],\n  [\"\u2581coldly\", \"\u0445\u043e\u043b\u043e\u0434\u043d\u043e\"],\n  [\"\u2581emphasizing\", \"\u043f\u043e\u0434\u0447\u0435\u0440\u043a\u0438\u0432\u0430\u044f\"],\n  [\"\u2581certain\", \"\u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435\"],\n  [\"\u2581chance\", \"\u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0435\"],\n  [\"\u2581words\", \"\u0441\u043b\u043e\u0432\u0430\"],\n  [\"\u2581that\", null],\n  [\"\u2581had\", null],\n  [\"\u2581no\", null],\n  [\"\u2581special\", \"\u043e\u0441\u043e\u0431\u043e\u0433\u043e\"],\n  [\"\u2581importance\", \"\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nHave you no sense to devise a system which will make you independent of all efforts, and all wills, but your own?\n\n\n---=== TOKENIZED TEXT ===---\n\n['Have', '\u2581you', '\u2581no', '\u2581sense', '\u2581to', '\u2581devise', '\u2581a', '\u2581system', '\u2581which', '\u2581will', '\u2581make', '\u2581you', '\u2581independent', '\u2581of', '\u2581all', '\u2581efforts', ',', '\u2581and', '\u2581all', '\u2581wills', ',', '\u2581but', '\u2581your', '\u2581own', '?']"}], "[\n[\"Have\", \"Avez\"],\n[\"\u2581you\", null],\n[\"\u2581no\", null],\n[\"\u2581sense\", \"sens\"],\n[\"\u2581to\", null],\n[\"\u2581devise\", \"concevoir\"],\n[\"\u2581a\", null],\n[\"\u2581system\", \"syst\u00e8me\"],\n[\"\u2581which\", null],\n[\"\u2581will\", null],\n[\"\u2581make\", \"rendra\"],\n[\"\u2581you\", null],\n[\"\u2581independent\", \"ind\u00e9pendant\"],\n[\"\u2581of\", null],\n[\"\u2581all\", null],\n[\"\u2581efforts\", \"efforts\"],\n[\",\", null],\n[\"\u2581and\", null],\n[\"\u2581all\", null],\n[\"\u2581wills\", \"volont\u00e9s\"],\n[\",\", null],\n[\"\u2581but\", null],\n[\"\u2581your\", null],\n[\"\u2581own\", \"propre\"],\n[\"?\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nHave you no sense to devise a system which will make you independent of all efforts, and all wills, but your own?\n\n\n---=== TOKENIZED TEXT ===---\n\n['Have', '\u2581you', '\u2581no', '\u2581sense', '\u2581to', '\u2581devise', '\u2581a', '\u2581system', '\u2581which', '\u2581will', '\u2581make', '\u2581you', '\u2581independent', '\u2581of', '\u2581all', '\u2581efforts', ',', '\u2581and', '\u2581all', '\u2581wills', ',', '\u2581but', '\u2581your', '\u2581own', '?']"}], "[\n  [\"Have\", \"\u041d\u0435\u0443\u0436\u0435\u043b\u0438\"],\n  [\"\u2581you\", null],\n  [\"\u2581no\", null],\n  [\"\u2581sense\", \"\u0441\u043c\u044b\u0441\u043b\u0430\"],\n  [\"\u2581to\", null],\n  [\"\u2581devise\", \"\u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c\"],\n  [\"\u2581a\", null],\n  [\"\u2581system\", \"\u0441\u0438\u0441\u0442\u0435\u043c\u0443\"],\n  [\"\u2581which\", null],\n  [\"\u2581will\", null],\n  [\"\u2581make\", \"\u0441\u0434\u0435\u043b\u0430\u0435\u0442\"],\n  [\"\u2581you\", \"\u0432\u0430\u0441\"],\n  [\"\u2581independent\", \"\u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u044b\u043c\"],\n  [\"\u2581of\", null],\n  [\"\u2581all\", \"\u0432\u0441\u0435\u0445\"],\n  [\"\u2581efforts\", \"\u0443\u0441\u0438\u043b\u0438\u0439\"],\n  [\",\", null],\n  [\"\u2581and\", null],\n  [\"\u2581all\", \"\u0432\u0441\u0435\u0445\"],\n  [\"\u2581wills\", \"\u0432\u043e\u043b\u044c\"],\n  [\",\", null],\n  [\"\u2581but\", \"\u043a\u0440\u043e\u043c\u0435\"],\n  [\"\u2581your\", \"\u0432\u0430\u0448\u0435\u0439\"],\n  [\"\u2581own\", \"\u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0439\"],\n  [\"?\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nThe manor-house of Ferndean was a building of considerable antiquity, moderate size, and no architectural pretensions, deep buried in a wood.\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', '\u2581manor', '-', 'house', '\u2581of', '\u2581Fer', 'nde', 'an', '\u2581was', '\u2581a', '\u2581building', '\u2581of', '\u2581considerable', '\u2581antiquity', ',', '\u2581moderate', '\u2581size', ',', '\u2581and', '\u2581no', '\u2581architectural', '\u2581pretensions', ',', '\u2581deep', '\u2581buried', '\u2581in', '\u2581a', '\u2581wood', '.']"}], "[\n    [\"The\", null],\n    [\"\u2581manor\", \"manoir\"],\n    [\"-\", null],\n    [\"house\", null],\n    [\"\u2581of\", null],\n    [\"\u2581Fer\", \"Ferndean\"],\n    [\"nde\", null],\n    [\"an\", null],\n    [\"\u2581was\", null],\n    [\"\u2581a\", null],\n    [\"\u2581building\", \"b\u00e2timent\"],\n    [\"\u2581of\", null],\n    [\"\u2581considerable\", \"consid\u00e9rable\"],\n    [\"\u2581antiquity\", \"anciennet\u00e9\"],\n    [\",\", null],\n    [\"\u2581moderate\", \"modeste\"],\n    [\"\u2581size\", \"taille\"],\n    [\",\", null],\n    [\"\u2581and\", null],\n    [\"\u2581no\", null],\n    [\"\u2581architectural\", \"architecturales\"],\n    [\"\u2581pretensions\", \"pr\u00e9tentions\"],\n    [\",\", null],\n    [\"\u2581deep\", \"profond\u00e9ment\"],\n    [\"\u2581buried\", \"enfoui\"],\n    [\"\u2581in\", null],\n    [\"\u2581a\", null],\n    [\"\u2581wood\", \"bois\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThe manor-house of Ferndean was a building of considerable antiquity, moderate size, and no architectural pretensions, deep buried in a wood.\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', '\u2581manor', '-', 'house', '\u2581of', '\u2581Fer', 'nde', 'an', '\u2581was', '\u2581a', '\u2581building', '\u2581of', '\u2581considerable', '\u2581antiquity', ',', '\u2581moderate', '\u2581size', ',', '\u2581and', '\u2581no', '\u2581architectural', '\u2581pretensions', ',', '\u2581deep', '\u2581buried', '\u2581in', '\u2581a', '\u2581wood', '.']"}], "[\n[\"The\", null],\n[\"\u2581manor\", \"\u043f\u043e\u043c\u0435\u0441\u0442\u044c\u0435\"],\n[\"-\", null],\n[\"house\", null],\n[\"\u2581of\", null],\n[\"\u2581Fer\", \"\u0424\u0435\u0440\u043d\u0434\u0438\u043d\"],\n[\"nde\", null],\n[\"an\", null],\n[\"\u2581was\", null],\n[\"\u2581a\", null],\n[\"\u2581building\", \"\u0437\u0434\u0430\u043d\u0438\u0435\"],\n[\"\u2581of\", null],\n[\"\u2581considerable\", \"\u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439\"],\n[\"\u2581antiquity\", \"\u0434\u0440\u0435\u0432\u043d\u043e\u0441\u0442\u044c\"],\n[\",\", null],\n[\"\u2581moderate\", \"\u0443\u043c\u0435\u0440\u0435\u043d\u043d\u044b\u0439\"],\n[\"\u2581size\", \"\u0440\u0430\u0437\u043c\u0435\u0440\"],\n[\",\", null],\n[\"\u2581and\", null],\n[\"\u2581no\", null],\n[\"\u2581architectural\", \"\u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u043d\u044b\u0439\"],\n[\"\u2581pretensions\", \"\u043f\u0440\u0435\u0442\u0435\u043d\u0437\u0438\u0438\"],\n[\",\", null],\n[\"\u2581deep\", \"\u0433\u043b\u0443\u0431\u043e\u043a\u043e\"],\n[\"\u2581buried\", \"\u043f\u043e\u0433\u0440\u0435\u0431\u0435\u043d\u043d\u044b\u0439\"],\n[\"\u2581in\", null],\n[\"\u2581a\", null],\n[\"\u2581wood\", \"\u043b\u0435\u0441\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nThe fear of failure in these points harassed me worse than the physical hardships of my lot; though these were no trifles.\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', '\u2581fear', '\u2581of', '\u2581failure', '\u2581in', '\u2581these', '\u2581points', '\u2581harassed', '\u2581me', '\u2581worse', '\u2581than', '\u2581the', '\u2581physical', '\u2581hardships', '\u2581of', '\u2581my', '\u2581lot', ';', '\u2581though', '\u2581these', '\u2581were', '\u2581no', '\u2581trif', 'les', '.']"}], "[\n    [\"The\", null],\n    [\"\u2581fear\", \"peur\"],\n    [\"\u2581of\", null],\n    [\"\u2581failure\", \"\u00e9chec\"],\n    [\"\u2581in\", null],\n    [\"\u2581these\", null],\n    [\"\u2581points\", \"points\"],\n    [\"\u2581harassed\", \"harcelait\"],\n    [\"\u2581me\", null],\n    [\"\u2581worse\", \"pire\"],\n    [\"\u2581than\", null],\n    [\"\u2581the\", null],\n    [\"\u2581physical\", \"physiques\"],\n    [\"\u2581hardships\", \"difficult\u00e9s\"],\n    [\"\u2581of\", null],\n    [\"\u2581my\", null],\n    [\"\u2581lot\", \"sort\"],\n    [\";\", null],\n    [\"\u2581though\", \"bien que\"],\n    [\"\u2581these\", null],\n    [\"\u2581were\", null],\n    [\"\u2581no\", null],\n    [\"\u2581trif\", \"bagatelles\"],\n    [\"les\", null],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThe fear of failure in these points harassed me worse than the physical hardships of my lot; though these were no trifles.\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', '\u2581fear', '\u2581of', '\u2581failure', '\u2581in', '\u2581these', '\u2581points', '\u2581harassed', '\u2581me', '\u2581worse', '\u2581than', '\u2581the', '\u2581physical', '\u2581hardships', '\u2581of', '\u2581my', '\u2581lot', ';', '\u2581though', '\u2581these', '\u2581were', '\u2581no', '\u2581trif', 'les', '.']"}], "[\n  [\"The\", null],\n  [\"\u2581fear\", \"\u0441\u0442\u0440\u0430\u0445\"],\n  [\"\u2581of\", null],\n  [\"\u2581failure\", \"\u043d\u0435\u0443\u0434\u0430\u0447\u0438\"],\n  [\"\u2581in\", null],\n  [\"\u2581these\", \"\u044d\u0442\u0438\u0445\"],\n  [\"\u2581points\", \"\u043c\u043e\u043c\u0435\u043d\u0442\u0430\u0445\"],\n  [\"\u2581harassed\", \"\u043f\u0440\u0435\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043b\"],\n  [\"\u2581me\", \"\u043c\u0435\u043d\u044f\"],\n  [\"\u2581worse\", \"\u0445\u0443\u0436\u0435\"],\n  [\"\u2581than\", null],\n  [\"\u2581the\", null],\n  [\"\u2581physical\", \"\u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0438\u0435\"],\n  [\"\u2581hardships\", \"\u0442\u0440\u0443\u0434\u043d\u043e\u0441\u0442\u0438\"],\n  [\"\u2581of\", null],\n  [\"\u2581my\", \"\u043c\u043e\u0435\u0439\"],\n  [\"\u2581lot\", \"\u0434\u043e\u043b\u0438\"],\n  [\";\", null],\n  [\"\u2581though\", \"\u0445\u043e\u0442\u044f\"],\n  [\"\u2581these\", \"\u043e\u043d\u0438\"],\n  [\"\u2581were\", null],\n  [\"\u2581no\", null],\n  [\"\u2581trif\", \"\u043f\u0443\u0441\u0442\u044f\u043a\"],\n  [\"les\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nThe archdeacon maintained silence for several minutes, during which Gringoire had time to observe him.\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', '\u2581arch', 'deacon', '\u2581maintained', '\u2581silence', '\u2581for', '\u2581several', '\u2581minutes', ',', '\u2581during', '\u2581which', '\u2581G', 'ring', 'oire', '\u2581had', '\u2581time', '\u2581to', '\u2581observe', '\u2581him', '.']"}], "[\n  [\"The\", null],\n  [\"\u2581arch\", \"archidiacre\"],\n  [\"deacon\", null],\n  [\"\u2581maintained\", \"maintint\"],\n  [\"\u2581silence\", \"silence\"],\n  [\"\u2581for\", null],\n  [\"\u2581several\", \"plusieurs\"],\n  [\"\u2581minutes\", \"minutes\"],\n  [\",\", null],\n  [\"\u2581during\", \"pendant\"],\n  [\"\u2581which\", null],\n  [\"\u2581G\", \"Gringoire\"],\n  [\"ring\", null],\n  [\"oire\", null],\n  [\"\u2581had\", null],\n  [\"\u2581time\", \"temps\"],\n  [\"\u2581to\", null],\n  [\"\u2581observe\", \"observer\"],\n  [\"\u2581him\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThe archdeacon maintained silence for several minutes, during which Gringoire had time to observe him.\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', '\u2581arch', 'deacon', '\u2581maintained', '\u2581silence', '\u2581for', '\u2581several', '\u2581minutes', ',', '\u2581during', '\u2581which', '\u2581G', 'ring', 'oire', '\u2581had', '\u2581time', '\u2581to', '\u2581observe', '\u2581him', '.']"}], "[\n[\"The\", null],\n[\"\u2581arch\", \"\u0410\u0440\u0445\u0438\u0434\u044c\u044f\u043a\u043e\u043d\"],\n[\"deacon\", null],\n[\"\u2581maintained\", \"\u0441\u043e\u0445\u0440\u0430\u043d\u044f\u043b\"],\n[\"\u2581silence\", \"\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u0435\"],\n[\"\u2581for\", null],\n[\"\u2581several\", \"\u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e\"],\n[\"\u2581minutes\", \"\u043c\u0438\u043d\u0443\u0442\"],\n[\",\", null],\n[\"\u2581during\", null],\n[\"\u2581which\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u0435\"],\n[\"\u2581G\", \"\u0413\u0440\u0438\u043d\u0433\u0443\u0430\u0440\"],\n[\"ring\", null],\n[\"oire\", null],\n[\"\u2581had\", null],\n[\"\u2581time\", \"\u0432\u0440\u0435\u043c\u044f\"],\n[\"\u2581to\", null],\n[\"\u2581observe\", \"\u043d\u0430\u0431\u043b\u044e\u0434\u0430\u0442\u044c\"],\n[\"\u2581him\", \"\u0437\u0430 \u043d\u0438\u043c\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nEach of those faces that with so much searching, so many faults and corrections, he had evolved with its own character, each representing so much pain and pleasure, and all of them so often placed and replaced to obtain harmony; all the shades of colour and tone elaborated with such effort all this, regarded as a whole from those others' point of view, now seemed trivialities a thousand times repeated.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Each', '\u2581of', '\u2581those', '\u2581faces', '\u2581that', '\u2581with', '\u2581so', '\u2581much', '\u2581searching', ',', '\u2581so', '\u2581many', '\u2581faults', '\u2581and', '\u2581corrections', ',', '\u2581he', '\u2581had', '\u2581evolved', '\u2581with', '\u2581its', '\u2581own', '\u2581character', ',', '\u2581each', '\u2581representing', '\u2581so', '\u2581much', '\u2581pain', '\u2581and', '\u2581pleasure', ',', '\u2581and', '\u2581all', '\u2581of', '\u2581them', '\u2581so', '\u2581often', '\u2581placed', '\u2581and', '\u2581replaced', '\u2581to', '\u2581obtain', '\u2581harmony', ';', '\u2581all', '\u2581the', '\u2581shades', '\u2581of', '\u2581colour', '\u2581and', '\u2581tone', '\u2581elaborated', '\u2581with', '\u2581such', '\u2581effort', '\u2581all', '\u2581this', ',', '\u2581regarded', '\u2581as', '\u2581a', '\u2581whole', '\u2581from', '\u2581those', '\u2581others', \"'\", '\u2581point', '\u2581of', '\u2581view', ',', '\u2581now', '\u2581seemed', '\u2581trivi', 'alities', '\u2581a', '\u2581thousand', '\u2581times', '\u2581repeated', '.']"}], "[\n[\"Each\", \"Chacun\"],\n[\"\u2581of\", null],\n[\"\u2581those\", \"ces\"],\n[\"\u2581faces\", \"visages\"],\n[\"\u2581that\", \"que\"],\n[\"\u2581with\", null],\n[\"\u2581so\", null],\n[\"\u2581much\", null],\n[\"\u2581searching\", \"recherche\"],\n[\",\", null],\n[\"\u2581so\", null],\n[\"\u2581many\", \"nombreux\"],\n[\"\u2581faults\", \"d\u00e9fauts\"],\n[\"\u2581and\", null],\n[\"\u2581corrections\", \"corrections\"],\n[\",\", null],\n[\"\u2581he\", \"il\"],\n[\"\u2581had\", null],\n[\"\u2581evolved\", \"avait \u00e9volu\u00e9\"],\n[\"\u2581with\", \"avec\"],\n[\"\u2581its\", \"son\"],\n[\"\u2581own\", \"propre\"],\n[\"\u2581character\", \"caract\u00e8re\"],\n[\",\", null],\n[\"\u2581each\", \"chacun\"],\n[\"\u2581representing\", \"repr\u00e9sentant\"],\n[\"\u2581so\", null],\n[\"\u2581much\", null],\n[\"\u2581pain\", \"douleur\"],\n[\"\u2581and\", null],\n[\"\u2581pleasure\", \"plaisir\"],\n[\",\", null],\n[\"\u2581and\", null],\n[\"\u2581all\", \"tous\"],\n[\"\u2581of\", null],\n[\"\u2581them\", \"eux\"],\n[\"\u2581so\", null],\n[\"\u2581often\", \"souvent\"],\n[\"\u2581placed\", \"plac\u00e9s\"],\n[\"\u2581and\", null],\n[\"\u2581replaced\", \"replac\u00e9s\"],\n[\"\u2581to\", \"pour\"],\n[\"\u2581obtain\", \"obtenir\"],\n[\"\u2581harmony\", \"harmonie\"],\n[\";\", null],\n[\"\u2581all\", \"toutes\"],\n[\"\u2581the\", null],\n[\"\u2581shades\", \"nuances\"],\n[\"\u2581of\", null],\n[\"\u2581colour\", \"couleur\"],\n[\"\u2581and\", null],\n[\"\u2581tone\", \"ton\"],\n[\"\u2581elaborated\", \"\u00e9labor\u00e9es\"],\n[\"\u2581with\", \"avec\"],\n[\"\u2581such\", null],\n[\"\u2581effort\", \"effort\"],\n[\"\u2581all\", \"tout\"],\n[\"\u2581this\", \"cela\"],\n[\",\", null],\n[\"\u2581regarded\", \"consid\u00e9r\u00e9\"],\n[\"\u2581as\", \"comme\"],\n[\"\u2581a\", null],\n[\"\u2581whole\", \"ensemble\"],\n[\"\u2581from\", \"du\"],\n[\"\u2581those\", \"ces\"],\n[\"\u2581others\", \"autres\"],\n[\"'\", null],\n[\"\u2581point\", \"point\"],\n[\"\u2581of\", null],\n[\"\u2581view\", \"vue\"],\n[\",\", null],\n[\"\u2581now\", \"maintenant\"],\n[\"\u2581seemed\", \"semblait\"],\n[\"\u2581trivi\", \"futilit\u00e9s\"],\n[\"alities\", null],\n[\"\u2581a\", null],\n[\"\u2581thousand\", \"mille\"],\n[\"\u2581times\", \"fois\"],\n[\"\u2581repeated\", \"r\u00e9p\u00e9t\u00e9es\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nEach of those faces that with so much searching, so many faults and corrections, he had evolved with its own character, each representing so much pain and pleasure, and all of them so often placed and replaced to obtain harmony; all the shades of colour and tone elaborated with such effort all this, regarded as a whole from those others' point of view, now seemed trivialities a thousand times repeated.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Each', '\u2581of', '\u2581those', '\u2581faces', '\u2581that', '\u2581with', '\u2581so', '\u2581much', '\u2581searching', ',', '\u2581so', '\u2581many', '\u2581faults', '\u2581and', '\u2581corrections', ',', '\u2581he', '\u2581had', '\u2581evolved', '\u2581with', '\u2581its', '\u2581own', '\u2581character', ',', '\u2581each', '\u2581representing', '\u2581so', '\u2581much', '\u2581pain', '\u2581and', '\u2581pleasure', ',', '\u2581and', '\u2581all', '\u2581of', '\u2581them', '\u2581so', '\u2581often', '\u2581placed', '\u2581and', '\u2581replaced', '\u2581to', '\u2581obtain', '\u2581harmony', ';', '\u2581all', '\u2581the', '\u2581shades', '\u2581of', '\u2581colour', '\u2581and', '\u2581tone', '\u2581elaborated', '\u2581with', '\u2581such', '\u2581effort', '\u2581all', '\u2581this', ',', '\u2581regarded', '\u2581as', '\u2581a', '\u2581whole', '\u2581from', '\u2581those', '\u2581others', \"'\", '\u2581point', '\u2581of', '\u2581view', ',', '\u2581now', '\u2581seemed', '\u2581trivi', 'alities', '\u2581a', '\u2581thousand', '\u2581times', '\u2581repeated', '.']"}], "[\n[\"Each\", \"\u041a\u0430\u0436\u0434\u043e\u0435\"],\n[\"\u2581of\", null],\n[\"\u2581those\", null],\n[\"\u2581faces\", \"\u043b\u0438\u0446\"],\n[\"\u2581that\", null],\n[\"\u2581with\", null],\n[\"\u2581so\", null],\n[\"\u2581much\", null],\n[\"\u2581searching\", \"\u043f\u043e\u0438\u0441\u043a\u0430\"],\n[\",\", null],\n[\"\u2581so\", null],\n[\"\u2581many\", null],\n[\"\u2581faults\", \"\u043d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043a\u043e\u0432\"],\n[\"\u2581and\", null],\n[\"\u2581corrections\", \"\u0438\u0441\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0439\"],\n[\",\", null],\n[\"\u2581he\", null],\n[\"\u2581had\", null],\n[\"\u2581evolved\", \"\u0440\u0430\u0437\u0432\u0438\u043b\"],\n[\"\u2581with\", null],\n[\"\u2581its\", null],\n[\"\u2581own\", null],\n[\"\u2581character\", \"\u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\"],\n[\",\", null],\n[\"\u2581each\", \"\u043a\u0430\u0436\u0434\u043e\u0435\"],\n[\"\u2581representing\", \"\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0449\u0435\u0435\"],\n[\"\u2581so\", null],\n[\"\u2581much\", null],\n[\"\u2581pain\", \"\u0431\u043e\u043b\u044c\"],\n[\"\u2581and\", null],\n[\"\u2581pleasure\", \"\u0443\u0434\u043e\u0432\u043e\u043b\u044c\u0441\u0442\u0432\u0438\u0435\"],\n[\",\", null],\n[\"\u2581and\", null],\n[\"\u2581all\", null],\n[\"\u2581of\", null],\n[\"\u2581them\", null],\n[\"\u2581so\", null],\n[\"\u2581often\", null],\n[\"\u2581placed\", \"\u0440\u0430\u0437\u043c\u0435\u0449\u0435\u043d\u043d\u044b\u0435\"],\n[\"\u2581and\", null],\n[\"\u2581replaced\", \"\u043f\u0435\u0440\u0435\u043c\u0435\u0449\u0435\u043d\u043d\u044b\u0435\"],\n[\"\u2581to\", null],\n[\"\u2581obtain\", \"\u0434\u043e\u0441\u0442\u0438\u0447\u044c\"],\n[\"\u2581harmony\", \"\u0433\u0430\u0440\u043c\u043e\u043d\u0438\u0438\"],\n[\";\", null],\n[\"\u2581all\", null],\n[\"\u2581the\", null],\n[\"\u2581shades\", \"\u043e\u0442\u0442\u0435\u043d\u043a\u0438\"],\n[\"\u2581of\", null],\n[\"\u2581colour\", \"\u0446\u0432\u0435\u0442\u0430\"],\n[\"\u2581and\", null],\n[\"\u2581tone\", \"\u0442\u043e\u043d\u0430\"],\n[\"\u2581elaborated\", \"\u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u044b\u0435\"],\n[\"\u2581with\", null],\n[\"\u2581such\", null],\n[\"\u2581effort\", \"\u0443\u0441\u0438\u043b\u0438\u0435\u043c\"],\n[\"\u2581all\", null],\n[\"\u2581this\", \"\u044d\u0442\u043e\"],\n[\",\", null],\n[\"\u2581regarded\", \"\u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0435\u043c\u043e\u0435\"],\n[\"\u2581as\", null],\n[\"\u2581a\", null],\n[\"\u2581whole\", \"\u0446\u0435\u043b\u043e\u0435\"],\n[\"\u2581from\", null],\n[\"\u2581those\", null],\n[\"\u2581others\", \"\u0434\u0440\u0443\u0433\u0438\u0445\"],\n[\"'\", null],\n[\"\u2581point\", \"\u0442\u043e\u0447\u043a\u0438\"],\n[\"\u2581of\", null],\n[\"\u2581view\", \"\u0437\u0440\u0435\u043d\u0438\u044f\"],\n[\",\", null],\n[\"\u2581now\", null],\n[\"\u2581seemed\", \"\u043a\u0430\u0437\u0430\u043b\u043e\u0441\u044c\"],\n[\"\u2581trivi\", \"\u0431\u0430\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044f\u043c\u0438\"],\n[\"alities\", null],\n[\"\u2581a\", null],\n[\"\u2581thousand\", \"\u0442\u044b\u0441\u044f\u0447\u0443\"],\n[\"\u2581times\", \"\u0440\u0430\u0437\"],\n[\"\u2581repeated\", \"\u043f\u043e\u0432\u0442\u043e\u0440\u0435\u043d\u043d\u044b\u043c\u0438\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nFor instance, it ought to consider the education of women injurious, yet it established courses of lectures and universities for women.'\n\n\n---=== TOKENIZED TEXT ===---\n\n['For', '\u2581instance', ',', '\u2581it', '\u2581ought', '\u2581to', '\u2581consider', '\u2581the', '\u2581education', '\u2581of', '\u2581women', '\u2581injurious', ',', '\u2581yet', '\u2581it', '\u2581established', '\u2581courses', '\u2581of', '\u2581lectures', '\u2581and', '\u2581universities', '\u2581for', '\u2581women', \".'\"]"}], "[\n    [\"For\", \"Par\"],\n    [\"\u2581instance\", \"exemple\"],\n    [\",\", null],\n    [\"\u2581it\", null],\n    [\"\u2581ought\", \"devrait\"],\n    [\"\u2581to\", null],\n    [\"\u2581consider\", \"consid\u00e9rer\"],\n    [\"\u2581the\", null],\n    [\"\u2581education\", \"\u00e9ducation\"],\n    [\"\u2581of\", null],\n    [\"\u2581women\", \"femmes\"],\n    [\"\u2581injurious\", \"pr\u00e9judiciable\"],\n    [\",\", null],\n    [\"\u2581yet\", \"pourtant\"],\n    [\"\u2581it\", null],\n    [\"\u2581established\", \"\u00e9tabli\"],\n    [\"\u2581courses\", \"cours\"],\n    [\"\u2581of\", null],\n    [\"\u2581lectures\", \"conf\u00e9rences\"],\n    [\"\u2581and\", null],\n    [\"\u2581universities\", \"universit\u00e9s\"],\n    [\"\u2581for\", \"pour\"],\n    [\"\u2581women\", \"femmes\"],\n    [\".'\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nFor instance, it ought to consider the education of women injurious, yet it established courses of lectures and universities for women.'\n\n\n---=== TOKENIZED TEXT ===---\n\n['For', '\u2581instance', ',', '\u2581it', '\u2581ought', '\u2581to', '\u2581consider', '\u2581the', '\u2581education', '\u2581of', '\u2581women', '\u2581injurious', ',', '\u2581yet', '\u2581it', '\u2581established', '\u2581courses', '\u2581of', '\u2581lectures', '\u2581and', '\u2581universities', '\u2581for', '\u2581women', \".'\"]"}], "[\n    [\"For\", null],\n    [\"\u2581instance\", \"\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440\"],\n    [\",\", null],\n    [\"\u2581it\", null],\n    [\"\u2581ought\", \"\u0434\u043e\u043b\u0436\u043d\u043e\"],\n    [\"\u2581to\", null],\n    [\"\u2581consider\", \"\u0441\u0447\u0438\u0442\u0430\u0442\u044c\"],\n    [\"\u2581the\", null],\n    [\"\u2581education\", \"\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435\"],\n    [\"\u2581of\", null],\n    [\"\u2581women\", \"\u0436\u0435\u043d\u0449\u0438\u043d\"],\n    [\"\u2581injurious\", \"\u0432\u0440\u0435\u0434\u043d\u044b\u043c\"],\n    [\",\", null],\n    [\"\u2581yet\", \"\u043e\u0434\u043d\u0430\u043a\u043e\"],\n    [\"\u2581it\", null],\n    [\"\u2581established\", \"\u0443\u0447\u0440\u0435\u0434\u0438\u043b\u043e\"],\n    [\"\u2581courses\", \"\u043a\u0443\u0440\u0441\u044b\"],\n    [\"\u2581of\", null],\n    [\"\u2581lectures\", \"\u043b\u0435\u043a\u0446\u0438\u0439\"],\n    [\"\u2581and\", null],\n    [\"\u2581universities\", \"\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442\u044b\"],\n    [\"\u2581for\", null],\n    [\"\u2581women\", \"\u0436\u0435\u043d\u0449\u0438\u043d\"],\n    [\".'\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\n'Oh, how sweet he is \u2013 don't frighten him!' exclaimed Kitty unexpectedly, looking at a sparrow that had settled on the railing, turned a raspberry stalk over, and was pecking at it.\n\n\n---=== TOKENIZED TEXT ===---\n\n[\"'\", 'Oh', ',', '\u2581how', '\u2581sweet', '\u2581he', '\u2581is', '\u2581\u2013', '\u2581don', \"'\", 't', '\u2581frighten', '\u2581him', \"!'\", '\u2581exclaimed', '\u2581Kitty', '\u2581unexpectedly', ',', '\u2581looking', '\u2581at', '\u2581a', '\u2581sparrow', '\u2581that', '\u2581had', '\u2581settled', '\u2581on', '\u2581the', '\u2581railing', ',', '\u2581turned', '\u2581a', '\u2581raspberry', '\u2581stalk', '\u2581over', ',', '\u2581and', '\u2581was', '\u2581pec', 'king', '\u2581at', '\u2581it', '.']"}], "[\n[\"'\", null],\n[\"Oh\", \"Oh\"],\n[\",\", null],\n[\"\u2581how\", \"comment\"],\n[\"\u2581sweet\", \"doux\"],\n[\"\u2581he\", \"il\"],\n[\"\u2581is\", \"est\"],\n[\"\u2581\u2013\", null],\n[\"\u2581don\", \"ne\"],\n[\"'\", null],\n[\"t\", null],\n[\"\u2581frighten\", \"effrayez\"],\n[\"\u2581him\", \"le\"],\n[\"!'\", null],\n[\"\u2581exclaimed\", \"s'exclama\"],\n[\"\u2581Kitty\", \"Kitty\"],\n[\"\u2581unexpectedly\", \"soudainement\"],\n[\",\", null],\n[\"\u2581looking\", \"regardant\"],\n[\"\u2581at\", null],\n[\"\u2581a\", null],\n[\"\u2581sparrow\", \"moineau\"],\n[\"\u2581that\", \"qui\"],\n[\"\u2581had\", \"s'\u00e9tait\"],\n[\"\u2581settled\", \"pos\u00e9\"],\n[\"\u2581on\", \"sur\"],\n[\"\u2581the\", null],\n[\"\u2581railing\", \"balustrade\"],\n[\",\", null],\n[\"\u2581turned\", \"retourn\u00e9\"],\n[\"\u2581a\", null],\n[\"\u2581raspberry\", \"framboise\"],\n[\"\u2581stalk\", \"tige\"],\n[\"\u2581over\", null],\n[\",\", null],\n[\"\u2581and\", \"et\"],\n[\"\u2581was\", null],\n[\"\u2581pec\", \"picorait\"],\n[\"king\", null],\n[\"\u2581at\", null],\n[\"\u2581it\", \"le\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\n'Oh, how sweet he is \u2013 don't frighten him!' exclaimed Kitty unexpectedly, looking at a sparrow that had settled on the railing, turned a raspberry stalk over, and was pecking at it.\n\n\n---=== TOKENIZED TEXT ===---\n\n[\"'\", 'Oh', ',', '\u2581how', '\u2581sweet', '\u2581he', '\u2581is', '\u2581\u2013', '\u2581don', \"'\", 't', '\u2581frighten', '\u2581him', \"!'\", '\u2581exclaimed', '\u2581Kitty', '\u2581unexpectedly', ',', '\u2581looking', '\u2581at', '\u2581a', '\u2581sparrow', '\u2581that', '\u2581had', '\u2581settled', '\u2581on', '\u2581the', '\u2581railing', ',', '\u2581turned', '\u2581a', '\u2581raspberry', '\u2581stalk', '\u2581over', ',', '\u2581and', '\u2581was', '\u2581pec', 'king', '\u2581at', '\u2581it', '.']"}], "[\n[\"'\", null],\n[\"Oh\", \"\u041e\u0439\"],\n[\",\", null],\n[\"\u2581how\", \"\u043a\u0430\u043a\"],\n[\"\u2581sweet\", \"\u043c\u0438\u043b\u044b\u0439\"],\n[\"\u2581he\", \"\u043e\u043d\"],\n[\"\u2581is\", null],\n[\"\u2581\u2013\", null],\n[\"\u2581don\", \"\u043d\u0435\"],\n[\"'\", null],\n[\"t\", null],\n[\"\u2581frighten\", \"\u043f\u0443\u0433\u0430\u0439\u0442\u0435\"],\n[\"\u2581him\", \"\u0435\u0433\u043e\"],\n[\"!'\", null],\n[\"\u2581exclaimed\", \"\u0432\u043e\u0441\u043a\u043b\u0438\u043a\u043d\u0443\u043b\u0430\"],\n[\"\u2581Kitty\", \"\u041a\u0438\u0442\u0442\u0438\"],\n[\"\u2581unexpectedly\", \"\u043d\u0435\u043e\u0436\u0438\u0434\u0430\u043d\u043d\u043e\"],\n[\",\", null],\n[\"\u2581looking\", \"\u0433\u043b\u044f\u0434\u044f\"],\n[\"\u2581at\", \"\u043d\u0430\"],\n[\"\u2581a\", null],\n[\"\u2581sparrow\", \"\u0432\u043e\u0440\u043e\u0431\u044c\u044f\"],\n[\"\u2581that\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u0439\"],\n[\"\u2581had\", null],\n[\"\u2581settled\", \"\u0441\u0435\u043b\"],\n[\"\u2581on\", \"\u043d\u0430\"],\n[\"\u2581the\", null],\n[\"\u2581railing\", \"\u043f\u0435\u0440\u0438\u043b\u0430\"],\n[\",\", null],\n[\"\u2581turned\", \"\u043f\u0435\u0440\u0435\u0432\u0435\u0440\u043d\u0443\u043b\"],\n[\"\u2581a\", null],\n[\"\u2581raspberry\", \"\u043c\u0430\u043b\u0438\u043d\u043e\u0432\u044b\u0439\"],\n[\"\u2581stalk\", \"\u0441\u0442\u0435\u0431\u0435\u043b\u0435\u043a\"],\n[\"\u2581over\", null],\n[\",\", null],\n[\"\u2581and\", \"\u0438\"],\n[\"\u2581was\", null],\n[\"\u2581pec\", \"\u043a\u043b\u0435\u0432\"],\n[\"king\", null],\n[\"\u2581at\", null],\n[\"\u2581it\", \"\u0435\u0433\u043e\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\n'There is one thing I want to say...' the Princess began, and from her serious look Kitty guessed what was coming.\n\n\n---=== TOKENIZED TEXT ===---\n\n[\"'\", 'There', '\u2581is', '\u2581one', '\u2581thing', '\u2581I', '\u2581want', '\u2581to', '\u2581say', \"...'\", '\u2581the', '\u2581Princess', '\u2581began', ',', '\u2581and', '\u2581from', '\u2581her', '\u2581serious', '\u2581look', '\u2581Kitty', '\u2581guessed', '\u2581what', '\u2581was', '\u2581coming', '.']"}], "[\n    [\"'\", null],\n    [\"There\", \"Il\"],\n    [\"\u2581is\", null],\n    [\"\u2581one\", \"une\"],\n    [\"\u2581thing\", \"chose\"],\n    [\"\u2581I\", \"je\"],\n    [\"\u2581want\", \"veux\"],\n    [\"\u2581to\", null],\n    [\"\u2581say\", \"dire\"],\n    [\"...'\", null],\n    [\"\u2581the\", null],\n    [\"\u2581Princess\", \"Princesse\"],\n    [\"\u2581began\", \"commen\u00e7a\"],\n    [\",\", null],\n    [\"\u2581and\", \"et\"],\n    [\"\u2581from\", \"d'apr\u00e8s\"],\n    [\"\u2581her\", \"son\"],\n    [\"\u2581serious\", \"s\u00e9rieux\"],\n    [\"\u2581look\", \"regard\"],\n    [\"\u2581Kitty\", \"Kitty\"],\n    [\"\u2581guessed\", \"devina\"],\n    [\"\u2581what\", \"ce\"],\n    [\"\u2581was\", null],\n    [\"\u2581coming\", \"qui allait suivre\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\n'There is one thing I want to say...' the Princess began, and from her serious look Kitty guessed what was coming.\n\n\n---=== TOKENIZED TEXT ===---\n\n[\"'\", 'There', '\u2581is', '\u2581one', '\u2581thing', '\u2581I', '\u2581want', '\u2581to', '\u2581say', \"...'\", '\u2581the', '\u2581Princess', '\u2581began', ',', '\u2581and', '\u2581from', '\u2581her', '\u2581serious', '\u2581look', '\u2581Kitty', '\u2581guessed', '\u2581what', '\u2581was', '\u2581coming', '.']"}], "[\n  [\"'\", null],\n  [\"There\", \"\u0415\u0441\u0442\u044c\"],\n  [\"\u2581is\", null],\n  [\"\u2581one\", \"\u043e\u0434\u043d\u0430\"],\n  [\"\u2581thing\", \"\u0432\u0435\u0449\u044c\"],\n  [\"\u2581I\", \"\u044f\"],\n  [\"\u2581want\", \"\u0445\u043e\u0447\u0443\"],\n  [\"\u2581to\", null],\n  [\"\u2581say\", \"\u0441\u043a\u0430\u0437\u0430\u0442\u044c\"],\n  [\"...'\", null],\n  [\"\u2581the\", null],\n  [\"\u2581Princess\", \"\u041a\u043d\u044f\u0433\u0438\u043d\u044f\"],\n  [\"\u2581began\", \"\u043d\u0430\u0447\u0430\u043b\u0430\"],\n  [\",\", null],\n  [\"\u2581and\", \"\u0438\"],\n  [\"\u2581from\", \"\u043f\u043e\"],\n  [\"\u2581her\", \"\u0435\u0451\"],\n  [\"\u2581serious\", \"\u0441\u0435\u0440\u044c\u0451\u0437\u043d\u043e\u043c\u0443\"],\n  [\"\u2581look\", \"\u0432\u0438\u0434\u0443\"],\n  [\"\u2581Kitty\", \"\u041a\u0438\u0442\u0438\"],\n  [\"\u2581guessed\", \"\u0434\u043e\u0433\u0430\u0434\u0430\u043b\u0430\u0441\u044c\"],\n  [\"\u2581what\", \"\u0447\u0442\u043e\"],\n  [\"\u2581was\", null],\n  [\"\u2581coming\", \"\u043f\u043e\u0441\u043b\u0435\u0434\u0443\u0435\u0442\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nCharles remained alone the whole afternoon; they had taken Berthe to Madame Homais'; Felicite was in the room upstairs with Madame Lefrancois.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Charles', '\u2581remained', '\u2581alone', '\u2581the', '\u2581whole', '\u2581afternoon', ';', '\u2581they', '\u2581had', '\u2581taken', '\u2581Berthe', '\u2581to', '\u2581Madame', '\u2581Ho', 'mais', \"';\", '\u2581Fe', 'licite', '\u2581was', '\u2581in', '\u2581the', '\u2581room', '\u2581upstairs', '\u2581with', '\u2581Madame', '\u2581Lef', 'ran', 'co', 'is', '.']"}], "[\n[\"Charles\", \"Charles\"],\n[\"\u2581remained\", \"restait\"],\n[\"\u2581alone\", \"seul\"],\n[\"\u2581the\", null],\n[\"\u2581whole\", \"tout\"],\n[\"\u2581afternoon\", \"l'apr\u00e8s-midi\"],\n[\";\", null],\n[\"\u2581they\", \"ils\"],\n[\"\u2581had\", \"avaient\"],\n[\"\u2581taken\", \"emmen\u00e9\"],\n[\"\u2581Berthe\", \"Berthe\"],\n[\"\u2581to\", null],\n[\"\u2581Madame\", \"Madame\"],\n[\"\u2581Ho\", null],\n[\"mais\", \"Homais\"],\n[\"'\", null],\n[\";\", null],\n[\"\u2581Fe\", null],\n[\"licite\", \"F\u00e9licit\u00e9\"],\n[\"\u2581was\", \"\u00e9tait\"],\n[\"\u2581in\", \"dans\"],\n[\"\u2581the\", null],\n[\"\u2581room\", \"chambre\"],\n[\"\u2581upstairs\", \"en haut\"],\n[\"\u2581with\", \"avec\"],\n[\"\u2581Madame\", \"Madame\"],\n[\"\u2581Lef\", null],\n[\"ran\", null],\n[\"co\", null],\n[\"is\", \"Lefran\u00e7ois\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nCharles remained alone the whole afternoon; they had taken Berthe to Madame Homais'; Felicite was in the room upstairs with Madame Lefrancois.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Charles', '\u2581remained', '\u2581alone', '\u2581the', '\u2581whole', '\u2581afternoon', ';', '\u2581they', '\u2581had', '\u2581taken', '\u2581Berthe', '\u2581to', '\u2581Madame', '\u2581Ho', 'mais', \"';\", '\u2581Fe', 'licite', '\u2581was', '\u2581in', '\u2581the', '\u2581room', '\u2581upstairs', '\u2581with', '\u2581Madame', '\u2581Lef', 'ran', 'co', 'is', '.']"}], "[\n[\"Charles\", \"\u0428\u0430\u0440\u043b\u044c\"],\n[\"\u2581remained\", \"\u043e\u0441\u0442\u0430\u0432\u0430\u043b\u0441\u044f\"],\n[\"\u2581alone\", \"\u043e\u0434\u0438\u043d\"],\n[\"\u2581the\", null],\n[\"\u2581whole\", \"\u0432\u0435\u0441\u044c\"],\n[\"\u2581afternoon\", \"\u0434\u0435\u043d\u044c\"],\n[\";\", null],\n[\"\u2581they\", \"\u043e\u043d\u0438\"],\n[\"\u2581had\", null],\n[\"\u2581taken\", \"\u043e\u0442\u0432\u0435\u043b\u0438\"],\n[\"\u2581Berthe\", \"\u0411\u0435\u0440\u0442\u0443\"],\n[\"\u2581to\", \"\u043a\"],\n[\"\u2581Madame\", \"\u043c\u0430\u0434\u0430\u043c\"],\n[\"\u2581Ho\", \"\u041e\u043c\u0430\"],\n[\"mais\", null],\n[\"'\", null],\n[\";\", null],\n[\"\u2581Fe\", \"\u0424\u0435\u043b\u0438\"],\n[\"licite\", null],\n[\"\u2581was\", null],\n[\"\u2581in\", \"\u0432\"],\n[\"\u2581the\", null],\n[\"\u2581room\", \"\u043a\u043e\u043c\u043d\u0430\u0442\u0435\"],\n[\"\u2581upstairs\", \"\u043d\u0430\u0432\u0435\u0440\u0445\u0443\"],\n[\"\u2581with\", \"\u0441\"],\n[\"\u2581Madame\", \"\u043c\u0430\u0434\u0430\u043c\"],\n[\"\u2581Lef\", \"\u041b\u0435\u0444\"],\n[\"ran\", null],\n[\"co\", null],\n[\"is\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nA splendid Midsummer shone over England: skies so pure, suns so radiant as were then seen in long succession, seldom favour even singly, our wave- girt land.\n\n\n---=== TOKENIZED TEXT ===---\n\n['A', '\u2581splendid', '\u2581Midsummer', '\u2581shone', '\u2581over', '\u2581England', ':', '\u2581skies', '\u2581so', '\u2581pure', ',', '\u2581suns', '\u2581so', '\u2581radiant', '\u2581as', '\u2581were', '\u2581then', '\u2581seen', '\u2581in', '\u2581long', '\u2581succession', ',', '\u2581seldom', '\u2581favour', '\u2581even', '\u2581singly', ',', '\u2581our', '\u2581wave', '-', '\u2581gir', 't', '\u2581land', '.']"}], "[\n    [\"A\", null],\n    [\"\u2581splendid\", \"splendide\"],\n    [\"\u2581Midsummer\", \"solstice d'\u00e9t\u00e9\"],\n    [\"\u2581shone\", \"brillait\"],\n    [\"\u2581over\", \"sur\"],\n    [\"\u2581England\", \"Angleterre\"],\n    [\":\", null],\n    [\"\u2581skies\", \"ciels\"],\n    [\"\u2581so\", null],\n    [\"\u2581pure\", \"purs\"],\n    [\",\", null],\n    [\"\u2581suns\", \"soleils\"],\n    [\"\u2581so\", null],\n    [\"\u2581radiant\", \"radieux\"],\n    [\"\u2581as\", null],\n    [\"\u2581were\", null],\n    [\"\u2581then\", null],\n    [\"\u2581seen\", \"vus\"],\n    [\"\u2581in\", null],\n    [\"\u2581long\", \"longue\"],\n    [\"\u2581succession\", \"succession\"],\n    [\",\", null],\n    [\"\u2581seldom\", \"rarement\"],\n    [\"\u2581favour\", \"favorisent\"],\n    [\"\u2581even\", \"m\u00eame\"],\n    [\"\u2581singly\", \"individuellement\"],\n    [\",\", null],\n    [\"\u2581our\", \"notre\"],\n    [\"\u2581wave\", \"mer\"],\n    [\"-\", null],\n    [\"\u2581gir\", null],\n    [\"t\", null],\n    [\"\u2581land\", \"terre\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nA splendid Midsummer shone over England: skies so pure, suns so radiant as were then seen in long succession, seldom favour even singly, our wave- girt land.\n\n\n---=== TOKENIZED TEXT ===---\n\n['A', '\u2581splendid', '\u2581Midsummer', '\u2581shone', '\u2581over', '\u2581England', ':', '\u2581skies', '\u2581so', '\u2581pure', ',', '\u2581suns', '\u2581so', '\u2581radiant', '\u2581as', '\u2581were', '\u2581then', '\u2581seen', '\u2581in', '\u2581long', '\u2581succession', ',', '\u2581seldom', '\u2581favour', '\u2581even', '\u2581singly', ',', '\u2581our', '\u2581wave', '-', '\u2581gir', 't', '\u2581land', '.']"}], "[\n[\"A\", null],\n[\"\u2581splendid\", \"\u0412\u0435\u043b\u0438\u043a\u043e\u043b\u0435\u043f\u043d\u043e\u0435\"],\n[\"\u2581Midsummer\", \"\u0421\u0435\u0440\u0435\u0434\u0438\u043d\u0430 \u043b\u0435\u0442\u0430\"],\n[\"\u2581shone\", \"\u0441\u0438\u044f\u043b\u043e\"],\n[\"\u2581over\", null],\n[\"\u2581England\", \"\u0410\u043d\u0433\u043b\u0438\u0435\u0439\"],\n[\":\", null],\n[\"\u2581skies\", \"\u043d\u0435\u0431\u0435\u0441\u0430\"],\n[\"\u2581so\", null],\n[\"\u2581pure\", \"\u0447\u0438\u0441\u0442\u044b\u0435\"],\n[\",\", null],\n[\"\u2581suns\", \"\u0441\u043e\u043b\u043d\u0446\u0430\"],\n[\"\u2581so\", null],\n[\"\u2581radiant\", \"\u043b\u0443\u0447\u0435\u0437\u0430\u0440\u043d\u044b\u0435\"],\n[\"\u2581as\", null],\n[\"\u2581were\", null],\n[\"\u2581then\", null],\n[\"\u2581seen\", \"\u0432\u0438\u0434\u0435\u043d\u043d\u044b\u0435\"],\n[\"\u2581in\", null],\n[\"\u2581long\", \"\u0434\u043e\u043b\u0433\u043e\u0439\"],\n[\"\u2581succession\", \"\u0447\u0435\u0440\u0435\u0434\u0435\"],\n[\",\", null],\n[\"\u2581seldom\", \"\u0440\u0435\u0434\u043a\u043e\"],\n[\"\u2581favour\", \"\u0431\u0430\u043b\u0443\u044e\u0442\"],\n[\"\u2581even\", \"\u0434\u0430\u0436\u0435\"],\n[\"\u2581singly\", \"\u043f\u043e \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438\"],\n[\",\", null],\n[\"\u2581our\", null],\n[\"\u2581wave\", \"\u0432\u043e\u043b\u043d\u0430\u043c\u0438\"],\n[\"-\", null],\n[\"\u2581gir\", \"\u043e\u043a\u0440\u0443\u0436\u0435\u043d\u043d\u0443\u044e\"],\n[\"t\", null],\n[\"\u2581land\", \"\u0437\u0435\u043c\u043b\u044e\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nAt the end, speaking about the entrance he says, 'Now I'll go and close it', although at the beginning of the story it says the door to the law is open as it always is, but if it's always open - always - that means it's open independently of the lifespan of the man it's intended for, and not even the doorkeeper will be able to close it.\n\n\n---=== TOKENIZED TEXT ===---\n\n['At', '\u2581the', '\u2581end', ',', '\u2581speaking', '\u2581about', '\u2581the', '\u2581entrance', '\u2581he', '\u2581says', ',', \"\u2581'\", 'Now', '\u2581I', \"'\", 'll', '\u2581go', '\u2581and', '\u2581close', '\u2581it', \"',\", '\u2581although', '\u2581at', '\u2581the', '\u2581beginning', '\u2581of', '\u2581the', '\u2581story', '\u2581it', '\u2581says', '\u2581the', '\u2581door', '\u2581to', '\u2581the', '\u2581law', '\u2581is', '\u2581open', '\u2581as', '\u2581it', '\u2581always', '\u2581is', ',', '\u2581but', '\u2581if', '\u2581it', \"'\", 's', '\u2581always', '\u2581open', '\u2581-', '\u2581always', '\u2581-', '\u2581that', '\u2581means', '\u2581it', \"'\", 's', '\u2581open', '\u2581independently', '\u2581of', '\u2581the', '\u2581lifespan', '\u2581of', '\u2581the', '\u2581man', '\u2581it', \"'\", 's', '\u2581intended', '\u2581for', ',', '\u2581and', '\u2581not', '\u2581even', '\u2581the', '\u2581door', 'keeper', '\u2581will', '\u2581be', '\u2581able', '\u2581to', '\u2581close', '\u2581it', '.']"}], "[\n[\"At\", null],\n[\"\u2581the\", null],\n[\"\u2581end\", \"fin\"],\n[\",\", null],\n[\"\u2581speaking\", \"parlant\"],\n[\"\u2581about\", null],\n[\"\u2581the\", null],\n[\"\u2581entrance\", \"entr\u00e9e\"],\n[\"\u2581he\", null],\n[\"\u2581says\", \"dit\"],\n[\",\", null],\n[\"\u2581'\", null],\n[\"Now\", \"Maintenant\"],\n[\"\u2581I\", null],\n[\"'\", null],\n[\"ll\", null],\n[\"\u2581go\", \"vais\"],\n[\"\u2581and\", null],\n[\"\u2581close\", \"fermer\"],\n[\"\u2581it\", null],\n[\"',\", null],\n[\"\u2581although\", \"bien que\"],\n[\"\u2581at\", null],\n[\"\u2581the\", null],\n[\"\u2581beginning\", \"d\u00e9but\"],\n[\"\u2581of\", null],\n[\"\u2581the\", null],\n[\"\u2581story\", \"histoire\"],\n[\"\u2581it\", null],\n[\"\u2581says\", \"dit\"],\n[\"\u2581the\", null],\n[\"\u2581door\", \"porte\"],\n[\"\u2581to\", null],\n[\"\u2581the\", null],\n[\"\u2581law\", \"loi\"],\n[\"\u2581is\", null],\n[\"\u2581open\", \"ouverte\"],\n[\"\u2581as\", null],\n[\"\u2581it\", null],\n[\"\u2581always\", \"toujours\"],\n[\"\u2581is\", null],\n[\",\", null],\n[\"\u2581but\", \"mais\"],\n[\"\u2581if\", \"si\"],\n[\"\u2581it\", null],\n[\"'\", null],\n[\"s\", null],\n[\"\u2581always\", \"toujours\"],\n[\"\u2581open\", \"ouverte\"],\n[\"\u2581-\", null],\n[\"\u2581always\", \"toujours\"],\n[\"\u2581-\", null],\n[\"\u2581that\", null],\n[\"\u2581means\", \"signifie\"],\n[\"\u2581it\", null],\n[\"'\", null],\n[\"s\", null],\n[\"\u2581open\", \"ouverte\"],\n[\"\u2581independently\", \"ind\u00e9pendamment\"],\n[\"\u2581of\", null],\n[\"\u2581the\", null],\n[\"\u2581lifespan\", \"dur\u00e9e de vie\"],\n[\"\u2581of\", null],\n[\"\u2581the\", null],\n[\"\u2581man\", \"homme\"],\n[\"\u2581it\", null],\n[\"'\", null],\n[\"s\", null],\n[\"\u2581intended\", \"destin\u00e9e\"],\n[\"\u2581for\", null],\n[\",\", null],\n[\"\u2581and\", null],\n[\"\u2581not\", null],\n[\"\u2581even\", \"m\u00eame\"],\n[\"\u2581the\", null],\n[\"\u2581door\", \"gardien\"],\n[\"keeper\", null],\n[\"\u2581will\", null],\n[\"\u2581be\", null],\n[\"\u2581able\", \"capable\"],\n[\"\u2581to\", null],\n[\"\u2581close\", \"fermer\"],\n[\"\u2581it\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nAt the end, speaking about the entrance he says, 'Now I'll go and close it', although at the beginning of the story it says the door to the law is open as it always is, but if it's always open - always - that means it's open independently of the lifespan of the man it's intended for, and not even the doorkeeper will be able to close it.\n\n\n---=== TOKENIZED TEXT ===---\n\n['At', '\u2581the', '\u2581end', ',', '\u2581speaking', '\u2581about', '\u2581the', '\u2581entrance', '\u2581he', '\u2581says', ',', \"\u2581'\", 'Now', '\u2581I', \"'\", 'll', '\u2581go', '\u2581and', '\u2581close', '\u2581it', \"',\", '\u2581although', '\u2581at', '\u2581the', '\u2581beginning', '\u2581of', '\u2581the', '\u2581story', '\u2581it', '\u2581says', '\u2581the', '\u2581door', '\u2581to', '\u2581the', '\u2581law', '\u2581is', '\u2581open', '\u2581as', '\u2581it', '\u2581always', '\u2581is', ',', '\u2581but', '\u2581if', '\u2581it', \"'\", 's', '\u2581always', '\u2581open', '\u2581-', '\u2581always', '\u2581-', '\u2581that', '\u2581means', '\u2581it', \"'\", 's', '\u2581open', '\u2581independently', '\u2581of', '\u2581the', '\u2581lifespan', '\u2581of', '\u2581the', '\u2581man', '\u2581it', \"'\", 's', '\u2581intended', '\u2581for', ',', '\u2581and', '\u2581not', '\u2581even', '\u2581the', '\u2581door', 'keeper', '\u2581will', '\u2581be', '\u2581able', '\u2581to', '\u2581close', '\u2581it', '.']"}], "[[\"At\", null], [\"\u2581the\", null], [\"\u2581end\", \"\u043a\u043e\u043d\u0446\u0435\"], [\",\", null], [\"\u2581speaking\", \"\u0433\u043e\u0432\u043e\u0440\u044f\"], [\"\u2581about\", null], [\"\u2581the\", null], [\"\u2581entrance\", \"\u0432\u0445\u043e\u0434\u0435\"], [\"\u2581he\", null], [\"\u2581says\", \"\u0433\u043e\u0432\u043e\u0440\u0438\u0442\"], [\",\", null], [\"\u2581'\", null], [\"Now\", \"\u0422\u0435\u043f\u0435\u0440\u044c\"], [\"\u2581I\", null], [\"'\", null], [\"ll\", null], [\"\u2581go\", \"\u043f\u043e\u0439\u0434\u0443\"], [\"\u2581and\", null], [\"\u2581close\", \"\u0437\u0430\u043a\u0440\u043e\u044e\"], [\"\u2581it\", null], [\"',\", null], [\"\u2581although\", \"\u0445\u043e\u0442\u044f\"], [\"\u2581at\", null], [\"\u2581the\", null], [\"\u2581beginning\", \"\u043d\u0430\u0447\u0430\u043b\u0435\"], [\"\u2581of\", null], [\"\u2581the\", null], [\"\u2581story\", \"\u0438\u0441\u0442\u043e\u0440\u0438\u0438\"], [\"\u2581it\", null], [\"\u2581says\", \"\u0433\u043e\u0432\u043e\u0440\u0438\u0442\u0441\u044f\"], [\"\u2581the\", null], [\"\u2581door\", \"\u0434\u0432\u0435\u0440\u044c\"], [\"\u2581to\", null], [\"\u2581the\", null], [\"\u2581law\", \"\u0437\u0430\u043a\u043e\u043d\u0443\"], [\"\u2581is\", null], [\"\u2581open\", \"\u043e\u0442\u043a\u0440\u044b\u0442\u0430\"], [\"\u2581as\", null], [\"\u2581it\", null], [\"\u2581always\", \"\u0432\u0441\u0435\u0433\u0434\u0430\"], [\"\u2581is\", null], [\",\", null], [\"\u2581but\", \"\u043d\u043e\"], [\"\u2581if\", \"\u0435\u0441\u043b\u0438\"], [\"\u2581it\", null], [\"'\", null], [\"s\", null], [\"\u2581always\", \"\u0432\u0441\u0435\u0433\u0434\u0430\"], [\"\u2581open\", \"\u043e\u0442\u043a\u0440\u044b\u0442\u0430\"], [\"\u2581-\", null], [\"\u2581always\", \"\u0432\u0441\u0435\u0433\u0434\u0430\"], [\"\u2581-\", null], [\"\u2581that\", null], [\"\u2581means\", \"\u0437\u043d\u0430\u0447\u0438\u0442\"], [\"\u2581it\", null], [\"'\", null], [\"s\", null], [\"\u2581open\", \"\u043e\u0442\u043a\u0440\u044b\u0442\u0430\"], [\"\u2581independently\", \"\u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\"], [\"\u2581of\", null], [\"\u2581the\", null], [\"\u2581lifespan\", \"\u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0436\u0438\u0437\u043d\u0438\"], [\"\u2581of\", null], [\"\u2581the\", null], [\"\u2581man\", \"\u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430\"], [\"\u2581it\", null], [\"'\", null], [\"s\", null], [\"\u2581intended\", \"\u043f\u0440\u0435\u0434\u043d\u0430\u0437\u043d\u0430\u0447\u0435\u043d\u0430\"], [\"\u2581for\", null], [\",\", null], [\"\u2581and\", \"\u0438\"], [\"\u2581not\", \"\u043d\u0435\"], [\"\u2581even\", \"\u0434\u0430\u0436\u0435\"], [\"\u2581the\", null], [\"\u2581door\", \"\u043f\u0440\u0438\u0432\u0440\u0430\u0442\u043d\u0438\u043a\"], [\"keeper\", null], [\"\u2581will\", null], [\"\u2581be\", null], [\"\u2581able\", \"\u0441\u043c\u043e\u0436\u0435\u0442\"], [\"\u2581to\", null], [\"\u2581close\", \"\u0437\u0430\u043a\u0440\u044b\u0442\u044c\"], [\"\u2581it\", null], [\".\", null]]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\n\"You damned ... ,\" in the first gurglings of his excitement his words could hardly be understood, K. was startled even though he had been expecting something of the sort and ran to his uncle with the intention, no doubt, of closing his mouth with both his hands.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\"', 'You', '\u2581damned', '\u2581...', '\u2581,\"', '\u2581in', '\u2581the', '\u2581first', '\u2581g', 'urg', 'lings', '\u2581of', '\u2581his', '\u2581excitement', '\u2581his', '\u2581words', '\u2581could', '\u2581hardly', '\u2581be', '\u2581understood', ',', '\u2581K', '.', '\u2581was', '\u2581startled', '\u2581even', '\u2581though', '\u2581he', '\u2581had', '\u2581been', '\u2581expecting', '\u2581something', '\u2581of', '\u2581the', '\u2581sort', '\u2581and', '\u2581ran', '\u2581to', '\u2581his', '\u2581uncle', '\u2581with', '\u2581the', '\u2581intention', ',', '\u2581no', '\u2581doubt', ',', '\u2581of', '\u2581closing', '\u2581his', '\u2581mouth', '\u2581with', '\u2581both', '\u2581his', '\u2581hands', '.']"}], "[\n['\"', null],\n[\"You\", \"Vous\"],\n[\"\u2581damned\", \"maudit\"],\n[\"\u2581...\", null],\n[\"\u2581,\\\"\", null],\n[\"\u2581in\", null],\n[\"\u2581the\", null],\n[\"\u2581first\", \"premiers\"],\n[\"\u2581g\", \"gargouillis\"],\n[\"urg\", null],\n[\"lings\", null],\n[\"\u2581of\", null],\n[\"\u2581his\", null],\n[\"\u2581excitement\", \"excitation\"],\n[\"\u2581his\", null],\n[\"\u2581words\", \"mots\"],\n[\"\u2581could\", null],\n[\"\u2581hardly\", \"difficilement\"],\n[\"\u2581be\", null],\n[\"\u2581understood\", \"compris\"],\n[\",\", null],\n[\"\u2581K\", \"K\"],\n[\".\", null],\n[\"\u2581was\", null],\n[\"\u2581startled\", \"sursauta\"],\n[\"\u2581even\", null],\n[\"\u2581though\", \"bien que\"],\n[\"\u2581he\", null],\n[\"\u2581had\", null],\n[\"\u2581been\", null],\n[\"\u2581expecting\", \"s'attendait\"],\n[\"\u2581something\", \"quelque chose\"],\n[\"\u2581of\", null],\n[\"\u2581the\", null],\n[\"\u2581sort\", \"genre\"],\n[\"\u2581and\", null],\n[\"\u2581ran\", \"courut\"],\n[\"\u2581to\", null],\n[\"\u2581his\", null],\n[\"\u2581uncle\", \"oncle\"],\n[\"\u2581with\", null],\n[\"\u2581the\", null],\n[\"\u2581intention\", \"intention\"],\n[\",\", null],\n[\"\u2581no\", null],\n[\"\u2581doubt\", \"doute\"],\n[\",\", null],\n[\"\u2581of\", null],\n[\"\u2581closing\", \"fermer\"],\n[\"\u2581his\", null],\n[\"\u2581mouth\", \"bouche\"],\n[\"\u2581with\", null],\n[\"\u2581both\", \"deux\"],\n[\"\u2581his\", null],\n[\"\u2581hands\", \"mains\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\n\"You damned ... ,\" in the first gurglings of his excitement his words could hardly be understood, K. was startled even though he had been expecting something of the sort and ran to his uncle with the intention, no doubt, of closing his mouth with both his hands.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\"', 'You', '\u2581damned', '\u2581...', '\u2581,\"', '\u2581in', '\u2581the', '\u2581first', '\u2581g', 'urg', 'lings', '\u2581of', '\u2581his', '\u2581excitement', '\u2581his', '\u2581words', '\u2581could', '\u2581hardly', '\u2581be', '\u2581understood', ',', '\u2581K', '.', '\u2581was', '\u2581startled', '\u2581even', '\u2581though', '\u2581he', '\u2581had', '\u2581been', '\u2581expecting', '\u2581something', '\u2581of', '\u2581the', '\u2581sort', '\u2581and', '\u2581ran', '\u2581to', '\u2581his', '\u2581uncle', '\u2581with', '\u2581the', '\u2581intention', ',', '\u2581no', '\u2581doubt', ',', '\u2581of', '\u2581closing', '\u2581his', '\u2581mouth', '\u2581with', '\u2581both', '\u2581his', '\u2581hands', '.']"}], "[\n  ['\"', null],\n  [\"You\", \"\u0422\u044b\"],\n  [\"\u2581damned\", \"\u043f\u0440\u043e\u043a\u043b\u044f\u0442\u044b\u0439\"],\n  [\"\u2581...\", null],\n  [\"\u2581,\\\"\", null],\n  [\"\u2581in\", null],\n  [\"\u2581the\", null],\n  [\"\u2581first\", \"\u043f\u0435\u0440\u0432\u044b\u0435\"],\n  [\"\u2581g\", \"\u0431\u0443\u043b\u044c\u043a\u0430\u043d\u044c\u044f\"],\n  [\"urg\", null],\n  [\"lings\", null],\n  [\"\u2581of\", null],\n  [\"\u2581his\", \"\u0435\u0433\u043e\"],\n  [\"\u2581excitement\", \"\u0432\u043e\u0437\u0431\u0443\u0436\u0434\u0435\u043d\u0438\u044f\"],\n  [\"\u2581his\", \"\u0435\u0433\u043e\"],\n  [\"\u2581words\", \"\u0441\u043b\u043e\u0432\u0430\"],\n  [\"\u2581could\", \"\u043c\u043e\u0433\u043b\u0438\"],\n  [\"\u2581hardly\", \"\u0435\u0434\u0432\u0430\"],\n  [\"\u2581be\", null],\n  [\"\u2581understood\", \"\u043f\u043e\u043d\u0438\u043c\u0430\u0442\u044c\u0441\u044f\"],\n  [\",\", null],\n  [\"\u2581K\", \"\u041a\"],\n  [\".\", null],\n  [\"\u2581was\", null],\n  [\"\u2581startled\", \"\u0432\u0437\u0434\u0440\u043e\u0433\u043d\u0443\u043b\"],\n  [\"\u2581even\", \"\u0434\u0430\u0436\u0435\"],\n  [\"\u2581though\", \"\u0445\u043e\u0442\u044f\"],\n  [\"\u2581he\", \"\u043e\u043d\"],\n  [\"\u2581had\", null],\n  [\"\u2581been\", null],\n  [\"\u2581expecting\", \"\u043e\u0436\u0438\u0434\u0430\u043b\"],\n  [\"\u2581something\", \"\u0447\u0442\u043e-\u0442\u043e\"],\n  [\"\u2581of\", null],\n  [\"\u2581the\", null],\n  [\"\u2581sort\", \"\u043f\u043e\u0434\u043e\u0431\u043d\u043e\u0435\"],\n  [\"\u2581and\", \"\u0438\"],\n  [\"\u2581ran\", \"\u043f\u043e\u0431\u0435\u0436\u0430\u043b\"],\n  [\"\u2581to\", \"\u043a\"],\n  [\"\u2581his\", \"\u0441\u0432\u043e\u0435\u043c\u0443\"],\n  [\"\u2581uncle\", \"\u0434\u044f\u0434\u0435\"],\n  [\"\u2581with\", \"\u0441\"],\n  [\"\u2581the\", null],\n  [\"\u2581intention\", \"\u043d\u0430\u043c\u0435\u0440\u0435\u043d\u0438\u0435\u043c\"],\n  [\",\", null],\n  [\"\u2581no\", \"\u0431\u0435\u0437\"],\n  [\"\u2581doubt\", \"\u0441\u043e\u043c\u043d\u0435\u043d\u0438\u044f\"],\n  [\",\", null],\n  [\"\u2581of\", null],\n  [\"\u2581closing\", \"\u0437\u0430\u043a\u0440\u044b\u0442\u044c\"],\n  [\"\u2581his\", \"\u0435\u0433\u043e\"],\n  [\"\u2581mouth\", \"\u0440\u043e\u0442\"],\n  [\"\u2581with\", null],\n  [\"\u2581both\", \"\u043e\u0431\u0435\u0438\u043c\u0438\"],\n  [\"\u2581his\", \"\u0441\u0432\u043e\u0438\u043c\u0438\"],\n  [\"\u2581hands\", \"\u0440\u0443\u043a\u0430\u043c\u0438\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nHence, the man to whom it had belonged had come thither and had died there.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Hence', ',', '\u2581the', '\u2581man', '\u2581to', '\u2581whom', '\u2581it', '\u2581had', '\u2581belonged', '\u2581had', '\u2581come', '\u2581thither', '\u2581and', '\u2581had', '\u2581died', '\u2581there', '.']"}], "[\n  [\"Hence\", \"Donc\"],\n  [\",\", null],\n  [\"\u2581the\", null],\n  [\"\u2581man\", \"homme\"],\n  [\"\u2581to\", null],\n  [\"\u2581whom\", \"qui\"],\n  [\"\u2581it\", null],\n  [\"\u2581had\", null],\n  [\"\u2581belonged\", \"appartenait\"],\n  [\"\u2581had\", null],\n  [\"\u2581come\", \"venu\"],\n  [\"\u2581thither\", \"l\u00e0-bas\"],\n  [\"\u2581and\", null],\n  [\"\u2581had\", null],\n  [\"\u2581died\", \"mort\"],\n  [\"\u2581there\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nHence, the man to whom it had belonged had come thither and had died there.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Hence', ',', '\u2581the', '\u2581man', '\u2581to', '\u2581whom', '\u2581it', '\u2581had', '\u2581belonged', '\u2581had', '\u2581come', '\u2581thither', '\u2581and', '\u2581had', '\u2581died', '\u2581there', '.']"}], "[\n  [\"Hence\", \"\u0421\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\"],\n  [\",\", null],\n  [\"\u2581the\", null],\n  [\"\u2581man\", \"\u0447\u0435\u043b\u043e\u0432\u0435\u043a\"],\n  [\"\u2581to\", null],\n  [\"\u2581whom\", \"\u043a\u043e\u0442\u043e\u0440\u043e\u043c\u0443\"],\n  [\"\u2581it\", null],\n  [\"\u2581had\", null],\n  [\"\u2581belonged\", \"\u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u0430\u043b\u043e\"],\n  [\"\u2581had\", null],\n  [\"\u2581come\", \"\u043f\u0440\u0438\u0448\u0435\u043b\"],\n  [\"\u2581thither\", \"\u0442\u0443\u0434\u0430\"],\n  [\"\u2581and\", null],\n  [\"\u2581had\", null],\n  [\"\u2581died\", \"\u0443\u043c\u0435\u0440\"],\n  [\"\u2581there\", \"\u0442\u0430\u043c\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\n\"Oh!\" she stammered, almost petrified, \"I knew well that it was he again!\"\n\n\n---=== TOKENIZED TEXT ===---\n\n['\"', 'Oh', '!\"', '\u2581she', '\u2581stammer', 'ed', ',', '\u2581almost', '\u2581petrified', ',', '\u2581\"', 'I', '\u2581knew', '\u2581well', '\u2581that', '\u2581it', '\u2581was', '\u2581he', '\u2581again', '!\"']"}], "[\n  ['\"', null],\n  [\"Oh\", \"Oh\"],\n  ['!\"', null],\n  [\"\u2581she\", \"elle\"],\n  [\"\u2581stammer\", \"balbutier\"],\n  [\"ed\", null],\n  [\",\", null],\n  [\"\u2581almost\", \"presque\"],\n  [\"\u2581petrified\", \"p\u00e9trifi\u00e9e\"],\n  [\",\", null],\n  [\"\u2581\\\"\", null],\n  [\"I\", \"Je\"],\n  [\"\u2581knew\", \"savais\"],\n  [\"\u2581well\", \"bien\"],\n  [\"\u2581that\", \"que\"],\n  [\"\u2581it\", \"c'\"],\n  [\"\u2581was\", \"\u00e9tait\"],\n  [\"\u2581he\", \"lui\"],\n  [\"\u2581again\", \"encore\"],\n  ['!\"', null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\n\"Oh!\" she stammered, almost petrified, \"I knew well that it was he again!\"\n\n\n---=== TOKENIZED TEXT ===---\n\n['\"', 'Oh', '!\"', '\u2581she', '\u2581stammer', 'ed', ',', '\u2581almost', '\u2581petrified', ',', '\u2581\"', 'I', '\u2581knew', '\u2581well', '\u2581that', '\u2581it', '\u2581was', '\u2581he', '\u2581again', '!\"']"}], "[\n  ['\"', null],\n  [\"Oh\", \"\u041e\u0445\"],\n  ['!\"', null],\n  [\"\u2581she\", \"\u043e\u043d\u0430\"],\n  [\"\u2581stammer\", \"\u0437\u0430\u043f\u0438\u043d\u0430\u0442\u044c\u0441\u044f\"],\n  [\"ed\", null],\n  [\",\", null],\n  [\"\u2581almost\", \"\u043f\u043e\u0447\u0442\u0438\"],\n  [\"\u2581petrified\", \"\u043e\u043a\u0430\u043c\u0435\u043d\u0435\u0432\"],\n  [\",\", null],\n  [\"\u2581\\\"\", null],\n  [\"I\", \"\u042f\"],\n  [\"\u2581knew\", \"\u0437\u043d\u0430\u043b\u0430\"],\n  [\"\u2581well\", \"\u0445\u043e\u0440\u043e\u0448\u043e\"],\n  [\"\u2581that\", null],\n  [\"\u2581it\", null],\n  [\"\u2581was\", null],\n  [\"\u2581he\", \"\u043e\u043d\"],\n  [\"\u2581again\", \"\u0441\u043d\u043e\u0432\u0430\"],\n  [\"!\\\"\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nNow it converts itself into a flock of birds, scatters itself to the four winds, and occupies all points of air and space at once.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Now', '\u2581it', '\u2581converts', '\u2581itself', '\u2581into', '\u2581a', '\u2581flock', '\u2581of', '\u2581birds', ',', '\u2581scat', 'ters', '\u2581itself', '\u2581to', '\u2581the', '\u2581four', '\u2581winds', ',', '\u2581and', '\u2581occupies', '\u2581all', '\u2581points', '\u2581of', '\u2581air', '\u2581and', '\u2581space', '\u2581at', '\u2581once', '.']"}], "[\n[\"Now\", \"Maintenant\"],\n[\"\u2581it\", null],\n[\"\u2581converts\", \"transforme\"],\n[\"\u2581itself\", \"se\"],\n[\"\u2581into\", null],\n[\"\u2581a\", null],\n[\"\u2581flock\", \"vol\u00e9e\"],\n[\"\u2581of\", null],\n[\"\u2581birds\", \"oiseaux\"],\n[\",\", null],\n[\"\u2581scat\", \"disperse\"],\n[\"ters\", null],\n[\"\u2581itself\", \"se\"],\n[\"\u2581to\", null],\n[\"\u2581the\", null],\n[\"\u2581four\", \"quatre\"],\n[\"\u2581winds\", \"vents\"],\n[\",\", null],\n[\"\u2581and\", null],\n[\"\u2581occupies\", \"occupe\"],\n[\"\u2581all\", \"tous\"],\n[\"\u2581points\", \"points\"],\n[\"\u2581of\", null],\n[\"\u2581air\", \"air\"],\n[\"\u2581and\", null],\n[\"\u2581space\", \"espace\"],\n[\"\u2581at\", null],\n[\"\u2581once\", \"fois\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nNow it converts itself into a flock of birds, scatters itself to the four winds, and occupies all points of air and space at once.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Now', '\u2581it', '\u2581converts', '\u2581itself', '\u2581into', '\u2581a', '\u2581flock', '\u2581of', '\u2581birds', ',', '\u2581scat', 'ters', '\u2581itself', '\u2581to', '\u2581the', '\u2581four', '\u2581winds', ',', '\u2581and', '\u2581occupies', '\u2581all', '\u2581points', '\u2581of', '\u2581air', '\u2581and', '\u2581space', '\u2581at', '\u2581once', '.']"}], "[\n  [\"Now\", \"\u0422\u0435\u043f\u0435\u0440\u044c\"],\n  [\"\u2581it\", null],\n  [\"\u2581converts\", \"\u043f\u0440\u0435\u0432\u0440\u0430\u0449\u0430\u0435\u0442\u0441\u044f\"],\n  [\"\u2581itself\", \"\u0441\u0435\u0431\u044f\"],\n  [\"\u2581into\", null],\n  [\"\u2581a\", null],\n  [\"\u2581flock\", \"\u0441\u0442\u0430\u044e\"],\n  [\"\u2581of\", null],\n  [\"\u2581birds\", \"\u043f\u0442\u0438\u0446\"],\n  [\",\", null],\n  [\"\u2581scat\", \"\u0440\u0430\u0441\u0441\u0435\u0438\u0432\u0430\u0435\u0442\u0441\u044f\"],\n  [\"ters\", null],\n  [\"\u2581itself\", \"\u0441\u0435\u0431\u044f\"],\n  [\"\u2581to\", null],\n  [\"\u2581the\", null],\n  [\"\u2581four\", \"\u0447\u0435\u0442\u044b\u0440\u0435\u043c\"],\n  [\"\u2581winds\", \"\u0432\u0435\u0442\u0440\u0430\u043c\"],\n  [\",\", null],\n  [\"\u2581and\", null],\n  [\"\u2581occupies\", \"\u0437\u0430\u043d\u0438\u043c\u0430\u0435\u0442\"],\n  [\"\u2581all\", \"\u0432\u0441\u0435\"],\n  [\"\u2581points\", \"\u0442\u043e\u0447\u043a\u0438\"],\n  [\"\u2581of\", null],\n  [\"\u2581air\", \"\u0432\u043e\u0437\u0434\u0443\u0445\u0430\"],\n  [\"\u2581and\", null],\n  [\"\u2581space\", \"\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0430\"],\n  [\"\u2581at\", null],\n  [\"\u2581once\", \"\u043e\u0434\u043d\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nVoil\u00e0 tout. \u00bb Un soldat montra au roi une serpe qu\u2019on avait saisie sur le truand.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Voil\u00e0', '\u2581tout', '.', '\u2581\u00bb', '\u2581Un', '\u2581soldat', '\u2581mont', 'ra', '\u2581au', '\u2581roi', '\u2581une', '\u2581ser', 'pe', '\u2581qu', '\u2019', 'on', '\u2581avait', '\u2581saisie', '\u2581sur', '\u2581le', '\u2581tru', 'and', '.']"}], "[\n    [\"Voil\u00e0\", \"That's\"],\n    [\"\u2581tout\", \"all\"],\n    [\".\", null],\n    [\"\u2581\u00bb\", null],\n    [\"\u2581Un\", null],\n    [\"\u2581soldat\", \"soldier\"],\n    [\"\u2581mont\", \"showed\"],\n    [\"ra\", null],\n    [\"\u2581au\", null],\n    [\"\u2581roi\", \"king\"],\n    [\"\u2581une\", null],\n    [\"\u2581ser\", \"sickle\"],\n    [\"pe\", null],\n    [\"\u2581qu\", null],\n    [\"'\", null],\n    [\"on\", null],\n    [\"\u2581avait\", null],\n    [\"\u2581saisie\", \"seized\"],\n    [\"\u2581sur\", null],\n    [\"\u2581le\", null],\n    [\"\u2581tru\", \"vagrant\"],\n    [\"and\", null],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nVoil\u00e0 tout. \u00bb Un soldat montra au roi une serpe qu\u2019on avait saisie sur le truand.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Voil\u00e0', '\u2581tout', '.', '\u2581\u00bb', '\u2581Un', '\u2581soldat', '\u2581mont', 'ra', '\u2581au', '\u2581roi', '\u2581une', '\u2581ser', 'pe', '\u2581qu', '\u2019', 'on', '\u2581avait', '\u2581saisie', '\u2581sur', '\u2581le', '\u2581tru', 'and', '.']"}], "[\n  [\"Voil\u00e0\", \"\u0412\u043e\u0442\"],\n  [\"\u2581tout\", \"\u0432\u0441\u0451\"],\n  [\".\", null],\n  [\"\u2581\u00bb\", null],\n  [\"\u2581Un\", null],\n  [\"\u2581soldat\", \"\u0421\u043e\u043b\u0434\u0430\u0442\"],\n  [\"\u2581mont\", \"\u043f\u043e\u043a\u0430\u0437\u0430\u043b\"],\n  [\"ra\", null],\n  [\"\u2581au\", null],\n  [\"\u2581roi\", \"\u043a\u043e\u0440\u043e\u043b\u044e\"],\n  [\"\u2581une\", null],\n  [\"\u2581ser\", \"\u0441\u0435\u0440\u043f\"],\n  [\"pe\", null],\n  [\"\u2581qu\", null],\n  [\"'\", null],\n  [\"on\", null],\n  [\"\u2581avait\", null],\n  [\"\u2581saisie\", \"\u0438\u0437\u044a\u044f\u043b\u0438\"],\n  [\"\u2581sur\", \"\u0443\"],\n  [\"\u2581le\", null],\n  [\"\u2581tru\", \"\u0431\u0440\u043e\u0434\u044f\u0433\u0438\"],\n  [\"and\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nL\u2019autre eut un geste d\u00e9daigneux plein de cordialit\u00e9, et, prenant son chapeau:\n\n\n---=== TOKENIZED TEXT ===---\n\n['L', '\u2019', 'autre', '\u2581eut', '\u2581un', '\u2581geste', '\u2581d\u00e9', 'da', 'igne', 'ux', '\u2581plein', '\u2581de', '\u2581cordial', 'it\u00e9', ',', '\u2581et', ',', '\u2581prenant', '\u2581son', '\u2581chapeau', ':']"}], "[\n  [\"L\", null],\n  [\"'\", null],\n  [\"autre\", \"other\"],\n  [\"\u2581eut\", \"had\"],\n  [\"\u2581un\", \"a\"],\n  [\"\u2581geste\", \"gesture\"],\n  [\"\u2581d\u00e9\", \"disdainful\"],\n  [\"da\", null],\n  [\"igne\", null],\n  [\"ux\", null],\n  [\"\u2581plein\", \"full\"],\n  [\"\u2581de\", null],\n  [\"\u2581cordial\", \"cordial\"],\n  [\"it\u00e9\", null],\n  [\",\", null],\n  [\"\u2581et\", null],\n  [\",\", null],\n  [\"\u2581prenant\", \"taking\"],\n  [\"\u2581son\", \"his\"],\n  [\"\u2581chapeau\", \"hat\"],\n  [\":\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nL\u2019autre eut un geste d\u00e9daigneux plein de cordialit\u00e9, et, prenant son chapeau:\n\n\n---=== TOKENIZED TEXT ===---\n\n['L', '\u2019', 'autre', '\u2581eut', '\u2581un', '\u2581geste', '\u2581d\u00e9', 'da', 'igne', 'ux', '\u2581plein', '\u2581de', '\u2581cordial', 'it\u00e9', ',', '\u2581et', ',', '\u2581prenant', '\u2581son', '\u2581chapeau', ':']"}], "[\n    [\"L\", null],\n    [\"'\", null],\n    [\"autre\", \"\u0414\u0440\u0443\u0433\u043e\u0439\"],\n    [\"\u2581eut\", \"\u0438\u043c\u0435\u043b\"],\n    [\"\u2581un\", null],\n    [\"\u2581geste\", \"\u0436\u0435\u0441\u0442\"],\n    [\"\u2581d\u00e9\", \"\u043f\u0440\u0435\u043d\u0435\u0431\u0440\u0435\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439\"],\n    [\"da\", null],\n    [\"igne\", null],\n    [\"ux\", null],\n    [\"\u2581plein\", \"\u043f\u043e\u043b\u043d\u044b\u0439\"],\n    [\"\u2581de\", null],\n    [\"\u2581cordial\", \"\u0441\u0435\u0440\u0434\u0435\u0447\u043d\u043e\u0441\u0442\u0438\"],\n    [\"it\u00e9\", null],\n    [\",\", null],\n    [\"\u2581et\", null],\n    [\",\", null],\n    [\"\u2581prenant\", \"\u0432\u0437\u044f\u0432\"],\n    [\"\u2581son\", null],\n    [\"\u2581chapeau\", \"\u0448\u043b\u044f\u043f\u0443\"],\n    [\":\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nJe vous dirai sur la voyageuse que vous avez recueillie tout ce que je puis dire sans compromettre la paix de mon esprit, ma propre s\u00e9curit\u00e9 morale et physique, et surtout celle des autres.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Je', '\u2581vous', '\u2581di', 'rai', '\u2581sur', '\u2581la', '\u2581voyage', 'use', '\u2581que', '\u2581vous', '\u2581avez', '\u2581recue', 'illie', '\u2581tout', '\u2581ce', '\u2581que', '\u2581je', '\u2581puis', '\u2581dire', '\u2581sans', '\u2581comprome', 'ttre', '\u2581la', '\u2581paix', '\u2581de', '\u2581mon', '\u2581esprit', ',', '\u2581ma', '\u2581propre', '\u2581s\u00e9curit\u00e9', '\u2581morale', '\u2581et', '\u2581physique', ',', '\u2581et', '\u2581surtout', '\u2581celle', '\u2581des', '\u2581autres', '.']"}], "[\n[\"Je\", \"I\"],\n[\"\u2581vous\", null],\n[\"\u2581di\", \"tell\"],\n[\"rai\", null],\n[\"\u2581sur\", null],\n[\"\u2581la\", null],\n[\"\u2581voyage\", \"traveler\"],\n[\"use\", null],\n[\"\u2581que\", null],\n[\"\u2581vous\", null],\n[\"\u2581avez\", null],\n[\"\u2581recue\", \"taken in\"],\n[\"illie\", null],\n[\"\u2581tout\", null],\n[\"\u2581ce\", null],\n[\"\u2581que\", null],\n[\"\u2581je\", null],\n[\"\u2581puis\", null],\n[\"\u2581dire\", \"say\"],\n[\"\u2581sans\", null],\n[\"\u2581comprome\", \"compromise\"],\n[\"ttre\", null],\n[\"\u2581la\", null],\n[\"\u2581paix\", \"peace\"],\n[\"\u2581de\", null],\n[\"\u2581mon\", null],\n[\"\u2581esprit\", \"mind\"],\n[\",\", null],\n[\"\u2581ma\", null],\n[\"\u2581propre\", \"own\"],\n[\"\u2581s\u00e9curit\u00e9\", \"safety\"],\n[\"\u2581morale\", \"moral\"],\n[\"\u2581et\", null],\n[\"\u2581physique\", \"physical\"],\n[\",\", null],\n[\"\u2581et\", null],\n[\"\u2581surtout\", \"especially\"],\n[\"\u2581celle\", null],\n[\"\u2581des\", null],\n[\"\u2581autres\", \"others\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nJe vous dirai sur la voyageuse que vous avez recueillie tout ce que je puis dire sans compromettre la paix de mon esprit, ma propre s\u00e9curit\u00e9 morale et physique, et surtout celle des autres.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Je', '\u2581vous', '\u2581di', 'rai', '\u2581sur', '\u2581la', '\u2581voyage', 'use', '\u2581que', '\u2581vous', '\u2581avez', '\u2581recue', 'illie', '\u2581tout', '\u2581ce', '\u2581que', '\u2581je', '\u2581puis', '\u2581dire', '\u2581sans', '\u2581comprome', 'ttre', '\u2581la', '\u2581paix', '\u2581de', '\u2581mon', '\u2581esprit', ',', '\u2581ma', '\u2581propre', '\u2581s\u00e9curit\u00e9', '\u2581morale', '\u2581et', '\u2581physique', ',', '\u2581et', '\u2581surtout', '\u2581celle', '\u2581des', '\u2581autres', '.']"}], "[\n[\"Je\", \"\u042f\"],\n[\"\u2581vous\", null],\n[\"\u2581di\", \"\u0441\u043a\u0430\u0436\u0443\"],\n[\"rai\", null],\n[\"\u2581sur\", null],\n[\"\u2581la\", null],\n[\"\u2581voyage\", \"\u043f\u0443\u0442\u0435\u0448\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u0438\u0446\u0435\"],\n[\"use\", null],\n[\"\u2581que\", null],\n[\"\u2581vous\", null],\n[\"\u2581avez\", null],\n[\"\u2581recue\", \"\u043f\u0440\u0438\u044e\u0442\u0438\u043b\u0438\"],\n[\"illie\", null],\n[\"\u2581tout\", null],\n[\"\u2581ce\", null],\n[\"\u2581que\", null],\n[\"\u2581je\", null],\n[\"\u2581puis\", \"\u043c\u043e\u0433\u0443\"],\n[\"\u2581dire\", \"\u0441\u043a\u0430\u0437\u0430\u0442\u044c\"],\n[\"\u2581sans\", null],\n[\"\u2581comprome\", \"\u0441\u043a\u043e\u043c\u043f\u0440\u043e\u043c\u0435\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c\"],\n[\"ttre\", null],\n[\"\u2581la\", null],\n[\"\u2581paix\", \"\u043f\u043e\u043a\u043e\u0439\"],\n[\"\u2581de\", null],\n[\"\u2581mon\", null],\n[\"\u2581esprit\", \"\u0440\u0430\u0437\u0443\u043c\u0430\"],\n[\",\", null],\n[\"\u2581ma\", null],\n[\"\u2581propre\", \"\u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u0443\u044e\"],\n[\"\u2581s\u00e9curit\u00e9\", \"\u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u044c\"],\n[\"\u2581morale\", \"\u043c\u043e\u0440\u0430\u043b\u044c\u043d\u0443\u044e\"],\n[\"\u2581et\", null],\n[\"\u2581physique\", \"\u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0443\u044e\"],\n[\",\", null],\n[\"\u2581et\", null],\n[\"\u2581surtout\", \"\u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\"],\n[\"\u2581celle\", \"\u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u044c\"],\n[\"\u2581des\", null],\n[\"\u2581autres\", \"\u0434\u0440\u0443\u0433\u0438\u0445\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n\u00c7\u00e0 et l\u00e0 \u00e9tincelaient des points lumineux, qui se croisaient dans tous les sens, s\u2019\u00e9teignaient et se rallumaient tour \u00e0 tour.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00c7', '\u00e0', '\u2581et', '\u2581l\u00e0', '\u2581\u00e9', 'tin', 'cel', 'aient', '\u2581des', '\u2581points', '\u2581lumineux', ',', '\u2581qui', '\u2581se', '\u2581crois', 'aient', '\u2581dans', '\u2581tous', '\u2581les', '\u2581sens', ',', '\u2581s', '\u2019', '\u00e9te', 'igna', 'ient', '\u2581et', '\u2581se', '\u2581rall', 'uma', 'ient', '\u2581tour', '\u2581\u00e0', '\u2581tour', '.']"}], "[\n  [\"\u00c7\", \"Here\"],\n  [\"\u00e0\", null],\n  [\"\u2581et\", null],\n  [\"\u2581l\u00e0\", null],\n  [\"\u2581\u00e9\", \"sparkled\"],\n  [\"tin\", null],\n  [\"cel\", null],\n  [\"aient\", null],\n  [\"\u2581des\", null],\n  [\"\u2581points\", \"points\"],\n  [\"\u2581lumineux\", \"of light\"],\n  [\",\", null],\n  [\"\u2581qui\", null],\n  [\"\u2581se\", null],\n  [\"\u2581crois\", \"crossed\"],\n  [\"aient\", null],\n  [\"\u2581dans\", null],\n  [\"\u2581tous\", null],\n  [\"\u2581les\", null],\n  [\"\u2581sens\", \"directions\"],\n  [\",\", null],\n  [\"\u2581s\", null],\n  [\"'\", null],\n  [\"\u00e9te\", \"extinguished\"],\n  [\"igna\", null],\n  [\"ient\", null],\n  [\"\u2581et\", null],\n  [\"\u2581se\", null],\n  [\"\u2581rall\", \"relit\"],\n  [\"uma\", null],\n  [\"ient\", null],\n  [\"\u2581tour\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581tour\", \"turn\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n\u00c7\u00e0 et l\u00e0 \u00e9tincelaient des points lumineux, qui se croisaient dans tous les sens, s\u2019\u00e9teignaient et se rallumaient tour \u00e0 tour.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00c7', '\u00e0', '\u2581et', '\u2581l\u00e0', '\u2581\u00e9', 'tin', 'cel', 'aient', '\u2581des', '\u2581points', '\u2581lumineux', ',', '\u2581qui', '\u2581se', '\u2581crois', 'aient', '\u2581dans', '\u2581tous', '\u2581les', '\u2581sens', ',', '\u2581s', '\u2019', '\u00e9te', 'igna', 'ient', '\u2581et', '\u2581se', '\u2581rall', 'uma', 'ient', '\u2581tour', '\u2581\u00e0', '\u2581tour', '.']"}], "[\n[\"\u00c7\", null],\n[\"\u00e0\", null],\n[\"\u2581et\", null],\n[\"\u2581l\u00e0\", \"\u0442\u0430\u043c\"],\n[\"\u2581\u00e9\", \"\u043c\u0435\u0440\u0446\u0430\u043b\u0438\"],\n[\"tin\", null],\n[\"cel\", null],\n[\"aient\", null],\n[\"\u2581des\", null],\n[\"\u2581points\", \"\u0442\u043e\u0447\u043a\u0438\"],\n[\"\u2581lumineux\", \"\u0441\u0432\u0435\u0442\u0430\"],\n[\",\", null],\n[\"\u2581qui\", null],\n[\"\u2581se\", null],\n[\"\u2581crois\", \"\u043f\u0435\u0440\u0435\u0441\u0435\u043a\u0430\u043b\u0438\u0441\u044c\"],\n[\"aient\", null],\n[\"\u2581dans\", null],\n[\"\u2581tous\", \"\u0432\u043e\"],\n[\"\u2581les\", \"\u0432\u0441\u0435\u0445\"],\n[\"\u2581sens\", \"\u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f\u0445\"],\n[\",\", null],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"\u00e9te\", \"\u043f\u043e\u0433\u0430\u0441\u0430\u043b\u0438\"],\n[\"igna\", null],\n[\"ient\", null],\n[\"\u2581et\", null],\n[\"\u2581se\", null],\n[\"\u2581rall\", \"\u0441\u043d\u043e\u0432\u0430\"],\n[\"uma\", \"\u0437\u0430\u0436\u0438\u0433\u0430\u043b\u0438\u0441\u044c\"],\n[\"ient\", null],\n[\"\u2581tour\", \"\u043f\u043e\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581tour\", \"\u043e\u0447\u0435\u0440\u0435\u0434\u0438\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n\u00c0 ces hardies paroles, lord Glenarvan avait tendu les bras \u00e0 sa jeune femme; il souriait, il la pressait sur son coeur, tandis que Mary et Robert lui baisaient les mains.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00c0', '\u2581ces', '\u2581har', 'dies', '\u2581paroles', ',', '\u2581lord', '\u2581Glen', 'ar', 'van', '\u2581avait', '\u2581tend', 'u', '\u2581les', '\u2581bras', '\u2581\u00e0', '\u2581sa', '\u2581jeune', '\u2581femme', ';', '\u2581il', '\u2581souri', 'ait', ',', '\u2581il', '\u2581la', '\u2581press', 'ait', '\u2581sur', '\u2581son', '\u2581coeur', ',', '\u2581tandis', '\u2581que', '\u2581Mary', '\u2581et', '\u2581Robert', '\u2581lui', '\u2581ba', 'is', 'aient', '\u2581les', '\u2581mains', '.']"}], "[\n[\"\u00c0\", null],\n[\"\u2581ces\", \"these\"],\n[\"\u2581har\", \"bold\"],\n[\"dies\", null],\n[\"\u2581paroles\", \"words\"],\n[\",\", null],\n[\"\u2581lord\", \"Lord\"],\n[\"\u2581Glen\", \"Glenarvan\"],\n[\"ar\", null],\n[\"van\", null],\n[\"\u2581avait\", \"had\"],\n[\"\u2581tend\", \"stretched\"],\n[\"u\", null],\n[\"\u2581les\", null],\n[\"\u2581bras\", \"arms\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581sa\", null],\n[\"\u2581jeune\", \"young\"],\n[\"\u2581femme\", \"wife\"],\n[\";\", null],\n[\"\u2581il\", null],\n[\"\u2581souri\", \"smiled\"],\n[\"ait\", null],\n[\",\", null],\n[\"\u2581il\", null],\n[\"\u2581la\", null],\n[\"\u2581press\", \"pressed\"],\n[\"ait\", null],\n[\"\u2581sur\", null],\n[\"\u2581son\", null],\n[\"\u2581coeur\", \"heart\"],\n[\",\", null],\n[\"\u2581tandis\", \"while\"],\n[\"\u2581que\", null],\n[\"\u2581Mary\", \"Mary\"],\n[\"\u2581et\", \"and\"],\n[\"\u2581Robert\", \"Robert\"],\n[\"\u2581lui\", null],\n[\"\u2581ba\", \"kissed\"],\n[\"is\", null],\n[\"aient\", null],\n[\"\u2581les\", null],\n[\"\u2581mains\", \"hands\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n\u00c0 ces hardies paroles, lord Glenarvan avait tendu les bras \u00e0 sa jeune femme; il souriait, il la pressait sur son coeur, tandis que Mary et Robert lui baisaient les mains.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00c0', '\u2581ces', '\u2581har', 'dies', '\u2581paroles', ',', '\u2581lord', '\u2581Glen', 'ar', 'van', '\u2581avait', '\u2581tend', 'u', '\u2581les', '\u2581bras', '\u2581\u00e0', '\u2581sa', '\u2581jeune', '\u2581femme', ';', '\u2581il', '\u2581souri', 'ait', ',', '\u2581il', '\u2581la', '\u2581press', 'ait', '\u2581sur', '\u2581son', '\u2581coeur', ',', '\u2581tandis', '\u2581que', '\u2581Mary', '\u2581et', '\u2581Robert', '\u2581lui', '\u2581ba', 'is', 'aient', '\u2581les', '\u2581mains', '.']"}], "[\n[\"\u00c0\", null],\n[\"\u2581ces\", \"\u042d\u0442\u0438\"],\n[\"\u2581har\", \"\u0441\u043c\u0435\u043b\u044b\u0435\"],\n[\"dies\", null],\n[\"\u2581paroles\", \"\u0441\u043b\u043e\u0432\u0430\"],\n[\",\", null],\n[\"\u2581lord\", \"\u043b\u043e\u0440\u0434\"],\n[\"\u2581Glen\", \"\u0413\u043b\u0435\u043d\u0430\u0440\u0432\u0430\u043d\"],\n[\"ar\", null],\n[\"van\", null],\n[\"\u2581avait\", \"\u043f\u0440\u043e\u0442\u044f\u043d\u0443\u043b\"],\n[\"\u2581tend\", null],\n[\"u\", null],\n[\"\u2581les\", null],\n[\"\u2581bras\", \"\u0440\u0443\u043a\u0438\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581sa\", null],\n[\"\u2581jeune\", \"\u043c\u043e\u043b\u043e\u0434\u043e\u0439\"],\n[\"\u2581femme\", \"\u0436\u0435\u043d\u0435\"],\n[\";\", null],\n[\"\u2581il\", null],\n[\"\u2581souri\", \"\u0443\u043b\u044b\u0431\u0430\u043b\u0441\u044f\"],\n[\"ait\", null],\n[\",\", null],\n[\"\u2581il\", null],\n[\"\u2581la\", null],\n[\"\u2581press\", \"\u043f\u0440\u0438\u0436\u0438\u043c\u0430\u043b\"],\n[\"ait\", null],\n[\"\u2581sur\", null],\n[\"\u2581son\", null],\n[\"\u2581coeur\", \"\u0441\u0435\u0440\u0434\u0446\u0443\"],\n[\",\", null],\n[\"\u2581tandis\", \"\u0432 \u0442\u043e \u0432\u0440\u0435\u043c\u044f \u043a\u0430\u043a\"],\n[\"\u2581que\", null],\n[\"\u2581Mary\", \"\u041c\u044d\u0440\u0438\"],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581Robert\", \"\u0420\u043e\u0431\u0435\u0440\u0442\"],\n[\"\u2581lui\", null],\n[\"\u2581ba\", \"\u0446\u0435\u043b\u043e\u0432\u0430\u043b\u0438\"],\n[\"is\", null],\n[\"aient\", null],\n[\"\u2581les\", null],\n[\"\u2581mains\", \"\u0440\u0443\u043a\u0438\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nCette force du sexe et du sang de l\u2019homme que, fol adolescent, j\u2019avais cru \u00e9touffer pour la vie, avait plus d\u2019une fois soulev\u00e9 convulsivement la cha\u00eene des v\u0153ux de fer qui me scellent, mis\u00e9rable, aux froides pierres de l\u2019autel.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Cette', '\u2581force', '\u2581du', '\u2581sexe', '\u2581et', '\u2581du', '\u2581sang', '\u2581de', '\u2581l', '\u2019', 'homme', '\u2581que', ',', '\u2581fol', '\u2581adolescent', ',', '\u2581j', '\u2019', 'avais', '\u2581cru', '\u2581\u00e9', 'tou', 'ffer', '\u2581pour', '\u2581la', '\u2581vie', ',', '\u2581avait', '\u2581plus', '\u2581d', '\u2019', 'une', '\u2581fois', '\u2581sou', 'lev\u00e9', '\u2581conv', 'ulsive', 'ment', '\u2581la', '\u2581cha\u00eene', '\u2581des', '\u2581v\u0153ux', '\u2581de', '\u2581fer', '\u2581qui', '\u2581me', '\u2581sc', 'ellent', ',', '\u2581mis', '\u00e9rable', ',', '\u2581aux', '\u2581froid', 'es', '\u2581pierres', '\u2581de', '\u2581l', '\u2019', 'au', 'tel', '.']"}], "[\n[\"Cette\", \"This\"],\n[\"\u2581force\", \"strength\"],\n[\"\u2581du\", null],\n[\"\u2581sexe\", \"sex\"],\n[\"\u2581et\", null],\n[\"\u2581du\", null],\n[\"\u2581sang\", \"blood\"],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"homme\", \"man\"],\n[\"\u2581que\", null],\n[\",\", null],\n[\"\u2581fol\", \"foolish\"],\n[\"\u2581adolescent\", \"adolescent\"],\n[\",\", null],\n[\"\u2581j\", null],\n[\"'\", null],\n[\"avais\", \"had\"],\n[\"\u2581cru\", \"believed\"],\n[\"\u2581\u00e9\", null],\n[\"tou\", null],\n[\"ffer\", \"smother\"],\n[\"\u2581pour\", null],\n[\"\u2581la\", null],\n[\"\u2581vie\", \"life\"],\n[\",\", null],\n[\"\u2581avait\", \"had\"],\n[\"\u2581plus\", \"more\"],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"une\", \"one\"],\n[\"\u2581fois\", \"time\"],\n[\"\u2581sou\", null],\n[\"lev\u00e9\", \"raised\"],\n[\"\u2581conv\", null],\n[\"ulsive\", null],\n[\"ment\", \"convulsively\"],\n[\"\u2581la\", null],\n[\"\u2581cha\u00eene\", \"chain\"],\n[\"\u2581des\", null],\n[\"\u2581v\u0153ux\", \"vows\"],\n[\"\u2581de\", null],\n[\"\u2581fer\", \"iron\"],\n[\"\u2581qui\", null],\n[\"\u2581me\", null],\n[\"\u2581sc\", null],\n[\"ellent\", \"seal\"],\n[\",\", null],\n[\"\u2581mis\", null],\n[\"\u00e9rable\", \"miserable\"],\n[\",\", null],\n[\"\u2581aux\", null],\n[\"\u2581froid\", null],\n[\"es\", \"cold\"],\n[\"\u2581pierres\", \"stones\"],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"au\", null],\n[\"tel\", \"altar\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nCette force du sexe et du sang de l\u2019homme que, fol adolescent, j\u2019avais cru \u00e9touffer pour la vie, avait plus d\u2019une fois soulev\u00e9 convulsivement la cha\u00eene des v\u0153ux de fer qui me scellent, mis\u00e9rable, aux froides pierres de l\u2019autel.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Cette', '\u2581force', '\u2581du', '\u2581sexe', '\u2581et', '\u2581du', '\u2581sang', '\u2581de', '\u2581l', '\u2019', 'homme', '\u2581que', ',', '\u2581fol', '\u2581adolescent', ',', '\u2581j', '\u2019', 'avais', '\u2581cru', '\u2581\u00e9', 'tou', 'ffer', '\u2581pour', '\u2581la', '\u2581vie', ',', '\u2581avait', '\u2581plus', '\u2581d', '\u2019', 'une', '\u2581fois', '\u2581sou', 'lev\u00e9', '\u2581conv', 'ulsive', 'ment', '\u2581la', '\u2581cha\u00eene', '\u2581des', '\u2581v\u0153ux', '\u2581de', '\u2581fer', '\u2581qui', '\u2581me', '\u2581sc', 'ellent', ',', '\u2581mis', '\u00e9rable', ',', '\u2581aux', '\u2581froid', 'es', '\u2581pierres', '\u2581de', '\u2581l', '\u2019', 'au', 'tel', '.']"}], "[\n[\"Cette\", \"\u042d\u0442\u0430\"],\n[\"\u2581force\", \"\u0441\u0438\u043b\u0430\"],\n[\"\u2581du\", null],\n[\"\u2581sexe\", \"\u043f\u043e\u043b\u0430\"],\n[\"\u2581et\", null],\n[\"\u2581du\", null],\n[\"\u2581sang\", \"\u043a\u0440\u043e\u0432\u0438\"],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"homme\", \"\u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430\"],\n[\"\u2581que\", null],\n[\",\", null],\n[\"\u2581fol\", \"\u0431\u0435\u0437\u0443\u043c\u043d\u044b\u0439\"],\n[\"\u2581adolescent\", \"\u043f\u043e\u0434\u0440\u043e\u0441\u0442\u043e\u043a\"],\n[\",\", null],\n[\"\u2581j\", null],\n[\"'\", null],\n[\"avais\", \"\u0434\u0443\u043c\u0430\u043b\"],\n[\"\u2581cru\", null],\n[\"\u2581\u00e9\", null],\n[\"tou\", null],\n[\"ffer\", \"\u043f\u043e\u0434\u0430\u0432\u0438\u0442\u044c\"],\n[\"\u2581pour\", null],\n[\"\u2581la\", null],\n[\"\u2581vie\", \"\u0436\u0438\u0437\u043d\u044c\"],\n[\",\", null],\n[\"\u2581avait\", \"\u043d\u0435\u043e\u0434\u043d\u043e\u043a\u0440\u0430\u0442\u043d\u043e\"],\n[\"\u2581plus\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"une\", null],\n[\"\u2581fois\", null],\n[\"\u2581sou\", null],\n[\"lev\u00e9\", \"\u043f\u043e\u0434\u043d\u0438\u043c\u0430\u043b\u0430\"],\n[\"\u2581conv\", null],\n[\"ulsive\", null],\n[\"ment\", \"\u0441\u0443\u0434\u043e\u0440\u043e\u0436\u043d\u043e\"],\n[\"\u2581la\", null],\n[\"\u2581cha\u00eene\", \"\u0446\u0435\u043f\u044c\"],\n[\"\u2581des\", null],\n[\"\u2581v\u0153ux\", \"\u043e\u0431\u0435\u0442\u043e\u0432\"],\n[\"\u2581de\", null],\n[\"\u2581fer\", \"\u0436\u0435\u043b\u0435\u0437\u043d\u044b\u0445\"],\n[\"\u2581qui\", null],\n[\"\u2581me\", null],\n[\"\u2581sc\", null],\n[\"ellent\", \"\u043f\u0440\u0438\u043a\u043e\u0432\u044b\u0432\u0430\u044e\u0442\"],\n[\",\", null],\n[\"\u2581mis\", null],\n[\"\u00e9rable\", \"\u0436\u0430\u043b\u043a\u043e\u0433\u043e\"],\n[\",\", null],\n[\"\u2581aux\", null],\n[\"\u2581froid\", null],\n[\"es\", \"\u0445\u043e\u043b\u043e\u0434\u043d\u044b\u043c\"],\n[\"\u2581pierres\", \"\u043a\u0430\u043c\u043d\u044f\u043c\"],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"au\", null],\n[\"tel\", \"\u0430\u043b\u0442\u0430\u0440\u044f\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nEnfin on disait, horreur dans un enfant de seize ans, que ses d\u00e9bordements allaient souventes fois jusqu\u2019\u00e0 la rue de Glatigny.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Enfin', '\u2581on', '\u2581disait', ',', '\u2581hor', 'reur', '\u2581dans', '\u2581un', '\u2581enfant', '\u2581de', '\u2581seize', '\u2581ans', ',', '\u2581que', '\u2581ses', '\u2581d\u00e9', 'borde', 'ments', '\u2581alla', 'ient', '\u2581souvent', 'es', '\u2581fois', '\u2581jusqu', '\u2019', '\u00e0', '\u2581la', '\u2581rue', '\u2581de', '\u2581G', 'lat', 'igny', '.']"}], "[\n[\"Enfin\", \"Finally\"],\n[\"\u2581on\", null],\n[\"\u2581disait\", \"said\"],\n[\",\", null],\n[\"\u2581hor\", \"horror\"],\n[\"reur\", null],\n[\"\u2581dans\", null],\n[\"\u2581un\", null],\n[\"\u2581enfant\", \"child\"],\n[\"\u2581de\", null],\n[\"\u2581seize\", \"sixteen\"],\n[\"\u2581ans\", \"years\"],\n[\",\", null],\n[\"\u2581que\", null],\n[\"\u2581ses\", null],\n[\"\u2581d\u00e9\", \"excesses\"],\n[\"borde\", null],\n[\"ments\", null],\n[\"\u2581alla\", \"went\"],\n[\"ient\", null],\n[\"\u2581souvent\", \"often\"],\n[\"es\", null],\n[\"\u2581fois\", \"times\"],\n[\"\u2581jusqu\", \"as far as\"],\n[\"'\", null],\n[\"\u00e0\", null],\n[\"\u2581la\", null],\n[\"\u2581rue\", \"street\"],\n[\"\u2581de\", null],\n[\"\u2581G\", \"Glatigny\"],\n[\"lat\", null],\n[\"igny\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nEnfin on disait, horreur dans un enfant de seize ans, que ses d\u00e9bordements allaient souventes fois jusqu\u2019\u00e0 la rue de Glatigny.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Enfin', '\u2581on', '\u2581disait', ',', '\u2581hor', 'reur', '\u2581dans', '\u2581un', '\u2581enfant', '\u2581de', '\u2581seize', '\u2581ans', ',', '\u2581que', '\u2581ses', '\u2581d\u00e9', 'borde', 'ments', '\u2581alla', 'ient', '\u2581souvent', 'es', '\u2581fois', '\u2581jusqu', '\u2019', '\u00e0', '\u2581la', '\u2581rue', '\u2581de', '\u2581G', 'lat', 'igny', '.']"}], "[\n    [\"Enfin\", \"\u041d\u0430\u043a\u043e\u043d\u0435\u0446\"],\n    [\"\u2581on\", null],\n    [\"\u2581disait\", \"\u0433\u043e\u0432\u043e\u0440\u0438\u043b\u0438\"],\n    [\",\", null],\n    [\"\u2581hor\", \"\u0443\u0436\u0430\u0441\"],\n    [\"reur\", null],\n    [\"\u2581dans\", null],\n    [\"\u2581un\", null],\n    [\"\u2581enfant\", \"\u0440\u0435\u0431\u0435\u043d\u043e\u043a\"],\n    [\"\u2581de\", null],\n    [\"\u2581seize\", \"\u0448\u0435\u0441\u0442\u043d\u0430\u0434\u0446\u0430\u0442\u0438\"],\n    [\"\u2581ans\", \"\u043b\u0435\u0442\"],\n    [\",\", null],\n    [\"\u2581que\", null],\n    [\"\u2581ses\", null],\n    [\"\u2581d\u00e9\", \"\u0440\u0430\u0441\u043f\u0443\u0442\u0441\u0442\u0432\u043e\"],\n    [\"borde\", null],\n    [\"ments\", null],\n    [\"\u2581alla\", \"\u0434\u043e\u0445\u043e\u0434\u0438\u043b\u043e\"],\n    [\"ient\", null],\n    [\"\u2581souvent\", \"\u0447\u0430\u0441\u0442\u043e\"],\n    [\"es\", null],\n    [\"\u2581fois\", null],\n    [\"\u2581jusqu\", \"\u0434\u043e\"],\n    [\"'\", null],\n    [\"\u00e0\", null],\n    [\"\u2581la\", null],\n    [\"\u2581rue\", \"\u0443\u043b\u0438\u0446\u044b\"],\n    [\"\u2581de\", null],\n    [\"\u2581G\", \"\u0413\u043b\u0430\u0442\u0438\u043d\u044c\u0438\"],\n    [\"lat\", null],\n    [\"igny\", null],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nQui e\u00fbt pu voir en ce moment la figure du malheureux coll\u00e9e aux barreaux vermoulus e\u00fbt cru voir une face de tigre regardant du fond d\u2019une cage quelque chacal qui d\u00e9vore une gazelle.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Qui', '\u2581e\u00fbt', '\u2581pu', '\u2581voir', '\u2581en', '\u2581ce', '\u2581moment', '\u2581la', '\u2581figure', '\u2581du', '\u2581malheureux', '\u2581coll', '\u00e9e', '\u2581aux', '\u2581barre', 'aux', '\u2581verm', 'oul', 'us', '\u2581e\u00fbt', '\u2581cru', '\u2581voir', '\u2581une', '\u2581face', '\u2581de', '\u2581tigre', '\u2581regardant', '\u2581du', '\u2581fond', '\u2581d', '\u2019', 'une', '\u2581cage', '\u2581quelque', '\u2581cha', 'cal', '\u2581qui', '\u2581d\u00e9', 'vore', '\u2581une', '\u2581gazelle', '.']"}], "[\n[\"Qui\", \"Who\"],\n[\"\u2581e\u00fbt\", null],\n[\"\u2581pu\", null],\n[\"\u2581voir\", \"see\"],\n[\"\u2581en\", null],\n[\"\u2581ce\", null],\n[\"\u2581moment\", \"moment\"],\n[\"\u2581la\", null],\n[\"\u2581figure\", \"face\"],\n[\"\u2581du\", null],\n[\"\u2581malheureux\", \"unfortunate\"],\n[\"\u2581coll\", \"pressed\"],\n[\"\u00e9e\", null],\n[\"\u2581aux\", null],\n[\"\u2581barre\", \"bars\"],\n[\"aux\", null],\n[\"\u2581verm\", \"worm-eaten\"],\n[\"oul\", null],\n[\"us\", null],\n[\"\u2581e\u00fbt\", \"would have\"],\n[\"\u2581cru\", \"believed\"],\n[\"\u2581voir\", \"seeing\"],\n[\"\u2581une\", null],\n[\"\u2581face\", \"face\"],\n[\"\u2581de\", null],\n[\"\u2581tigre\", \"tiger\"],\n[\"\u2581regardant\", \"looking\"],\n[\"\u2581du\", null],\n[\"\u2581fond\", \"bottom\"],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"une\", null],\n[\"\u2581cage\", \"cage\"],\n[\"\u2581quelque\", \"some\"],\n[\"\u2581cha\", \"jackal\"],\n[\"cal\", null],\n[\"\u2581qui\", null],\n[\"\u2581d\u00e9\", \"devours\"],\n[\"vore\", null],\n[\"\u2581une\", null],\n[\"\u2581gazelle\", \"gazelle\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nQui e\u00fbt pu voir en ce moment la figure du malheureux coll\u00e9e aux barreaux vermoulus e\u00fbt cru voir une face de tigre regardant du fond d\u2019une cage quelque chacal qui d\u00e9vore une gazelle.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Qui', '\u2581e\u00fbt', '\u2581pu', '\u2581voir', '\u2581en', '\u2581ce', '\u2581moment', '\u2581la', '\u2581figure', '\u2581du', '\u2581malheureux', '\u2581coll', '\u00e9e', '\u2581aux', '\u2581barre', 'aux', '\u2581verm', 'oul', 'us', '\u2581e\u00fbt', '\u2581cru', '\u2581voir', '\u2581une', '\u2581face', '\u2581de', '\u2581tigre', '\u2581regardant', '\u2581du', '\u2581fond', '\u2581d', '\u2019', 'une', '\u2581cage', '\u2581quelque', '\u2581cha', 'cal', '\u2581qui', '\u2581d\u00e9', 'vore', '\u2581une', '\u2581gazelle', '.']"}], "[\n[\"Qui\", \"\u041a\u0442\u043e\"],\n[\"\u2581e\u00fbt\", \"\u043c\u043e\u0433\"],\n[\"\u2581pu\", null],\n[\"\u2581voir\", \"\u0443\u0432\u0438\u0434\u0435\u0442\u044c\"],\n[\"\u2581en\", null],\n[\"\u2581ce\", null],\n[\"\u2581moment\", \"\u043c\u043e\u043c\u0435\u043d\u0442\"],\n[\"\u2581la\", null],\n[\"\u2581figure\", \"\u043b\u0438\u0446\u043e\"],\n[\"\u2581du\", null],\n[\"\u2581malheureux\", \"\u043d\u0435\u0441\u0447\u0430\u0441\u0442\u043d\u043e\u0433\u043e\"],\n[\"\u2581coll\", \"\u043f\u0440\u0438\u0436\u0430\u0442\u043e\u0435\"],\n[\"\u00e9e\", null],\n[\"\u2581aux\", null],\n[\"\u2581barre\", \"\u0440\u0435\u0448\u0435\u0442\u043a\u0435\"],\n[\"aux\", null],\n[\"\u2581verm\", \"\u043f\u0440\u043e\u0433\u043d\u0438\u0432\u0448\u0435\u0439\"],\n[\"oul\", null],\n[\"us\", null],\n[\"\u2581e\u00fbt\", \"\u043c\u043e\u0433\"],\n[\"\u2581cru\", \"\u043f\u043e\u0434\u0443\u043c\u0430\u0442\u044c\"],\n[\"\u2581voir\", \"\u0447\u0442\u043e \u0432\u0438\u0434\u0438\u0442\"],\n[\"\u2581une\", null],\n[\"\u2581face\", \"\u043c\u043e\u0440\u0434\u0443\"],\n[\"\u2581de\", null],\n[\"\u2581tigre\", \"\u0442\u0438\u0433\u0440\u0430\"],\n[\"\u2581regardant\", \"\u0441\u043c\u043e\u0442\u0440\u044f\u0449\u0435\u0433\u043e\"],\n[\"\u2581du\", null],\n[\"\u2581fond\", \"\u0438\u0437 \u0433\u043b\u0443\u0431\u0438\u043d\u044b\"],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"une\", null],\n[\"\u2581cage\", \"\u043a\u043b\u0435\u0442\u043a\u0438\"],\n[\"\u2581quelque\", \"\u043d\u0430 \u043a\u0430\u043a\u043e\u0433\u043e-\u0442\u043e\"],\n[\"\u2581cha\", \"\u0448\u0430\u043a\u0430\u043b\u0430\"],\n[\"cal\", null],\n[\"\u2581qui\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u0439\"],\n[\"\u2581d\u00e9\", \"\u043f\u043e\u0436\u0438\u0440\u0430\u0435\u0442\"],\n[\"vore\", null],\n[\"\u2581une\", null],\n[\"\u2581gazelle\", \"\u0433\u0430\u0437\u0435\u043b\u044c\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nGlenarvan garda pour lui le secret de ses angoisses, et ni lady Helena, ni Mary Grant, ni Robert ne soup\u00e7onn\u00e8rent la gravit\u00e9 des p\u00e9rils auxquels ils venaient d\u2019\u00e9chapper.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Glen', 'ar', 'van', '\u2581garda', '\u2581pour', '\u2581lui', '\u2581le', '\u2581secret', '\u2581de', '\u2581ses', '\u2581ang', 'o', 'isses', ',', '\u2581et', '\u2581ni', '\u2581lady', '\u2581Helena', ',', '\u2581ni', '\u2581Mary', '\u2581Grant', ',', '\u2581ni', '\u2581Robert', '\u2581ne', '\u2581soup', '\u00e7on', 'n', '\u00e8rent', '\u2581la', '\u2581grav', 'it\u00e9', '\u2581des', '\u2581p\u00e9ri', 'ls', '\u2581auxquels', '\u2581ils', '\u2581ven', 'aient', '\u2581d', '\u2019', '\u00e9', 'chapper', '.']"}], "[\n[\"Glen\", \"Glenarvan\"],\n[\"ar\", null],\n[\"van\", null],\n[\"\u2581garda\", \"kept\"],\n[\"\u2581pour\", null],\n[\"\u2581lui\", null],\n[\"\u2581le\", null],\n[\"\u2581secret\", \"secret\"],\n[\"\u2581de\", null],\n[\"\u2581ses\", null],\n[\"\u2581ang\", \"anxieties\"],\n[\"o\", null],\n[\"isses\", null],\n[\",\", null],\n[\"\u2581et\", null],\n[\"\u2581ni\", null],\n[\"\u2581lady\", \"Lady\"],\n[\"\u2581Helena\", \"Helena\"],\n[\",\", null],\n[\"\u2581ni\", null],\n[\"\u2581Mary\", \"Mary\"],\n[\"\u2581Grant\", \"Grant\"],\n[\",\", null],\n[\"\u2581ni\", null],\n[\"\u2581Robert\", \"Robert\"],\n[\"\u2581ne\", null],\n[\"\u2581soup\", \"suspected\"],\n[\"\u00e7on\", null],\n[\"n\", null],\n[\"\u00e8rent\", null],\n[\"\u2581la\", null],\n[\"\u2581grav\", \"seriousness\"],\n[\"it\u00e9\", null],\n[\"\u2581des\", null],\n[\"\u2581p\u00e9ri\", \"dangers\"],\n[\"ls\", null],\n[\"\u2581auxquels\", \"which\"],\n[\"\u2581ils\", null],\n[\"\u2581ven\", \"had\"],\n[\"aient\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"\u00e9\", \"escaped\"],\n[\"chapper\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nGlenarvan garda pour lui le secret de ses angoisses, et ni lady Helena, ni Mary Grant, ni Robert ne soup\u00e7onn\u00e8rent la gravit\u00e9 des p\u00e9rils auxquels ils venaient d\u2019\u00e9chapper.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Glen', 'ar', 'van', '\u2581garda', '\u2581pour', '\u2581lui', '\u2581le', '\u2581secret', '\u2581de', '\u2581ses', '\u2581ang', 'o', 'isses', ',', '\u2581et', '\u2581ni', '\u2581lady', '\u2581Helena', ',', '\u2581ni', '\u2581Mary', '\u2581Grant', ',', '\u2581ni', '\u2581Robert', '\u2581ne', '\u2581soup', '\u00e7on', 'n', '\u00e8rent', '\u2581la', '\u2581grav', 'it\u00e9', '\u2581des', '\u2581p\u00e9ri', 'ls', '\u2581auxquels', '\u2581ils', '\u2581ven', 'aient', '\u2581d', '\u2019', '\u00e9', 'chapper', '.']"}], "[\n[\"Glen\", \"\u0413\u043b\u0435\u043d\u0430\u0440\u0432\u0430\u043d\"],\n[\"ar\", null],\n[\"van\", null],\n[\"\u2581garda\", \"\u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043b\"],\n[\"\u2581pour\", null],\n[\"\u2581lui\", null],\n[\"\u2581le\", null],\n[\"\u2581secret\", \"\u0442\u0430\u0439\u043d\u0443\"],\n[\"\u2581de\", null],\n[\"\u2581ses\", null],\n[\"\u2581ang\", \"\u0442\u0440\u0435\u0432\u043e\u0433\"],\n[\"o\", null],\n[\"isses\", null],\n[\",\", null],\n[\"\u2581et\", null],\n[\"\u2581ni\", null],\n[\"\u2581lady\", \"\u043b\u0435\u0434\u0438\"],\n[\"\u2581Helena\", \"\u0415\u043b\u0435\u043d\u0430\"],\n[\",\", null],\n[\"\u2581ni\", null],\n[\"\u2581Mary\", \"\u041c\u044d\u0440\u0438\"],\n[\"\u2581Grant\", \"\u0413\u0440\u0430\u043d\u0442\"],\n[\",\", null],\n[\"\u2581ni\", null],\n[\"\u2581Robert\", \"\u0420\u043e\u0431\u0435\u0440\u0442\"],\n[\"\u2581ne\", null],\n[\"\u2581soup\", \"\u043f\u043e\u0434\u043e\u0437\u0440\u0435\u0432\u0430\u043b\u0438\"],\n[\"\u00e7on\", null],\n[\"n\", null],\n[\"\u00e8rent\", null],\n[\"\u2581la\", null],\n[\"\u2581grav\", \"\u0441\u0435\u0440\u044c\u0435\u0437\u043d\u043e\u0441\u0442\u044c\"],\n[\"it\u00e9\", null],\n[\"\u2581des\", null],\n[\"\u2581p\u00e9ri\", \"\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u0435\u0439\"],\n[\"ls\", null],\n[\"\u2581auxquels\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u043c\"],\n[\"\u2581ils\", null],\n[\"\u2581ven\", \"\u0442\u043e\u043b\u044c\u043a\u043e\"],\n[\"aient\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"\u00e9\", \"\u0438\u0437\u0431\u0435\u0436\u0430\u043b\u0438\"],\n[\"chapper\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nJe commen\u00e7ai \u00e0 comprendre ce qu'elle devait \u00eatre dans un sermon que je l'entendis faire \u00e0 sa propre paroisse de Morton: il n'est pas en mon pouvoir de raconter ce sermon;\n\n\n---=== TOKENIZED TEXT ===---\n\n['Je', '\u2581commen', '\u00e7', 'ai', '\u2581\u00e0', '\u2581comprendre', '\u2581ce', '\u2581qu', \"'\", 'elle', '\u2581devait', '\u2581\u00eatre', '\u2581dans', '\u2581un', '\u2581sermon', '\u2581que', '\u2581je', '\u2581l', \"'\", 'ent', 'endis', '\u2581faire', '\u2581\u00e0', '\u2581sa', '\u2581propre', '\u2581paroisse', '\u2581de', '\u2581Morton', ':', '\u2581il', '\u2581n', \"'\", 'est', '\u2581pas', '\u2581en', '\u2581mon', '\u2581pouvoir', '\u2581de', '\u2581raconter', '\u2581ce', '\u2581sermon', ';']"}], "[\n[\"Je\", \"I\"],\n[\"\u2581commen\", \"begin\"],\n[\"\u00e7\", null],\n[\"ai\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581comprendre\", \"understand\"],\n[\"\u2581ce\", null],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"elle\", \"she\"],\n[\"\u2581devait\", \"must\"],\n[\"\u2581\u00eatre\", \"be\"],\n[\"\u2581dans\", \"in\"],\n[\"\u2581un\", null],\n[\"\u2581sermon\", \"sermon\"],\n[\"\u2581que\", \"that\"],\n[\"\u2581je\", \"I\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"ent\", \"heard\"],\n[\"endis\", null],\n[\"\u2581faire\", \"give\"],\n[\"\u2581\u00e0\", \"to\"],\n[\"\u2581sa\", \"her\"],\n[\"\u2581propre\", \"own\"],\n[\"\u2581paroisse\", \"parish\"],\n[\"\u2581de\", \"of\"],\n[\"\u2581Morton\", \"Morton\"],\n[\":\", null],\n[\"\u2581il\", \"it\"],\n[\"\u2581n\", null],\n[\"'\", null],\n[\"est\", \"is\"],\n[\"\u2581pas\", null],\n[\"\u2581en\", \"in\"],\n[\"\u2581mon\", \"my\"],\n[\"\u2581pouvoir\", \"power\"],\n[\"\u2581de\", \"to\"],\n[\"\u2581raconter\", \"recount\"],\n[\"\u2581ce\", \"this\"],\n[\"\u2581sermon\", \"sermon\"],\n[\";\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nJe commen\u00e7ai \u00e0 comprendre ce qu'elle devait \u00eatre dans un sermon que je l'entendis faire \u00e0 sa propre paroisse de Morton: il n'est pas en mon pouvoir de raconter ce sermon;\n\n\n---=== TOKENIZED TEXT ===---\n\n['Je', '\u2581commen', '\u00e7', 'ai', '\u2581\u00e0', '\u2581comprendre', '\u2581ce', '\u2581qu', \"'\", 'elle', '\u2581devait', '\u2581\u00eatre', '\u2581dans', '\u2581un', '\u2581sermon', '\u2581que', '\u2581je', '\u2581l', \"'\", 'ent', 'endis', '\u2581faire', '\u2581\u00e0', '\u2581sa', '\u2581propre', '\u2581paroisse', '\u2581de', '\u2581Morton', ':', '\u2581il', '\u2581n', \"'\", 'est', '\u2581pas', '\u2581en', '\u2581mon', '\u2581pouvoir', '\u2581de', '\u2581raconter', '\u2581ce', '\u2581sermon', ';']"}], "[\n[\"Je\", \"\u042f\"],\n[\"\u2581commen\", \"\u043d\u0430\u0447\u0438\u043d\u0430\u0442\u044c\"],\n[\"\u00e7\", null],\n[\"ai\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581comprendre\", \"\u043f\u043e\u043d\u0438\u043c\u0430\u0442\u044c\"],\n[\"\u2581ce\", null],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"elle\", \"\u043e\u043d\u0430\"],\n[\"\u2581devait\", \"\u0434\u043e\u043b\u0436\u043d\u0430 \u0431\u044b\u043b\u0430\"],\n[\"\u2581\u00eatre\", \"\u0431\u044b\u0442\u044c\"],\n[\"\u2581dans\", \"\u0432\"],\n[\"\u2581un\", null],\n[\"\u2581sermon\", \"\u043f\u0440\u043e\u043f\u043e\u0432\u0435\u0434\u044c\"],\n[\"\u2581que\", null],\n[\"\u2581je\", \"\u044f\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"ent\", \"\u0441\u043b\u044b\u0448\u0430\u0442\u044c\"],\n[\"endis\", null],\n[\"\u2581faire\", \"\u0434\u0435\u043b\u0430\u0442\u044c\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581sa\", \"\u0435\u0451\"],\n[\"\u2581propre\", \"\u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439\"],\n[\"\u2581paroisse\", \"\u043f\u0440\u0438\u0445\u043e\u0434\"],\n[\"\u2581de\", null],\n[\"\u2581Morton\", \"\u041c\u043e\u0440\u0442\u043e\u043d\"],\n[\":\", null],\n[\"\u2581il\", null],\n[\"\u2581n\", null],\n[\"'\", null],\n[\"est\", \"\u0435\u0441\u0442\u044c\"],\n[\"\u2581pas\", null],\n[\"\u2581en\", \"\u0432\"],\n[\"\u2581mon\", \"\u043c\u043e\u0435\u0439\"],\n[\"\u2581pouvoir\", \"\u0432\u043b\u0430\u0441\u0442\u0438\"],\n[\"\u2581de\", null],\n[\"\u2581raconter\", \"\u0440\u0430\u0441\u0441\u043a\u0430\u0437\u0430\u0442\u044c\"],\n[\"\u2581ce\", \"\u044d\u0442\u0443\"],\n[\"\u2581sermon\", \"\u043f\u0440\u043e\u043f\u043e\u0432\u0435\u0434\u044c\"],\n[\";\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nHuit avirons \u00e0 l\u2019avant le faisaient voler \u00e0 la surface des eaux, pendant qu\u2019un homme, assis \u00e0 l\u2019arri\u00e8re, le dirigeait au moyen d\u2019une pagaie mobile.\n\n\n---=== TOKENIZED TEXT ===---\n\n['H', 'uit', '\u2581a', 'virons', '\u2581\u00e0', '\u2581l', '\u2019', 'avant', '\u2581le', '\u2581faisaient', '\u2581voler', '\u2581\u00e0', '\u2581la', '\u2581surface', '\u2581des', '\u2581eaux', ',', '\u2581pendant', '\u2581qu', '\u2019', 'un', '\u2581homme', ',', '\u2581assis', '\u2581\u00e0', '\u2581l', '\u2019', 'arri\u00e8re', ',', '\u2581le', '\u2581dirig', 'eait', '\u2581au', '\u2581moyen', '\u2581d', '\u2019', 'une', '\u2581paga', 'ie', '\u2581mobile', '.']"}], "[\n[\"H\", null],\n[\"uit\", \"Eight\"],\n[\"\u2581a\", null],\n[\"virons\", \"oars\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"avant\", \"front\"],\n[\"\u2581le\", null],\n[\"\u2581faisaient\", \"made\"],\n[\"\u2581voler\", \"fly\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581la\", null],\n[\"\u2581surface\", \"surface\"],\n[\"\u2581des\", null],\n[\"\u2581eaux\", \"waters\"],\n[\",\", null],\n[\"\u2581pendant\", \"while\"],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"un\", \"a\"],\n[\"\u2581homme\", \"man\"],\n[\",\", null],\n[\"\u2581assis\", \"seated\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"arri\u00e8re\", \"back\"],\n[\",\", null],\n[\"\u2581le\", null],\n[\"\u2581dirig\", null],\n[\"eait\", \"steered\"],\n[\"\u2581au\", null],\n[\"\u2581moyen\", \"using\"],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"une\", \"a\"],\n[\"\u2581paga\", null],\n[\"ie\", \"paddle\"],\n[\"\u2581mobile\", \"movable\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nHuit avirons \u00e0 l\u2019avant le faisaient voler \u00e0 la surface des eaux, pendant qu\u2019un homme, assis \u00e0 l\u2019arri\u00e8re, le dirigeait au moyen d\u2019une pagaie mobile.\n\n\n---=== TOKENIZED TEXT ===---\n\n['H', 'uit', '\u2581a', 'virons', '\u2581\u00e0', '\u2581l', '\u2019', 'avant', '\u2581le', '\u2581faisaient', '\u2581voler', '\u2581\u00e0', '\u2581la', '\u2581surface', '\u2581des', '\u2581eaux', ',', '\u2581pendant', '\u2581qu', '\u2019', 'un', '\u2581homme', ',', '\u2581assis', '\u2581\u00e0', '\u2581l', '\u2019', 'arri\u00e8re', ',', '\u2581le', '\u2581dirig', 'eait', '\u2581au', '\u2581moyen', '\u2581d', '\u2019', 'une', '\u2581paga', 'ie', '\u2581mobile', '.']"}], "[\n[\"H\", \"\u0412\u043e\u0441\u0435\u043c\u044c\"],\n[\"uit\", null],\n[\"\u2581a\", \"\u0432\u0435\u0441\u043b\u0430\"],\n[\"virons\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"avant\", \"\u0441\u043f\u0435\u0440\u0435\u0434\u0438\"],\n[\"\u2581le\", null],\n[\"\u2581faisaient\", \"\u0437\u0430\u0441\u0442\u0430\u0432\u043b\u044f\u043b\u0438\"],\n[\"\u2581voler\", \"\u043b\u0435\u0442\u0435\u0442\u044c\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581la\", null],\n[\"\u2581surface\", \"\u043f\u043e\u0432\u0435\u0440\u0445\u043d\u043e\u0441\u0442\u0438\"],\n[\"\u2581des\", null],\n[\"\u2581eaux\", \"\u0432\u043e\u0434\u044b\"],\n[\",\", null],\n[\"\u2581pendant\", \"\u043f\u043e\u043a\u0430\"],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"un\", \"\u043e\u0434\u0438\u043d\"],\n[\"\u2581homme\", \"\u0447\u0435\u043b\u043e\u0432\u0435\u043a\"],\n[\",\", null],\n[\"\u2581assis\", \"\u0441\u0438\u0434\u044f\u0449\u0438\u0439\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"arri\u00e8re\", \"\u0441\u0437\u0430\u0434\u0438\"],\n[\",\", null],\n[\"\u2581le\", null],\n[\"\u2581dirig\", \"\u0443\u043f\u0440\u0430\u0432\u043b\u044f\u043b\"],\n[\"eait\", null],\n[\"\u2581au\", null],\n[\"\u2581moyen\", \"\u043f\u043e\u043c\u043e\u0449\u044c\u044e\"],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"une\", \"\u043f\u043e\u0434\u0432\u0438\u0436\u043d\u043e\u0433\u043e\"],\n[\"\u2581paga\", \"\u0432\u0435\u0441\u043b\u0430\"],\n[\"ie\", null],\n[\"\u2581mobile\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nLa r\u00e9ponse co\u00efncidait si peu avec la question, que le fou rire recommen\u00e7a \u00e0 circuler, et que messire Robert s\u2019\u00e9cria, rouge de col\u00e8re :\n\n\n---=== TOKENIZED TEXT ===---\n\n['La', '\u2581r\u00e9ponse', '\u2581co', '\u00ef', 'nc', 'id', 'ait', '\u2581si', '\u2581peu', '\u2581avec', '\u2581la', '\u2581question', ',', '\u2581que', '\u2581le', '\u2581fou', '\u2581rire', '\u2581recommen', '\u00e7a', '\u2581\u00e0', '\u2581circ', 'uler', ',', '\u2581et', '\u2581que', '\u2581messi', 're', '\u2581Robert', '\u2581s', '\u2019', '\u00e9cri', 'a', ',', '\u2581rouge', '\u2581de', '\u2581col\u00e8re', '\u2581:']"}], "[\n[\"La\", null],\n[\"\u2581r\u00e9ponse\", \"answer\"],\n[\"\u2581co\", \"coincide\"],\n[\"\u00ef\", null],\n[\"nc\", null],\n[\"id\", null],\n[\"ait\", null],\n[\"\u2581si\", \"so\"],\n[\"\u2581peu\", \"little\"],\n[\"\u2581avec\", \"with\"],\n[\"\u2581la\", null],\n[\"\u2581question\", \"question\"],\n[\",\", null],\n[\"\u2581que\", \"that\"],\n[\"\u2581le\", null],\n[\"\u2581fou\", \"crazy\"],\n[\"\u2581rire\", \"laughter\"],\n[\"\u2581recommen\", \"start\"],\n[\"\u00e7a\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581circ\", \"circulate\"],\n[\"uler\", null],\n[\",\", null],\n[\"\u2581et\", \"and\"],\n[\"\u2581que\", \"that\"],\n[\"\u2581messi\", \"sir\"],\n[\"re\", null],\n[\"\u2581Robert\", \"Robert\"],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"\u00e9cri\", \"exclaimed\"],\n[\"a\", null],\n[\",\", null],\n[\"\u2581rouge\", \"red\"],\n[\"\u2581de\", \"with\"],\n[\"\u2581col\u00e8re\", \"anger\"],\n[\"\u2581:\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nLa r\u00e9ponse co\u00efncidait si peu avec la question, que le fou rire recommen\u00e7a \u00e0 circuler, et que messire Robert s\u2019\u00e9cria, rouge de col\u00e8re :\n\n\n---=== TOKENIZED TEXT ===---\n\n['La', '\u2581r\u00e9ponse', '\u2581co', '\u00ef', 'nc', 'id', 'ait', '\u2581si', '\u2581peu', '\u2581avec', '\u2581la', '\u2581question', ',', '\u2581que', '\u2581le', '\u2581fou', '\u2581rire', '\u2581recommen', '\u00e7a', '\u2581\u00e0', '\u2581circ', 'uler', ',', '\u2581et', '\u2581que', '\u2581messi', 're', '\u2581Robert', '\u2581s', '\u2019', '\u00e9cri', 'a', ',', '\u2581rouge', '\u2581de', '\u2581col\u00e8re', '\u2581:']"}], "[\n[\"La\", null],\n[\"\u2581r\u00e9ponse\", \"\u041e\u0442\u0432\u0435\u0442\"],\n[\"\u2581co\", \"\u0441\u043e\u0432\u043f\u0430\u0434\u0430\u043b\"],\n[\"\u00ef\", null],\n[\"nc\", null],\n[\"id\", null],\n[\"ait\", null],\n[\"\u2581si\", \"\u0442\u0430\u043a\"],\n[\"\u2581peu\", \"\u043c\u0430\u043b\u043e\"],\n[\"\u2581avec\", \"\u0441\"],\n[\"\u2581la\", null],\n[\"\u2581question\", \"\u0432\u043e\u043f\u0440\u043e\u0441\u043e\u043c\"],\n[\",\", null],\n[\"\u2581que\", \"\u0447\u0442\u043e\"],\n[\"\u2581le\", null],\n[\"\u2581fou\", \"\u0431\u0435\u0437\u0443\u043c\u043d\u044b\u0439\"],\n[\"\u2581rire\", \"\u0441\u043c\u0435\u0445\"],\n[\"\u2581recommen\", \"\u0441\u043d\u043e\u0432\u0430\"],\n[\"\u00e7a\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581circ\", \"\u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u044f\u0442\u044c\u0441\u044f\"],\n[\"uler\", null],\n[\",\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581que\", null],\n[\"\u2581messi\", \"\u0433\u043e\u0441\u043f\u043e\u0434\u0438\u043d\"],\n[\"re\", null],\n[\"\u2581Robert\", \"\u0420\u043e\u0431\u0435\u0440\u0442\"],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"\u00e9cri\", \"\u0432\u043e\u0441\u043a\u043b\u0438\u043a\u043d\u0443\u043b\"],\n[\"a\", null],\n[\",\", null],\n[\"\u2581rouge\", \"\u043a\u0440\u0430\u0441\u043d\u044b\u0439\"],\n[\"\u2581de\", \"\u043e\u0442\"],\n[\"\u2581col\u00e8re\", \"\u0433\u043d\u0435\u0432\u0430\"],\n[\"\u2581:\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nEn 1697, trois b\u00e2timents hollandais de la compagnie des Indes y rel\u00e2ch\u00e8rent, et en d\u00e9termin\u00e8rent les coordonn\u00e9es, laissant au grand astronome Halley le soin de revoir leurs calculs en l\u2019an 1700.\n\n\n---=== TOKENIZED TEXT ===---\n\n['En', '\u2581', '1', '6', '9', '7', ',', '\u2581trois', '\u2581b\u00e2timents', '\u2581holl', 'anda', 'is', '\u2581de', '\u2581la', '\u2581compagnie', '\u2581des', '\u2581Indes', '\u2581y', '\u2581rel', '\u00e2', 'ch\u00e8', 'rent', ',', '\u2581et', '\u2581en', '\u2581d\u00e9termin', '\u00e8rent', '\u2581les', '\u2581coordonn\u00e9es', ',', '\u2581laissant', '\u2581au', '\u2581grand', '\u2581astron', 'ome', '\u2581Halley', '\u2581le', '\u2581soin', '\u2581de', '\u2581revoir', '\u2581leurs', '\u2581calcul', 's', '\u2581en', '\u2581l', '\u2019', 'an', '\u2581', '1', '7', '0', '0', '.']"}], "[\n[\"En\", \"In\"],\n[\"\u2581\", null],\n[\"1\", null],\n[\"6\", null],\n[\"9\", null],\n[\"7\", null],\n[\",\", null],\n[\"\u2581trois\", \"three\"],\n[\"\u2581b\u00e2timents\", \"ships\"],\n[\"\u2581holl\", \"Dutch\"],\n[\"anda\", null],\n[\"is\", null],\n[\"\u2581de\", null],\n[\"\u2581la\", null],\n[\"\u2581compagnie\", \"company\"],\n[\"\u2581des\", null],\n[\"\u2581Indes\", \"Indies\"],\n[\"\u2581y\", null],\n[\"\u2581rel\", \"stopped\"],\n[\"\u00e2\", null],\n[\"ch\u00e8\", null],\n[\"rent\", null],\n[\",\", null],\n[\"\u2581et\", \"and\"],\n[\"\u2581en\", null],\n[\"\u2581d\u00e9termin\", \"determined\"],\n[\"\u00e8rent\", null],\n[\"\u2581les\", null],\n[\"\u2581coordonn\u00e9es\", \"coordinates\"],\n[\",\", null],\n[\"\u2581laissant\", \"leaving\"],\n[\"\u2581au\", null],\n[\"\u2581grand\", \"great\"],\n[\"\u2581astron\", \"astronomer\"],\n[\"ome\", null],\n[\"\u2581Halley\", \"Halley\"],\n[\"\u2581le\", null],\n[\"\u2581soin\", \"task\"],\n[\"\u2581de\", null],\n[\"\u2581revoir\", \"review\"],\n[\"\u2581leurs\", \"their\"],\n[\"\u2581calcul\", \"calculations\"],\n[\"s\", null],\n[\"\u2581en\", \"in\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"an\", \"year\"],\n[\"\u2581\", null],\n[\"1\", null],\n[\"7\", null],\n[\"0\", null],\n[\"0\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nEn 1697, trois b\u00e2timents hollandais de la compagnie des Indes y rel\u00e2ch\u00e8rent, et en d\u00e9termin\u00e8rent les coordonn\u00e9es, laissant au grand astronome Halley le soin de revoir leurs calculs en l\u2019an 1700.\n\n\n---=== TOKENIZED TEXT ===---\n\n['En', '\u2581', '1', '6', '9', '7', ',', '\u2581trois', '\u2581b\u00e2timents', '\u2581holl', 'anda', 'is', '\u2581de', '\u2581la', '\u2581compagnie', '\u2581des', '\u2581Indes', '\u2581y', '\u2581rel', '\u00e2', 'ch\u00e8', 'rent', ',', '\u2581et', '\u2581en', '\u2581d\u00e9termin', '\u00e8rent', '\u2581les', '\u2581coordonn\u00e9es', ',', '\u2581laissant', '\u2581au', '\u2581grand', '\u2581astron', 'ome', '\u2581Halley', '\u2581le', '\u2581soin', '\u2581de', '\u2581revoir', '\u2581leurs', '\u2581calcul', 's', '\u2581en', '\u2581l', '\u2019', 'an', '\u2581', '1', '7', '0', '0', '.']"}], "[\n[\"En\", \"\u0412\"],\n[\"\u2581\", null],\n[\"1\", null],\n[\"6\", null],\n[\"9\", null],\n[\"7\", null],\n[\",\", null],\n[\"\u2581trois\", \"\u0442\u0440\u0438\"],\n[\"\u2581b\u00e2timents\", \"\u043a\u043e\u0440\u0430\u0431\u043b\u044f\"],\n[\"\u2581holl\", \"\u0433\u043e\u043b\u043b\u0430\u043d\u0434\u0441\u043a\u043e\u0439\"],\n[\"anda\", null],\n[\"is\", null],\n[\"\u2581de\", null],\n[\"\u2581la\", null],\n[\"\u2581compagnie\", \"\u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438\"],\n[\"\u2581des\", null],\n[\"\u2581Indes\", \"\u041e\u0441\u0442-\u0418\u043d\u0434\u0441\u043a\u043e\u0439\"],\n[\"\u2581y\", \"\u0442\u0430\u043c\"],\n[\"\u2581rel\", \"\u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043b\u0438\u0441\u044c\"],\n[\"\u00e2\", null],\n[\"ch\u00e8\", null],\n[\"rent\", null],\n[\",\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581en\", null],\n[\"\u2581d\u00e9termin\", \"\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043b\u0438\"],\n[\"\u00e8rent\", null],\n[\"\u2581les\", null],\n[\"\u2581coordonn\u00e9es\", \"\u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u044b\"],\n[\",\", null],\n[\"\u2581laissant\", \"\u043e\u0441\u0442\u0430\u0432\u0438\u0432\"],\n[\"\u2581au\", null],\n[\"\u2581grand\", \"\u0432\u0435\u043b\u0438\u043a\u043e\u043c\u0443\"],\n[\"\u2581astron\", \"\u0430\u0441\u0442\u0440\u043e\u043d\u043e\u043c\u0443\"],\n[\"ome\", null],\n[\"\u2581Halley\", \"\u0413\u0430\u043b\u043b\u0435\u044e\"],\n[\"\u2581le\", null],\n[\"\u2581soin\", \"\u0437\u0430\u0434\u0430\u0447\u0443\"],\n[\"\u2581de\", null],\n[\"\u2581revoir\", \"\u043f\u0435\u0440\u0435\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c\"],\n[\"\u2581leurs\", \"\u0438\u0445\"],\n[\"\u2581calcul\", \"\u0440\u0430\u0441\u0447\u0435\u0442\u044b\"],\n[\"s\", null],\n[\"\u2581en\", \"\u0432\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"an\", \"\u0433\u043e\u0434\u0443\"],\n[\"\u2581\", null],\n[\"1\", null],\n[\"7\", null],\n[\"0\", null],\n[\"0\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nIl a l'intention, je crois, de la faire \u00e9lever dans le comt\u00e9 de...\n\n\n---=== TOKENIZED TEXT ===---\n\n['Il', '\u2581a', '\u2581l', \"'\", 'intention', ',', '\u2581je', '\u2581crois', ',', '\u2581de', '\u2581la', '\u2581faire', '\u2581\u00e9', 'lever', '\u2581dans', '\u2581le', '\u2581comt\u00e9', '\u2581de', '...']"}], "[\n  [\"Il\", \"He\"],\n  [\"\u2581a\", null],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"intention\", \"intention\"],\n  [\",\", null],\n  [\"\u2581je\", null],\n  [\"\u2581crois\", \"believe\"],\n  [\",\", null],\n  [\"\u2581de\", null],\n  [\"\u2581la\", null],\n  [\"\u2581faire\", \"have\"],\n  [\"\u2581\u00e9\", null],\n  [\"lever\", \"raised\"],\n  [\"\u2581dans\", null],\n  [\"\u2581le\", null],\n  [\"\u2581comt\u00e9\", \"county\"],\n  [\"\u2581de\", null],\n  [\"...\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nIl a l'intention, je crois, de la faire \u00e9lever dans le comt\u00e9 de...\n\n\n---=== TOKENIZED TEXT ===---\n\n['Il', '\u2581a', '\u2581l', \"'\", 'intention', ',', '\u2581je', '\u2581crois', ',', '\u2581de', '\u2581la', '\u2581faire', '\u2581\u00e9', 'lever', '\u2581dans', '\u2581le', '\u2581comt\u00e9', '\u2581de', '...']"}], "[\n  [\"Il\", \"\u041e\u043d\"],\n  [\"\u2581a\", null],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"intention\", \"\u043d\u0430\u043c\u0435\u0440\u0435\u043d\u0438\u0435\"],\n  [\",\", null],\n  [\"\u2581je\", null],\n  [\"\u2581crois\", \"\u0434\u0443\u043c\u0430\u044e\"],\n  [\",\", null],\n  [\"\u2581de\", null],\n  [\"\u2581la\", null],\n  [\"\u2581faire\", \"\u0441\u0434\u0435\u043b\u0430\u0442\u044c\"],\n  [\"\u2581\u00e9\", null],\n  [\"lever\", \"\u0432\u043e\u0441\u043f\u0438\u0442\u0430\u0442\u044c\"],\n  [\"\u2581dans\", null],\n  [\"\u2581le\", null],\n  [\"\u2581comt\u00e9\", \"\u0433\u0440\u0430\u0444\u0441\u0442\u0432\u0435\"],\n  [\"\u2581de\", null],\n  [\"...\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nIl la pla\u00e7a dans l'esp\u00e8ce de charrette qui devait nous conduire; je montai ensuite, et, avant qu'il referm\u00e2t la porti\u00e8re, je lui demandai \u00e0 quelle distance nous \u00e9tions de Thornfield.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Il', '\u2581la', '\u2581pla', '\u00e7a', '\u2581dans', '\u2581l', \"'\", 'esp\u00e8ce', '\u2581de', '\u2581char', 'rette', '\u2581qui', '\u2581devait', '\u2581nous', '\u2581conduire', ';', '\u2581je', '\u2581mon', 'tai', '\u2581ensuite', ',', '\u2581et', ',', '\u2581avant', '\u2581qu', \"'\", 'il', '\u2581refer', 'm', '\u00e2t', '\u2581la', '\u2581por', 'ti\u00e8re', ',', '\u2581je', '\u2581lui', '\u2581demand', 'ai', '\u2581\u00e0', '\u2581quelle', '\u2581distance', '\u2581nous', '\u2581\u00e9tions', '\u2581de', '\u2581Thorn', 'field', '.']"}], "[\n[\"Il\", \"He\"],\n[\"\u2581la\", null],\n[\"\u2581pla\", \"placed\"],\n[\"\u00e7a\", null],\n[\"\u2581dans\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"esp\u00e8ce\", \"kind\"],\n[\"\u2581de\", null],\n[\"\u2581char\", \"cart\"],\n[\"rette\", null],\n[\"\u2581qui\", null],\n[\"\u2581devait\", \"was supposed to\"],\n[\"\u2581nous\", null],\n[\"\u2581conduire\", \"take us\"],\n[\";\", null],\n[\"\u2581je\", null],\n[\"\u2581mon\", \"climbed\"],\n[\"tai\", null],\n[\"\u2581ensuite\", \"then\"],\n[\",\", null],\n[\"\u2581et\", null],\n[\",\", null],\n[\"\u2581avant\", \"before\"],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"il\", null],\n[\"\u2581refer\", \"closed\"],\n[\"m\", null],\n[\"\u00e2t\", null],\n[\"\u2581la\", null],\n[\"\u2581por\", \"door\"],\n[\"ti\u00e8re\", null],\n[\",\", null],\n[\"\u2581je\", null],\n[\"\u2581lui\", null],\n[\"\u2581demand\", \"asked\"],\n[\"ai\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581quelle\", \"what\"],\n[\"\u2581distance\", \"distance\"],\n[\"\u2581nous\", null],\n[\"\u2581\u00e9tions\", \"were\"],\n[\"\u2581de\", null],\n[\"\u2581Thorn\", \"Thornfield\"],\n[\"field\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nIl la pla\u00e7a dans l'esp\u00e8ce de charrette qui devait nous conduire; je montai ensuite, et, avant qu'il referm\u00e2t la porti\u00e8re, je lui demandai \u00e0 quelle distance nous \u00e9tions de Thornfield.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Il', '\u2581la', '\u2581pla', '\u00e7a', '\u2581dans', '\u2581l', \"'\", 'esp\u00e8ce', '\u2581de', '\u2581char', 'rette', '\u2581qui', '\u2581devait', '\u2581nous', '\u2581conduire', ';', '\u2581je', '\u2581mon', 'tai', '\u2581ensuite', ',', '\u2581et', ',', '\u2581avant', '\u2581qu', \"'\", 'il', '\u2581refer', 'm', '\u00e2t', '\u2581la', '\u2581por', 'ti\u00e8re', ',', '\u2581je', '\u2581lui', '\u2581demand', 'ai', '\u2581\u00e0', '\u2581quelle', '\u2581distance', '\u2581nous', '\u2581\u00e9tions', '\u2581de', '\u2581Thorn', 'field', '.']"}], "[\n[\"Il\", \"\u041e\u043d\"],\n[\"\u2581la\", null],\n[\"\u2581pla\", \"\u043f\u043e\u043c\u0435\u0441\u0442\u0438\u043b\"],\n[\"\u00e7a\", null],\n[\"\u2581dans\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"esp\u00e8ce\", \"\u043d\u0435\u0447\u0442\u043e\"],\n[\"\u2581de\", null],\n[\"\u2581char\", \"\u043f\u043e\u0432\u043e\u0437\u043a\u0430\"],\n[\"rette\", null],\n[\"\u2581qui\", null],\n[\"\u2581devait\", \"\u0434\u043e\u043b\u0436\u043d\u0430\"],\n[\"\u2581nous\", null],\n[\"\u2581conduire\", \"\u0432\u0435\u0437\u0442\u0438\"],\n[\";\", null],\n[\"\u2581je\", null],\n[\"\u2581mon\", \"\u043f\u043e\u0434\u043d\u044f\u043b\u0441\u044f\"],\n[\"tai\", null],\n[\"\u2581ensuite\", \"\u0437\u0430\u0442\u0435\u043c\"],\n[\",\", null],\n[\"\u2581et\", null],\n[\",\", null],\n[\"\u2581avant\", \"\u043f\u0440\u0435\u0436\u0434\u0435\"],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"il\", null],\n[\"\u2581refer\", \"\u0437\u0430\u043a\u0440\u044b\u043b\"],\n[\"m\", null],\n[\"\u00e2t\", null],\n[\"\u2581la\", null],\n[\"\u2581por\", \"\u0434\u0432\u0435\u0440\u0446\u0443\"],\n[\"ti\u00e8re\", null],\n[\",\", null],\n[\"\u2581je\", null],\n[\"\u2581lui\", null],\n[\"\u2581demand\", \"\u0441\u043f\u0440\u043e\u0441\u0438\u043b\"],\n[\"ai\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581quelle\", \"\u043a\u0430\u043a\u043e\u0435\"],\n[\"\u2581distance\", \"\u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0435\"],\n[\"\u2581nous\", null],\n[\"\u2581\u00e9tions\", \"\u043d\u0430\u0445\u043e\u0434\u0438\u043c\u0441\u044f\"],\n[\"\u2581de\", null],\n[\"\u2581Thorn\", \"\u0422\u043e\u0440\u043d\u0444\u0438\u043b\u0434\u0430\"],\n[\"field\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nTous ces animaux, achet\u00e9s maigres dans les plaines des montagnes Bleues, allaient s\u2019engraisser au milieu des p\u00e2turages salutaires de l\u2019Australie m\u00e9ridionale, o\u00f9 ils sont revendus avec grand b\u00e9n\u00e9fice. Ainsi, Sam Machell, gagnant deux livres par boeuf et une demi-livre par mouton, devait r\u00e9aliser un b\u00e9n\u00e9fice de cinquante mille francs.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Tous', '\u2581ces', '\u2581animaux', ',', '\u2581achet', '\u00e9s', '\u2581maig', 'res', '\u2581dans', '\u2581les', '\u2581pla', 'ines', '\u2581des', '\u2581montagnes', '\u2581Ble', 'ues', ',', '\u2581alla', 'ient', '\u2581s', '\u2019', 'engra', 'isser', '\u2581au', '\u2581milieu', '\u2581des', '\u2581p\u00e2', 'tura', 'ges', '\u2581sal', 'uta', 'ires', '\u2581de', '\u2581l', '\u2019', 'Australie', '\u2581m\u00e9ri', 'dion', 'ale', ',', '\u2581o\u00f9', '\u2581ils', '\u2581sont', '\u2581revend', 'us', '\u2581avec', '\u2581grand', '\u2581b\u00e9n\u00e9fice', '.', '\u2581Ainsi', ',', '\u2581Sam', '\u2581Mache', 'll', ',', '\u2581gag', 'nant', '\u2581deux', '\u2581livres', '\u2581par', '\u2581boe', 'uf', '\u2581et', '\u2581une', '\u2581demi', '-', 'livre', '\u2581par', '\u2581m', 'outon', ',', '\u2581devait', '\u2581r\u00e9aliser', '\u2581un', '\u2581b\u00e9n\u00e9fice', '\u2581de', '\u2581cinquante', '\u2581mille', '\u2581francs', '.']"}], "[\n[\"Tous\", \"All\"],\n[\"\u2581ces\", null],\n[\"\u2581animaux\", \"animals\"],\n[\",\", null],\n[\"\u2581achet\", \"bought\"],\n[\"\u00e9s\", null],\n[\"\u2581maig\", \"thin\"],\n[\"res\", null],\n[\"\u2581dans\", null],\n[\"\u2581les\", null],\n[\"\u2581pla\", \"plains\"],\n[\"ines\", null],\n[\"\u2581des\", null],\n[\"\u2581montagnes\", \"mountains\"],\n[\"\u2581Ble\", \"Blue\"],\n[\"ues\", null],\n[\",\", null],\n[\"\u2581alla\", \"were going\"],\n[\"ient\", null],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"engra\", \"fatten\"],\n[\"isser\", null],\n[\"\u2581au\", null],\n[\"\u2581milieu\", \"midst\"],\n[\"\u2581des\", null],\n[\"\u2581p\u00e2\", \"pastures\"],\n[\"tura\", null],\n[\"ges\", null],\n[\"\u2581sal\", \"healthy\"],\n[\"uta\", null],\n[\"ires\", null],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"Australie\", \"Australia\"],\n[\"\u2581m\u00e9ri\", \"southern\"],\n[\"dion\", null],\n[\"ale\", null],\n[\",\", null],\n[\"\u2581o\u00f9\", \"where\"],\n[\"\u2581ils\", null],\n[\"\u2581sont\", null],\n[\"\u2581revend\", \"resold\"],\n[\"us\", null],\n[\"\u2581avec\", null],\n[\"\u2581grand\", \"great\"],\n[\"\u2581b\u00e9n\u00e9fice\", \"profit\"],\n[\".\", null],\n[\"\u2581Ainsi\", \"Thus\"],\n[\",\", null],\n[\"\u2581Sam\", \"Sam\"],\n[\"\u2581Mache\", \"Machell\"],\n[\"ll\", null],\n[\",\", null],\n[\"\u2581gag\", \"earning\"],\n[\"nant\", null],\n[\"\u2581deux\", \"two\"],\n[\"\u2581livres\", \"pounds\"],\n[\"\u2581par\", null],\n[\"\u2581boe\", \"ox\"],\n[\"uf\", null],\n[\"\u2581et\", null],\n[\"\u2581une\", null],\n[\"\u2581demi\", \"half\"],\n[\"-\", null],\n[\"livre\", \"pound\"],\n[\"\u2581par\", null],\n[\"\u2581m\", \"sheep\"],\n[\"outon\", null],\n[\",\", null],\n[\"\u2581devait\", \"was to\"],\n[\"\u2581r\u00e9aliser\", \"make\"],\n[\"\u2581un\", null],\n[\"\u2581b\u00e9n\u00e9fice\", \"profit\"],\n[\"\u2581de\", null],\n[\"\u2581cinquante\", \"fifty\"],\n[\"\u2581mille\", \"thousand\"],\n[\"\u2581francs\", \"francs\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nTous ces animaux, achet\u00e9s maigres dans les plaines des montagnes Bleues, allaient s\u2019engraisser au milieu des p\u00e2turages salutaires de l\u2019Australie m\u00e9ridionale, o\u00f9 ils sont revendus avec grand b\u00e9n\u00e9fice. Ainsi, Sam Machell, gagnant deux livres par boeuf et une demi-livre par mouton, devait r\u00e9aliser un b\u00e9n\u00e9fice de cinquante mille francs.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Tous', '\u2581ces', '\u2581animaux', ',', '\u2581achet', '\u00e9s', '\u2581maig', 'res', '\u2581dans', '\u2581les', '\u2581pla', 'ines', '\u2581des', '\u2581montagnes', '\u2581Ble', 'ues', ',', '\u2581alla', 'ient', '\u2581s', '\u2019', 'engra', 'isser', '\u2581au', '\u2581milieu', '\u2581des', '\u2581p\u00e2', 'tura', 'ges', '\u2581sal', 'uta', 'ires', '\u2581de', '\u2581l', '\u2019', 'Australie', '\u2581m\u00e9ri', 'dion', 'ale', ',', '\u2581o\u00f9', '\u2581ils', '\u2581sont', '\u2581revend', 'us', '\u2581avec', '\u2581grand', '\u2581b\u00e9n\u00e9fice', '.', '\u2581Ainsi', ',', '\u2581Sam', '\u2581Mache', 'll', ',', '\u2581gag', 'nant', '\u2581deux', '\u2581livres', '\u2581par', '\u2581boe', 'uf', '\u2581et', '\u2581une', '\u2581demi', '-', 'livre', '\u2581par', '\u2581m', 'outon', ',', '\u2581devait', '\u2581r\u00e9aliser', '\u2581un', '\u2581b\u00e9n\u00e9fice', '\u2581de', '\u2581cinquante', '\u2581mille', '\u2581francs', '.']"}], "[\n[\"Tous\", \"\u0412\u0441\u0435\"],\n[\"\u2581ces\", null],\n[\"\u2581animaux\", \"\u0436\u0438\u0432\u043e\u0442\u043d\u044b\u0435\"],\n[\",\", null],\n[\"\u2581achet\", \"\u043a\u0443\u043f\u043b\u0435\u043d\u043d\u044b\u0435\"],\n[\"\u00e9s\", null],\n[\"\u2581maig\", \"\u0445\u0443\u0434\u044b\u0435\"],\n[\"res\", null],\n[\"\u2581dans\", null],\n[\"\u2581les\", null],\n[\"\u2581pla\", \"\u0440\u0430\u0432\u043d\u0438\u043d\u044b\"],\n[\"ines\", null],\n[\"\u2581des\", null],\n[\"\u2581montagnes\", \"\u0433\u043e\u0440\"],\n[\"\u2581Ble\", \"\u0413\u043e\u043b\u0443\u0431\u044b\u0445\"],\n[\"ues\", null],\n[\",\", null],\n[\"\u2581alla\", \"\u0441\u043e\u0431\u0438\u0440\u0430\u043b\u0438\u0441\u044c\"],\n[\"ient\", null],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"engra\", \"\u043e\u0442\u043a\u0430\u0440\u043c\u043b\u0438\u0432\u0430\u0442\u044c\u0441\u044f\"],\n[\"isser\", null],\n[\"\u2581au\", null],\n[\"\u2581milieu\", \"\u043f\u043e\u0441\u0440\u0435\u0434\u0438\"],\n[\"\u2581des\", null],\n[\"\u2581p\u00e2\", \"\u043f\u0430\u0441\u0442\u0431\u0438\u0449\"],\n[\"tura\", null],\n[\"ges\", null],\n[\"\u2581sal\", \"\u0431\u043b\u0430\u0433\u043e\u043f\u0440\u0438\u044f\u0442\u043d\u044b\u0445\"],\n[\"uta\", null],\n[\"ires\", null],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"Australie\", \"\u042e\u0436\u043d\u043e\u0439\"],\n[\"\u2581m\u00e9ri\", \"\u0410\u0432\u0441\u0442\u0440\u0430\u043b\u0438\u0438\"],\n[\"dion\", null],\n[\"ale\", null],\n[\",\", null],\n[\"\u2581o\u00f9\", \"\u0433\u0434\u0435\"],\n[\"\u2581ils\", null],\n[\"\u2581sont\", null],\n[\"\u2581revend\", \"\u043f\u0435\u0440\u0435\u043f\u0440\u043e\u0434\u0430\u0432\u0430\u043b\u0438\u0441\u044c\"],\n[\"us\", null],\n[\"\u2581avec\", null],\n[\"\u2581grand\", \"\u0431\u043e\u043b\u044c\u0448\u043e\u0439\"],\n[\"\u2581b\u00e9n\u00e9fice\", \"\u043f\u0440\u0438\u0431\u044b\u043b\u044c\u044e\"],\n[\".\", null],\n[\"\u2581Ainsi\", \"\u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c\"],\n[\",\", null],\n[\"\u2581Sam\", \"\u0421\u044d\u043c\"],\n[\"\u2581Mache\", \"\u041c\u044d\u0447\u0435\u043b\u043b\"],\n[\"ll\", null],\n[\",\", null],\n[\"\u2581gag\", \"\u0437\u0430\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u044f\"],\n[\"nant\", null],\n[\"\u2581deux\", \"\u0434\u0432\u0430\"],\n[\"\u2581livres\", \"\u0444\u0443\u043d\u0442\u0430\"],\n[\"\u2581par\", null],\n[\"\u2581boe\", \"\u0431\u044b\u043a\u0430\"],\n[\"uf\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581une\", null],\n[\"\u2581demi\", \"\u043f\u043e\u043b\u0444\u0443\u043d\u0442\u0430\"],\n[\"-\", null],\n[\"livre\", null],\n[\"\u2581par\", null],\n[\"\u2581m\", \"\u043e\u0432\u0446\u0443\"],\n[\"outon\", null],\n[\",\", null],\n[\"\u2581devait\", \"\u0434\u043e\u043b\u0436\u0435\u043d \u0431\u044b\u043b\"],\n[\"\u2581r\u00e9aliser\", \"\u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c\"],\n[\"\u2581un\", null],\n[\"\u2581b\u00e9n\u00e9fice\", \"\u043f\u0440\u0438\u0431\u044b\u043b\u044c\"],\n[\"\u2581de\", null],\n[\"\u2581cinquante\", \"\u043f\u044f\u0442\u044c\u0434\u0435\u0441\u044f\u0442\"],\n[\"\u2581mille\", \"\u0442\u044b\u0441\u044f\u0447\"],\n[\"\u2581francs\", \"\u0444\u0440\u0430\u043d\u043a\u043e\u0432\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nGr\u00e8ve, aboye, Gr\u00e8ve, grouille !File, file, ma quenouille,File sa corde au bourreau Qui siffle dans le pr\u00e9au.Gr\u00e8ve, aboye, Gr\u00e8ve, grouille !\n\n\n---=== TOKENIZED TEXT ===---\n\n['Gr', '\u00e8ve', ',', '\u2581ab', 'oye', ',', '\u2581Gr', '\u00e8ve', ',', '\u2581grou', 'ille', '\u2581!', 'File', ',', '\u2581file', ',', '\u2581ma', '\u2581quen', 'ouille', ',', 'File', '\u2581sa', '\u2581corde', '\u2581au', '\u2581bour', 'reau', '\u2581Qui', '\u2581siff', 'le', '\u2581dans', '\u2581le', '\u2581pr\u00e9', 'au', '.', 'Gr', '\u00e8ve', ',', '\u2581ab', 'oye', ',', '\u2581Gr', '\u00e8ve', ',', '\u2581grou', 'ille', '\u2581!']"}], "[\n  [\"Gr\", \"Strike\"],\n  [\"\u00e8ve\", null],\n  [\",\", null],\n  [\"\u2581ab\", \"bark\"],\n  [\"oye\", null],\n  [\",\", null],\n  [\"\u2581Gr\", \"Strike\"],\n  [\"\u00e8ve\", null],\n  [\",\", null],\n  [\"\u2581grou\", \"crawl\"],\n  [\"ille\", null],\n  [\"\u2581!\", null],\n  [\"File\", \"Spin\"],\n  [\",\", null],\n  [\"\u2581file\", \"spin\"],\n  [\",\", null],\n  [\"\u2581ma\", null],\n  [\"\u2581quen\", \"distaff\"],\n  [\"ouille\", null],\n  [\",\", null],\n  [\"File\", \"Spin\"],\n  [\"\u2581sa\", null],\n  [\"\u2581corde\", \"rope\"],\n  [\"\u2581au\", null],\n  [\"\u2581bour\", \"executioner\"],\n  [\"reau\", null],\n  [\"\u2581Qui\", \"Who\"],\n  [\"\u2581siff\", \"whistles\"],\n  [\"le\", null],\n  [\"\u2581dans\", null],\n  [\"\u2581le\", null],\n  [\"\u2581pr\u00e9\", \"courtyard\"],\n  [\"au\", null],\n  [\".\", null],\n  [\"Gr\", \"Strike\"],\n  [\"\u00e8ve\", null],\n  [\",\", null],\n  [\"\u2581ab\", \"bark\"],\n  [\"oye\", null],\n  [\",\", null],\n  [\"\u2581Gr\", \"Strike\"],\n  [\"\u00e8ve\", null],\n  [\",\", null],\n  [\"\u2581grou\", \"crawl\"],\n  [\"ille\", null],\n  [\"\u2581!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nGr\u00e8ve, aboye, Gr\u00e8ve, grouille !File, file, ma quenouille,File sa corde au bourreau Qui siffle dans le pr\u00e9au.Gr\u00e8ve, aboye, Gr\u00e8ve, grouille !\n\n\n---=== TOKENIZED TEXT ===---\n\n['Gr', '\u00e8ve', ',', '\u2581ab', 'oye', ',', '\u2581Gr', '\u00e8ve', ',', '\u2581grou', 'ille', '\u2581!', 'File', ',', '\u2581file', ',', '\u2581ma', '\u2581quen', 'ouille', ',', 'File', '\u2581sa', '\u2581corde', '\u2581au', '\u2581bour', 'reau', '\u2581Qui', '\u2581siff', 'le', '\u2581dans', '\u2581le', '\u2581pr\u00e9', 'au', '.', 'Gr', '\u00e8ve', ',', '\u2581ab', 'oye', ',', '\u2581Gr', '\u00e8ve', ',', '\u2581grou', 'ille', '\u2581!']"}], "[\n  [\"Gr\", \"\u0411\u0430\u0441\u0442\u0443\u0439\"],\n  [\"\u00e8ve\", null],\n  [\",\", null],\n  [\"\u2581ab\", \"\u043b\u0430\u0439\"],\n  [\"oye\", null],\n  [\",\", null],\n  [\"\u2581Gr\", \"\u0411\u0430\u0441\u0442\u0443\u0439\"],\n  [\"\u00e8ve\", null],\n  [\",\", null],\n  [\"\u2581grou\", \"\u0448\u0435\u0432\u0435\u043b\u0438\u0441\u044c\"],\n  [\"ille\", null],\n  [\"\u2581!\", null],\n  [\"File\", \"\u041f\u0440\u044f\u0434\u0438\u0441\u044c\"],\n  [\",\", null],\n  [\"\u2581file\", \"\u043f\u0440\u044f\u0434\u0438\u0441\u044c\"],\n  [\",\", null],\n  [\"\u2581ma\", null],\n  [\"\u2581quen\", \"\u043f\u0440\u044f\u043b\u043a\u0430\"],\n  [\"ouille\", null],\n  [\",\", null],\n  [\"File\", \"\u041f\u0440\u044f\u0434\u0438\"],\n  [\"\u2581sa\", null],\n  [\"\u2581corde\", \"\u0432\u0435\u0440\u0435\u0432\u043a\u0443\"],\n  [\"\u2581au\", null],\n  [\"\u2581bour\", \"\u043f\u0430\u043b\u0430\u0447\u0443\"],\n  [\"reau\", null],\n  [\"\u2581Qui\", \"\u041a\u043e\u0442\u043e\u0440\u044b\u0439\"],\n  [\"\u2581siff\", \"\u0441\u0432\u0438\u0441\u0442\u0438\u0442\"],\n  [\"le\", null],\n  [\"\u2581dans\", \"\u0432\"],\n  [\"\u2581le\", null],\n  [\"\u2581pr\u00e9\", \"\u0434\u0432\u043e\u0440\u0435\"],\n  [\"au\", null],\n  [\".\", null],\n  [\"Gr\", \"\u0411\u0430\u0441\u0442\u0443\u0439\"],\n  [\"\u00e8ve\", null],\n  [\",\", null],\n  [\"\u2581ab\", \"\u043b\u0430\u0439\"],\n  [\"oye\", null],\n  [\",\", null],\n  [\"\u2581Gr\", \"\u0411\u0430\u0441\u0442\u0443\u0439\"],\n  [\"\u00e8ve\", null],\n  [\",\", null],\n  [\"\u2581grou\", \"\u0448\u0435\u0432\u0435\u043b\u0438\u0441\u044c\"],\n  [\"ille\", null],\n  [\"\u2581!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nQuand elle eut un enfant, il le fallut mettre en nourrice. Rentr\u00e9 chez eux, le marmot fut g\u00e2t\u00e9 comme un prince.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Quand', '\u2581elle', '\u2581eut', '\u2581un', '\u2581enfant', ',', '\u2581il', '\u2581le', '\u2581fall', 'ut', '\u2581mettre', '\u2581en', '\u2581nour', 'rice', '.', '\u2581R', 'entr', '\u00e9', '\u2581chez', '\u2581eux', ',', '\u2581le', '\u2581mar', 'mot', '\u2581fut', '\u2581g\u00e2', 't\u00e9', '\u2581comme', '\u2581un', '\u2581prince', '.']"}], "[\n[\"Quand\", \"When\"],\n[\"\u2581elle\", \"she\"],\n[\"\u2581eut\", \"had\"],\n[\"\u2581un\", \"a\"],\n[\"\u2581enfant\", \"child\"],\n[\",\", null],\n[\"\u2581il\", \"it\"],\n[\"\u2581le\", null],\n[\"\u2581fall\", \"was\"],\n[\"ut\", null],\n[\"\u2581mettre\", \"put\"],\n[\"\u2581en\", null],\n[\"\u2581nour\", \"wet\"],\n[\"rice\", null],\n[\".\", null],\n[\"\u2581R\", \"When\"],\n[\"entr\", null],\n[\"\u00e9\", null],\n[\"\u2581chez\", null],\n[\"\u2581eux\", \"home\"],\n[\",\", null],\n[\"\u2581le\", \"the\"],\n[\"\u2581mar\", \"child\"],\n[\"mot\", null],\n[\"\u2581fut\", \"was\"],\n[\"\u2581g\u00e2\", \"spoiled\"],\n[\"t\u00e9\", null],\n[\"\u2581comme\", \"like\"],\n[\"\u2581un\", \"a\"],\n[\"\u2581prince\", \"prince\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nQuand elle eut un enfant, il le fallut mettre en nourrice. Rentr\u00e9 chez eux, le marmot fut g\u00e2t\u00e9 comme un prince.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Quand', '\u2581elle', '\u2581eut', '\u2581un', '\u2581enfant', ',', '\u2581il', '\u2581le', '\u2581fall', 'ut', '\u2581mettre', '\u2581en', '\u2581nour', 'rice', '.', '\u2581R', 'entr', '\u00e9', '\u2581chez', '\u2581eux', ',', '\u2581le', '\u2581mar', 'mot', '\u2581fut', '\u2581g\u00e2', 't\u00e9', '\u2581comme', '\u2581un', '\u2581prince', '.']"}], "[\n[\"Quand\", \"\u041a\u043e\u0433\u0434\u0430\"],\n[\"\u2581elle\", \"\u043e\u043d\u0430\"],\n[\"\u2581eut\", \"\u0440\u043e\u0434\u0438\u043b\u0430\"],\n[\"\u2581un\", \"null\"],\n[\"\u2581enfant\", \"\u0440\u0435\u0431\u0435\u043d\u043a\u0430\"],\n[\",\", \"null\"],\n[\"\u2581il\", \"null\"],\n[\"\u2581le\", \"null\"],\n[\"\u2581fall\", \"\u043f\u0440\u0438\u0448\u043b\u043e\u0441\u044c\"],\n[\"ut\", \"null\"],\n[\"\u2581mettre\", \"\u043e\u0442\u0434\u0430\u0442\u044c\"],\n[\"\u2581en\", \"null\"],\n[\"\u2581nour\", \"\u043a\u043e\u0440\u043c\u0438\u043b\u0438\u0446\u0435\"],\n[\"rice\", \"null\"],\n[\".\", \"null\"],\n[\"\u2581R\", \"\u0412\u0435\u0440\u043d\u0443\u0432\u0448\u0438\u0441\u044c\"],\n[\"entr\", \"null\"],\n[\"\u00e9\", \"null\"],\n[\"\u2581chez\", \"\u0434\u043e\u043c\u043e\u0439\"],\n[\"\u2581eux\", \"null\"],\n[\",\", \"null\"],\n[\"\u2581le\", \"null\"],\n[\"\u2581mar\", \"\u043c\u0430\u043b\u044b\u0448\"],\n[\"mot\", \"null\"],\n[\"\u2581fut\", \"\u0431\u044b\u043b\"],\n[\"\u2581g\u00e2\", \"\u0438\u0437\u0431\u0430\u043b\u043e\u0432\u0430\u043d\"],\n[\"t\u00e9\", \"null\"],\n[\"\u2581comme\", \"\u043a\u0430\u043a\"],\n[\"\u2581un\", \"null\"],\n[\"\u2581prince\", \"\u043f\u0440\u0438\u043d\u0446\"],\n[\".\", \"null\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n\u00ab Voil\u00e0 une histoire bien vraisemblable ! \u00bb dit le Pigeon d\u2019un air de profond m\u00e9pris. \u00ab J\u2019ai vu bien des petites filles dans mon temps, mais je n\u2019en ai jamais vu avec un cou comme cela.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00ab', '\u2581Voil\u00e0', '\u2581une', '\u2581histoire', '\u2581bien', '\u2581v', 'raisemb', 'lable', '\u2581!', '\u2581\u00bb', '\u2581dit', '\u2581le', '\u2581Pigeon', '\u2581d', '\u2019', 'un', '\u2581air', '\u2581de', '\u2581profond', '\u2581m\u00e9pris', '.', '\u2581\u00ab', '\u2581J', '\u2019', 'ai', '\u2581vu', '\u2581bien', '\u2581des', '\u2581petites', '\u2581filles', '\u2581dans', '\u2581mon', '\u2581temps', ',', '\u2581mais', '\u2581je', '\u2581n', '\u2019', 'en', '\u2581ai', '\u2581jamais', '\u2581vu', '\u2581avec', '\u2581un', '\u2581cou', '\u2581comme', '\u2581cela', '.']"}], "[\n[\"\u00ab\", null],\n[\"\u2581Voil\u00e0\", \"Here's\"],\n[\"\u2581une\", null],\n[\"\u2581histoire\", \"story\"],\n[\"\u2581bien\", \"quite\"],\n[\"\u2581v\", \"plausible\"],\n[\"raisemb\", null],\n[\"lable\", null],\n[\"\u2581!\", null],\n[\"\u2581\u00bb\", null],\n[\"\u2581dit\", \"said\"],\n[\"\u2581le\", null],\n[\"\u2581Pigeon\", \"Pigeon\"],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"un\", null],\n[\"\u2581air\", \"tone\"],\n[\"\u2581de\", null],\n[\"\u2581profond\", \"deep\"],\n[\"\u2581m\u00e9pris\", \"contempt\"],\n[\".\", null],\n[\"\u2581\u00ab\", null],\n[\"\u2581J\", \"I\"],\n[\"'\", null],\n[\"ai\", null],\n[\"\u2581vu\", \"seen\"],\n[\"\u2581bien\", \"many\"],\n[\"\u2581des\", null],\n[\"\u2581petites\", \"little\"],\n[\"\u2581filles\", \"girls\"],\n[\"\u2581dans\", \"in\"],\n[\"\u2581mon\", \"my\"],\n[\"\u2581temps\", \"time\"],\n[\",\", null],\n[\"\u2581mais\", \"but\"],\n[\"\u2581je\", \"I\"],\n[\"\u2581n\", null],\n[\"'\", null],\n[\"en\", null],\n[\"\u2581ai\", \"have\"],\n[\"\u2581jamais\", \"never\"],\n[\"\u2581vu\", \"seen\"],\n[\"\u2581avec\", \"with\"],\n[\"\u2581un\", null],\n[\"\u2581cou\", \"neck\"],\n[\"\u2581comme\", \"like\"],\n[\"\u2581cela\", \"that\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n\u00ab Voil\u00e0 une histoire bien vraisemblable ! \u00bb dit le Pigeon d\u2019un air de profond m\u00e9pris. \u00ab J\u2019ai vu bien des petites filles dans mon temps, mais je n\u2019en ai jamais vu avec un cou comme cela.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00ab', '\u2581Voil\u00e0', '\u2581une', '\u2581histoire', '\u2581bien', '\u2581v', 'raisemb', 'lable', '\u2581!', '\u2581\u00bb', '\u2581dit', '\u2581le', '\u2581Pigeon', '\u2581d', '\u2019', 'un', '\u2581air', '\u2581de', '\u2581profond', '\u2581m\u00e9pris', '.', '\u2581\u00ab', '\u2581J', '\u2019', 'ai', '\u2581vu', '\u2581bien', '\u2581des', '\u2581petites', '\u2581filles', '\u2581dans', '\u2581mon', '\u2581temps', ',', '\u2581mais', '\u2581je', '\u2581n', '\u2019', 'en', '\u2581ai', '\u2581jamais', '\u2581vu', '\u2581avec', '\u2581un', '\u2581cou', '\u2581comme', '\u2581cela', '.']"}], "[\n[\"\u00ab\", null],\n[\"\u2581Voil\u00e0\", \"\u0412\u043e\u0442\"],\n[\"\u2581une\", null],\n[\"\u2581histoire\", \"\u0438\u0441\u0442\u043e\u0440\u0438\u044f\"],\n[\"\u2581bien\", \"\u043e\u0447\u0435\u043d\u044c\"],\n[\"\u2581v\", \"\u043f\u0440\u0430\u0432\u0434\u043e\u043f\u043e\u0434\u043e\u0431\u043d\u0430\u044f\"],\n[\"raisemb\", null],\n[\"lable\", null],\n[\"\u2581!\", null],\n[\"\u2581\u00bb\", null],\n[\"\u2581dit\", \"\u0441\u043a\u0430\u0437\u0430\u043b\"],\n[\"\u2581le\", null],\n[\"\u2581Pigeon\", \"\u0413\u043e\u043b\u0443\u0431\u044c\"],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"un\", null],\n[\"\u2581air\", \"\u0441 \u0432\u0438\u0434\u043e\u043c\"],\n[\"\u2581de\", null],\n[\"\u2581profond\", \"\u0433\u043b\u0443\u0431\u043e\u043a\u043e\u0433\u043e\"],\n[\"\u2581m\u00e9pris\", \"\u043f\u0440\u0435\u0437\u0440\u0435\u043d\u0438\u044f\"],\n[\".\", null],\n[\"\u2581\u00ab\", null],\n[\"\u2581J\", \"\u042f\"],\n[\"'\", null],\n[\"ai\", null],\n[\"\u2581vu\", \"\u0432\u0438\u0434\u0435\u043b\"],\n[\"\u2581bien\", \"\u043c\u043d\u043e\u0433\u043e\"],\n[\"\u2581des\", null],\n[\"\u2581petites\", \"\u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0438\u0445\"],\n[\"\u2581filles\", \"\u0434\u0435\u0432\u043e\u0447\u0435\u043a\"],\n[\"\u2581dans\", \"\u0437\u0430\"],\n[\"\u2581mon\", \"\u0441\u0432\u043e\u0435\"],\n[\"\u2581temps\", \"\u0432\u0440\u0435\u043c\u044f\"],\n[\",\", null],\n[\"\u2581mais\", \"\u043d\u043e\"],\n[\"\u2581je\", \"\u044f\"],\n[\"\u2581n\", null],\n[\"'\", null],\n[\"en\", null],\n[\"\u2581ai\", null],\n[\"\u2581jamais\", \"\u043d\u0438\u043a\u043e\u0433\u0434\u0430\"],\n[\"\u2581vu\", \"\u043d\u0435 \u0432\u0438\u0434\u0435\u043b\"],\n[\"\u2581avec\", \"\u0441\"],\n[\"\u2581un\", null],\n[\"\u2581cou\", \"\u0448\u0435\u0435\u0439\"],\n[\"\u2581comme\", \"\u043a\u0430\u043a\"],\n[\"\u2581cela\", \"\u044d\u0442\u0430\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nLe salon \u00e9tait une petite pi\u00e8ce simplement meubl\u00e9e, mais propre et confortable.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Le', '\u2581salon', '\u2581\u00e9tait', '\u2581une', '\u2581petite', '\u2581pi\u00e8ce', '\u2581simplement', '\u2581meub', 'l\u00e9e', ',', '\u2581mais', '\u2581propre', '\u2581et', '\u2581confortable', '.']"}], "[\n    [\"Le\", null],\n    [\"\u2581salon\", \"living room\"],\n    [\"\u2581\u00e9tait\", null],\n    [\"\u2581une\", null],\n    [\"\u2581petite\", \"small\"],\n    [\"\u2581pi\u00e8ce\", \"room\"],\n    [\"\u2581simplement\", \"simply\"],\n    [\"\u2581meub\", \"furnished\"],\n    [\"l\u00e9e\", null],\n    [\",\", null],\n    [\"\u2581mais\", null],\n    [\"\u2581propre\", \"clean\"],\n    [\"\u2581et\", null],\n    [\"\u2581confortable\", \"comfortable\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nLe salon \u00e9tait une petite pi\u00e8ce simplement meubl\u00e9e, mais propre et confortable.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Le', '\u2581salon', '\u2581\u00e9tait', '\u2581une', '\u2581petite', '\u2581pi\u00e8ce', '\u2581simplement', '\u2581meub', 'l\u00e9e', ',', '\u2581mais', '\u2581propre', '\u2581et', '\u2581confortable', '.']"}], "[\n    [\"Le\", null],\n    [\"\u2581salon\", \"\u0413\u043e\u0441\u0442\u0438\u043d\u0430\u044f\"],\n    [\"\u2581\u00e9tait\", \"\u0431\u044b\u043b\u0430\"],\n    [\"\u2581une\", null],\n    [\"\u2581petite\", \"\u043c\u0430\u043b\u0435\u043d\u044c\u043a\u043e\u0439\"],\n    [\"\u2581pi\u00e8ce\", \"\u043a\u043e\u043c\u043d\u0430\u0442\u043e\u0439\"],\n    [\"\u2581simplement\", \"\u043f\u0440\u043e\u0441\u0442\u043e\"],\n    [\"\u2581meub\", \"\u043c\u0435\u0431\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0439\"],\n    [\"l\u00e9e\", null],\n    [\",\", null],\n    [\"\u2581mais\", \"\u043d\u043e\"],\n    [\"\u2581propre\", \"\u0447\u0438\u0441\u0442\u043e\u0439\"],\n    [\"\u2581et\", null],\n    [\"\u2581confortable\", \"\u0443\u044e\u0442\u043d\u043e\u0439\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nSi vous saviez que je n\u2019ai qu\u2019elle, que je suis vieille, que c\u2019est une b\u00e9n\u00e9diction que la sainte Vierge m\u2019envoie.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Si', '\u2581vous', '\u2581sav', 'iez', '\u2581que', '\u2581je', '\u2581n', '\u2019', 'ai', '\u2581qu', '\u2019', 'elle', ',', '\u2581que', '\u2581je', '\u2581suis', '\u2581vieille', ',', '\u2581que', '\u2581c', '\u2019', 'est', '\u2581une', '\u2581b\u00e9n\u00e9', 'diction', '\u2581que', '\u2581la', '\u2581sainte', '\u2581Vierge', '\u2581m', '\u2019', 'en', 'voie', '.']"}], "[\n[\"Si\", \"If\"],\n[\"\u2581vous\", \"you\"],\n[\"\u2581sav\", \"knew\"],\n[\"iez\", null],\n[\"\u2581que\", null],\n[\"\u2581je\", \"I\"],\n[\"\u2581n\", null],\n[\"'\", null],\n[\"ai\", null],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"elle\", \"her\"],\n[\",\", null],\n[\"\u2581que\", null],\n[\"\u2581je\", \"I\"],\n[\"\u2581suis\", \"am\"],\n[\"\u2581vieille\", \"old\"],\n[\",\", null],\n[\"\u2581que\", null],\n[\"\u2581c\", null],\n[\"'\", null],\n[\"est\", null],\n[\"\u2581une\", \"a\"],\n[\"\u2581b\u00e9n\u00e9\", \"blessing\"],\n[\"diction\", null],\n[\"\u2581que\", null],\n[\"\u2581la\", \"the\"],\n[\"\u2581sainte\", \"Holy\"],\n[\"\u2581Vierge\", \"Virgin\"],\n[\"\u2581m\", null],\n[\"'\", null],\n[\"en\", null],\n[\"voie\", \"sends\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nSi vous saviez que je n\u2019ai qu\u2019elle, que je suis vieille, que c\u2019est une b\u00e9n\u00e9diction que la sainte Vierge m\u2019envoie.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Si', '\u2581vous', '\u2581sav', 'iez', '\u2581que', '\u2581je', '\u2581n', '\u2019', 'ai', '\u2581qu', '\u2019', 'elle', ',', '\u2581que', '\u2581je', '\u2581suis', '\u2581vieille', ',', '\u2581que', '\u2581c', '\u2019', 'est', '\u2581une', '\u2581b\u00e9n\u00e9', 'diction', '\u2581que', '\u2581la', '\u2581sainte', '\u2581Vierge', '\u2581m', '\u2019', 'en', 'voie', '.']"}], "[\n[\"Si\", \"\u0415\u0441\u043b\u0438\"],\n[\"\u2581vous\", \"\u0432\u044b\"],\n[\"\u2581sav\", \"\u0437\u043d\u0430\u043b\u0438\"],\n[\"iez\", null],\n[\"\u2581que\", null],\n[\"\u2581je\", \"\u044f\"],\n[\"\u2581n\", null],\n[\"'\", null],\n[\"ai\", null],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"elle\", \"\u043e\u043d\u0430\"],\n[\",\", null],\n[\"\u2581que\", null],\n[\"\u2581je\", \"\u044f\"],\n[\"\u2581suis\", \"\u044f\u0432\u043b\u044f\u044e\u0441\u044c\"],\n[\"\u2581vieille\", \"\u0441\u0442\u0430\u0440\u043e\u0439\"],\n[\",\", null],\n[\"\u2581que\", null],\n[\"\u2581c\", null],\n[\"'\", null],\n[\"est\", null],\n[\"\u2581une\", null],\n[\"\u2581b\u00e9n\u00e9\", \"\u0431\u043b\u0430\u0433\u043e\u0441\u043b\u043e\u0432\u0435\u043d\u0438\u0435\"],\n[\"diction\", null],\n[\"\u2581que\", null],\n[\"\u2581la\", null],\n[\"\u2581sainte\", \"\u0441\u0432\u044f\u0442\u0430\u044f\"],\n[\"\u2581Vierge\", \"\u0414\u0435\u0432\u0430\"],\n[\"\u2581m\", null],\n[\"'\", null],\n[\"en\", null],\n[\"voie\", \"\u043f\u043e\u0441\u044b\u043b\u0430\u0435\u0442\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nAu del\u00e0 verdoyaient les pr\u00e9s, au del\u00e0 s\u2019enfuyaient les routes, le long desquelles tra\u00eenaient encore quelques maisons de faubourg, d\u2019autant plus rares qu\u2019elles s\u2019\u00e9loignaient plus. Quelques-uns de ces faubourgs avaient de l\u2019importance.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Au', '\u2581del\u00e0', '\u2581verdo', 'y', 'aient', '\u2581les', '\u2581pr\u00e9s', ',', '\u2581au', '\u2581del\u00e0', '\u2581s', '\u2019', 'enf', 'uy', 'aient', '\u2581les', '\u2581routes', ',', '\u2581le', '\u2581long', '\u2581des', 'quelles', '\u2581tra', '\u00ee', 'na', 'ient', '\u2581encore', '\u2581quelques', '\u2581maisons', '\u2581de', '\u2581fa', 'ubourg', ',', '\u2581d', '\u2019', 'autant', '\u2581plus', '\u2581rares', '\u2581qu', '\u2019', 'elles', '\u2581s', '\u2019', '\u00e9lo', 'igna', 'ient', '\u2581plus', '.', '\u2581Quelques', '-', 'uns', '\u2581de', '\u2581ces', '\u2581fa', 'ub', 'our', 'gs', '\u2581avaient', '\u2581de', '\u2581l', '\u2019', 'importance', '.']"}], "[\n[\"Au\", \"Beyond\"],\n[\"\u2581del\u00e0\", null],\n[\"\u2581verdo\", \"were green\"],\n[\"y\", null],\n[\"aient\", null],\n[\"\u2581les\", null],\n[\"\u2581pr\u00e9s\", \"meadows\"],\n[\",\", null],\n[\"\u2581au\", null],\n[\"\u2581del\u00e0\", \"beyond\"],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"enf\", \"fled\"],\n[\"uy\", null],\n[\"aient\", null],\n[\"\u2581les\", null],\n[\"\u2581routes\", \"roads\"],\n[\",\", null],\n[\"\u2581le\", null],\n[\"\u2581long\", \"along\"],\n[\"\u2581des\", null],\n[\"quelles\", \"which\"],\n[\"\u2581tra\", \"stretched\"],\n[\"\u00ee\", null],\n[\"na\", null],\n[\"ient\", null],\n[\"\u2581encore\", \"still\"],\n[\"\u2581quelques\", \"some\"],\n[\"\u2581maisons\", \"houses\"],\n[\"\u2581de\", null],\n[\"\u2581fa\", \"suburb\"],\n[\"ubourg\", null],\n[\",\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"autant\", \"all the more\"],\n[\"\u2581plus\", null],\n[\"\u2581rares\", \"rare\"],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"elles\", \"they\"],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"\u00e9lo\", \"moved\"],\n[\"igna\", null],\n[\"ient\", null],\n[\"\u2581plus\", \"further\"],\n[\".\", null],\n[\"\u2581Quelques\", \"Some\"],\n[\"-\", null],\n[\"uns\", null],\n[\"\u2581de\", null],\n[\"\u2581ces\", \"these\"],\n[\"\u2581fa\", \"suburbs\"],\n[\"ub\", null],\n[\"our\", null],\n[\"gs\", null],\n[\"\u2581avaient\", \"had\"],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"importance\", \"importance\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nAu del\u00e0 verdoyaient les pr\u00e9s, au del\u00e0 s\u2019enfuyaient les routes, le long desquelles tra\u00eenaient encore quelques maisons de faubourg, d\u2019autant plus rares qu\u2019elles s\u2019\u00e9loignaient plus. Quelques-uns de ces faubourgs avaient de l\u2019importance.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Au', '\u2581del\u00e0', '\u2581verdo', 'y', 'aient', '\u2581les', '\u2581pr\u00e9s', ',', '\u2581au', '\u2581del\u00e0', '\u2581s', '\u2019', 'enf', 'uy', 'aient', '\u2581les', '\u2581routes', ',', '\u2581le', '\u2581long', '\u2581des', 'quelles', '\u2581tra', '\u00ee', 'na', 'ient', '\u2581encore', '\u2581quelques', '\u2581maisons', '\u2581de', '\u2581fa', 'ubourg', ',', '\u2581d', '\u2019', 'autant', '\u2581plus', '\u2581rares', '\u2581qu', '\u2019', 'elles', '\u2581s', '\u2019', '\u00e9lo', 'igna', 'ient', '\u2581plus', '.', '\u2581Quelques', '-', 'uns', '\u2581de', '\u2581ces', '\u2581fa', 'ub', 'our', 'gs', '\u2581avaient', '\u2581de', '\u2581l', '\u2019', 'importance', '.']"}], "[\n[\"Au\", null],\n[\"\u2581del\u00e0\", \"\u0417\u0430\"],\n[\"\u2581verdo\", \"\u0437\u0435\u043b\u0435\u043d\u0435\u043b\u0438\"],\n[\"y\", null],\n[\"aient\", null],\n[\"\u2581les\", null],\n[\"\u2581pr\u00e9s\", \"\u043b\u0443\u0433\u0430\"],\n[\",\", null],\n[\"\u2581au\", null],\n[\"\u2581del\u00e0\", \"\u0434\u0430\u043b\u044c\u0448\u0435\"],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"enf\", \"\u0443\u0431\u0435\u0433\u0430\u043b\u0438\"],\n[\"uy\", null],\n[\"aient\", null],\n[\"\u2581les\", null],\n[\"\u2581routes\", \"\u0434\u043e\u0440\u043e\u0433\u0438\"],\n[\",\", null],\n[\"\u2581le\", null],\n[\"\u2581long\", \"\u0432\u0434\u043e\u043b\u044c\"],\n[\"\u2581des\", null],\n[\"quelles\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u0445\"],\n[\"\u2581tra\", \"\u0442\u044f\u043d\u0443\u043b\u0438\u0441\u044c\"],\n[\"\u00ee\", null],\n[\"na\", null],\n[\"ient\", null],\n[\"\u2581encore\", \"\u0435\u0449\u0435\"],\n[\"\u2581quelques\", \"\u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e\"],\n[\"\u2581maisons\", \"\u0434\u043e\u043c\u043e\u0432\"],\n[\"\u2581de\", null],\n[\"\u2581fa\", \"\u043f\u0440\u0435\u0434\u043c\u0435\u0441\u0442\u044c\u044f\"],\n[\"ubourg\", null],\n[\",\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"autant\", \"\u0442\u0435\u043c\"],\n[\"\u2581plus\", \"\u0431\u043e\u043b\u0435\u0435\"],\n[\"\u2581rares\", \"\u0440\u0435\u0434\u043a\u0438\u0435\"],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"elles\", \"\u043e\u043d\u0438\"],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"\u00e9lo\", \"\u0443\u0434\u0430\u043b\u044f\u043b\u0438\u0441\u044c\"],\n[\"igna\", null],\n[\"ient\", null],\n[\"\u2581plus\", \"\u0434\u0430\u043b\u044c\u0448\u0435\"],\n[\".\", null],\n[\"\u2581Quelques\", \"\u041d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435\"],\n[\"-\", null],\n[\"uns\", null],\n[\"\u2581de\", null],\n[\"\u2581ces\", \"\u0438\u0437 \u044d\u0442\u0438\u0445\"],\n[\"\u2581fa\", \"\u043f\u0440\u0435\u0434\u043c\u0435\u0441\u0442\u0438\u0439\"],\n[\"ub\", null],\n[\"our\", null],\n[\"gs\", null],\n[\"\u2581avaient\", \"\u0438\u043c\u0435\u043b\u0438\"],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"importance\", \"\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n\u00c0 la Wimerra, il a communiqu\u00e9 avec l\u2019un des siens, le forgeron de Black-Point, et a laiss\u00e9 des traces reconnaissables de notre passage.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00c0', '\u2581la', '\u2581W', 'imer', 'ra', ',', '\u2581il', '\u2581a', '\u2581communiqu\u00e9', '\u2581avec', '\u2581l', '\u2019', 'un', '\u2581des', '\u2581si', 'ens', ',', '\u2581le', '\u2581for', 'ger', 'on', '\u2581de', '\u2581Black', '-', 'Point', ',', '\u2581et', '\u2581a', '\u2581laiss\u00e9', '\u2581des', '\u2581traces', '\u2581reconna', 'iss', 'ables', '\u2581de', '\u2581notre', '\u2581passage', '.']"}], "[\n  [\"\u00c0\", null],\n  [\"\u2581la\", null],\n  [\"\u2581W\", \"Wimerra\"],\n  [\"imer\", null],\n  [\"ra\", null],\n  [\",\", null],\n  [\"\u2581il\", \"he\"],\n  [\"\u2581a\", null],\n  [\"\u2581communiqu\u00e9\", \"communicated\"],\n  [\"\u2581avec\", \"with\"],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"un\", \"one\"],\n  [\"\u2581des\", null],\n  [\"\u2581si\", \"his\"],\n  [\"ens\", null],\n  [\",\", null],\n  [\"\u2581le\", null],\n  [\"\u2581for\", \"blacksmith\"],\n  [\"ger\", null],\n  [\"on\", null],\n  [\"\u2581de\", \"of\"],\n  [\"\u2581Black\", \"Black\"],\n  [\"-\", null],\n  [\"Point\", \"Point\"],\n  [\",\", null],\n  [\"\u2581et\", \"and\"],\n  [\"\u2581a\", null],\n  [\"\u2581laiss\u00e9\", \"left\"],\n  [\"\u2581des\", null],\n  [\"\u2581traces\", \"traces\"],\n  [\"\u2581reconna\", \"recognizable\"],\n  [\"iss\", null],\n  [\"ables\", null],\n  [\"\u2581de\", \"of\"],\n  [\"\u2581notre\", \"our\"],\n  [\"\u2581passage\", \"passage\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n\u00c0 la Wimerra, il a communiqu\u00e9 avec l\u2019un des siens, le forgeron de Black-Point, et a laiss\u00e9 des traces reconnaissables de notre passage.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00c0', '\u2581la', '\u2581W', 'imer', 'ra', ',', '\u2581il', '\u2581a', '\u2581communiqu\u00e9', '\u2581avec', '\u2581l', '\u2019', 'un', '\u2581des', '\u2581si', 'ens', ',', '\u2581le', '\u2581for', 'ger', 'on', '\u2581de', '\u2581Black', '-', 'Point', ',', '\u2581et', '\u2581a', '\u2581laiss\u00e9', '\u2581des', '\u2581traces', '\u2581reconna', 'iss', 'ables', '\u2581de', '\u2581notre', '\u2581passage', '.']"}], "[\n[\"\u00c0\", null],\n[\"\u2581la\", null],\n[\"\u2581W\", \"\u0412\u0438\u043c\u0435\u0440\u0440\u0430\"],\n[\"imer\", null],\n[\"ra\", null],\n[\",\", null],\n[\"\u2581il\", \"\u043e\u043d\"],\n[\"\u2581a\", null],\n[\"\u2581communiqu\u00e9\", \"\u043e\u0431\u0449\u0430\u043b\u0441\u044f\"],\n[\"\u2581avec\", \"\u0441\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"un\", \"\u043e\u0434\u043d\u0438\u043c\"],\n[\"\u2581des\", \"\u0438\u0437\"],\n[\"\u2581si\", \"\u0441\u0432\u043e\u0438\u0445\"],\n[\"ens\", null],\n[\",\", null],\n[\"\u2581le\", null],\n[\"\u2581for\", \"\u043a\u0443\u0437\u043d\u0435\u0446\u043e\u043c\"],\n[\"ger\", null],\n[\"on\", null],\n[\"\u2581de\", \"\u0438\u0437\"],\n[\"\u2581Black\", \"\u0411\u043b\u044d\u043a\"],\n[\"-\", null],\n[\"Point\", \"\u041f\u043e\u0439\u043d\u0442\u0430\"],\n[\",\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581a\", null],\n[\"\u2581laiss\u00e9\", \"\u043e\u0441\u0442\u0430\u0432\u0438\u043b\"],\n[\"\u2581des\", null],\n[\"\u2581traces\", \"\u0441\u043b\u0435\u0434\u044b\"],\n[\"\u2581reconna\", \"\u0443\u0437\u043d\u0430\u0432\u0430\u0435\u043c\u044b\u0435\"],\n[\"iss\", null],\n[\"ables\", null],\n[\"\u2581de\", null],\n[\"\u2581notre\", \"\u043d\u0430\u0448\u0435\u0433\u043e\"],\n[\"\u2581passage\", \"\u043f\u0440\u043e\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nMonsieur l\u2019archev\u00eaque s\u2019est int\u00e9ress\u00e9 \u00e0 l\u2019enfant d\u2019\u00c9gypte, l\u2019a exorcis\u00e9, l\u2019a b\u00e9ni, lui a \u00f4t\u00e9 bien soigneusement le diable du corps, et l\u2019a envoy\u00e9 \u00e0 Paris pour \u00eatre expos\u00e9 sur le lit de bois, \u00e0 Notre-Dame, comme enfant trouv\u00e9.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Monsieur', '\u2581l', '\u2019', 'ar', 'chev\u00eaque', '\u2581s', '\u2019', 'est', '\u2581int\u00e9ress', '\u00e9', '\u2581\u00e0', '\u2581l', '\u2019', 'enfant', '\u2581d', '\u2019', '\u00c9gypte', ',', '\u2581l', '\u2019', 'a', '\u2581exor', 'cis', '\u00e9', ',', '\u2581l', '\u2019', 'a', '\u2581b\u00e9', 'ni', ',', '\u2581lui', '\u2581a', '\u2581\u00f4', 't\u00e9', '\u2581bien', '\u2581soigneusement', '\u2581le', '\u2581diable', '\u2581du', '\u2581corps', ',', '\u2581et', '\u2581l', '\u2019', 'a', '\u2581envoy\u00e9', '\u2581\u00e0', '\u2581Paris', '\u2581pour', '\u2581\u00eatre', '\u2581expos\u00e9', '\u2581sur', '\u2581le', '\u2581lit', '\u2581de', '\u2581bois', ',', '\u2581\u00e0', '\u2581Notre', '-', 'Dame', ',', '\u2581comme', '\u2581enfant', '\u2581trouv\u00e9', '.']"}], "[\n[\"Monsieur\", \"Mister\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"ar\", null],\n[\"chev\u00eaque\", \"archbishop\"],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"est\", null],\n[\"\u2581int\u00e9ress\", null],\n[\"\u00e9\", \"interested\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"enfant\", \"child\"],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"\u00c9gypte\", \"Egypt\"],\n[\",\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"a\", null],\n[\"\u2581exor\", null],\n[\"cis\", null],\n[\"\u00e9\", \"exorcised\"],\n[\",\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"a\", null],\n[\"\u2581b\u00e9\", null],\n[\"ni\", \"blessed\"],\n[\",\", null],\n[\"\u2581lui\", \"him\"],\n[\"\u2581a\", null],\n[\"\u2581\u00f4\", null],\n[\"t\u00e9\", \"removed\"],\n[\"\u2581bien\", \"very\"],\n[\"\u2581soigneusement\", \"carefully\"],\n[\"\u2581le\", \"the\"],\n[\"\u2581diable\", \"devil\"],\n[\"\u2581du\", \"from\"],\n[\"\u2581corps\", \"body\"],\n[\",\", null],\n[\"\u2581et\", \"and\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"a\", null],\n[\"\u2581envoy\u00e9\", \"sent\"],\n[\"\u2581\u00e0\", \"to\"],\n[\"\u2581Paris\", \"Paris\"],\n[\"\u2581pour\", \"to\"],\n[\"\u2581\u00eatre\", \"be\"],\n[\"\u2581expos\u00e9\", \"displayed\"],\n[\"\u2581sur\", \"on\"],\n[\"\u2581le\", \"the\"],\n[\"\u2581lit\", \"bed\"],\n[\"\u2581de\", \"of\"],\n[\"\u2581bois\", \"wood\"],\n[\",\", null],\n[\"\u2581\u00e0\", \"at\"],\n[\"\u2581Notre\", \"Notre\"],\n[\"-\", null],\n[\"Dame\", \"Dame\"],\n[\",\", null],\n[\"\u2581comme\", \"as\"],\n[\"\u2581enfant\", \"child\"],\n[\"\u2581trouv\u00e9\", \"foundling\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nMonsieur l\u2019archev\u00eaque s\u2019est int\u00e9ress\u00e9 \u00e0 l\u2019enfant d\u2019\u00c9gypte, l\u2019a exorcis\u00e9, l\u2019a b\u00e9ni, lui a \u00f4t\u00e9 bien soigneusement le diable du corps, et l\u2019a envoy\u00e9 \u00e0 Paris pour \u00eatre expos\u00e9 sur le lit de bois, \u00e0 Notre-Dame, comme enfant trouv\u00e9.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Monsieur', '\u2581l', '\u2019', 'ar', 'chev\u00eaque', '\u2581s', '\u2019', 'est', '\u2581int\u00e9ress', '\u00e9', '\u2581\u00e0', '\u2581l', '\u2019', 'enfant', '\u2581d', '\u2019', '\u00c9gypte', ',', '\u2581l', '\u2019', 'a', '\u2581exor', 'cis', '\u00e9', ',', '\u2581l', '\u2019', 'a', '\u2581b\u00e9', 'ni', ',', '\u2581lui', '\u2581a', '\u2581\u00f4', 't\u00e9', '\u2581bien', '\u2581soigneusement', '\u2581le', '\u2581diable', '\u2581du', '\u2581corps', ',', '\u2581et', '\u2581l', '\u2019', 'a', '\u2581envoy\u00e9', '\u2581\u00e0', '\u2581Paris', '\u2581pour', '\u2581\u00eatre', '\u2581expos\u00e9', '\u2581sur', '\u2581le', '\u2581lit', '\u2581de', '\u2581bois', ',', '\u2581\u00e0', '\u2581Notre', '-', 'Dame', ',', '\u2581comme', '\u2581enfant', '\u2581trouv\u00e9', '.']"}], "[\n[\"Monsieur\", \"\u0413\u043e\u0441\u043f\u043e\u0434\u0438\u043d\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"ar\", null],\n[\"chev\u00eaque\", \"\u0430\u0440\u0445\u0438\u0435\u043f\u0438\u0441\u043a\u043e\u043f\"],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"est\", null],\n[\"\u2581int\u00e9ress\", null],\n[\"\u00e9\", \"\u0437\u0430\u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043e\u0432\u0430\u043b\u0441\u044f\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"enfant\", \"\u0440\u0435\u0431\u0435\u043d\u043a\u043e\u043c\"],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"\u00c9gypte\", \"\u0415\u0433\u0438\u043f\u0442\u0430\"],\n[\",\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"a\", null],\n[\"\u2581exor\", null],\n[\"cis\", null],\n[\"\u00e9\", \"\u0438\u0437\u0433\u043d\u0430\u043b\"],\n[\",\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"a\", null],\n[\"\u2581b\u00e9\", null],\n[\"ni\", \"\u0431\u043b\u0430\u0433\u043e\u0441\u043b\u043e\u0432\u0438\u043b\"],\n[\",\", null],\n[\"\u2581lui\", \"\u0443 \u043d\u0435\u0433\u043e\"],\n[\"\u2581a\", null],\n[\"\u2581\u00f4\", null],\n[\"t\u00e9\", \"\u0438\u0437\u0433\u043d\u0430\u043b\"],\n[\"\u2581bien\", \"\u0442\u0449\u0430\u0442\u0435\u043b\u044c\u043d\u043e\"],\n[\"\u2581soigneusement\", \"\u0441\u0442\u0430\u0440\u0430\u0442\u0435\u043b\u044c\u043d\u043e\"],\n[\"\u2581le\", null],\n[\"\u2581diable\", \"\u0434\u044c\u044f\u0432\u043e\u043b\u0430\"],\n[\"\u2581du\", \"\u0438\u0437\"],\n[\"\u2581corps\", \"\u0442\u0435\u043b\u0430\"],\n[\",\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"a\", null],\n[\"\u2581envoy\u00e9\", \"\u043e\u0442\u043f\u0440\u0430\u0432\u0438\u043b\"],\n[\"\u2581\u00e0\", \"\u0432\"],\n[\"\u2581Paris\", \"\u041f\u0430\u0440\u0438\u0436\"],\n[\"\u2581pour\", \"\u0447\u0442\u043e\u0431\u044b\"],\n[\"\u2581\u00eatre\", null],\n[\"\u2581expos\u00e9\", \"\u0432\u044b\u0441\u0442\u0430\u0432\u0438\u0442\u044c\"],\n[\"\u2581sur\", \"\u043d\u0430\"],\n[\"\u2581le\", null],\n[\"\u2581lit\", \"\u043a\u0440\u043e\u0432\u0430\u0442\u0438\"],\n[\"\u2581de\", \"\u0438\u0437\"],\n[\"\u2581bois\", \"\u0434\u0435\u0440\u0435\u0432\u0430\"],\n[\",\", null],\n[\"\u2581\u00e0\", \"\u0432\"],\n[\"\u2581Notre\", \"\u041d\u043e\u0442\u0440\"],\n[\"-\", null],\n[\"Dame\", \"\u0414\u0430\u043c\"],\n[\",\", null],\n[\"\u2581comme\", \"\u043a\u0430\u043a\"],\n[\"\u2581enfant\", \"\u043d\u0430\u0439\u0434\u0435\u043d\u044b\u0448\u0430\"],\n[\"\u2581trouv\u00e9\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nDes cygnes \u00e0 t\u00eate noire s\u2019y \u00e9battaient capricieusement et disputaient l\u2019empire des eaux \u00e0 de nombreuses autruches qui gambadaient \u00e0 travers les ilanos.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Des', '\u2581cy', 'gnes', '\u2581\u00e0', '\u2581t\u00eate', '\u2581noire', '\u2581s', '\u2019', 'y', '\u2581\u00e9', 'bat', 'taient', '\u2581cap', 'ric', 'ie', 'usement', '\u2581et', '\u2581disputa', 'ient', '\u2581l', '\u2019', 'empire', '\u2581des', '\u2581eaux', '\u2581\u00e0', '\u2581de', '\u2581nombreuses', '\u2581au', 'tru', 'ches', '\u2581qui', '\u2581gamb', 'ad', 'aient', '\u2581\u00e0', '\u2581travers', '\u2581les', '\u2581ilan', 'os', '.']"}], "[\n[\"Des\", null],\n[\"\u2581cy\", \"black\"],\n[\"gnes\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581t\u00eate\", null],\n[\"\u2581noire\", null],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"y\", null],\n[\"\u2581\u00e9\", \"frolicked\"],\n[\"bat\", null],\n[\"taient\", null],\n[\"\u2581cap\", \"capriciously\"],\n[\"ric\", null],\n[\"ie\", null],\n[\"usement\", null],\n[\"\u2581et\", null],\n[\"\u2581disputa\", \"disputed\"],\n[\"ient\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"empire\", \"dominion\"],\n[\"\u2581des\", null],\n[\"\u2581eaux\", \"waters\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581de\", null],\n[\"\u2581nombreuses\", \"numerous\"],\n[\"\u2581au\", \"ostriches\"],\n[\"tru\", null],\n[\"ches\", null],\n[\"\u2581qui\", null],\n[\"\u2581gamb\", \"pranced\"],\n[\"ad\", null],\n[\"aient\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581travers\", \"across\"],\n[\"\u2581les\", null],\n[\"\u2581ilan\", \"llanos\"],\n[\"os\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nDes cygnes \u00e0 t\u00eate noire s\u2019y \u00e9battaient capricieusement et disputaient l\u2019empire des eaux \u00e0 de nombreuses autruches qui gambadaient \u00e0 travers les ilanos.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Des', '\u2581cy', 'gnes', '\u2581\u00e0', '\u2581t\u00eate', '\u2581noire', '\u2581s', '\u2019', 'y', '\u2581\u00e9', 'bat', 'taient', '\u2581cap', 'ric', 'ie', 'usement', '\u2581et', '\u2581disputa', 'ient', '\u2581l', '\u2019', 'empire', '\u2581des', '\u2581eaux', '\u2581\u00e0', '\u2581de', '\u2581nombreuses', '\u2581au', 'tru', 'ches', '\u2581qui', '\u2581gamb', 'ad', 'aient', '\u2581\u00e0', '\u2581travers', '\u2581les', '\u2581ilan', 'os', '.']"}], "[\n[\"Des\", null],\n[\"\u2581cy\", \"\u0447\u0435\u0440\u043d\u043e\u0448\u0435\u0438\u0435\"],\n[\"gnes\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581t\u00eate\", null],\n[\"\u2581noire\", null],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"y\", null],\n[\"\u2581\u00e9\", \"\u0440\u0435\u0437\u0432\u0438\u043b\u0438\u0441\u044c\"],\n[\"bat\", null],\n[\"taient\", null],\n[\"\u2581cap\", null],\n[\"ric\", null],\n[\"ie\", null],\n[\"usement\", null],\n[\"\u2581et\", null],\n[\"\u2581disputa\", \"\u043e\u0441\u043f\u0430\u0440\u0438\u0432\u0430\u043b\u0438\"],\n[\"ient\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"empire\", \"\u0433\u043e\u0441\u043f\u043e\u0434\u0441\u0442\u0432\u043e\"],\n[\"\u2581des\", null],\n[\"\u2581eaux\", \"\u0432\u043e\u0434\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581de\", null],\n[\"\u2581nombreuses\", \"\u043c\u043d\u043e\u0433\u043e\u0447\u0438\u0441\u043b\u0435\u043d\u043d\u044b\u0445\"],\n[\"\u2581au\", \"\u0441\u0442\u0440\u0430\u0443\u0441\u043e\u0432\"],\n[\"tru\", null],\n[\"ches\", null],\n[\"\u2581qui\", null],\n[\"\u2581gamb\", \"\u0441\u043a\u0430\u043a\u0430\u043b\u0438\"],\n[\"ad\", null],\n[\"aient\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581travers\", \"\u0447\u0435\u0440\u0435\u0437\"],\n[\"\u2581les\", null],\n[\"\u2581ilan\", \"\u043b\u044c\u044f\u043d\u043e\u0441\u044b\"],\n[\"os\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nAlors, emport\u00e9e dans ses souvenirs comme dans un torrent qui bouillonne, elle arriva bient\u00f4t \u00e0 se rappeler la journ\u00e9e de la veille.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Alors', ',', '\u2581emp', 'ort', '\u00e9e', '\u2581dans', '\u2581ses', '\u2581souvenirs', '\u2581comme', '\u2581dans', '\u2581un', '\u2581torrent', '\u2581qui', '\u2581bou', 'ill', 'onne', ',', '\u2581elle', '\u2581arriva', '\u2581bient\u00f4t', '\u2581\u00e0', '\u2581se', '\u2581rappeler', '\u2581la', '\u2581journ\u00e9e', '\u2581de', '\u2581la', '\u2581veille', '.']"}], "[\n[\"Alors\", \"Then\"],\n[\",\", null],\n[\"\u2581emp\", \"carried\"],\n[\"ort\", null],\n[\"\u00e9e\", null],\n[\"\u2581dans\", null],\n[\"\u2581ses\", null],\n[\"\u2581souvenirs\", \"memories\"],\n[\"\u2581comme\", null],\n[\"\u2581dans\", null],\n[\"\u2581un\", null],\n[\"\u2581torrent\", \"torrent\"],\n[\"\u2581qui\", null],\n[\"\u2581bou\", \"boils\"],\n[\"ill\", null],\n[\"onne\", null],\n[\",\", null],\n[\"\u2581elle\", null],\n[\"\u2581arriva\", \"arrived\"],\n[\"\u2581bient\u00f4t\", \"soon\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581se\", null],\n[\"\u2581rappeler\", \"remember\"],\n[\"\u2581la\", null],\n[\"\u2581journ\u00e9e\", \"day\"],\n[\"\u2581de\", null],\n[\"\u2581la\", null],\n[\"\u2581veille\", \"previous\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nAlors, emport\u00e9e dans ses souvenirs comme dans un torrent qui bouillonne, elle arriva bient\u00f4t \u00e0 se rappeler la journ\u00e9e de la veille.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Alors', ',', '\u2581emp', 'ort', '\u00e9e', '\u2581dans', '\u2581ses', '\u2581souvenirs', '\u2581comme', '\u2581dans', '\u2581un', '\u2581torrent', '\u2581qui', '\u2581bou', 'ill', 'onne', ',', '\u2581elle', '\u2581arriva', '\u2581bient\u00f4t', '\u2581\u00e0', '\u2581se', '\u2581rappeler', '\u2581la', '\u2581journ\u00e9e', '\u2581de', '\u2581la', '\u2581veille', '.']"}], "[\n[\"Alors\", \"\u0418\u0442\u0430\u043a\"],\n[\",\", null],\n[\"\u2581emp\", \"\u0443\u043d\u0435\u0441\u0435\u043d\u043d\u0430\u044f\"],\n[\"ort\", null],\n[\"\u00e9e\", null],\n[\"\u2581dans\", null],\n[\"\u2581ses\", null],\n[\"\u2581souvenirs\", \"\u0432\u043e\u0441\u043f\u043e\u043c\u0438\u043d\u0430\u043d\u0438\u044f\"],\n[\"\u2581comme\", null],\n[\"\u2581dans\", null],\n[\"\u2581un\", null],\n[\"\u2581torrent\", \"\u043f\u043e\u0442\u043e\u043a\"],\n[\"\u2581qui\", null],\n[\"\u2581bou\", \"\u043a\u0438\u043f\u044f\u0449\u0438\u0439\"],\n[\"ill\", null],\n[\"onne\", null],\n[\",\", null],\n[\"\u2581elle\", null],\n[\"\u2581arriva\", \"\u043f\u0440\u0438\u0448\u043b\u0430\"],\n[\"\u2581bient\u00f4t\", \"\u0432\u0441\u043a\u043e\u0440\u0435\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581se\", null],\n[\"\u2581rappeler\", \"\u0432\u0441\u043f\u043e\u043c\u043d\u0438\u0442\u044c\"],\n[\"\u2581la\", null],\n[\"\u2581journ\u00e9e\", \"\u0434\u0435\u043d\u044c\"],\n[\"\u2581de\", null],\n[\"\u2581la\", null],\n[\"\u2581veille\", \"\u043d\u0430\u043a\u0430\u043d\u0443\u043d\u0435\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nL\u00e0-bas, c\u2019est l\u2019abbaye Saint-Martin, chanteuse aigre et f\u00eal\u00e9e ; ici, la voix sinistre et bourrue de la Bastille ; \u00e0 l\u2019autre bout, la grosse Tour du Louvre, avec sa basse-taille.\n\n\n---=== TOKENIZED TEXT ===---\n\n['L\u00e0', '-', 'bas', ',', '\u2581c', '\u2019', 'est', '\u2581l', '\u2019', 'abbaye', '\u2581Saint', '-', 'Martin', ',', '\u2581chanteuse', '\u2581a', 'igre', '\u2581et', '\u2581f', '\u00ea', 'l\u00e9e', '\u2581;', '\u2581ici', ',', '\u2581la', '\u2581voix', '\u2581sin', 'istre', '\u2581et', '\u2581bour', 'rue', '\u2581de', '\u2581la', '\u2581Bastille', '\u2581;', '\u2581\u00e0', '\u2581l', '\u2019', 'autre', '\u2581bout', ',', '\u2581la', '\u2581grosse', '\u2581Tour', '\u2581du', '\u2581Louvre', ',', '\u2581avec', '\u2581sa', '\u2581basse', '-', 'taille', '.']"}], "[\n[\"L\u00e0\", \"There\"],\n[\"-\", null],\n[\"bas\", null],\n[\",\", null],\n[\"\u2581c\", null],\n[\"'\", null],\n[\"est\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"abbaye\", \"abbey\"],\n[\"\u2581Saint\", \"Saint\"],\n[\"-\", null],\n[\"Martin\", \"Martin\"],\n[\",\", null],\n[\"\u2581chanteuse\", \"singer\"],\n[\"\u2581a\", null],\n[\"igre\", null],\n[\"\u2581et\", null],\n[\"\u2581f\", null],\n[\"\u00ea\", null],\n[\"l\u00e9e\", null],\n[\"\u2581;\", null],\n[\"\u2581ici\", \"here\"],\n[\",\", null],\n[\"\u2581la\", null],\n[\"\u2581voix\", \"voice\"],\n[\"\u2581sin\", null],\n[\"istre\", null],\n[\"\u2581et\", null],\n[\"\u2581bour\", null],\n[\"rue\", null],\n[\"\u2581de\", null],\n[\"\u2581la\", null],\n[\"\u2581Bastille\", \"Bastille\"],\n[\"\u2581;\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"autre\", \"other\"],\n[\"\u2581bout\", \"end\"],\n[\",\", null],\n[\"\u2581la\", null],\n[\"\u2581grosse\", \"big\"],\n[\"\u2581Tour\", \"Tower\"],\n[\"\u2581du\", null],\n[\"\u2581Louvre\", \"Louvre\"],\n[\",\", null],\n[\"\u2581avec\", \"with\"],\n[\"\u2581sa\", \"its\"],\n[\"\u2581basse\", \"low\"],\n[\"-\", null],\n[\"taille\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nL\u00e0-bas, c\u2019est l\u2019abbaye Saint-Martin, chanteuse aigre et f\u00eal\u00e9e ; ici, la voix sinistre et bourrue de la Bastille ; \u00e0 l\u2019autre bout, la grosse Tour du Louvre, avec sa basse-taille.\n\n\n---=== TOKENIZED TEXT ===---\n\n['L\u00e0', '-', 'bas', ',', '\u2581c', '\u2019', 'est', '\u2581l', '\u2019', 'abbaye', '\u2581Saint', '-', 'Martin', ',', '\u2581chanteuse', '\u2581a', 'igre', '\u2581et', '\u2581f', '\u00ea', 'l\u00e9e', '\u2581;', '\u2581ici', ',', '\u2581la', '\u2581voix', '\u2581sin', 'istre', '\u2581et', '\u2581bour', 'rue', '\u2581de', '\u2581la', '\u2581Bastille', '\u2581;', '\u2581\u00e0', '\u2581l', '\u2019', 'autre', '\u2581bout', ',', '\u2581la', '\u2581grosse', '\u2581Tour', '\u2581du', '\u2581Louvre', ',', '\u2581avec', '\u2581sa', '\u2581basse', '-', 'taille', '.']"}], "[\n[\"L\u00e0\", \"\u0422\u0430\u043c\"],\n[\"-\", null],\n[\"bas\", null],\n[\",\", null],\n[\"\u2581c\", null],\n[\"'\", null],\n[\"est\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"abbaye\", \"\u0430\u0431\u0431\u0430\u0442\u0441\u0442\u0432\u043e\"],\n[\"\u2581Saint\", \"\u0421\u0432\u044f\u0442\u043e\u0433\u043e\"],\n[\"-\", null],\n[\"Martin\", \"\u041c\u0430\u0440\u0442\u0438\u043d\u0430\"],\n[\",\", null],\n[\"\u2581chanteuse\", \"\u043f\u0435\u0432\u0438\u0446\u0430\"],\n[\"\u2581a\", null],\n[\"igre\", null],\n[\"\u2581et\", null],\n[\"\u2581f\", null],\n[\"\u00ea\", null],\n[\"l\u00e9e\", null],\n[\"\u2581;\", null],\n[\"\u2581ici\", \"\u0437\u0434\u0435\u0441\u044c\"],\n[\",\", null],\n[\"\u2581la\", null],\n[\"\u2581voix\", \"\u0433\u043e\u043b\u043e\u0441\"],\n[\"\u2581sin\", null],\n[\"istre\", null],\n[\"\u2581et\", null],\n[\"\u2581bour\", null],\n[\"rue\", null],\n[\"\u2581de\", null],\n[\"\u2581la\", null],\n[\"\u2581Bastille\", \"\u0411\u0430\u0441\u0442\u0438\u043b\u0438\u0438\"],\n[\"\u2581;\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"autre\", \"\u0434\u0440\u0443\u0433\u043e\u043c\"],\n[\"\u2581bout\", \"\u043a\u043e\u043d\u0446\u0435\"],\n[\",\", null],\n[\"\u2581la\", null],\n[\"\u2581grosse\", \"\u0431\u043e\u043b\u044c\u0448\u0430\u044f\"],\n[\"\u2581Tour\", \"\u0411\u0430\u0448\u043d\u044f\"],\n[\"\u2581du\", null],\n[\"\u2581Louvre\", \"\u041b\u0443\u0432\u0440\u0430\"],\n[\",\", null],\n[\"\u2581avec\", \"\u0441\u043e\"],\n[\"\u2581sa\", \"\u0441\u0432\u043e\u0438\u043c\"],\n[\"\u2581basse\", \"\u043d\u0438\u0437\u043a\u0438\u043c\"],\n[\"-\", null],\n[\"taille\", \"\u0433\u043e\u043b\u043e\u0441\u043e\u043c\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nJe n'avais pour tout ornement qu'une petite perle, dernier souvenir de Mlle Temple. Je la mis et nous descend\u00eemes.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Je', '\u2581n', \"'\", 'avais', '\u2581pour', '\u2581tout', '\u2581or', 'nement', '\u2581qu', \"'\", 'une', '\u2581petite', '\u2581perle', ',', '\u2581dernier', '\u2581souvenir', '\u2581de', '\u2581Mlle', '\u2581Temple', '.', '\u2581Je', '\u2581la', '\u2581mis', '\u2581et', '\u2581nous', '\u2581descend', '\u00eemes', '.']"}], "[\n[\"Je\", \"I\"],\n[\"\u2581n\", null],\n[\"'\", null],\n[\"avais\", \"had\"],\n[\"\u2581pour\", \"for\"],\n[\"\u2581tout\", \"all\"],\n[\"\u2581or\", null],\n[\"nement\", \"ornament\"],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"une\", \"a\"],\n[\"\u2581petite\", \"small\"],\n[\"\u2581perle\", \"pearl\"],\n[\",\", null],\n[\"\u2581dernier\", \"last\"],\n[\"\u2581souvenir\", \"memento\"],\n[\"\u2581de\", \"of\"],\n[\"\u2581Mlle\", \"Miss\"],\n[\"\u2581Temple\", \"Temple\"],\n[\".\", null],\n[\"\u2581Je\", \"I\"],\n[\"\u2581la\", \"it\"],\n[\"\u2581mis\", \"put\"],\n[\"\u2581et\", \"and\"],\n[\"\u2581nous\", \"we\"],\n[\"\u2581descend\", null],\n[\"\u00eemes\", \"descended\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nJe n'avais pour tout ornement qu'une petite perle, dernier souvenir de Mlle Temple. Je la mis et nous descend\u00eemes.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Je', '\u2581n', \"'\", 'avais', '\u2581pour', '\u2581tout', '\u2581or', 'nement', '\u2581qu', \"'\", 'une', '\u2581petite', '\u2581perle', ',', '\u2581dernier', '\u2581souvenir', '\u2581de', '\u2581Mlle', '\u2581Temple', '.', '\u2581Je', '\u2581la', '\u2581mis', '\u2581et', '\u2581nous', '\u2581descend', '\u00eemes', '.']"}], "[\n[\"Je\", \"\u042f\"],\n[\"\u2581n\", null],\n[\"'\", null],\n[\"avais\", \"\u0438\u043c\u0435\u043b\u0430\"],\n[\"\u2581pour\", \"\u0434\u043b\u044f\"],\n[\"\u2581tout\", \"\u0432\u0441\u0435\u0433\u043e\"],\n[\"\u2581or\", null],\n[\"nement\", \"\u0443\u043a\u0440\u0430\u0448\u0435\u043d\u0438\u044f\"],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"une\", \"\u043e\u0434\u043d\u0443\"],\n[\"\u2581petite\", \"\u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0443\u044e\"],\n[\"\u2581perle\", \"\u0436\u0435\u043c\u0447\u0443\u0436\u0438\u043d\u0443\"],\n[\",\", null],\n[\"\u2581dernier\", \"\u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435\"],\n[\"\u2581souvenir\", \"\u0432\u043e\u0441\u043f\u043e\u043c\u0438\u043d\u0430\u043d\u0438\u0435\"],\n[\"\u2581de\", \"\u043e\"],\n[\"\u2581Mlle\", \"\u043c\u0438\u0441\u0441\"],\n[\"\u2581Temple\", \"\u0422\u0435\u043c\u043f\u043b\"],\n[\".\", null],\n[\"\u2581Je\", \"\u042f\"],\n[\"\u2581la\", \"\u0435\u0435\"],\n[\"\u2581mis\", \"\u043d\u0430\u0434\u0435\u043b\u0430\"],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581nous\", \"\u043c\u044b\"],\n[\"\u2581descend\", null],\n[\"\u00eemes\", \"\u0441\u043f\u0443\u0441\u0442\u0438\u043b\u0438\u0441\u044c\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n-- Des documents! des documents! s\u2019\u00e9cria lady Helena. -- Seulement, r\u00e9pondit Glenarvan, ils paraissent \u00eatre rong\u00e9s par l\u2019humidit\u00e9, et il est impossible de les retirer, car ils adh\u00e8rent aux parois de la bouteille.\n\n\n---=== TOKENIZED TEXT ===---\n\n['--', '\u2581Des', '\u2581documents', '!', '\u2581des', '\u2581documents', '!', '\u2581s', '\u2019', '\u00e9cri', 'a', '\u2581lady', '\u2581Helena', '.', '\u2581--', '\u2581Seulement', ',', '\u2581r\u00e9pondit', '\u2581Glen', 'ar', 'van', ',', '\u2581ils', '\u2581para', 'issent', '\u2581\u00eatre', '\u2581rong', '\u00e9s', '\u2581par', '\u2581l', '\u2019', 'humidit\u00e9', ',', '\u2581et', '\u2581il', '\u2581est', '\u2581impossible', '\u2581de', '\u2581les', '\u2581retirer', ',', '\u2581car', '\u2581ils', '\u2581adh', '\u00e8rent', '\u2581aux', '\u2581par', 'ois', '\u2581de', '\u2581la', '\u2581bouteille', '.']"}], "[\n    [\"--\", null],\n    [\"\u2581Des\", \"Documents\"],\n    [\"\u2581documents\", null],\n    [\"!\", null],\n    [\"\u2581des\", \"documents\"],\n    [\"\u2581documents\", null],\n    [\"!\", null],\n    [\"\u2581s\", \"exclaimed\"],\n    [\"'\", null],\n    [\"\u00e9cri\", null],\n    [\"a\", null],\n    [\"\u2581lady\", \"Lady\"],\n    [\"\u2581Helena\", \"Helena\"],\n    [\".\", null],\n    [\"\u2581--\", null],\n    [\"\u2581Seulement\", \"Only\"],\n    [\",\", null],\n    [\"\u2581r\u00e9pondit\", \"replied\"],\n    [\"\u2581Glen\", \"Glenarvan\"],\n    [\"ar\", null],\n    [\"van\", null],\n    [\",\", null],\n    [\"\u2581ils\", \"they\"],\n    [\"\u2581para\", \"appear\"],\n    [\"issent\", null],\n    [\"\u2581\u00eatre\", \"to be\"],\n    [\"\u2581rong\", \"eaten\"],\n    [\"\u00e9s\", null],\n    [\"\u2581par\", \"by\"],\n    [\"\u2581l\", \"the\"],\n    [\"'\", null],\n    [\"humidit\u00e9\", \"moisture\"],\n    [\",\", null],\n    [\"\u2581et\", \"and\"],\n    [\"\u2581il\", \"it\"],\n    [\"\u2581est\", \"is\"],\n    [\"\u2581impossible\", \"impossible\"],\n    [\"\u2581de\", \"to\"],\n    [\"\u2581les\", \"them\"],\n    [\"\u2581retirer\", \"remove\"],\n    [\",\", null],\n    [\"\u2581car\", \"because\"],\n    [\"\u2581ils\", \"they\"],\n    [\"\u2581adh\", \"adhere\"],\n    [\"\u00e8rent\", null],\n    [\"\u2581aux\", \"to the\"],\n    [\"\u2581par\", \"walls\"],\n    [\"ois\", null],\n    [\"\u2581de\", \"of\"],\n    [\"\u2581la\", \"the\"],\n    [\"\u2581bouteille\", \"bottle\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n-- Des documents! des documents! s\u2019\u00e9cria lady Helena. -- Seulement, r\u00e9pondit Glenarvan, ils paraissent \u00eatre rong\u00e9s par l\u2019humidit\u00e9, et il est impossible de les retirer, car ils adh\u00e8rent aux parois de la bouteille.\n\n\n---=== TOKENIZED TEXT ===---\n\n['--', '\u2581Des', '\u2581documents', '!', '\u2581des', '\u2581documents', '!', '\u2581s', '\u2019', '\u00e9cri', 'a', '\u2581lady', '\u2581Helena', '.', '\u2581--', '\u2581Seulement', ',', '\u2581r\u00e9pondit', '\u2581Glen', 'ar', 'van', ',', '\u2581ils', '\u2581para', 'issent', '\u2581\u00eatre', '\u2581rong', '\u00e9s', '\u2581par', '\u2581l', '\u2019', 'humidit\u00e9', ',', '\u2581et', '\u2581il', '\u2581est', '\u2581impossible', '\u2581de', '\u2581les', '\u2581retirer', ',', '\u2581car', '\u2581ils', '\u2581adh', '\u00e8rent', '\u2581aux', '\u2581par', 'ois', '\u2581de', '\u2581la', '\u2581bouteille', '.']"}], "[\n  [\"--\", null],\n  [\"\u2581Des\", null],\n  [\"\u2581documents\", \"\u0414\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u044b\"],\n  [\"!\", null],\n  [\"\u2581des\", null],\n  [\"\u2581documents\", \"\u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u044b\"],\n  [\"!\", null],\n  [\"\u2581s\", null],\n  [\"'\", null],\n  [\"\u00e9cri\", null],\n  [\"a\", null],\n  [\"\u2581lady\", \"\u043b\u0435\u0434\u0438\"],\n  [\"\u2581Helena\", \"\u0415\u043b\u0435\u043d\u0430\"],\n  [\".\", null],\n  [\"\u2581--\", null],\n  [\"\u2581Seulement\", \"\u0422\u043e\u043b\u044c\u043a\u043e\"],\n  [\",\", null],\n  [\"\u2581r\u00e9pondit\", \"\u043e\u0442\u0432\u0435\u0442\u0438\u043b\"],\n  [\"\u2581Glen\", \"\u0413\u043b\u0435\u043d\"],\n  [\"ar\", null],\n  [\"van\", null],\n  [\",\", null],\n  [\"\u2581ils\", \"\u043e\u043d\u0438\"],\n  [\"\u2581para\", null],\n  [\"issent\", null],\n  [\"\u2581\u00eatre\", \"\u043a\u0430\u0436\u0443\u0442\u0441\u044f\"],\n  [\"\u2581rong\", null],\n  [\"\u00e9s\", null],\n  [\"\u2581par\", \"\u043e\u0442\"],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"humidit\u00e9\", \"\u0432\u043b\u0430\u0433\u0438\"],\n  [\",\", null],\n  [\"\u2581et\", \"\u0438\"],\n  [\"\u2581il\", \"\u043d\u0435\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\"],\n  [\"\u2581est\", null],\n  [\"\u2581impossible\", null],\n  [\"\u2581de\", null],\n  [\"\u2581les\", \"\u0438\u0445\"],\n  [\"\u2581retirer\", \"\u0438\u0437\u0432\u043b\u0435\u0447\u044c\"],\n  [\",\", null],\n  [\"\u2581car\", \"\u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e\"],\n  [\"\u2581ils\", \"\u043e\u043d\u0438\"],\n  [\"\u2581adh\", null],\n  [\"\u00e8rent\", null],\n  [\"\u2581aux\", \"\u043a\"],\n  [\"\u2581par\", null],\n  [\"ois\", null],\n  [\"\u2581de\", null],\n  [\"\u2581la\", null],\n  [\"\u2581bouteille\", \"\u0431\u0443\u0442\u044b\u043b\u043a\u0438\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nJ'\u00e9tais dans une petite chambre. Pr\u00e8s du feu se trouvait une table ronde; sur un fauteuil \u00e0 dos \u00e9lev\u00e9 et de forme antique \u00e9tait assise la plus propre et la plus mignonne petite dame qu'on puisse imaginer.\n\n\n---=== TOKENIZED TEXT ===---\n\n['J', \"'\", '\u00e9tais', '\u2581dans', '\u2581une', '\u2581petite', '\u2581chambre', '.', '\u2581Pr', '\u00e8s', '\u2581du', '\u2581feu', '\u2581se', '\u2581trouvait', '\u2581une', '\u2581table', '\u2581ronde', ';', '\u2581sur', '\u2581un', '\u2581fauteuil', '\u2581\u00e0', '\u2581dos', '\u2581\u00e9lev\u00e9', '\u2581et', '\u2581de', '\u2581forme', '\u2581antique', '\u2581\u00e9tait', '\u2581assise', '\u2581la', '\u2581plus', '\u2581propre', '\u2581et', '\u2581la', '\u2581plus', '\u2581m', 'ign', 'onne', '\u2581petite', '\u2581dame', '\u2581qu', \"'\", 'on', '\u2581puisse', '\u2581imagin', 'er', '.']"}], "[\n[\"J\", \"I\"],\n[\"'\", null],\n[\"\u00e9tais\", null],\n[\"\u2581dans\", null],\n[\"\u2581une\", null],\n[\"\u2581petite\", \"small\"],\n[\"\u2581chambre\", \"room\"],\n[\".\", null],\n[\"\u2581Pr\", \"Near\"],\n[\"\u00e8s\", null],\n[\"\u2581du\", null],\n[\"\u2581feu\", \"fire\"],\n[\"\u2581se\", null],\n[\"\u2581trouvait\", \"was\"],\n[\"\u2581une\", null],\n[\"\u2581table\", \"table\"],\n[\"\u2581ronde\", \"round\"],\n[\";\", null],\n[\"\u2581sur\", null],\n[\"\u2581un\", null],\n[\"\u2581fauteuil\", \"armchair\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581dos\", \"back\"],\n[\"\u2581\u00e9lev\u00e9\", \"high\"],\n[\"\u2581et\", null],\n[\"\u2581de\", null],\n[\"\u2581forme\", \"shape\"],\n[\"\u2581antique\", \"antique\"],\n[\"\u2581\u00e9tait\", null],\n[\"\u2581assise\", \"seated\"],\n[\"\u2581la\", null],\n[\"\u2581plus\", null],\n[\"\u2581propre\", \"neatest\"],\n[\"\u2581et\", null],\n[\"\u2581la\", null],\n[\"\u2581plus\", null],\n[\"\u2581m\", \"daintiest\"],\n[\"ign\", null],\n[\"onne\", null],\n[\"\u2581petite\", \"little\"],\n[\"\u2581dame\", \"lady\"],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"on\", null],\n[\"\u2581puisse\", \"could\"],\n[\"\u2581imagin\", \"imagine\"],\n[\"er\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nJ'\u00e9tais dans une petite chambre. Pr\u00e8s du feu se trouvait une table ronde; sur un fauteuil \u00e0 dos \u00e9lev\u00e9 et de forme antique \u00e9tait assise la plus propre et la plus mignonne petite dame qu'on puisse imaginer.\n\n\n---=== TOKENIZED TEXT ===---\n\n['J', \"'\", '\u00e9tais', '\u2581dans', '\u2581une', '\u2581petite', '\u2581chambre', '.', '\u2581Pr', '\u00e8s', '\u2581du', '\u2581feu', '\u2581se', '\u2581trouvait', '\u2581une', '\u2581table', '\u2581ronde', ';', '\u2581sur', '\u2581un', '\u2581fauteuil', '\u2581\u00e0', '\u2581dos', '\u2581\u00e9lev\u00e9', '\u2581et', '\u2581de', '\u2581forme', '\u2581antique', '\u2581\u00e9tait', '\u2581assise', '\u2581la', '\u2581plus', '\u2581propre', '\u2581et', '\u2581la', '\u2581plus', '\u2581m', 'ign', 'onne', '\u2581petite', '\u2581dame', '\u2581qu', \"'\", 'on', '\u2581puisse', '\u2581imagin', 'er', '.']"}], "[\n[\"J\", \"\u042f\"],\n[\"'\", null],\n[\"\u00e9tais\", null],\n[\"\u2581dans\", null],\n[\"\u2581une\", null],\n[\"\u2581petite\", \"\u043c\u0430\u043b\u0435\u043d\u044c\u043a\u043e\u0439\"],\n[\"\u2581chambre\", \"\u043a\u043e\u043c\u043d\u0430\u0442\u0435\"],\n[\".\", null],\n[\"\u2581Pr\", \"\u0412\u043e\u0437\u043b\u0435\"],\n[\"\u00e8s\", null],\n[\"\u2581du\", null],\n[\"\u2581feu\", \"\u043e\u0433\u043d\u044f\"],\n[\"\u2581se\", null],\n[\"\u2581trouvait\", \"\u043d\u0430\u0445\u043e\u0434\u0438\u043b\u0441\u044f\"],\n[\"\u2581une\", null],\n[\"\u2581table\", \"\u0441\u0442\u043e\u043b\"],\n[\"\u2581ronde\", \"\u043a\u0440\u0443\u0433\u043b\u044b\u0439\"],\n[\";\", null],\n[\"\u2581sur\", null],\n[\"\u2581un\", null],\n[\"\u2581fauteuil\", \"\u043a\u0440\u0435\u0441\u043b\u0435\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581dos\", \"\u0441\u043f\u0438\u043d\u043a\u043e\u0439\"],\n[\"\u2581\u00e9lev\u00e9\", \"\u0432\u044b\u0441\u043e\u043a\u043e\u0439\"],\n[\"\u2581et\", null],\n[\"\u2581de\", null],\n[\"\u2581forme\", \"\u0444\u043e\u0440\u043c\u044b\"],\n[\"\u2581antique\", \"\u0441\u0442\u0430\u0440\u0438\u043d\u043d\u043e\u0439\"],\n[\"\u2581\u00e9tait\", null],\n[\"\u2581assise\", \"\u0441\u0438\u0434\u0435\u043b\u0430\"],\n[\"\u2581la\", null],\n[\"\u2581plus\", null],\n[\"\u2581propre\", \"\u043e\u043f\u0440\u044f\u0442\u043d\u0430\u044f\"],\n[\"\u2581et\", null],\n[\"\u2581la\", null],\n[\"\u2581plus\", null],\n[\"\u2581m\", \"\u043c\u0438\u043b\u043e\u0432\u0438\u0434\u043d\u0430\u044f\"],\n[\"ign\", null],\n[\"onne\", null],\n[\"\u2581petite\", \"\u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0430\u044f\"],\n[\"\u2581dame\", \"\u0434\u0430\u043c\u0430\"],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"on\", null],\n[\"\u2581puisse\", null],\n[\"\u2581imagin\", \"\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u044c\"],\n[\"er\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nJohn n'avait beaucoup d'affection ni pour sa m\u00e8re ni pour ses soeurs.\n\n\n---=== TOKENIZED TEXT ===---\n\n['John', '\u2581n', \"'\", 'avait', '\u2581beaucoup', '\u2581d', \"'\", 'affection', '\u2581ni', '\u2581pour', '\u2581sa', '\u2581m\u00e8re', '\u2581ni', '\u2581pour', '\u2581ses', '\u2581so', 'eurs', '.']"}], "[\n  [\"John\", \"John\"],\n  [\"\u2581n\", null],\n  [\"'\", null],\n  [\"avait\", \"had\"],\n  [\"\u2581beaucoup\", \"much\"],\n  [\"\u2581d\", null],\n  [\"'\", null],\n  [\"affection\", \"affection\"],\n  [\"\u2581ni\", null],\n  [\"\u2581pour\", null],\n  [\"\u2581sa\", null],\n  [\"\u2581m\u00e8re\", \"mother\"],\n  [\"\u2581ni\", null],\n  [\"\u2581pour\", null],\n  [\"\u2581ses\", null],\n  [\"\u2581so\", \"sisters\"],\n  [\"eurs\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nJohn n'avait beaucoup d'affection ni pour sa m\u00e8re ni pour ses soeurs.\n\n\n---=== TOKENIZED TEXT ===---\n\n['John', '\u2581n', \"'\", 'avait', '\u2581beaucoup', '\u2581d', \"'\", 'affection', '\u2581ni', '\u2581pour', '\u2581sa', '\u2581m\u00e8re', '\u2581ni', '\u2581pour', '\u2581ses', '\u2581so', 'eurs', '.']"}], "[\n  [\"John\", \"\u0414\u0436\u043e\u043d\"],\n  [\"\u2581n\", null],\n  [\"'\", null],\n  [\"avait\", \"\u0438\u0441\u043f\u044b\u0442\u044b\u0432\u0430\u043b\"],\n  [\"\u2581beaucoup\", null],\n  [\"\u2581d\", null],\n  [\"'\", null],\n  [\"affection\", \"\u043f\u0440\u0438\u0432\u044f\u0437\u0430\u043d\u043d\u043e\u0441\u0442\u0438\"],\n  [\"\u2581ni\", null],\n  [\"\u2581pour\", null],\n  [\"\u2581sa\", null],\n  [\"\u2581m\u00e8re\", \"\u043c\u0430\u0442\u0435\u0440\u0438\"],\n  [\"\u2581ni\", null],\n  [\"\u2581pour\", null],\n  [\"\u2581ses\", null],\n  [\"\u2581so\", \"\u0441\u0435\u0441\u0442\u0440\u0430\u043c\"],\n  [\"eurs\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n\u00ab Mais non, je rampe encore ; je m\u2019\u00e9corche la face et les genoux aux cailloux de la voie souterraine.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00ab', '\u2581Mais', '\u2581non', ',', '\u2581je', '\u2581ram', 'pe', '\u2581encore', '\u2581;', '\u2581je', '\u2581m', '\u2019', '\u00e9', 'cor', 'che', '\u2581la', '\u2581face', '\u2581et', '\u2581les', '\u2581genoux', '\u2581aux', '\u2581ca', 'ill', 'oux', '\u2581de', '\u2581la', '\u2581voie', '\u2581sou', 'ter', 'raine', '.']"}], "[\n[\"\u00ab\", null],\n[\"\u2581Mais\", \"But\"],\n[\"\u2581non\", null],\n[\",\", null],\n[\"\u2581je\", null],\n[\"\u2581ram\", \"crawl\"],\n[\"pe\", null],\n[\"\u2581encore\", \"still\"],\n[\"\u2581;\", null],\n[\"\u2581je\", null],\n[\"\u2581m\", null],\n[\"'\", null],\n[\"\u00e9\", null],\n[\"cor\", \"scratch\"],\n[\"che\", null],\n[\"\u2581la\", null],\n[\"\u2581face\", \"face\"],\n[\"\u2581et\", null],\n[\"\u2581les\", null],\n[\"\u2581genoux\", \"knees\"],\n[\"\u2581aux\", null],\n[\"\u2581ca\", \"stones\"],\n[\"ill\", null],\n[\"oux\", null],\n[\"\u2581de\", null],\n[\"\u2581la\", null],\n[\"\u2581voie\", \"path\"],\n[\"\u2581sou\", \"underground\"],\n[\"ter\", null],\n[\"raine\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n\u00ab Mais non, je rampe encore ; je m\u2019\u00e9corche la face et les genoux aux cailloux de la voie souterraine.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00ab', '\u2581Mais', '\u2581non', ',', '\u2581je', '\u2581ram', 'pe', '\u2581encore', '\u2581;', '\u2581je', '\u2581m', '\u2019', '\u00e9', 'cor', 'che', '\u2581la', '\u2581face', '\u2581et', '\u2581les', '\u2581genoux', '\u2581aux', '\u2581ca', 'ill', 'oux', '\u2581de', '\u2581la', '\u2581voie', '\u2581sou', 'ter', 'raine', '.']"}], "[\n[\"\u00ab\", null],\n[\"\u2581Mais\", \"\u041d\u043e\"],\n[\"\u2581non\", null],\n[\",\", null],\n[\"\u2581je\", null],\n[\"\u2581ram\", \"\u043f\u043e\u043b\u0437\u0430\u0442\u044c\"],\n[\"pe\", null],\n[\"\u2581encore\", \"\u0432\u0441\u0435 \u0435\u0449\u0435\"],\n[\"\u2581;\", null],\n[\"\u2581je\", null],\n[\"\u2581m\", \"\u043e\u0431\u0434\u0438\u0440\u0430\u0442\u044c\"],\n[\"'\", null],\n[\"\u00e9\", null],\n[\"cor\", null],\n[\"che\", null],\n[\"\u2581la\", null],\n[\"\u2581face\", \"\u043b\u0438\u0446\u043e\"],\n[\"\u2581et\", null],\n[\"\u2581les\", null],\n[\"\u2581genoux\", \"\u043a\u043e\u043b\u0435\u043d\u0438\"],\n[\"\u2581aux\", null],\n[\"\u2581ca\", \"\u043a\u0430\u043c\u043d\u0438\"],\n[\"ill\", null],\n[\"oux\", null],\n[\"\u2581de\", null],\n[\"\u2581la\", null],\n[\"\u2581voie\", \"\u043f\u0443\u0442\u044c\"],\n[\"\u2581sou\", \"\u043f\u043e\u0434\u0437\u0435\u043c\u043d\u044b\u0439\"],\n[\"ter\", null],\n[\"raine\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nTout salut semblait impossible, quand la voix du major se fit entendre.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Tout', '\u2581salut', '\u2581semblait', '\u2581impossible', ',', '\u2581quand', '\u2581la', '\u2581voix', '\u2581du', '\u2581major', '\u2581se', '\u2581fit', '\u2581entendre', '.']"}], "[\n    [\"Tout\", \"All\"],\n    [\"\u2581salut\", \"salvation\"],\n    [\"\u2581semblait\", \"seemed\"],\n    [\"\u2581impossible\", \"impossible\"],\n    [\",\", null],\n    [\"\u2581quand\", \"when\"],\n    [\"\u2581la\", null],\n    [\"\u2581voix\", \"voice\"],\n    [\"\u2581du\", null],\n    [\"\u2581major\", \"major\"],\n    [\"\u2581se\", null],\n    [\"\u2581fit\", \"made\"],\n    [\"\u2581entendre\", \"heard\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nTout salut semblait impossible, quand la voix du major se fit entendre.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Tout', '\u2581salut', '\u2581semblait', '\u2581impossible', ',', '\u2581quand', '\u2581la', '\u2581voix', '\u2581du', '\u2581major', '\u2581se', '\u2581fit', '\u2581entendre', '.']"}], "[\n  [\"Tout\", \"\u0412\u0441\u044f\u043a\u043e\u0435\"],\n  [\"\u2581salut\", \"\u0441\u043f\u0430\u0441\u0435\u043d\u0438\u0435\"],\n  [\"\u2581semblait\", \"\u043a\u0430\u0437\u0430\u043b\u043e\u0441\u044c\"],\n  [\"\u2581impossible\", \"\u043d\u0435\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u043c\"],\n  [\",\", null],\n  [\"\u2581quand\", \"\u043a\u043e\u0433\u0434\u0430\"],\n  [\"\u2581la\", null],\n  [\"\u2581voix\", \"\u0433\u043e\u043b\u043e\u0441\"],\n  [\"\u2581du\", null],\n  [\"\u2581major\", \"\u043c\u0430\u0439\u043e\u0440\u0430\"],\n  [\"\u2581se\", null],\n  [\"\u2581fit\", \"\u0440\u0430\u0437\u0434\u0430\u043b\u0441\u044f\"],\n  [\"\u2581entendre\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nMais Paganel lui apprit que, sous cette latitude, les sauvages fr\u00e9quentaient principalement les plaines du Murray, situ\u00e9es \u00e0 cent milles dans l\u2019est.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Mais', '\u2581Pag', 'anel', '\u2581lui', '\u2581app', 'rit', '\u2581que', ',', '\u2581sous', '\u2581cette', '\u2581latitude', ',', '\u2581les', '\u2581sauvages', '\u2581fr\u00e9qu', 'enta', 'ient', '\u2581principalement', '\u2581les', '\u2581pla', 'ines', '\u2581du', '\u2581Murray', ',', '\u2581situ', '\u00e9es', '\u2581\u00e0', '\u2581cent', '\u2581mill', 'es', '\u2581dans', '\u2581l', '\u2019', 'est', '.']"}], "[\n    [\"Mais\", \"But\"],\n    [\"\u2581Pag\", \"Paganel\"],\n    [\"anel\", null],\n    [\"\u2581lui\", null],\n    [\"\u2581app\", \"informed\"],\n    [\"rit\", null],\n    [\"\u2581que\", null],\n    [\",\", null],\n    [\"\u2581sous\", \"under\"],\n    [\"\u2581cette\", \"this\"],\n    [\"\u2581latitude\", \"latitude\"],\n    [\",\", null],\n    [\"\u2581les\", null],\n    [\"\u2581sauvages\", \"savages\"],\n    [\"\u2581fr\u00e9qu\", \"frequented\"],\n    [\"enta\", null],\n    [\"ient\", null],\n    [\"\u2581principalement\", \"mainly\"],\n    [\"\u2581les\", null],\n    [\"\u2581pla\", \"plains\"],\n    [\"ines\", null],\n    [\"\u2581du\", \"of\"],\n    [\"\u2581Murray\", \"Murray\"],\n    [\",\", null],\n    [\"\u2581situ\", \"located\"],\n    [\"\u00e9es\", null],\n    [\"\u2581\u00e0\", null],\n    [\"\u2581cent\", \"hundred\"],\n    [\"\u2581mill\", \"miles\"],\n    [\"es\", null],\n    [\"\u2581dans\", \"in\"],\n    [\"\u2581l\", null],\n    [\"'\", null],\n    [\"est\", \"east\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nMais Paganel lui apprit que, sous cette latitude, les sauvages fr\u00e9quentaient principalement les plaines du Murray, situ\u00e9es \u00e0 cent milles dans l\u2019est.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Mais', '\u2581Pag', 'anel', '\u2581lui', '\u2581app', 'rit', '\u2581que', ',', '\u2581sous', '\u2581cette', '\u2581latitude', ',', '\u2581les', '\u2581sauvages', '\u2581fr\u00e9qu', 'enta', 'ient', '\u2581principalement', '\u2581les', '\u2581pla', 'ines', '\u2581du', '\u2581Murray', ',', '\u2581situ', '\u00e9es', '\u2581\u00e0', '\u2581cent', '\u2581mill', 'es', '\u2581dans', '\u2581l', '\u2019', 'est', '.']"}], "[\n[\"Mais\", \"\u041d\u043e\"],\n[\"\u2581Pag\", \"\u041f\u0430\u0433\u0430\u043d\u0435\u043b\u044c\"],\n[\"anel\", null],\n[\"\u2581lui\", null],\n[\"\u2581app\", \"\u0441\u043e\u043e\u0431\u0449\u0438\u043b\"],\n[\"rit\", null],\n[\"\u2581que\", null],\n[\",\", null],\n[\"\u2581sous\", \"\u043f\u043e\u0434\"],\n[\"\u2581cette\", \"\u044d\u0442\u043e\u0439\"],\n[\"\u2581latitude\", \"\u0448\u0438\u0440\u043e\u0442\u043e\u0439\"],\n[\",\", null],\n[\"\u2581les\", null],\n[\"\u2581sauvages\", \"\u0434\u0438\u043a\u0430\u0440\u0438\"],\n[\"\u2581fr\u00e9qu\", \"\u0447\u0430\u0441\u0442\u043e\"],\n[\"enta\", null],\n[\"ient\", null],\n[\"\u2581principalement\", \"\u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c\"],\n[\"\u2581les\", null],\n[\"\u2581pla\", \"\u0440\u0430\u0432\u043d\u0438\u043d\u044b\"],\n[\"ines\", null],\n[\"\u2581du\", null],\n[\"\u2581Murray\", \"\u041c\u0443\u0440\u0440\u0435\u044f\"],\n[\",\", null],\n[\"\u2581situ\", \"\u0440\u0430\u0441\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u043d\u044b\u0435\"],\n[\"\u00e9es\", null],\n[\"\u2581\u00e0\", \"\u0432\"],\n[\"\u2581cent\", \"\u0441\u0442\u0430\"],\n[\"\u2581mill\", \"\u043c\u0438\u043b\u044f\u0445\"],\n[\"es\", null],\n[\"\u2581dans\", \"\u043d\u0430\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"est\", \"\u0432\u043e\u0441\u0442\u043e\u043a\u0435\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n\u2013 Est-ce que vous ne trouvez pas, r\u00e9pondit l\u2019archidiacre d\u2019un air de profonde r\u00e9flexion, que l\u2019habit de ces cavaliers que nous venons de voir est plus beau que le v\u00f4tre et que le mien ? \u00bb Gringoire hocha la t\u00eate.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2013', '\u2581Est', '-', 'ce', '\u2581que', '\u2581vous', '\u2581ne', '\u2581trouvez', '\u2581pas', ',', '\u2581r\u00e9pondit', '\u2581l', '\u2019', 'arch', 'idi', 'acre', '\u2581d', '\u2019', 'un', '\u2581air', '\u2581de', '\u2581profonde', '\u2581r\u00e9flexion', ',', '\u2581que', '\u2581l', '\u2019', 'habit', '\u2581de', '\u2581ces', '\u2581cav', 'aliers', '\u2581que', '\u2581nous', '\u2581venons', '\u2581de', '\u2581voir', '\u2581est', '\u2581plus', '\u2581beau', '\u2581que', '\u2581le', '\u2581v\u00f4tre', '\u2581et', '\u2581que', '\u2581le', '\u2581mien', '\u2581?', '\u2581\u00bb', '\u2581G', 'ring', 'oire', '\u2581h', 'ocha', '\u2581la', '\u2581t\u00eate', '.']"}], "[\n  [\"-\", null],\n  [\"\u2581Est\", \"Don't\"],\n  [\"-\", null],\n  [\"ce\", null],\n  [\"\u2581que\", null],\n  [\"\u2581vous\", \"you\"],\n  [\"\u2581ne\", null],\n  [\"\u2581trouvez\", \"think\"],\n  [\"\u2581pas\", null],\n  [\",\", null],\n  [\"\u2581r\u00e9pondit\", \"replied\"],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"arch\", \"archdeacon\"],\n  [\"idi\", null],\n  [\"acre\", null],\n  [\"\u2581d\", null],\n  [\"'\", null],\n  [\"un\", null],\n  [\"\u2581air\", \"air\"],\n  [\"\u2581de\", null],\n  [\"\u2581profonde\", \"deep\"],\n  [\"\u2581r\u00e9flexion\", \"reflection\"],\n  [\",\", null],\n  [\"\u2581que\", \"that\"],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"habit\", \"clothing\"],\n  [\"\u2581de\", null],\n  [\"\u2581ces\", \"these\"],\n  [\"\u2581cav\", \"horsemen\"],\n  [\"aliers\", null],\n  [\"\u2581que\", \"that\"],\n  [\"\u2581nous\", \"we\"],\n  [\"\u2581venons\", \"just\"],\n  [\"\u2581de\", null],\n  [\"\u2581voir\", \"saw\"],\n  [\"\u2581est\", \"is\"],\n  [\"\u2581plus\", \"more\"],\n  [\"\u2581beau\", \"beautiful\"],\n  [\"\u2581que\", \"than\"],\n  [\"\u2581le\", null],\n  [\"\u2581v\u00f4tre\", \"yours\"],\n  [\"\u2581et\", \"and\"],\n  [\"\u2581que\", null],\n  [\"\u2581le\", null],\n  [\"\u2581mien\", \"mine\"],\n  [\"\u2581?\", null],\n  [\"\u2581\u00bb\", null],\n  [\"\u2581G\", \"Gringoire\"],\n  [\"ring\", null],\n  [\"oire\", null],\n  [\"\u2581h\", \"nodded\"],\n  [\"ocha\", null],\n  [\"\u2581la\", null],\n  [\"\u2581t\u00eate\", \"head\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n\u2013 Est-ce que vous ne trouvez pas, r\u00e9pondit l\u2019archidiacre d\u2019un air de profonde r\u00e9flexion, que l\u2019habit de ces cavaliers que nous venons de voir est plus beau que le v\u00f4tre et que le mien ? \u00bb Gringoire hocha la t\u00eate.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2013', '\u2581Est', '-', 'ce', '\u2581que', '\u2581vous', '\u2581ne', '\u2581trouvez', '\u2581pas', ',', '\u2581r\u00e9pondit', '\u2581l', '\u2019', 'arch', 'idi', 'acre', '\u2581d', '\u2019', 'un', '\u2581air', '\u2581de', '\u2581profonde', '\u2581r\u00e9flexion', ',', '\u2581que', '\u2581l', '\u2019', 'habit', '\u2581de', '\u2581ces', '\u2581cav', 'aliers', '\u2581que', '\u2581nous', '\u2581venons', '\u2581de', '\u2581voir', '\u2581est', '\u2581plus', '\u2581beau', '\u2581que', '\u2581le', '\u2581v\u00f4tre', '\u2581et', '\u2581que', '\u2581le', '\u2581mien', '\u2581?', '\u2581\u00bb', '\u2581G', 'ring', 'oire', '\u2581h', 'ocha', '\u2581la', '\u2581t\u00eate', '.']"}], "[\n[\"\u2013\", null],\n[\"\u2581Est\", \"\u0420\u0430\u0437\u0432\u0435\"],\n[\"-\", null],\n[\"ce\", null],\n[\"\u2581que\", null],\n[\"\u2581vous\", \"\u0432\u044b\"],\n[\"\u2581ne\", null],\n[\"\u2581trouvez\", \"\u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0435\"],\n[\"\u2581pas\", null],\n[\",\", null],\n[\"\u2581r\u00e9pondit\", \"\u043e\u0442\u0432\u0435\u0442\u0438\u043b\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"arch\", null],\n[\"idi\", null],\n[\"acre\", \"\u0430\u0440\u0445\u0438\u0434\u044c\u044f\u043a\u043e\u043d\"],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"un\", null],\n[\"\u2581air\", \"\u0432\u0438\u0434\u043e\u043c\"],\n[\"\u2581de\", null],\n[\"\u2581profonde\", \"\u0433\u043b\u0443\u0431\u043e\u043a\u043e\u0439\"],\n[\"\u2581r\u00e9flexion\", \"\u0437\u0430\u0434\u0443\u043c\u0447\u0438\u0432\u043e\u0441\u0442\u0438\"],\n[\",\", null],\n[\"\u2581que\", \"\u0447\u0442\u043e\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"habit\", \"\u043e\u0434\u0435\u0436\u0434\u0430\"],\n[\"\u2581de\", null],\n[\"\u2581ces\", \"\u044d\u0442\u0438\u0445\"],\n[\"\u2581cav\", null],\n[\"aliers\", \"\u0432\u0441\u0430\u0434\u043d\u0438\u043a\u043e\u0432\"],\n[\"\u2581que\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u0445\"],\n[\"\u2581nous\", \"\u043c\u044b\"],\n[\"\u2581venons\", \"\u0442\u043e\u043b\u044c\u043a\u043e\"],\n[\"\u2581de\", \"\u0447\u0442\u043e\"],\n[\"\u2581voir\", \"\u0432\u0438\u0434\u0435\u043b\u0438\"],\n[\"\u2581est\", null],\n[\"\u2581plus\", \"\u043a\u0440\u0430\u0441\u0438\u0432\u0435\u0435\"],\n[\"\u2581beau\", null],\n[\"\u2581que\", \"\u0447\u0435\u043c\"],\n[\"\u2581le\", null],\n[\"\u2581v\u00f4tre\", \"\u0432\u0430\u0448\u0430\"],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581que\", \"\u0447\u0435\u043c\"],\n[\"\u2581le\", null],\n[\"\u2581mien\", \"\u043c\u043e\u044f\"],\n[\"\u2581?\", null],\n[\"\u2581\u00bb\", null],\n[\"\u2581G\", null],\n[\"ring\", null],\n[\"oire\", \"\u0413\u0440\u0435\u043d\u0433\u0443\u0430\u0440\"],\n[\"\u2581h\", null],\n[\"ocha\", \"\u043f\u043e\u043a\u0430\u0447\u0430\u043b\"],\n[\"\u2581la\", null],\n[\"\u2581t\u00eate\", \"\u0433\u043e\u043b\u043e\u0432\u043e\u0439\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nEn moins de cinq minutes, ils atteignirent le sommet du mont; l\u00e0, ils se retourn\u00e8rent afin de juger la situation et de prendre une direction qui p\u00fbt d\u00e9pister les maoris.\n\n\n---=== TOKENIZED TEXT ===---\n\n['En', '\u2581moins', '\u2581de', '\u2581cinq', '\u2581minutes', ',', '\u2581ils', '\u2581atte', 'ign', 'irent', '\u2581le', '\u2581sommet', '\u2581du', '\u2581mont', ';', '\u2581l\u00e0', ',', '\u2581ils', '\u2581se', '\u2581retour', 'n', '\u00e8rent', '\u2581afin', '\u2581de', '\u2581juger', '\u2581la', '\u2581situation', '\u2581et', '\u2581de', '\u2581prendre', '\u2581une', '\u2581direction', '\u2581qui', '\u2581p', '\u00fbt', '\u2581d\u00e9p', 'ister', '\u2581les', '\u2581ma', 'oris', '.']"}], "[\n  [\"En\", null],\n  [\"\u2581moins\", \"In less\"],\n  [\"\u2581de\", null],\n  [\"\u2581cinq\", \"five\"],\n  [\"\u2581minutes\", \"minutes\"],\n  [\",\", null],\n  [\"\u2581ils\", \"they\"],\n  [\"\u2581atte\", \"reached\"],\n  [\"ign\", null],\n  [\"irent\", null],\n  [\"\u2581le\", null],\n  [\"\u2581sommet\", \"summit\"],\n  [\"\u2581du\", null],\n  [\"\u2581mont\", \"mountain\"],\n  [\";\", null],\n  [\"\u2581l\u00e0\", \"there\"],\n  [\",\", null],\n  [\"\u2581ils\", \"they\"],\n  [\"\u2581se\", null],\n  [\"\u2581retour\", \"turned\"],\n  [\"n\", null],\n  [\"\u00e8rent\", null],\n  [\"\u2581afin\", \"in order\"],\n  [\"\u2581de\", null],\n  [\"\u2581juger\", \"assess\"],\n  [\"\u2581la\", null],\n  [\"\u2581situation\", \"situation\"],\n  [\"\u2581et\", \"and\"],\n  [\"\u2581de\", null],\n  [\"\u2581prendre\", \"choose\"],\n  [\"\u2581une\", null],\n  [\"\u2581direction\", \"direction\"],\n  [\"\u2581qui\", \"that\"],\n  [\"\u2581p\", \"could\"],\n  [\"\u00fbt\", null],\n  [\"\u2581d\u00e9p\", \"throw off\"],\n  [\"ister\", null],\n  [\"\u2581les\", null],\n  [\"\u2581ma\", \"Maoris\"],\n  [\"oris\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nEn moins de cinq minutes, ils atteignirent le sommet du mont; l\u00e0, ils se retourn\u00e8rent afin de juger la situation et de prendre une direction qui p\u00fbt d\u00e9pister les maoris.\n\n\n---=== TOKENIZED TEXT ===---\n\n['En', '\u2581moins', '\u2581de', '\u2581cinq', '\u2581minutes', ',', '\u2581ils', '\u2581atte', 'ign', 'irent', '\u2581le', '\u2581sommet', '\u2581du', '\u2581mont', ';', '\u2581l\u00e0', ',', '\u2581ils', '\u2581se', '\u2581retour', 'n', '\u00e8rent', '\u2581afin', '\u2581de', '\u2581juger', '\u2581la', '\u2581situation', '\u2581et', '\u2581de', '\u2581prendre', '\u2581une', '\u2581direction', '\u2581qui', '\u2581p', '\u00fbt', '\u2581d\u00e9p', 'ister', '\u2581les', '\u2581ma', 'oris', '.']"}], "[\n[\"En\", null],\n[\"\u2581moins\", \"\u041c\u0435\u043d\u0435\u0435\"],\n[\"\u2581de\", null],\n[\"\u2581cinq\", \"\u043f\u044f\u0442\u0438\"],\n[\"\u2581minutes\", \"\u043c\u0438\u043d\u0443\u0442\"],\n[\",\", null],\n[\"\u2581ils\", \"\u043e\u043d\u0438\"],\n[\"\u2581atte\", \"\u0434\u043e\u0441\u0442\u0438\u0433\u043b\u0438\"],\n[\"ign\", null],\n[\"irent\", null],\n[\"\u2581le\", null],\n[\"\u2581sommet\", \"\u0432\u0435\u0440\u0448\u0438\u043d\u044b\"],\n[\"\u2581du\", null],\n[\"\u2581mont\", \"\u0433\u043e\u0440\u044b\"],\n[\";\", null],\n[\"\u2581l\u00e0\", \"\u0442\u0430\u043c\"],\n[\",\", null],\n[\"\u2581ils\", \"\u043e\u043d\u0438\"],\n[\"\u2581se\", null],\n[\"\u2581retour\", \"\u043f\u043e\u0432\u0435\u0440\u043d\u0443\u043b\u0438\u0441\u044c\"],\n[\"n\", null],\n[\"\u00e8rent\", null],\n[\"\u2581afin\", \"\u0447\u0442\u043e\u0431\u044b\"],\n[\"\u2581de\", null],\n[\"\u2581juger\", \"\u043e\u0446\u0435\u043d\u0438\u0442\u044c\"],\n[\"\u2581la\", null],\n[\"\u2581situation\", \"\u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044e\"],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581de\", null],\n[\"\u2581prendre\", \"\u0432\u044b\u0431\u0440\u0430\u0442\u044c\"],\n[\"\u2581une\", null],\n[\"\u2581direction\", \"\u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435\"],\n[\"\u2581qui\", \"\u043a\u043e\u0442\u043e\u0440\u043e\u0435\"],\n[\"\u2581p\", \"\u043c\u043e\u0433\u043b\u043e\"],\n[\"\u00fbt\", null],\n[\"\u2581d\u00e9p\", \"\u043e\u0431\u043c\u0430\u043d\u0443\u0442\u044c\"],\n[\"ister\", null],\n[\"\u2581les\", null],\n[\"\u2581ma\", \"\u043c\u0430\u043e\u0440\u0438\"],\n[\"oris\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0422\u043e\u043b\u044c\u043a\u043e \u0432 \u0441\u0430\u043c\u043e\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435 \u0432\u0440\u0435\u043c\u044f, \u043f\u043e \u043f\u043e\u0432\u043e\u0434\u0443 \u0441\u0432\u043e\u0438\u0445 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0439 \u043a \u0410\u043d\u043d\u0435, \u0412\u0440\u043e\u043d\u0441\u043a\u0438\u0439 \u043d\u0430\u0447\u0438\u043d\u0430\u043b \u0447\u0443\u0432\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c, \u0447\u0442\u043e \u0441\u0432\u043e\u0434 \u0435\u0433\u043e \u043f\u0440\u0430\u0432\u0438\u043b \u043d\u0435 \u0432\u043f\u043e\u043b\u043d\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u043b \u0432\u0441\u0435 \u0443\u0441\u043b\u043e\u0432\u0438\u044f, \u0438 \u0432 \u0431\u0443\u0434\u0443\u0449\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u043b\u0438\u0441\u044c \u0442\u0440\u0443\u0434\u043d\u043e\u0441\u0442\u0438 \u0438 \u0441\u043e\u043c\u043d\u0435\u043d\u0438\u044f, \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0412\u0440\u043e\u043d\u0441\u043a\u0438\u0439 \u0443\u0436\u0435 \u043d\u0435 \u043d\u0430\u0445\u043e\u0434\u0438\u043b \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u044f\u0449\u0435\u0439 \u043d\u0438\u0442\u0438.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0422\u043e\u043b\u044c\u043a\u043e', '\u2581\u0432', '\u2581\u0441\u0430\u043c\u043e\u0435', '\u2581\u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435', '\u2581\u0432\u0440\u0435\u043c\u044f', ',', '\u2581\u043f\u043e', '\u2581\u043f\u043e\u0432\u043e\u0434\u0443', '\u2581\u0441\u0432\u043e\u0438\u0445', '\u2581\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0439', '\u2581\u043a', '\u2581\u0410\u043d', '\u043d\u0435', ',', '\u2581\u0412', '\u0440\u043e\u043d', '\u0441\u043a\u0438\u0439', '\u2581\u043d\u0430\u0447\u0438\u043d\u0430', '\u043b', '\u2581\u0447\u0443\u0432', '\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0441\u0432\u043e', '\u0434', '\u2581\u0435\u0433\u043e', '\u2581\u043f\u0440\u0430\u0432\u0438\u043b', '\u2581\u043d\u0435', '\u2581\u0432\u043f\u043e\u043b\u043d\u0435', '\u2581\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f', '\u043b', '\u2581\u0432\u0441\u0435', '\u2581\u0443\u0441\u043b\u043e\u0432\u0438\u044f', ',', '\u2581\u0438', '\u2581\u0432', '\u2581\u0431\u0443\u0434\u0443\u0449\u0435\u043c', '\u2581\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f', '\u043b\u0438\u0441\u044c', '\u2581\u0442\u0440\u0443\u0434', '\u043d\u043e\u0441\u0442\u0438', '\u2581\u0438', '\u2581\u0441\u043e\u043c', '\u043d\u0435\u043d\u0438\u044f', ',', '\u2581\u0432', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0445', '\u2581\u0412', '\u0440\u043e\u043d', '\u0441\u043a\u0438\u0439', '\u2581\u0443\u0436\u0435', '\u2581\u043d\u0435', '\u2581\u043d\u0430\u0445\u043e\u0434\u0438', '\u043b', '\u2581\u0440\u0443\u043a\u043e\u0432\u043e\u0434', '\u044f', '\u0449\u0435\u0439', '\u2581\u043d\u0438', '\u0442\u0438', '.']"}], "[\n[\"\u0422\u043e\u043b\u044c\u043a\u043e\", \"Only\"],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0441\u0430\u043c\u043e\u0435\", \"most\"],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435\", \"recent\"],\n[\"\u2581\u0432\u0440\u0435\u043c\u044f\", \"time\"],\n[\",\", null],\n[\"\u2581\u043f\u043e\", null],\n[\"\u2581\u043f\u043e\u0432\u043e\u0434\u0443\", \"regarding\"],\n[\"\u2581\u0441\u0432\u043e\u0438\u0445\", \"his\"],\n[\"\u2581\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0439\", \"relations\"],\n[\"\u2581\u043a\", null],\n[\"\u2581\u0410\u043d\", \"Anna\"],\n[\"\u043d\u0435\", null],\n[\",\", null],\n[\"\u2581\u0412\", \"Vronsky\"],\n[\"\u0440\u043e\u043d\", null],\n[\"\u0441\u043a\u0438\u0439\", null],\n[\"\u2581\u043d\u0430\u0447\u0438\u043d\u0430\", \"began\"],\n[\"\u043b\", null],\n[\"\u2581\u0447\u0443\u0432\", \"feel\"],\n[\"\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", \"that\"],\n[\"\u2581\u0441\u0432\u043e\", \"code\"],\n[\"\u0434\", null],\n[\"\u2581\u0435\u0433\u043e\", \"his\"],\n[\"\u2581\u043f\u0440\u0430\u0432\u0438\u043b\", \"rules\"],\n[\"\u2581\u043d\u0435\", \"not\"],\n[\"\u2581\u0432\u043f\u043e\u043b\u043d\u0435\", \"fully\"],\n[\"\u2581\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\", \"define\"],\n[\"\u043b\", null],\n[\"\u2581\u0432\u0441\u0435\", \"all\"],\n[\"\u2581\u0443\u0441\u043b\u043e\u0432\u0438\u044f\", \"conditions\"],\n[\",\", null],\n[\"\u2581\u0438\", \"and\"],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0431\u0443\u0434\u0443\u0449\u0435\u043c\", \"future\"],\n[\"\u2581\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\", \"presented\"],\n[\"\u043b\u0438\u0441\u044c\", null],\n[\"\u2581\u0442\u0440\u0443\u0434\", \"difficulties\"],\n[\"\u043d\u043e\u0441\u0442\u0438\", null],\n[\"\u2581\u0438\", \"and\"],\n[\"\u2581\u0441\u043e\u043c\", \"doubts\"],\n[\"\u043d\u0435\u043d\u0438\u044f\", null],\n[\",\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0445\", \"which\"],\n[\"\u2581\u0412\", \"Vronsky\"],\n[\"\u0440\u043e\u043d\", null],\n[\"\u0441\u043a\u0438\u0439\", null],\n[\"\u2581\u0443\u0436\u0435\", \"already\"],\n[\"\u2581\u043d\u0435\", \"not\"],\n[\"\u2581\u043d\u0430\u0445\u043e\u0434\u0438\", \"find\"],\n[\"\u043b\", null],\n[\"\u2581\u0440\u0443\u043a\u043e\u0432\u043e\u0434\", \"guiding\"],\n[\"\u044f\", null],\n[\"\u0449\u0435\u0439\", null],\n[\"\u2581\u043d\u0438\", \"thread\"],\n[\"\u0442\u0438\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u0422\u043e\u043b\u044c\u043a\u043e \u0432 \u0441\u0430\u043c\u043e\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435 \u0432\u0440\u0435\u043c\u044f, \u043f\u043e \u043f\u043e\u0432\u043e\u0434\u0443 \u0441\u0432\u043e\u0438\u0445 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0439 \u043a \u0410\u043d\u043d\u0435, \u0412\u0440\u043e\u043d\u0441\u043a\u0438\u0439 \u043d\u0430\u0447\u0438\u043d\u0430\u043b \u0447\u0443\u0432\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c, \u0447\u0442\u043e \u0441\u0432\u043e\u0434 \u0435\u0433\u043e \u043f\u0440\u0430\u0432\u0438\u043b \u043d\u0435 \u0432\u043f\u043e\u043b\u043d\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u043b \u0432\u0441\u0435 \u0443\u0441\u043b\u043e\u0432\u0438\u044f, \u0438 \u0432 \u0431\u0443\u0434\u0443\u0449\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u043b\u0438\u0441\u044c \u0442\u0440\u0443\u0434\u043d\u043e\u0441\u0442\u0438 \u0438 \u0441\u043e\u043c\u043d\u0435\u043d\u0438\u044f, \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0412\u0440\u043e\u043d\u0441\u043a\u0438\u0439 \u0443\u0436\u0435 \u043d\u0435 \u043d\u0430\u0445\u043e\u0434\u0438\u043b \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u044f\u0449\u0435\u0439 \u043d\u0438\u0442\u0438.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0422\u043e\u043b\u044c\u043a\u043e', '\u2581\u0432', '\u2581\u0441\u0430\u043c\u043e\u0435', '\u2581\u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435', '\u2581\u0432\u0440\u0435\u043c\u044f', ',', '\u2581\u043f\u043e', '\u2581\u043f\u043e\u0432\u043e\u0434\u0443', '\u2581\u0441\u0432\u043e\u0438\u0445', '\u2581\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0439', '\u2581\u043a', '\u2581\u0410\u043d', '\u043d\u0435', ',', '\u2581\u0412', '\u0440\u043e\u043d', '\u0441\u043a\u0438\u0439', '\u2581\u043d\u0430\u0447\u0438\u043d\u0430', '\u043b', '\u2581\u0447\u0443\u0432', '\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0441\u0432\u043e', '\u0434', '\u2581\u0435\u0433\u043e', '\u2581\u043f\u0440\u0430\u0432\u0438\u043b', '\u2581\u043d\u0435', '\u2581\u0432\u043f\u043e\u043b\u043d\u0435', '\u2581\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f', '\u043b', '\u2581\u0432\u0441\u0435', '\u2581\u0443\u0441\u043b\u043e\u0432\u0438\u044f', ',', '\u2581\u0438', '\u2581\u0432', '\u2581\u0431\u0443\u0434\u0443\u0449\u0435\u043c', '\u2581\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f', '\u043b\u0438\u0441\u044c', '\u2581\u0442\u0440\u0443\u0434', '\u043d\u043e\u0441\u0442\u0438', '\u2581\u0438', '\u2581\u0441\u043e\u043c', '\u043d\u0435\u043d\u0438\u044f', ',', '\u2581\u0432', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0445', '\u2581\u0412', '\u0440\u043e\u043d', '\u0441\u043a\u0438\u0439', '\u2581\u0443\u0436\u0435', '\u2581\u043d\u0435', '\u2581\u043d\u0430\u0445\u043e\u0434\u0438', '\u043b', '\u2581\u0440\u0443\u043a\u043e\u0432\u043e\u0434', '\u044f', '\u0449\u0435\u0439', '\u2581\u043d\u0438', '\u0442\u0438', '.']"}], "[\n[\"\u0422\u043e\u043b\u044c\u043a\u043e\", \"Seulement\"],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0441\u0430\u043c\u043e\u0435\", \"tout\"],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435\", \"r\u00e9cent\"],\n[\"\u2581\u0432\u0440\u0435\u043c\u044f\", \"temps\"],\n[\",\", null],\n[\"\u2581\u043f\u043e\", null],\n[\"\u2581\u043f\u043e\u0432\u043e\u0434\u0443\", \"concernant\"],\n[\"\u2581\u0441\u0432\u043e\u0438\u0445\", \"ses\"],\n[\"\u2581\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0439\", \"relations\"],\n[\"\u2581\u043a\", \"avec\"],\n[\"\u2581\u0410\u043d\", \"Anne\"],\n[\"\u043d\u0435\", null],\n[\",\", null],\n[\"\u2581\u0412\", \"Vronski\"],\n[\"\u0440\u043e\u043d\", null],\n[\"\u0441\u043a\u0438\u0439\", null],\n[\"\u2581\u043d\u0430\u0447\u0438\u043d\u0430\", \"commen\u00e7ait\"],\n[\"\u043b\", null],\n[\"\u2581\u0447\u0443\u0432\", \"sentir\"],\n[\"\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", \"que\"],\n[\"\u2581\u0441\u0432\u043e\", \"code\"],\n[\"\u0434\", null],\n[\"\u2581\u0435\u0433\u043e\", \"ses\"],\n[\"\u2581\u043f\u0440\u0430\u0432\u0438\u043b\", \"r\u00e8gles\"],\n[\"\u2581\u043d\u0435\", \"ne\"],\n[\"\u2581\u0432\u043f\u043e\u043b\u043d\u0435\", \"pas compl\u00e8tement\"],\n[\"\u2581\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\", \"d\u00e9finissait\"],\n[\"\u043b\", null],\n[\"\u2581\u0432\u0441\u0435\", \"toutes\"],\n[\"\u2581\u0443\u0441\u043b\u043e\u0432\u0438\u044f\", \"conditions\"],\n[\",\", null],\n[\"\u2581\u0438\", \"et\"],\n[\"\u2581\u0432\", \"dans\"],\n[\"\u2581\u0431\u0443\u0434\u0443\u0449\u0435\u043c\", \"avenir\"],\n[\"\u2581\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\", \"apparaissaient\"],\n[\"\u043b\u0438\u0441\u044c\", null],\n[\"\u2581\u0442\u0440\u0443\u0434\", \"difficult\u00e9s\"],\n[\"\u043d\u043e\u0441\u0442\u0438\", null],\n[\"\u2581\u0438\", \"et\"],\n[\"\u2581\u0441\u043e\u043c\", \"doutes\"],\n[\"\u043d\u0435\u043d\u0438\u044f\", null],\n[\",\", null],\n[\"\u2581\u0432\", \"dans\"],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0445\", \"lesquels\"],\n[\"\u2581\u0412\", \"Vronski\"],\n[\"\u0440\u043e\u043d\", null],\n[\"\u0441\u043a\u0438\u0439\", null],\n[\"\u2581\u0443\u0436\u0435\", \"d\u00e9j\u00e0\"],\n[\"\u2581\u043d\u0435\", \"ne\"],\n[\"\u2581\u043d\u0430\u0445\u043e\u0434\u0438\", \"trouvait\"],\n[\"\u043b\", null],\n[\"\u2581\u0440\u0443\u043a\u043e\u0432\u043e\u0434\", \"guide\"],\n[\"\u044f\", null],\n[\"\u0449\u0435\u0439\", null],\n[\"\u2581\u043d\u0438\", \"fil\"],\n[\"\u0442\u0438\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\nPardon, -- \u043f\u0440\u0438\u0431\u0430\u0432\u0438\u043b\u0430 \u043e\u043d\u0430, \u0433\u043b\u044f\u0434\u044f \u043d\u0430 \u043e\u043f\u044f\u0442\u044c \u0432\u043e\u0448\u0435\u0434\u0448\u0435\u0433\u043e \u0441 \u0434\u0440\u0443\u0433\u043e\u0439 \u0437\u0430\u043f\u0438\u0441\u043a\u043e\u0439 \u043b\u0430\u043a\u0435\u044f.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Pardon', ',', '\u2581--', '\u2581\u043f\u0440\u0438', '\u0431\u0430', '\u0432\u0438\u043b\u0430', '\u2581\u043e\u043d\u0430', ',', '\u2581\u0433\u043b\u044f', '\u0434\u044f', '\u2581\u043d\u0430', '\u2581\u043e\u043f\u044f\u0442\u044c', '\u2581\u0432\u043e', '\u0448\u0435\u0434', '\u0448\u0435\u0433\u043e', '\u2581\u0441', '\u2581\u0434\u0440\u0443\u0433\u043e\u0439', '\u2581\u0437\u0430\u043f\u0438', '\u0441\u043a\u043e\u0439', '\u2581\u043b\u0430', '\u043a\u0435', '\u044f', '.']"}], "[\n    [\"Pardon\", \"Pardon\"],\n    [\",\", null],\n    [\"\u2581--\", null],\n    [\"\u2581\u043f\u0440\u0438\", \"added\"],\n    [\"\u0431\u0430\", null],\n    [\"\u0432\u0438\u043b\u0430\", null],\n    [\"\u2581\u043e\u043d\u0430\", \"she\"],\n    [\",\", null],\n    [\"\u2581\u0433\u043b\u044f\", \"looking\"],\n    [\"\u0434\u044f\", null],\n    [\"\u2581\u043d\u0430\", \"at\"],\n    [\"\u2581\u043e\u043f\u044f\u0442\u044c\", \"again\"],\n    [\"\u2581\u0432\u043e\", \"entered\"],\n    [\"\u0448\u0435\u0434\", null],\n    [\"\u0448\u0435\u0433\u043e\", null],\n    [\"\u2581\u0441\", \"with\"],\n    [\"\u2581\u0434\u0440\u0443\u0433\u043e\u0439\", \"another\"],\n    [\"\u2581\u0437\u0430\u043f\u0438\", \"note\"],\n    [\"\u0441\u043a\u043e\u0439\", null],\n    [\"\u2581\u043b\u0430\", \"footman\"],\n    [\"\u043a\u0435\", null],\n    [\"\u044f\", null],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\nPardon, -- \u043f\u0440\u0438\u0431\u0430\u0432\u0438\u043b\u0430 \u043e\u043d\u0430, \u0433\u043b\u044f\u0434\u044f \u043d\u0430 \u043e\u043f\u044f\u0442\u044c \u0432\u043e\u0448\u0435\u0434\u0448\u0435\u0433\u043e \u0441 \u0434\u0440\u0443\u0433\u043e\u0439 \u0437\u0430\u043f\u0438\u0441\u043a\u043e\u0439 \u043b\u0430\u043a\u0435\u044f.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Pardon', ',', '\u2581--', '\u2581\u043f\u0440\u0438', '\u0431\u0430', '\u0432\u0438\u043b\u0430', '\u2581\u043e\u043d\u0430', ',', '\u2581\u0433\u043b\u044f', '\u0434\u044f', '\u2581\u043d\u0430', '\u2581\u043e\u043f\u044f\u0442\u044c', '\u2581\u0432\u043e', '\u0448\u0435\u0434', '\u0448\u0435\u0433\u043e', '\u2581\u0441', '\u2581\u0434\u0440\u0443\u0433\u043e\u0439', '\u2581\u0437\u0430\u043f\u0438', '\u0441\u043a\u043e\u0439', '\u2581\u043b\u0430', '\u043a\u0435', '\u044f', '.']"}], "[\n    [\"Pardon\", \"Pardon\"],\n    [\",\", null],\n    [\"\u2581--\", null],\n    [\"\u2581\u043f\u0440\u0438\", \"ajouta\"],\n    [\"\u0431\u0430\", null],\n    [\"\u0432\u0438\u043b\u0430\", null],\n    [\"\u2581\u043e\u043d\u0430\", \"elle\"],\n    [\",\", null],\n    [\"\u2581\u0433\u043b\u044f\", \"regardant\"],\n    [\"\u0434\u044f\", null],\n    [\"\u2581\u043d\u0430\", null],\n    [\"\u2581\u043e\u043f\u044f\u0442\u044c\", \"\u00e0 nouveau\"],\n    [\"\u2581\u0432\u043e\", \"entr\u00e9\"],\n    [\"\u0448\u0435\u0434\", null],\n    [\"\u0448\u0435\u0433\u043e\", null],\n    [\"\u2581\u0441\", \"avec\"],\n    [\"\u2581\u0434\u0440\u0443\u0433\u043e\u0439\", \"autre\"],\n    [\"\u2581\u0437\u0430\u043f\u0438\", \"note\"],\n    [\"\u0441\u043a\u043e\u0439\", null],\n    [\"\u2581\u043b\u0430\", \"valet\"],\n    [\"\u043a\u0435\", null],\n    [\"\u044f\", null],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0422\u0443\u0442 \u0431\u044b\u043b\u0430 \u0438 \u043f\u0440\u043e\u0441\u044c\u0431\u0430 \u043e \u043f\u0440\u043e\u0449\u0435\u043d\u0438\u0438, \u0438 \u0434\u043e\u0432\u0435\u0440\u0438\u0435 \u043a \u043d\u0435\u043c\u0443, \u0438 \u043b\u0430\u0441\u043a\u0430, \u043d\u0435\u0436\u043d\u0430\u044f, \u0440\u043e\u0431\u043a\u0430\u044f \u043b\u0430\u0441\u043a\u0430, \u0438 \u043e\u0431\u0435\u0449\u0430\u043d\u0438\u0435, \u0438 \u043d\u0430\u0434\u0435\u0436\u0434\u0430, \u0438 \u043b\u044e\u0431\u043e\u0432\u044c \u043a \u043d\u0435\u043c\u0443, \u0432 \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043e\u043d \u043d\u0435 \u043c\u043e\u0433 \u043d\u0435 \u0432\u0435\u0440\u0438\u0442\u044c \u0438 \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0434\u0443\u0448\u0438\u043b\u0430 \u0435\u0433\u043e \u0441\u0447\u0430\u0441\u0442\u044c\u0435\u043c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0422\u0443\u0442', '\u2581\u0431\u044b\u043b\u0430', '\u2581\u0438', '\u2581\u043f\u0440\u043e\u0441\u044c', '\u0431\u0430', '\u2581\u043e', '\u2581\u043f\u0440\u043e', '\u0449\u0435\u043d\u0438\u0438', ',', '\u2581\u0438', '\u2581\u0434\u043e', '\u0432\u0435\u0440\u0438', '\u0435', '\u2581\u043a', '\u2581\u043d\u0435\u043c\u0443', ',', '\u2581\u0438', '\u2581\u043b\u0430', '\u0441\u043a\u0430', ',', '\u2581\u043d\u0435', '\u0436\u043d\u0430\u044f', ',', '\u2581\u0440\u043e', '\u0431', '\u043a\u0430\u044f', '\u2581\u043b\u0430', '\u0441\u043a\u0430', ',', '\u2581\u0438', '\u2581\u043e\u0431\u0435\u0449\u0430', '\u043d\u0438\u0435', ',', '\u2581\u0438', '\u2581\u043d\u0430\u0434\u0435', '\u0436\u0434\u0430', ',', '\u2581\u0438', '\u2581\u043b\u044e\u0431\u043e\u0432\u044c', '\u2581\u043a', '\u2581\u043d\u0435\u043c\u0443', ',', '\u2581\u0432', '\u2581\u043a\u043e\u0442\u043e\u0440\u0443\u044e', '\u2581\u043e\u043d', '\u2581\u043d\u0435', '\u2581\u043c\u043e\u0433', '\u2581\u043d\u0435', '\u2581\u0432\u0435', '\u0440\u0438\u0442\u044c', '\u2581\u0438', '\u2581\u043a\u043e\u0442\u043e\u0440\u0430\u044f', '\u2581\u0434\u0443\u0448\u0438', '\u043b\u0430', '\u2581\u0435\u0433\u043e', '\u2581\u0441\u0447\u0430', '\u0441\u0442\u044c', '\u0435\u043c', '.']"}], "[\n[\"\u0422\u0443\u0442\", \"Here\"],\n[\"\u2581\u0431\u044b\u043b\u0430\", \"was\"],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043f\u0440\u043e\u0441\u044c\", \"request\"],\n[\"\u0431\u0430\", null],\n[\"\u2581\u043e\", null],\n[\"\u2581\u043f\u0440\u043e\", \"forgiveness\"],\n[\"\u0449\u0435\u043d\u0438\u0438\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0434\u043e\", \"trust\"],\n[\"\u0432\u0435\u0440\u0438\", null],\n[\"\u0435\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u043d\u0435\u043c\u0443\", \"him\"],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043b\u0430\", \"caress\"],\n[\"\u0441\u043a\u0430\", null],\n[\",\", null],\n[\"\u2581\u043d\u0435\", \"gentle\"],\n[\"\u0436\u043d\u0430\u044f\", null],\n[\",\", null],\n[\"\u2581\u0440\u043e\", \"timid\"],\n[\"\u0431\", null],\n[\"\u043a\u0430\u044f\", null],\n[\"\u2581\u043b\u0430\", \"caress\"],\n[\"\u0441\u043a\u0430\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043e\u0431\u0435\u0449\u0430\", \"promise\"],\n[\"\u043d\u0438\u0435\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043d\u0430\u0434\u0435\", \"hope\"],\n[\"\u0436\u0434\u0430\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043b\u044e\u0431\u043e\u0432\u044c\", \"love\"],\n[\"\u2581\u043a\", null],\n[\"\u2581\u043d\u0435\u043c\u0443\", \"him\"],\n[\",\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u0443\u044e\", \"which\"],\n[\"\u2581\u043e\u043d\", \"he\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u043c\u043e\u0433\", \"could\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0432\u0435\", \"believe\"],\n[\"\u0440\u0438\u0442\u044c\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u0430\u044f\", \"which\"],\n[\"\u2581\u0434\u0443\u0448\u0438\", \"suffocated\"],\n[\"\u043b\u0430\", null],\n[\"\u2581\u0435\u0433\u043e\", \"him\"],\n[\"\u2581\u0441\u0447\u0430\", \"happiness\"],\n[\"\u0441\u0442\u044c\", null],\n[\"\u0435\u043c\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u0422\u0443\u0442 \u0431\u044b\u043b\u0430 \u0438 \u043f\u0440\u043e\u0441\u044c\u0431\u0430 \u043e \u043f\u0440\u043e\u0449\u0435\u043d\u0438\u0438, \u0438 \u0434\u043e\u0432\u0435\u0440\u0438\u0435 \u043a \u043d\u0435\u043c\u0443, \u0438 \u043b\u0430\u0441\u043a\u0430, \u043d\u0435\u0436\u043d\u0430\u044f, \u0440\u043e\u0431\u043a\u0430\u044f \u043b\u0430\u0441\u043a\u0430, \u0438 \u043e\u0431\u0435\u0449\u0430\u043d\u0438\u0435, \u0438 \u043d\u0430\u0434\u0435\u0436\u0434\u0430, \u0438 \u043b\u044e\u0431\u043e\u0432\u044c \u043a \u043d\u0435\u043c\u0443, \u0432 \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043e\u043d \u043d\u0435 \u043c\u043e\u0433 \u043d\u0435 \u0432\u0435\u0440\u0438\u0442\u044c \u0438 \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0434\u0443\u0448\u0438\u043b\u0430 \u0435\u0433\u043e \u0441\u0447\u0430\u0441\u0442\u044c\u0435\u043c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0422\u0443\u0442', '\u2581\u0431\u044b\u043b\u0430', '\u2581\u0438', '\u2581\u043f\u0440\u043e\u0441\u044c', '\u0431\u0430', '\u2581\u043e', '\u2581\u043f\u0440\u043e', '\u0449\u0435\u043d\u0438\u0438', ',', '\u2581\u0438', '\u2581\u0434\u043e', '\u0432\u0435\u0440\u0438', '\u0435', '\u2581\u043a', '\u2581\u043d\u0435\u043c\u0443', ',', '\u2581\u0438', '\u2581\u043b\u0430', '\u0441\u043a\u0430', ',', '\u2581\u043d\u0435', '\u0436\u043d\u0430\u044f', ',', '\u2581\u0440\u043e', '\u0431', '\u043a\u0430\u044f', '\u2581\u043b\u0430', '\u0441\u043a\u0430', ',', '\u2581\u0438', '\u2581\u043e\u0431\u0435\u0449\u0430', '\u043d\u0438\u0435', ',', '\u2581\u0438', '\u2581\u043d\u0430\u0434\u0435', '\u0436\u0434\u0430', ',', '\u2581\u0438', '\u2581\u043b\u044e\u0431\u043e\u0432\u044c', '\u2581\u043a', '\u2581\u043d\u0435\u043c\u0443', ',', '\u2581\u0432', '\u2581\u043a\u043e\u0442\u043e\u0440\u0443\u044e', '\u2581\u043e\u043d', '\u2581\u043d\u0435', '\u2581\u043c\u043e\u0433', '\u2581\u043d\u0435', '\u2581\u0432\u0435', '\u0440\u0438\u0442\u044c', '\u2581\u0438', '\u2581\u043a\u043e\u0442\u043e\u0440\u0430\u044f', '\u2581\u0434\u0443\u0448\u0438', '\u043b\u0430', '\u2581\u0435\u0433\u043e', '\u2581\u0441\u0447\u0430', '\u0441\u0442\u044c', '\u0435\u043c', '.']"}], "[\n[\"\u0422\u0443\u0442\", \"Ici\"],\n[\"\u2581\u0431\u044b\u043b\u0430\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043f\u0440\u043e\u0441\u044c\", \"demande\"],\n[\"\u0431\u0430\", null],\n[\"\u2581\u043e\", null],\n[\"\u2581\u043f\u0440\u043e\", \"pardon\"],\n[\"\u0449\u0435\u043d\u0438\u0438\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0434\u043e\", \"confiance\"],\n[\"\u0432\u0435\u0440\u0438\", null],\n[\"\u0435\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u043d\u0435\u043c\u0443\", \"lui\"],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043b\u0430\", \"caresse\"],\n[\"\u0441\u043a\u0430\", null],\n[\",\", null],\n[\"\u2581\u043d\u0435\", \"tendre\"],\n[\"\u0436\u043d\u0430\u044f\", null],\n[\",\", null],\n[\"\u2581\u0440\u043e\", \"timide\"],\n[\"\u0431\", null],\n[\"\u043a\u0430\u044f\", null],\n[\"\u2581\u043b\u0430\", \"caresse\"],\n[\"\u0441\u043a\u0430\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043e\u0431\u0435\u0449\u0430\", \"promesse\"],\n[\"\u043d\u0438\u0435\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043d\u0430\u0434\u0435\", \"espoir\"],\n[\"\u0436\u0434\u0430\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043b\u044e\u0431\u043e\u0432\u044c\", \"amour\"],\n[\"\u2581\u043a\", null],\n[\"\u2581\u043d\u0435\u043c\u0443\", \"lui\"],\n[\",\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u0443\u044e\", \"laquelle\"],\n[\"\u2581\u043e\u043d\", \"il\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u043c\u043e\u0433\", \"pouvait\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0432\u0435\", \"croire\"],\n[\"\u0440\u0438\u0442\u044c\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u0430\u044f\", \"qui\"],\n[\"\u2581\u0434\u0443\u0448\u0438\", \"\u00e9touffait\"],\n[\"\u043b\u0430\", null],\n[\"\u2581\u0435\u0433\u043e\", \"le\"],\n[\"\u2581\u0441\u0447\u0430\", \"bonheur\"],\n[\"\u0441\u0442\u044c\", null],\n[\"\u0435\u043c\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0421\u0443\u0445\u043e\u0441\u0442\u044c \u044d\u0442\u0430 \u043e\u0433\u043e\u0440\u0447\u0438\u043b\u0430 \u041a\u0438\u0442\u0438, \u0438 \u043e\u043d\u0430 \u043d\u0435 \u043c\u043e\u0433\u043b\u0430 \u0443\u0434\u0435\u0440\u0436\u0430\u0442\u044c\u0441\u044f \u043e\u0442 \u0436\u0435\u043b\u0430\u043d\u0438\u044f \u0437\u0430\u0433\u043b\u0430\u0434\u0438\u0442\u044c \u0445\u043e\u043b\u043e\u0434\u043d\u043e\u0441\u0442\u044c \u043c\u0430\u0442\u0435\u0440\u0438.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0421\u0443', '\u0445\u043e', '\u0441\u0442\u044c', '\u2581\u044d\u0442\u0430', '\u2581\u043e', '\u0433\u043e\u0440', '\u0447\u0438\u043b\u0430', '\u2581\u041a\u0438', '\u0442\u0438', ',', '\u2581\u0438', '\u2581\u043e\u043d\u0430', '\u2581\u043d\u0435', '\u2581\u043c\u043e\u0433\u043b\u0430', '\u2581\u0443', '\u0434\u0435\u0440\u0436\u0430', '\u0442\u044c\u0441\u044f', '\u2581\u043e\u0442', '\u2581\u0436\u0435\u043b\u0430\u043d\u0438\u044f', '\u2581\u0437\u0430', '\u0433\u043b\u0430', '\u0434\u0438\u0442\u044c', '\u2581\u0445\u043e\u043b\u043e\u0434', '\u043d\u043e\u0441\u0442\u044c', '\u2581\u043c\u0430\u0442\u0435\u0440\u0438', '.']"}], "[\n[\"\u0421\u0443\", \"Dryness\"],\n[\"\u0445\u043e\", null],\n[\"\u0441\u0442\u044c\", null],\n[\"\u2581\u044d\u0442\u0430\", \"this\"],\n[\"\u2581\u043e\", null],\n[\"\u0433\u043e\u0440\", \"upset\"],\n[\"\u0447\u0438\u043b\u0430\", null],\n[\"\u2581\u041a\u0438\", \"Kitty\"],\n[\"\u0442\u0438\", null],\n[\",\", null],\n[\"\u2581\u0438\", \"and\"],\n[\"\u2581\u043e\u043d\u0430\", \"she\"],\n[\"\u2581\u043d\u0435\", \"not\"],\n[\"\u2581\u043c\u043e\u0433\u043b\u0430\", \"could\"],\n[\"\u2581\u0443\", null],\n[\"\u0434\u0435\u0440\u0436\u0430\", \"restrain\"],\n[\"\u0442\u044c\u0441\u044f\", null],\n[\"\u2581\u043e\u0442\", \"from\"],\n[\"\u2581\u0436\u0435\u043b\u0430\u043d\u0438\u044f\", \"desire\"],\n[\"\u2581\u0437\u0430\", null],\n[\"\u0433\u043b\u0430\", \"smooth\"],\n[\"\u0434\u0438\u0442\u044c\", null],\n[\"\u2581\u0445\u043e\u043b\u043e\u0434\", \"coldness\"],\n[\"\u043d\u043e\u0441\u0442\u044c\", null],\n[\"\u2581\u043c\u0430\u0442\u0435\u0440\u0438\", \"mother\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u0421\u0443\u0445\u043e\u0441\u0442\u044c \u044d\u0442\u0430 \u043e\u0433\u043e\u0440\u0447\u0438\u043b\u0430 \u041a\u0438\u0442\u0438, \u0438 \u043e\u043d\u0430 \u043d\u0435 \u043c\u043e\u0433\u043b\u0430 \u0443\u0434\u0435\u0440\u0436\u0430\u0442\u044c\u0441\u044f \u043e\u0442 \u0436\u0435\u043b\u0430\u043d\u0438\u044f \u0437\u0430\u0433\u043b\u0430\u0434\u0438\u0442\u044c \u0445\u043e\u043b\u043e\u0434\u043d\u043e\u0441\u0442\u044c \u043c\u0430\u0442\u0435\u0440\u0438.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0421\u0443', '\u0445\u043e', '\u0441\u0442\u044c', '\u2581\u044d\u0442\u0430', '\u2581\u043e', '\u0433\u043e\u0440', '\u0447\u0438\u043b\u0430', '\u2581\u041a\u0438', '\u0442\u0438', ',', '\u2581\u0438', '\u2581\u043e\u043d\u0430', '\u2581\u043d\u0435', '\u2581\u043c\u043e\u0433\u043b\u0430', '\u2581\u0443', '\u0434\u0435\u0440\u0436\u0430', '\u0442\u044c\u0441\u044f', '\u2581\u043e\u0442', '\u2581\u0436\u0435\u043b\u0430\u043d\u0438\u044f', '\u2581\u0437\u0430', '\u0433\u043b\u0430', '\u0434\u0438\u0442\u044c', '\u2581\u0445\u043e\u043b\u043e\u0434', '\u043d\u043e\u0441\u0442\u044c', '\u2581\u043c\u0430\u0442\u0435\u0440\u0438', '.']"}], "[\n  [\"\u0421\u0443\", \"S\u00e9cheresse\"],\n  [\"\u0445\u043e\", null],\n  [\"\u0441\u0442\u044c\", null],\n  [\"\u2581\u044d\u0442\u0430\", \"cette\"],\n  [\"\u2581\u043e\", null],\n  [\"\u0433\u043e\u0440\", null],\n  [\"\u0447\u0438\u043b\u0430\", null],\n  [\"\u2581\u041a\u0438\", \"Kitty\"],\n  [\"\u0442\u0438\", null],\n  [\",\", null],\n  [\"\u2581\u0438\", \"et\"],\n  [\"\u2581\u043e\u043d\u0430\", \"elle\"],\n  [\"\u2581\u043d\u0435\", \"ne\"],\n  [\"\u2581\u043c\u043e\u0433\u043b\u0430\", \"pouvait\"],\n  [\"\u2581\u0443\", null],\n  [\"\u0434\u0435\u0440\u0436\u0430\", null],\n  [\"\u0442\u044c\u0441\u044f\", null],\n  [\"\u2581\u043e\u0442\", \"de\"],\n  [\"\u2581\u0436\u0435\u043b\u0430\u043d\u0438\u044f\", \"d\u00e9sir\"],\n  [\"\u2581\u0437\u0430\", null],\n  [\"\u0433\u043b\u0430\", null],\n  [\"\u0434\u0438\u0442\u044c\", null],\n  [\"\u2581\u0445\u043e\u043b\u043e\u0434\", \"froideur\"],\n  [\"\u043d\u043e\u0441\u0442\u044c\", null],\n  [\"\u2581\u043c\u0430\u0442\u0435\u0440\u0438\", \"m\u00e8re\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0410\u043d\u043d\u0430 \u0437\u0430\u043c\u0435\u0442\u0438\u043b\u0430 \u044d\u0442\u043e\u0442 \u0432\u0437\u0434\u043e\u0445, \u0432\u044b\u043a\u0430\u0437\u044b\u0432\u0430\u0432\u0448\u0438\u0439 \u043d\u0435\u0441\u043e\u0433\u043b\u0430\u0441\u0438\u0435, \u0438 \u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0430\u043b\u0430.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0410\u043d\u043d\u0430', '\u2581\u0437\u0430\u043c\u0435\u0442\u0438', '\u043b\u0430', '\u2581\u044d\u0442\u043e\u0442', '\u2581\u0432\u0437', '\u0434\u043e\u0445', ',', '\u2581\u0432\u044b', '\u043a\u0430\u0437\u044b\u0432\u0430', '\u0432\u0448\u0438\u0439', '\u2581\u043d\u0435', '\u0441\u043e', '\u0433\u043b\u0430', '\u0441\u0438', '\u0435', ',', '\u2581\u0438', '\u2581\u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0430', '\u043b\u0430', '.']"}], "[\n  [\"\u0410\u043d\u043d\u0430\", \"Anna\"],\n  [\"\u2581\u0437\u0430\u043c\u0435\u0442\u0438\", \"noticed\"],\n  [\"\u043b\u0430\", null],\n  [\"\u2581\u044d\u0442\u043e\u0442\", \"this\"],\n  [\"\u2581\u0432\u0437\", \"sigh\"],\n  [\"\u0434\u043e\u0445\", null],\n  [\",\", null],\n  [\"\u2581\u0432\u044b\", \"expressing\"],\n  [\"\u043a\u0430\u0437\u044b\u0432\u0430\", null],\n  [\"\u0432\u0448\u0438\u0439\", null],\n  [\"\u2581\u043d\u0435\", \"dis\"],\n  [\"\u0441\u043e\", null],\n  [\"\u0433\u043b\u0430\", null],\n  [\"\u0441\u0438\", null],\n  [\"\u0435\", null],\n  [\",\", null],\n  [\"\u2581\u0438\", \"and\"],\n  [\"\u2581\u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0430\", \"continued\"],\n  [\"\u043b\u0430\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u0410\u043d\u043d\u0430 \u0437\u0430\u043c\u0435\u0442\u0438\u043b\u0430 \u044d\u0442\u043e\u0442 \u0432\u0437\u0434\u043e\u0445, \u0432\u044b\u043a\u0430\u0437\u044b\u0432\u0430\u0432\u0448\u0438\u0439 \u043d\u0435\u0441\u043e\u0433\u043b\u0430\u0441\u0438\u0435, \u0438 \u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0430\u043b\u0430.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0410\u043d\u043d\u0430', '\u2581\u0437\u0430\u043c\u0435\u0442\u0438', '\u043b\u0430', '\u2581\u044d\u0442\u043e\u0442', '\u2581\u0432\u0437', '\u0434\u043e\u0445', ',', '\u2581\u0432\u044b', '\u043a\u0430\u0437\u044b\u0432\u0430', '\u0432\u0448\u0438\u0439', '\u2581\u043d\u0435', '\u0441\u043e', '\u0433\u043b\u0430', '\u0441\u0438', '\u0435', ',', '\u2581\u0438', '\u2581\u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0430', '\u043b\u0430', '.']"}], "[\n[\"\u0410\u043d\u043d\u0430\", \"Anna\"],\n[\"\u2581\u0437\u0430\u043c\u0435\u0442\u0438\", \"remarquer\"],\n[\"\u043b\u0430\", null],\n[\"\u2581\u044d\u0442\u043e\u0442\", \"ce\"],\n[\"\u2581\u0432\u0437\", \"soupir\"],\n[\"\u0434\u043e\u0445\", null],\n[\",\", null],\n[\"\u2581\u0432\u044b\", \"r\u00e9v\u00e9ler\"],\n[\"\u043a\u0430\u0437\u044b\u0432\u0430\", null],\n[\"\u0432\u0448\u0438\u0439\", null],\n[\"\u2581\u043d\u0435\", null],\n[\"\u0441\u043e\", null],\n[\"\u0433\u043b\u0430\", null],\n[\"\u0441\u0438\", null],\n[\"\u0435\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0430\", \"continuer\"],\n[\"\u043b\u0430\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n-- \u0414\u0430, \u044f \u043f\u043e\u0442\u0435\u0440\u044f\u043b \u0434\u0430\u0436\u0435 \u043b\u044e\u0431\u043e\u0432\u044c \u043a \u0441\u044b\u043d\u0443, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0441 \u043d\u0438\u043c \u0441\u0432\u044f\u0437\u0430\u043d\u043e \u043c\u043e\u0435 \u043e\u0442\u0432\u0440\u0430\u0449\u0435\u043d\u0438\u0435 \u043a \u0432\u0430\u043c.\u041d\u043e \u044f \u0432\u0441\u0435-\u0442\u0430\u043a\u0438 \u0432\u043e\u0437\u044c\u043c\u0443 \u0435\u0433\u043e.\n\n\n---=== TOKENIZED TEXT ===---\n\n['--', '\u2581\u0414\u0430', ',', '\u2581\u044f', '\u2581\u043f\u043e\u0442\u0435\u0440\u044f', '\u043b', '\u2581\u0434\u0430\u0436\u0435', '\u2581\u043b\u044e\u0431\u043e\u0432\u044c', '\u2581\u043a', '\u2581\u0441\u044b', '\u043d\u0443', ',', '\u2581\u043f\u043e\u0442\u043e\u043c\u0443', '\u2581\u0447\u0442\u043e', '\u2581\u0441', '\u2581\u043d\u0438\u043c', '\u2581\u0441\u0432\u044f\u0437\u0430\u043d\u043e', '\u2581\u043c\u043e\u0435', '\u2581\u043e\u0442', '\u0432\u0440\u0430', '\u0449\u0435\u043d\u0438\u0435', '\u2581\u043a', '\u2581\u0432\u0430\u043c', '.', '\u041d\u043e', '\u2581\u044f', '\u2581\u0432\u0441\u0435', '-', '\u0442\u0430\u043a\u0438', '\u2581\u0432\u043e\u0437\u044c', '\u043c\u0443', '\u2581\u0435\u0433\u043e', '.']"}], "[\n[\"--\", null],\n[\"\u2581\u0414\u0430\", \"Yes\"],\n[\",\", null],\n[\"\u2581\u044f\", \"I\"],\n[\"\u2581\u043f\u043e\u0442\u0435\u0440\u044f\", \"lost\"],\n[\"\u043b\", null],\n[\"\u2581\u0434\u0430\u0436\u0435\", \"even\"],\n[\"\u2581\u043b\u044e\u0431\u043e\u0432\u044c\", \"love\"],\n[\"\u2581\u043a\", null],\n[\"\u2581\u0441\u044b\", \"son\"],\n[\"\u043d\u0443\", null],\n[\",\", null],\n[\"\u2581\u043f\u043e\u0442\u043e\u043c\u0443\", \"because\"],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0441\", null],\n[\"\u2581\u043d\u0438\u043c\", \"him\"],\n[\"\u2581\u0441\u0432\u044f\u0437\u0430\u043d\u043e\", \"associated\"],\n[\"\u2581\u043c\u043e\u0435\", \"my\"],\n[\"\u2581\u043e\u0442\", \"disgust\"],\n[\"\u0432\u0440\u0430\", null],\n[\"\u0449\u0435\u043d\u0438\u0435\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u0432\u0430\u043c\", \"you\"],\n[\".\", null],\n[\"\u041d\u043e\", \"But\"],\n[\"\u2581\u044f\", \"I\"],\n[\"\u2581\u0432\u0441\u0435\", \"still\"],\n[\"-\", null],\n[\"\u0442\u0430\u043a\u0438\", null],\n[\"\u2581\u0432\u043e\u0437\u044c\", \"take\"],\n[\"\u043c\u0443\", null],\n[\"\u2581\u0435\u0433\u043e\", \"him\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n-- \u0414\u0430, \u044f \u043f\u043e\u0442\u0435\u0440\u044f\u043b \u0434\u0430\u0436\u0435 \u043b\u044e\u0431\u043e\u0432\u044c \u043a \u0441\u044b\u043d\u0443, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0441 \u043d\u0438\u043c \u0441\u0432\u044f\u0437\u0430\u043d\u043e \u043c\u043e\u0435 \u043e\u0442\u0432\u0440\u0430\u0449\u0435\u043d\u0438\u0435 \u043a \u0432\u0430\u043c.\u041d\u043e \u044f \u0432\u0441\u0435-\u0442\u0430\u043a\u0438 \u0432\u043e\u0437\u044c\u043c\u0443 \u0435\u0433\u043e.\n\n\n---=== TOKENIZED TEXT ===---\n\n['--', '\u2581\u0414\u0430', ',', '\u2581\u044f', '\u2581\u043f\u043e\u0442\u0435\u0440\u044f', '\u043b', '\u2581\u0434\u0430\u0436\u0435', '\u2581\u043b\u044e\u0431\u043e\u0432\u044c', '\u2581\u043a', '\u2581\u0441\u044b', '\u043d\u0443', ',', '\u2581\u043f\u043e\u0442\u043e\u043c\u0443', '\u2581\u0447\u0442\u043e', '\u2581\u0441', '\u2581\u043d\u0438\u043c', '\u2581\u0441\u0432\u044f\u0437\u0430\u043d\u043e', '\u2581\u043c\u043e\u0435', '\u2581\u043e\u0442', '\u0432\u0440\u0430', '\u0449\u0435\u043d\u0438\u0435', '\u2581\u043a', '\u2581\u0432\u0430\u043c', '.', '\u041d\u043e', '\u2581\u044f', '\u2581\u0432\u0441\u0435', '-', '\u0442\u0430\u043a\u0438', '\u2581\u0432\u043e\u0437\u044c', '\u043c\u0443', '\u2581\u0435\u0433\u043e', '.']"}], "[\n[\"--\", null],\n[\"\u2581\u0414\u0430\", \"Oui\"],\n[\",\", null],\n[\"\u2581\u044f\", \"je\"],\n[\"\u2581\u043f\u043e\u0442\u0435\u0440\u044f\", \"perdu\"],\n[\"\u043b\", null],\n[\"\u2581\u0434\u0430\u0436\u0435\", \"m\u00eame\"],\n[\"\u2581\u043b\u044e\u0431\u043e\u0432\u044c\", \"l'amour\"],\n[\"\u2581\u043a\", null],\n[\"\u2581\u0441\u044b\", \"fils\"],\n[\"\u043d\u0443\", null],\n[\",\", null],\n[\"\u2581\u043f\u043e\u0442\u043e\u043c\u0443\", \"parce\"],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0441\", null],\n[\"\u2581\u043d\u0438\u043c\", \"lui\"],\n[\"\u2581\u0441\u0432\u044f\u0437\u0430\u043d\u043e\", \"li\u00e9\"],\n[\"\u2581\u043c\u043e\u0435\", \"mon\"],\n[\"\u2581\u043e\u0442\", \"d\u00e9go\u00fbt\"],\n[\"\u0432\u0440\u0430\", null],\n[\"\u0449\u0435\u043d\u0438\u0435\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u0432\u0430\u043c\", \"vous\"],\n[\".\", null],\n[\"\u041d\u043e\", \"Mais\"],\n[\"\u2581\u044f\", \"je\"],\n[\"\u2581\u0432\u0441\u0435\", \"quand\"],\n[\"-\", null],\n[\"\u0442\u0430\u043a\u0438\", \"m\u00eame\"],\n[\"\u2581\u0432\u043e\u0437\u044c\", \"prendrai\"],\n[\"\u043c\u0443\", null],\n[\"\u2581\u0435\u0433\u043e\", \"le\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041f\u0440\u0430\u0432\u0434\u0430, \u0447\u0442\u043e \u043d\u0430 \u0441\u043a\u043e\u0442\u043d\u043e\u043c \u0434\u0432\u043e\u0440\u0435 \u0434\u0435\u043b\u043e \u0448\u043b\u043e \u0434\u043e \u0441\u0438\u0445 \u043f\u043e\u0440 \u043d\u0435 \u043b\u0443\u0447\u0448\u0435, \u0447\u0435\u043c \u043f\u0440\u0435\u0436\u0434\u0435, \u0438 \u0418\u0432\u0430\u043d \u0441\u0438\u043b\u044c\u043d\u043e \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u043e\u0432\u0430\u043b \u0442\u0435\u043f\u043b\u043e\u043c\u0443 \u043f\u043e\u043c\u0435\u0449\u0435\u043d\u0438\u044e \u043a\u043e\u0440\u043e\u0432 \u0438 \u0441\u043b\u0438\u0432\u043e\u0447\u043d\u043e\u043c\u0443 \u043c\u0430\u0441\u043b\u0443, \u0443\u0442\u0432\u0435\u0440\u0436\u0434\u0430\u044f, \u0447\u0442\u043e \u043a\u043e\u0440\u043e\u0432\u0435 \u043d\u0430 \u0445\u043e\u043b\u043e\u0434\u0443 \u043f\u043e\u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u043c\u0435\u043d\u044c\u0448\u0435 \u043a\u043e\u0440\u043c\u0443 \u0438 \u0447\u0442\u043e \u0441\u043c\u0435\u0442\u0430\u043d\u043d\u043e\u0435 \u043c\u0430\u0441\u043b\u043e \u0441\u043f\u043e\u0440\u0435\u0435, \u0438 \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043b \u0436\u0430\u043b\u043e\u0432\u0430\u043d\u044c\u044f, \u043a\u0430\u043a \u0438 \u0432 \u0441\u0442\u0430\u0440\u0438\u043d\u0443, \u0438 \u043d\u0438\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043d\u0435 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043e\u0432\u0430\u043b\u0441\u044f \u0442\u0435\u043c, \u0447\u0442\u043e \u0434\u0435\u043d\u044c\u0433\u0438, \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c\u044b\u0435 \u0438\u043c, \u0431\u044b\u043b\u0438 \u043d\u0435 \u0436\u0430\u043b\u043e\u0432\u0430\u043d\u044c\u0435, \u0430 \u0432\u044b\u0434\u0430\u0447\u0430 \u0432\u043f\u0435\u0440\u0435\u0434 \u0434\u043e\u043b\u0438 \u0431\u0430\u0440\u044b\u0448\u0430.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u041f', '\u0440\u0430\u0432\u0434\u0430', ',', '\u2581\u0447\u0442\u043e', '\u2581\u043d\u0430', '\u2581\u0441\u043a\u043e', '\u0442\u043d\u043e\u043c', '\u2581\u0434\u0432\u043e', '\u0440\u0435', '\u2581\u0434\u0435\u043b\u043e', '\u2581\u0448', '\u043b\u043e', '\u2581\u0434\u043e', '\u2581\u0441\u0438\u0445', '\u2581\u043f\u043e\u0440', '\u2581\u043d\u0435', '\u2581\u043b\u0443\u0447\u0448\u0435', ',', '\u2581\u0447\u0435\u043c', '\u2581\u043f\u0440\u0435\u0436\u0434\u0435', ',', '\u2581\u0438', '\u2581\u0418\u0432\u0430\u043d', '\u2581\u0441\u0438\u043b\u044c\u043d\u043e', '\u2581\u043f\u0440\u043e\u0442\u0438\u0432\u043e', '\u0434\u0435\u0439', '\u0441\u0442\u0432\u043e\u0432\u0430\u043b', '\u2581\u0442\u0435\u043f\u043b\u043e', '\u043c\u0443', '\u2581\u043f\u043e\u043c\u0435', '\u0449\u0435\u043d\u0438\u044e', '\u2581\u043a\u043e', '\u0440\u043e\u0432', '\u2581\u0438', '\u2581\u0441\u043b\u0438\u0432\u043e', '\u0447\u043d\u043e\u043c\u0443', '\u2581\u043c\u0430', '\u0441\u043b\u0443', ',', '\u2581\u0443\u0442\u0432\u0435\u0440', '\u0436\u0434\u0430\u044f', ',', '\u2581\u0447\u0442\u043e', '\u2581\u043a\u043e\u0440\u043e', '\u0432\u0435', '\u2581\u043d\u0430', '\u2581\u0445\u043e\u043b\u043e', '\u0434\u0443', '\u2581\u043f\u043e\u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f', '\u2581\u043c\u0435\u043d\u044c\u0448\u0435', '\u2581\u043a\u043e\u0440', '\u043c\u0443', '\u2581\u0438', '\u2581\u0447\u0442\u043e', '\u2581\u0441\u043c\u0435', '\u0442\u0430\u043d', '\u043d\u043e\u0435', '\u2581\u043c\u0430\u0441\u043b\u043e', '\u2581\u0441\u043f\u043e', '\u0440\u0435\u0435', ',', '\u2581\u0438', '\u2581\u0442\u0440\u0435\u0431\u043e', '\u0432\u0430\u043b', '\u2581\u0436\u0430\u043b\u043e', '\u0432\u0430\u043d', '\u044c\u044f', ',', '\u2581\u043a\u0430\u043a', '\u2581\u0438', '\u2581\u0432', '\u2581\u0441\u0442\u0430', '\u0440\u0438', '\u043d\u0443', ',', '\u2581\u0438', '\u2581\u043d\u0438', '\u0441\u043a\u043e\u043b\u044c\u043a\u043e', '\u2581\u043d\u0435', '\u2581\u0438\u043d\u0442\u0435\u0440\u0435', '\u0441\u043e\u0432\u0430', '\u043b\u0441\u044f', '\u2581\u0442\u0435\u043c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0434\u0435\u043d\u044c\u0433\u0438', ',', '\u2581\u043f\u043e\u043b\u0443\u0447\u0430', '\u0435\u043c\u044b\u0435', '\u2581\u0438\u043c', ',', '\u2581\u0431\u044b\u043b\u0438', '\u2581\u043d\u0435', '\u2581\u0436\u0430\u043b\u043e', '\u0432\u0430\u043d', '\u044c\u0435', ',', '\u2581\u0430', '\u2581\u0432\u044b', '\u0434\u0430\u0447\u0430', '\u2581\u0432\u043f\u0435\u0440\u0435\u0434', '\u2581\u0434\u043e\u043b\u0438', '\u2581\u0431\u0430', '\u0440\u044b', '\u0448\u0430', '.']"}], "[\n[\"\u041f\", null],\n[\"\u0440\u0430\u0432\u0434\u0430\", \"Truth\"],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u0441\u043a\u043e\", null],\n[\"\u0442\u043d\u043e\u043c\", \"farmyard\"],\n[\"\u2581\u0434\u0432\u043e\", null],\n[\"\u0440\u0435\", null],\n[\"\u2581\u0434\u0435\u043b\u043e\", \"things\"],\n[\"\u2581\u0448\", null],\n[\"\u043b\u043e\", null],\n[\"\u2581\u0434\u043e\", null],\n[\"\u2581\u0441\u0438\u0445\", null],\n[\"\u2581\u043f\u043e\u0440\", \"still\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u043b\u0443\u0447\u0448\u0435\", \"better\"],\n[\",\", null],\n[\"\u2581\u0447\u0435\u043c\", null],\n[\"\u2581\u043f\u0440\u0435\u0436\u0434\u0435\", \"before\"],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0418\u0432\u0430\u043d\", \"Ivan\"],\n[\"\u2581\u0441\u0438\u043b\u044c\u043d\u043e\", \"strongly\"],\n[\"\u2581\u043f\u0440\u043e\u0442\u0438\u0432\u043e\", null],\n[\"\u0434\u0435\u0439\", null],\n[\"\u0441\u0442\u0432\u043e\u0432\u0430\u043b\", \"opposed\"],\n[\"\u2581\u0442\u0435\u043f\u043b\u043e\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u043f\u043e\u043c\u0435\", null],\n[\"\u0449\u0435\u043d\u0438\u044e\", \"housing\"],\n[\"\u2581\u043a\u043e\", null],\n[\"\u0440\u043e\u0432\", \"cows\"],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0441\u043b\u0438\u0432\u043e\", null],\n[\"\u0447\u043d\u043e\u043c\u0443\", \"cream\"],\n[\"\u2581\u043c\u0430\", null],\n[\"\u0441\u043b\u0443\", \"butter\"],\n[\",\", null],\n[\"\u2581\u0443\u0442\u0432\u0435\u0440\", null],\n[\"\u0436\u0434\u0430\u044f\", \"claiming\"],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u043a\u043e\u0440\u043e\", null],\n[\"\u0432\u0435\", \"cow\"],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u0445\u043e\u043b\u043e\", null],\n[\"\u0434\u0443\", \"cold\"],\n[\"\u2581\u043f\u043e\u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f\", \"would need\"],\n[\"\u2581\u043c\u0435\u043d\u044c\u0448\u0435\", \"less\"],\n[\"\u2581\u043a\u043e\u0440\", null],\n[\"\u043c\u0443\", \"feed\"],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0441\u043c\u0435\", null],\n[\"\u0442\u0430\u043d\", null],\n[\"\u043d\u043e\u0435\", \"sour cream\"],\n[\"\u2581\u043c\u0430\u0441\u043b\u043e\", \"butter\"],\n[\"\u2581\u0441\u043f\u043e\", null],\n[\"\u0440\u0435\u0435\", \"faster\"],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0442\u0440\u0435\u0431\u043e\", null],\n[\"\u0432\u0430\u043b\", \"demanded\"],\n[\"\u2581\u0436\u0430\u043b\u043e\", null],\n[\"\u0432\u0430\u043d\", null],\n[\"\u044c\u044f\", \"salary\"],\n[\",\", null],\n[\"\u2581\u043a\u0430\u043a\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0441\u0442\u0430\", null],\n[\"\u0440\u0438\", null],\n[\"\u043d\u0443\", \"old times\"],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043d\u0438\", null],\n[\"\u0441\u043a\u043e\u043b\u044c\u043a\u043e\", \"not at all\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0438\u043d\u0442\u0435\u0440\u0435\", null],\n[\"\u0441\u043e\u0432\u0430\", null],\n[\"\u043b\u0441\u044f\", \"interested\"],\n[\"\u2581\u0442\u0435\u043c\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0434\u0435\u043d\u044c\u0433\u0438\", \"money\"],\n[\",\", null],\n[\"\u2581\u043f\u043e\u043b\u0443\u0447\u0430\", null],\n[\"\u0435\u043c\u044b\u0435\", \"received\"],\n[\"\u2581\u0438\u043c\", null],\n[\",\", null],\n[\"\u2581\u0431\u044b\u043b\u0438\", \"were\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0436\u0430\u043b\u043e\", null],\n[\"\u0432\u0430\u043d\", null],\n[\"\u044c\u0435\", \"salary\"],\n[\",\", null],\n[\"\u2581\u0430\", null],\n[\"\u2581\u0432\u044b\", null],\n[\"\u0434\u0430\u0447\u0430\", \"advance\"],\n[\"\u2581\u0432\u043f\u0435\u0440\u0435\u0434\", \"forward\"],\n[\"\u2581\u0434\u043e\u043b\u0438\", \"share\"],\n[\"\u2581\u0431\u0430\", null],\n[\"\u0440\u044b\", null],\n[\"\u0448\u0430\", \"profit\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u041f\u0440\u0430\u0432\u0434\u0430, \u0447\u0442\u043e \u043d\u0430 \u0441\u043a\u043e\u0442\u043d\u043e\u043c \u0434\u0432\u043e\u0440\u0435 \u0434\u0435\u043b\u043e \u0448\u043b\u043e \u0434\u043e \u0441\u0438\u0445 \u043f\u043e\u0440 \u043d\u0435 \u043b\u0443\u0447\u0448\u0435, \u0447\u0435\u043c \u043f\u0440\u0435\u0436\u0434\u0435, \u0438 \u0418\u0432\u0430\u043d \u0441\u0438\u043b\u044c\u043d\u043e \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u043e\u0432\u0430\u043b \u0442\u0435\u043f\u043b\u043e\u043c\u0443 \u043f\u043e\u043c\u0435\u0449\u0435\u043d\u0438\u044e \u043a\u043e\u0440\u043e\u0432 \u0438 \u0441\u043b\u0438\u0432\u043e\u0447\u043d\u043e\u043c\u0443 \u043c\u0430\u0441\u043b\u0443, \u0443\u0442\u0432\u0435\u0440\u0436\u0434\u0430\u044f, \u0447\u0442\u043e \u043a\u043e\u0440\u043e\u0432\u0435 \u043d\u0430 \u0445\u043e\u043b\u043e\u0434\u0443 \u043f\u043e\u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u043c\u0435\u043d\u044c\u0448\u0435 \u043a\u043e\u0440\u043c\u0443 \u0438 \u0447\u0442\u043e \u0441\u043c\u0435\u0442\u0430\u043d\u043d\u043e\u0435 \u043c\u0430\u0441\u043b\u043e \u0441\u043f\u043e\u0440\u0435\u0435, \u0438 \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043b \u0436\u0430\u043b\u043e\u0432\u0430\u043d\u044c\u044f, \u043a\u0430\u043a \u0438 \u0432 \u0441\u0442\u0430\u0440\u0438\u043d\u0443, \u0438 \u043d\u0438\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043d\u0435 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043e\u0432\u0430\u043b\u0441\u044f \u0442\u0435\u043c, \u0447\u0442\u043e \u0434\u0435\u043d\u044c\u0433\u0438, \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c\u044b\u0435 \u0438\u043c, \u0431\u044b\u043b\u0438 \u043d\u0435 \u0436\u0430\u043b\u043e\u0432\u0430\u043d\u044c\u0435, \u0430 \u0432\u044b\u0434\u0430\u0447\u0430 \u0432\u043f\u0435\u0440\u0435\u0434 \u0434\u043e\u043b\u0438 \u0431\u0430\u0440\u044b\u0448\u0430.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u041f', '\u0440\u0430\u0432\u0434\u0430', ',', '\u2581\u0447\u0442\u043e', '\u2581\u043d\u0430', '\u2581\u0441\u043a\u043e', '\u0442\u043d\u043e\u043c', '\u2581\u0434\u0432\u043e', '\u0440\u0435', '\u2581\u0434\u0435\u043b\u043e', '\u2581\u0448', '\u043b\u043e', '\u2581\u0434\u043e', '\u2581\u0441\u0438\u0445', '\u2581\u043f\u043e\u0440', '\u2581\u043d\u0435', '\u2581\u043b\u0443\u0447\u0448\u0435', ',', '\u2581\u0447\u0435\u043c', '\u2581\u043f\u0440\u0435\u0436\u0434\u0435', ',', '\u2581\u0438', '\u2581\u0418\u0432\u0430\u043d', '\u2581\u0441\u0438\u043b\u044c\u043d\u043e', '\u2581\u043f\u0440\u043e\u0442\u0438\u0432\u043e', '\u0434\u0435\u0439', '\u0441\u0442\u0432\u043e\u0432\u0430\u043b', '\u2581\u0442\u0435\u043f\u043b\u043e', '\u043c\u0443', '\u2581\u043f\u043e\u043c\u0435', '\u0449\u0435\u043d\u0438\u044e', '\u2581\u043a\u043e', '\u0440\u043e\u0432', '\u2581\u0438', '\u2581\u0441\u043b\u0438\u0432\u043e', '\u0447\u043d\u043e\u043c\u0443', '\u2581\u043c\u0430', '\u0441\u043b\u0443', ',', '\u2581\u0443\u0442\u0432\u0435\u0440', '\u0436\u0434\u0430\u044f', ',', '\u2581\u0447\u0442\u043e', '\u2581\u043a\u043e\u0440\u043e', '\u0432\u0435', '\u2581\u043d\u0430', '\u2581\u0445\u043e\u043b\u043e', '\u0434\u0443', '\u2581\u043f\u043e\u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f', '\u2581\u043c\u0435\u043d\u044c\u0448\u0435', '\u2581\u043a\u043e\u0440', '\u043c\u0443', '\u2581\u0438', '\u2581\u0447\u0442\u043e', '\u2581\u0441\u043c\u0435', '\u0442\u0430\u043d', '\u043d\u043e\u0435', '\u2581\u043c\u0430\u0441\u043b\u043e', '\u2581\u0441\u043f\u043e', '\u0440\u0435\u0435', ',', '\u2581\u0438', '\u2581\u0442\u0440\u0435\u0431\u043e', '\u0432\u0430\u043b', '\u2581\u0436\u0430\u043b\u043e', '\u0432\u0430\u043d', '\u044c\u044f', ',', '\u2581\u043a\u0430\u043a', '\u2581\u0438', '\u2581\u0432', '\u2581\u0441\u0442\u0430', '\u0440\u0438', '\u043d\u0443', ',', '\u2581\u0438', '\u2581\u043d\u0438', '\u0441\u043a\u043e\u043b\u044c\u043a\u043e', '\u2581\u043d\u0435', '\u2581\u0438\u043d\u0442\u0435\u0440\u0435', '\u0441\u043e\u0432\u0430', '\u043b\u0441\u044f', '\u2581\u0442\u0435\u043c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0434\u0435\u043d\u044c\u0433\u0438', ',', '\u2581\u043f\u043e\u043b\u0443\u0447\u0430', '\u0435\u043c\u044b\u0435', '\u2581\u0438\u043c', ',', '\u2581\u0431\u044b\u043b\u0438', '\u2581\u043d\u0435', '\u2581\u0436\u0430\u043b\u043e', '\u0432\u0430\u043d', '\u044c\u0435', ',', '\u2581\u0430', '\u2581\u0432\u044b', '\u0434\u0430\u0447\u0430', '\u2581\u0432\u043f\u0435\u0440\u0435\u0434', '\u2581\u0434\u043e\u043b\u0438', '\u2581\u0431\u0430', '\u0440\u044b', '\u0448\u0430', '.']"}], "[\n[\"\u041f\", null],\n[\"\u0440\u0430\u0432\u0434\u0430\", \"Vrai\"],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u0441\u043a\u043e\", null],\n[\"\u0442\u043d\u043e\u043c\", null],\n[\"\u2581\u0434\u0432\u043e\", null],\n[\"\u0440\u0435\", \"ferme\"],\n[\"\u2581\u0434\u0435\u043b\u043e\", \"affaire\"],\n[\"\u2581\u0448\", null],\n[\"\u043b\u043e\", null],\n[\"\u2581\u0434\u043e\", null],\n[\"\u2581\u0441\u0438\u0445\", null],\n[\"\u2581\u043f\u043e\u0440\", \"jusqu'\u00e0 pr\u00e9sent\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u043b\u0443\u0447\u0448\u0435\", \"mieux\"],\n[\",\", null],\n[\"\u2581\u0447\u0435\u043c\", null],\n[\"\u2581\u043f\u0440\u0435\u0436\u0434\u0435\", \"avant\"],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0418\u0432\u0430\u043d\", \"Ivan\"],\n[\"\u2581\u0441\u0438\u043b\u044c\u043d\u043e\", \"fortement\"],\n[\"\u2581\u043f\u0440\u043e\u0442\u0438\u0432\u043e\", null],\n[\"\u0434\u0435\u0439\", null],\n[\"\u0441\u0442\u0432\u043e\u0432\u0430\u043b\", \"s'opposait\"],\n[\"\u2581\u0442\u0435\u043f\u043b\u043e\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u043f\u043e\u043c\u0435\", null],\n[\"\u0449\u0435\u043d\u0438\u044e\", \"logement\"],\n[\"\u2581\u043a\u043e\", null],\n[\"\u0440\u043e\u0432\", \"vaches\"],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0441\u043b\u0438\u0432\u043e\", null],\n[\"\u0447\u043d\u043e\u043c\u0443\", null],\n[\"\u2581\u043c\u0430\", null],\n[\"\u0441\u043b\u0443\", \"beurre\"],\n[\",\", null],\n[\"\u2581\u0443\u0442\u0432\u0435\u0440\", null],\n[\"\u0436\u0434\u0430\u044f\", \"affirmant\"],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u043a\u043e\u0440\u043e\", null],\n[\"\u0432\u0435\", \"vache\"],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u0445\u043e\u043b\u043e\", null],\n[\"\u0434\u0443\", \"froid\"],\n[\"\u2581\u043f\u043e\u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f\", \"n\u00e9cessiterait\"],\n[\"\u2581\u043c\u0435\u043d\u044c\u0448\u0435\", \"moins\"],\n[\"\u2581\u043a\u043e\u0440\", null],\n[\"\u043c\u0443\", \"nourriture\"],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0441\u043c\u0435\", null],\n[\"\u0442\u0430\u043d\", null],\n[\"\u043d\u043e\u0435\", null],\n[\"\u2581\u043c\u0430\u0441\u043b\u043e\", \"beurre\"],\n[\"\u2581\u0441\u043f\u043e\", null],\n[\"\u0440\u0435\u0435\", \"plus rapidement\"],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0442\u0440\u0435\u0431\u043e\", null],\n[\"\u0432\u0430\u043b\", \"exigeait\"],\n[\"\u2581\u0436\u0430\u043b\u043e\", null],\n[\"\u0432\u0430\u043d\", null],\n[\"\u044c\u044f\", \"salaire\"],\n[\",\", null],\n[\"\u2581\u043a\u0430\u043a\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0441\u0442\u0430\", null],\n[\"\u0440\u0438\", null],\n[\"\u043d\u0443\", \"ancien temps\"],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043d\u0438\", null],\n[\"\u0441\u043a\u043e\u043b\u044c\u043a\u043e\", \"aucunement\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0438\u043d\u0442\u0435\u0440\u0435\", null],\n[\"\u0441\u043e\u0432\u0430\", null],\n[\"\u043b\u0441\u044f\", \"s'int\u00e9ressait\"],\n[\"\u2581\u0442\u0435\u043c\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0434\u0435\u043d\u044c\u0433\u0438\", \"argent\"],\n[\",\", null],\n[\"\u2581\u043f\u043e\u043b\u0443\u0447\u0430\", null],\n[\"\u0435\u043c\u044b\u0435\", null],\n[\"\u2581\u0438\u043c\", null],\n[\",\", null],\n[\"\u2581\u0431\u044b\u043b\u0438\", \"\u00e9taient\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0436\u0430\u043b\u043e\", null],\n[\"\u0432\u0430\u043d\", null],\n[\"\u044c\u0435\", \"salaire\"],\n[\",\", null],\n[\"\u2581\u0430\", null],\n[\"\u2581\u0432\u044b\", null],\n[\"\u0434\u0430\u0447\u0430\", \"avance\"],\n[\"\u2581\u0432\u043f\u0435\u0440\u0435\u0434\", null],\n[\"\u2581\u0434\u043e\u043b\u0438\", \"part\"],\n[\"\u2581\u0431\u0430\", null],\n[\"\u0440\u044b\", null],\n[\"\u0448\u0430\", \"profit\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041a\u043d\u044f\u0433\u0438\u043d\u044f, \u0443\u0441\u043b\u044b\u0445\u0430\u0432 \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u0412\u0430\u0440\u0435\u043d\u044c\u043a\u0430 \u0445\u043e\u0440\u043e\u0448\u043e \u043f\u043e\u0435\u0442, \u0432\u043e\u043f\u0440\u043e\u0441\u0438\u043b\u0430 \u0435\u0435 \u043f\u0440\u0438\u0439\u0442\u0438 \u043a \u043d\u0438\u043c \u043f\u0435\u0442\u044c \u0432\u0435\u0447\u0435\u0440\u043e\u043c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u041a', '\u043d\u044f', '\u0433\u0438', '\u043d\u044f', ',', '\u2581\u0443\u0441', '\u043b\u044b', '\u0445\u0430', '\u0432', '\u2581\u043e', '\u2581\u0442\u043e\u043c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0412\u0430\u0440', '\u0435\u043d\u044c', '\u043a\u0430', '\u2581\u0445\u043e\u0440\u043e\u0448\u043e', '\u2581\u043f\u043e', '\u0435\u0442', ',', '\u2581\u0432\u043e\u043f\u0440\u043e', '\u0441\u0438\u043b\u0430', '\u2581\u0435\u0435', '\u2581\u043f\u0440\u0438', '\u0439\u0442\u0438', '\u2581\u043a', '\u2581\u043d\u0438\u043c', '\u2581\u043f\u0435', '\u0442\u044c', '\u2581\u0432\u0435\u0447\u0435\u0440\u043e\u043c', '.']"}], "[\n[\"\u041a\", \"Princess\"],\n[\"\u043d\u044f\", null],\n[\"\u0433\u0438\", null],\n[\"\u043d\u044f\", null],\n[\",\", null],\n[\"\u2581\u0443\u0441\", \"hearing\"],\n[\"\u043b\u044b\", null],\n[\"\u0445\u0430\", null],\n[\"\u0432\", null],\n[\"\u2581\u043e\", null],\n[\"\u2581\u0442\u043e\u043c\", \"that\"],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", \"that\"],\n[\"\u2581\u0412\u0430\u0440\", \"Varenka\"],\n[\"\u0435\u043d\u044c\", null],\n[\"\u043a\u0430\", null],\n[\"\u2581\u0445\u043e\u0440\u043e\u0448\u043e\", \"well\"],\n[\"\u2581\u043f\u043e\", \"sings\"],\n[\"\u0435\u0442\", null],\n[\",\", null],\n[\"\u2581\u0432\u043e\u043f\u0440\u043e\", \"asked\"],\n[\"\u0441\u0438\u043b\u0430\", null],\n[\"\u2581\u0435\u0435\", \"her\"],\n[\"\u2581\u043f\u0440\u0438\", \"to come\"],\n[\"\u0439\u0442\u0438\", null],\n[\"\u2581\u043a\", \"to\"],\n[\"\u2581\u043d\u0438\u043c\", \"them\"],\n[\"\u2581\u043f\u0435\", \"sing\"],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0432\u0435\u0447\u0435\u0440\u043e\u043c\", \"in the evening\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u041a\u043d\u044f\u0433\u0438\u043d\u044f, \u0443\u0441\u043b\u044b\u0445\u0430\u0432 \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u0412\u0430\u0440\u0435\u043d\u044c\u043a\u0430 \u0445\u043e\u0440\u043e\u0448\u043e \u043f\u043e\u0435\u0442, \u0432\u043e\u043f\u0440\u043e\u0441\u0438\u043b\u0430 \u0435\u0435 \u043f\u0440\u0438\u0439\u0442\u0438 \u043a \u043d\u0438\u043c \u043f\u0435\u0442\u044c \u0432\u0435\u0447\u0435\u0440\u043e\u043c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u041a', '\u043d\u044f', '\u0433\u0438', '\u043d\u044f', ',', '\u2581\u0443\u0441', '\u043b\u044b', '\u0445\u0430', '\u0432', '\u2581\u043e', '\u2581\u0442\u043e\u043c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0412\u0430\u0440', '\u0435\u043d\u044c', '\u043a\u0430', '\u2581\u0445\u043e\u0440\u043e\u0448\u043e', '\u2581\u043f\u043e', '\u0435\u0442', ',', '\u2581\u0432\u043e\u043f\u0440\u043e', '\u0441\u0438\u043b\u0430', '\u2581\u0435\u0435', '\u2581\u043f\u0440\u0438', '\u0439\u0442\u0438', '\u2581\u043a', '\u2581\u043d\u0438\u043c', '\u2581\u043f\u0435', '\u0442\u044c', '\u2581\u0432\u0435\u0447\u0435\u0440\u043e\u043c', '.']"}], "[\n[\"\u041a\", \"Princesse\"],\n[\"\u043d\u044f\", null],\n[\"\u0433\u0438\", null],\n[\"\u043d\u044f\", null],\n[\",\", null],\n[\"\u2581\u0443\u0441\", \"apprenant\"],\n[\"\u043b\u044b\", null],\n[\"\u0445\u0430\", null],\n[\"\u0432\", null],\n[\"\u2581\u043e\", null],\n[\"\u2581\u0442\u043e\u043c\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0412\u0430\u0440\", \"Varenka\"],\n[\"\u0435\u043d\u044c\", null],\n[\"\u043a\u0430\", null],\n[\"\u2581\u0445\u043e\u0440\u043e\u0448\u043e\", \"bien\"],\n[\"\u2581\u043f\u043e\", \"chantait\"],\n[\"\u0435\u0442\", null],\n[\",\", null],\n[\"\u2581\u0432\u043e\u043f\u0440\u043e\", \"demanda\"],\n[\"\u0441\u0438\u043b\u0430\", null],\n[\"\u2581\u0435\u0435\", \"lui\"],\n[\"\u2581\u043f\u0440\u0438\", \"venir\"],\n[\"\u0439\u0442\u0438\", null],\n[\"\u2581\u043a\", \"chez\"],\n[\"\u2581\u043d\u0438\u043c\", \"eux\"],\n[\"\u2581\u043f\u0435\", \"chanter\"],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0432\u0435\u0447\u0435\u0440\u043e\u043c\", \"soir\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041f\u043e \u043c\u043e\u0435\u043c\u0443 \u043c\u043d\u0435\u043d\u0438\u044e, \u0434\u0435\u0439\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c \u043d\u0430 \u0434\u0440\u0443\u0433\u043e\u0439 \u043d\u0430\u0440\u043e\u0434 \u043c\u043e\u0436\u0435\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u043e\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0438\u043c\u0435\u0435\u0442 \u0432\u044b\u0441\u0448\u0435\u0435 \u0440\u0430\u0437\u0432\u0438\u0442\u0438\u0435, \u043a\u043e\u0442\u043e\u0440\u044b\u0439...\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u041f\u043e', '\u2581\u043c\u043e\u0435', '\u043c\u0443', '\u2581\u043c\u043d\u0435\u043d\u0438\u044e', ',', '\u2581\u0434\u0435\u0439', '\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c', '\u2581\u043d\u0430', '\u2581\u0434\u0440\u0443\u0433\u043e\u0439', '\u2581\u043d\u0430\u0440\u043e\u0434', '\u2581\u043c\u043e\u0436\u0435\u0442', '\u2581\u0442\u043e\u043b\u044c\u043a\u043e', '\u2581\u0442\u043e\u0442', ',', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439', '\u2581\u0438\u043c\u0435\u0435\u0442', '\u2581\u0432\u044b\u0441', '\u0448\u0435\u0435', '\u2581\u0440\u0430\u0437\u0432\u0438\u0442\u0438\u0435', ',', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439', '...']"}], "[\n  [\"\u041f\u043e\", null],\n  [\"\u2581\u043c\u043e\u0435\", \"my\"],\n  [\"\u043c\u0443\", null],\n  [\"\u2581\u043c\u043d\u0435\u043d\u0438\u044e\", \"opinion\"],\n  [\",\", null],\n  [\"\u2581\u0434\u0435\u0439\", \"act\"],\n  [\"\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c\", null],\n  [\"\u2581\u043d\u0430\", null],\n  [\"\u2581\u0434\u0440\u0443\u0433\u043e\u0439\", \"another\"],\n  [\"\u2581\u043d\u0430\u0440\u043e\u0434\", \"people\"],\n  [\"\u2581\u043c\u043e\u0436\u0435\u0442\", \"can\"],\n  [\"\u2581\u0442\u043e\u043b\u044c\u043a\u043e\", \"only\"],\n  [\"\u2581\u0442\u043e\u0442\", \"the one\"],\n  [\",\", null],\n  [\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439\", \"who\"],\n  [\"\u2581\u0438\u043c\u0435\u0435\u0442\", \"has\"],\n  [\"\u2581\u0432\u044b\u0441\", \"higher\"],\n  [\"\u0448\u0435\u0435\", null],\n  [\"\u2581\u0440\u0430\u0437\u0432\u0438\u0442\u0438\u0435\", \"development\"],\n  [\",\", null],\n  [\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439\", \"who\"],\n  [\"...\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u041f\u043e \u043c\u043e\u0435\u043c\u0443 \u043c\u043d\u0435\u043d\u0438\u044e, \u0434\u0435\u0439\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c \u043d\u0430 \u0434\u0440\u0443\u0433\u043e\u0439 \u043d\u0430\u0440\u043e\u0434 \u043c\u043e\u0436\u0435\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u043e\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0438\u043c\u0435\u0435\u0442 \u0432\u044b\u0441\u0448\u0435\u0435 \u0440\u0430\u0437\u0432\u0438\u0442\u0438\u0435, \u043a\u043e\u0442\u043e\u0440\u044b\u0439...\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u041f\u043e', '\u2581\u043c\u043e\u0435', '\u043c\u0443', '\u2581\u043c\u043d\u0435\u043d\u0438\u044e', ',', '\u2581\u0434\u0435\u0439', '\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c', '\u2581\u043d\u0430', '\u2581\u0434\u0440\u0443\u0433\u043e\u0439', '\u2581\u043d\u0430\u0440\u043e\u0434', '\u2581\u043c\u043e\u0436\u0435\u0442', '\u2581\u0442\u043e\u043b\u044c\u043a\u043e', '\u2581\u0442\u043e\u0442', ',', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439', '\u2581\u0438\u043c\u0435\u0435\u0442', '\u2581\u0432\u044b\u0441', '\u0448\u0435\u0435', '\u2581\u0440\u0430\u0437\u0432\u0438\u0442\u0438\u0435', ',', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439', '...']"}], "[\n    [\"\u041f\u043e\", null],\n    [\"\u2581\u043c\u043e\u0435\", \"mon\"],\n    [\"\u043c\u0443\", null],\n    [\"\u2581\u043c\u043d\u0435\u043d\u0438\u044e\", \"avis\"],\n    [\",\", null],\n    [\"\u2581\u0434\u0435\u0439\", \"agir\"],\n    [\"\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c\", null],\n    [\"\u2581\u043d\u0430\", null],\n    [\"\u2581\u0434\u0440\u0443\u0433\u043e\u0439\", \"autre\"],\n    [\"\u2581\u043d\u0430\u0440\u043e\u0434\", \"peuple\"],\n    [\"\u2581\u043c\u043e\u0436\u0435\u0442\", \"peut\"],\n    [\"\u2581\u0442\u043e\u043b\u044c\u043a\u043e\", \"seulement\"],\n    [\"\u2581\u0442\u043e\u0442\", \"celui\"],\n    [\",\", null],\n    [\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439\", \"qui\"],\n    [\"\u2581\u0438\u043c\u0435\u0435\u0442\", \"a\"],\n    [\"\u2581\u0432\u044b\u0441\", \"sup\u00e9rieur\"],\n    [\"\u0448\u0435\u0435\", null],\n    [\"\u2581\u0440\u0430\u0437\u0432\u0438\u0442\u0438\u0435\", \"d\u00e9veloppement\"],\n    [\",\", null],\n    [\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439\", \"qui\"],\n    [\"...\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0411\u044b\u043b \u043e\u0434\u0438\u043d \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442\u0441\u043a\u0438\u0439 \u0442\u043e\u0432\u0430\u0440\u0438\u0449, \u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043e\u043d \u0441\u0431\u043b\u0438\u0437\u0438\u043b\u0441\u044f \u043f\u043e\u0441\u043b\u0435 \u0438 \u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043e\u043d \u043c\u043e\u0433 \u0431\u044b \u043f\u043e\u0433\u043e\u0432\u043e\u0440\u0438\u0442\u044c \u043e \u043b\u0438\u0447\u043d\u043e\u043c \u0433\u043e\u0440\u0435; \u043d\u043e \u0442\u043e\u0432\u0430\u0440\u0438\u0449 \u044d\u0442\u043e\u0442 \u0431\u044b\u043b \u043f\u043e\u043f\u0435\u0447\u0438\u0442\u0435\u043b\u0435\u043c \u0432 \u0434\u0430\u043b\u044c\u043d\u0435\u043c \u0443\u0447\u0435\u0431\u043d\u043e\u043c \u043e\u043a\u0440\u0443\u0433\u0435.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0411', '\u044b\u043b', '\u2581\u043e\u0434\u0438\u043d', '\u2581\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442', '\u0441\u043a\u0438\u0439', '\u2581\u0442\u043e\u0432\u0430\u0440\u0438', '\u0449', ',', '\u2581\u0441', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u043c', '\u2581\u043e\u043d', '\u2581\u0441', '\u0431\u043b\u0438', '\u0437\u0438', '\u043b\u0441\u044f', '\u2581\u043f\u043e\u0441\u043b\u0435', '\u2581\u0438', '\u2581\u0441', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u043c', '\u2581\u043e\u043d', '\u2581\u043c\u043e\u0433', '\u2581\u0431\u044b', '\u2581\u043f\u043e\u0433\u043e\u0432\u043e\u0440\u0438', '\u0442\u044c', '\u2581\u043e', '\u2581\u043b\u0438', '\u0447\u043d\u043e\u043c', '\u2581\u0433\u043e\u0440\u0435', ';', '\u2581\u043d\u043e', '\u2581\u0442\u043e\u0432\u0430\u0440\u0438', '\u0449', '\u2581\u044d\u0442\u043e\u0442', '\u2581\u0431\u044b\u043b', '\u2581\u043f\u043e', '\u043f\u0435', '\u0447\u0438', '\u0442\u0435\u043b\u0435\u043c', '\u2581\u0432', '\u2581\u0434\u0430\u043b\u044c', '\u043d\u0435\u043c', '\u2581\u0443\u0447\u0435\u0431', '\u043d\u043e\u043c', '\u2581\u043e\u043a\u0440\u0443', '\u0433\u0435', '.']"}], "[\n[\"\u0411\", null],\n[\"\u044b\u043b\", \"There\"],\n[\"\u2581\u043e\u0434\u0438\u043d\", \"was\"],\n[\"\u2581\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442\", \"university\"],\n[\"\u0441\u043a\u0438\u0439\", null],\n[\"\u2581\u0442\u043e\u0432\u0430\u0440\u0438\", \"friend\"],\n[\"\u0449\", null],\n[\",\", null],\n[\"\u2581\u0441\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u043c\", \"whom\"],\n[\"\u2581\u043e\u043d\", \"he\"],\n[\"\u2581\u0441\", null],\n[\"\u0431\u043b\u0438\", null],\n[\"\u0437\u0438\", null],\n[\"\u043b\u0441\u044f\", \"became close\"],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\", \"later\"],\n[\"\u2581\u0438\", \"and\"],\n[\"\u2581\u0441\", \"with\"],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u043c\", \"whom\"],\n[\"\u2581\u043e\u043d\", \"he\"],\n[\"\u2581\u043c\u043e\u0433\", \"could\"],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u043f\u043e\u0433\u043e\u0432\u043e\u0440\u0438\", \"talk\"],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043e\", \"about\"],\n[\"\u2581\u043b\u0438\", null],\n[\"\u0447\u043d\u043e\u043c\", \"personal\"],\n[\"\u2581\u0433\u043e\u0440\u0435\", \"sorrow\"],\n[\";\", null],\n[\"\u2581\u043d\u043e\", \"but\"],\n[\"\u2581\u0442\u043e\u0432\u0430\u0440\u0438\", \"friend\"],\n[\"\u0449\", null],\n[\"\u2581\u044d\u0442\u043e\u0442\", \"this\"],\n[\"\u2581\u0431\u044b\u043b\", \"was\"],\n[\"\u2581\u043f\u043e\", null],\n[\"\u043f\u0435\", null],\n[\"\u0447\u0438\", null],\n[\"\u0442\u0435\u043b\u0435\u043c\", \"curator\"],\n[\"\u2581\u0432\", \"in\"],\n[\"\u2581\u0434\u0430\u043b\u044c\", \"distant\"],\n[\"\u043d\u0435\u043c\", null],\n[\"\u2581\u0443\u0447\u0435\u0431\", \"educational\"],\n[\"\u043d\u043e\u043c\", null],\n[\"\u2581\u043e\u043a\u0440\u0443\", \"district\"],\n[\"\u0433\u0435\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u0411\u044b\u043b \u043e\u0434\u0438\u043d \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442\u0441\u043a\u0438\u0439 \u0442\u043e\u0432\u0430\u0440\u0438\u0449, \u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043e\u043d \u0441\u0431\u043b\u0438\u0437\u0438\u043b\u0441\u044f \u043f\u043e\u0441\u043b\u0435 \u0438 \u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043e\u043d \u043c\u043e\u0433 \u0431\u044b \u043f\u043e\u0433\u043e\u0432\u043e\u0440\u0438\u0442\u044c \u043e \u043b\u0438\u0447\u043d\u043e\u043c \u0433\u043e\u0440\u0435; \u043d\u043e \u0442\u043e\u0432\u0430\u0440\u0438\u0449 \u044d\u0442\u043e\u0442 \u0431\u044b\u043b \u043f\u043e\u043f\u0435\u0447\u0438\u0442\u0435\u043b\u0435\u043c \u0432 \u0434\u0430\u043b\u044c\u043d\u0435\u043c \u0443\u0447\u0435\u0431\u043d\u043e\u043c \u043e\u043a\u0440\u0443\u0433\u0435.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0411', '\u044b\u043b', '\u2581\u043e\u0434\u0438\u043d', '\u2581\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442', '\u0441\u043a\u0438\u0439', '\u2581\u0442\u043e\u0432\u0430\u0440\u0438', '\u0449', ',', '\u2581\u0441', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u043c', '\u2581\u043e\u043d', '\u2581\u0441', '\u0431\u043b\u0438', '\u0437\u0438', '\u043b\u0441\u044f', '\u2581\u043f\u043e\u0441\u043b\u0435', '\u2581\u0438', '\u2581\u0441', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u043c', '\u2581\u043e\u043d', '\u2581\u043c\u043e\u0433', '\u2581\u0431\u044b', '\u2581\u043f\u043e\u0433\u043e\u0432\u043e\u0440\u0438', '\u0442\u044c', '\u2581\u043e', '\u2581\u043b\u0438', '\u0447\u043d\u043e\u043c', '\u2581\u0433\u043e\u0440\u0435', ';', '\u2581\u043d\u043e', '\u2581\u0442\u043e\u0432\u0430\u0440\u0438', '\u0449', '\u2581\u044d\u0442\u043e\u0442', '\u2581\u0431\u044b\u043b', '\u2581\u043f\u043e', '\u043f\u0435', '\u0447\u0438', '\u0442\u0435\u043b\u0435\u043c', '\u2581\u0432', '\u2581\u0434\u0430\u043b\u044c', '\u043d\u0435\u043c', '\u2581\u0443\u0447\u0435\u0431', '\u043d\u043e\u043c', '\u2581\u043e\u043a\u0440\u0443', '\u0433\u0435', '.']"}], "[\n[\"\u0411\", \"Il\"],\n[\"\u044b\u043b\", null],\n[\"\u2581\u043e\u0434\u0438\u043d\", \"un\"],\n[\"\u2581\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442\", \"camarade\"],\n[\"\u0441\u043a\u0438\u0439\", null],\n[\"\u2581\u0442\u043e\u0432\u0430\u0440\u0438\", null],\n[\"\u0449\", null],\n[\",\", null],\n[\"\u2581\u0441\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u043c\", \"avec lequel\"],\n[\"\u2581\u043e\u043d\", \"il\"],\n[\"\u2581\u0441\", null],\n[\"\u0431\u043b\u0438\", null],\n[\"\u0437\u0438\", null],\n[\"\u043b\u0441\u044f\", null],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\", \"plus tard\"],\n[\"\u2581\u0438\", \"et\"],\n[\"\u2581\u0441\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u043c\", \"avec lequel\"],\n[\"\u2581\u043e\u043d\", \"il\"],\n[\"\u2581\u043c\u043e\u0433\", \"pouvait\"],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u043f\u043e\u0433\u043e\u0432\u043e\u0440\u0438\", \"parler\"],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043e\", \"de\"],\n[\"\u2581\u043b\u0438\", null],\n[\"\u0447\u043d\u043e\u043c\", null],\n[\"\u2581\u0433\u043e\u0440\u0435\", \"chagrin personnel\"],\n[\";\", null],\n[\"\u2581\u043d\u043e\", \"mais\"],\n[\"\u2581\u0442\u043e\u0432\u0430\u0440\u0438\", null],\n[\"\u0449\", null],\n[\"\u2581\u044d\u0442\u043e\u0442\", \"ce\"],\n[\"\u2581\u0431\u044b\u043b\", \"\u00e9tait\"],\n[\"\u2581\u043f\u043e\", null],\n[\"\u043f\u0435\", null],\n[\"\u0447\u0438\", null],\n[\"\u0442\u0435\u043b\u0435\u043c\", null],\n[\"\u2581\u0432\", \"dans\"],\n[\"\u2581\u0434\u0430\u043b\u044c\", null],\n[\"\u043d\u0435\u043c\", null],\n[\"\u2581\u0443\u0447\u0435\u0431\", null],\n[\"\u043d\u043e\u043c\", null],\n[\"\u2581\u043e\u043a\u0440\u0443\", null],\n[\"\u0433\u0435\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041d\u0430 \u0442\u0440\u0435\u0442\u0438\u0439 \u0434\u0435\u043d\u044c \u041d\u0438\u043a\u043e\u043b\u0430\u0439 \u0432\u044b\u0437\u0432\u0430\u043b \u0431\u0440\u0430\u0442\u0430 \u0432\u044b\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u043e\u043f\u044f\u0442\u044c \u0435\u043c\u0443 \u0441\u0432\u043e\u0439 \u043f\u043b\u0430\u043d \u0438 \u0441\u0442\u0430\u043b \u043d\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0441\u0443\u0436\u0434\u0430\u0442\u044c \u0435\u0433\u043e, \u043d\u043e \u0441\u0442\u0430\u043b \u0443\u043c\u044b\u0448\u043b\u0435\u043d\u043d\u043e \u0441\u043c\u0435\u0448\u0438\u0432\u0430\u0442\u044c \u0435\u0433\u043e \u0441 \u043a\u043e\u043c\u043c\u0443\u043d\u0438\u0437\u043c\u043e\u043c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u041d\u0430', '\u2581\u0442\u0440\u0435\u0442\u0438\u0439', '\u2581\u0434\u0435\u043d\u044c', '\u2581\u041d\u0438\u043a\u043e\u043b\u0430\u0439', '\u2581\u0432\u044b\u0437', '\u0432\u0430\u043b', '\u2581\u0431\u0440\u0430', '\u0442\u0430', '\u2581\u0432\u044b', '\u0441\u043a\u0430\u0437\u0430', '\u0442\u044c', '\u2581\u043e\u043f\u044f\u0442\u044c', '\u2581\u0435\u043c\u0443', '\u2581\u0441\u0432\u043e\u0439', '\u2581\u043f\u043b\u0430\u043d', '\u2581\u0438', '\u2581\u0441\u0442\u0430\u043b', '\u2581\u043d\u0435', '\u2581\u0442\u043e\u043b\u044c\u043a\u043e', '\u2581\u043e\u0441\u0443', '\u0436\u0434\u0430\u0442\u044c', '\u2581\u0435\u0433\u043e', ',', '\u2581\u043d\u043e', '\u2581\u0441\u0442\u0430\u043b', '\u2581\u0443', '\u043c\u044b\u0448\u043b\u0435\u043d', '\u043d\u043e', '\u2581\u0441\u043c\u0435', '\u0448\u0438', '\u0432\u0430\u0442\u044c', '\u2581\u0435\u0433\u043e', '\u2581\u0441', '\u2581\u043a\u043e\u043c\u043c\u0443', '\u043d\u0438\u0437', '\u043c\u043e\u043c', '.']"}], "[\n[\"\u041d\u0430\", null],\n[\"\u2581\u0442\u0440\u0435\u0442\u0438\u0439\", \"third\"],\n[\"\u2581\u0434\u0435\u043d\u044c\", \"day\"],\n[\"\u2581\u041d\u0438\u043a\u043e\u043b\u0430\u0439\", \"Nikolai\"],\n[\"\u2581\u0432\u044b\u0437\", \"summon\"],\n[\"\u0432\u0430\u043b\", null],\n[\"\u2581\u0431\u0440\u0430\", \"brother\"],\n[\"\u0442\u0430\", null],\n[\"\u2581\u0432\u044b\", null],\n[\"\u0441\u043a\u0430\u0437\u0430\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043e\u043f\u044f\u0442\u044c\", \"again\"],\n[\"\u2581\u0435\u043c\u0443\", null],\n[\"\u2581\u0441\u0432\u043e\u0439\", \"his\"],\n[\"\u2581\u043f\u043b\u0430\u043d\", \"plan\"],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0441\u0442\u0430\u043b\", \"began\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0442\u043e\u043b\u044c\u043a\u043e\", \"only\"],\n[\"\u2581\u043e\u0441\u0443\", \"condemn\"],\n[\"\u0436\u0434\u0430\u0442\u044c\", null],\n[\"\u2581\u0435\u0433\u043e\", null],\n[\",\", null],\n[\"\u2581\u043d\u043e\", null],\n[\"\u2581\u0441\u0442\u0430\u043b\", \"began\"],\n[\"\u2581\u0443\", null],\n[\"\u043c\u044b\u0448\u043b\u0435\u043d\", \"deliberately\"],\n[\"\u043d\u043e\", null],\n[\"\u2581\u0441\u043c\u0435\", \"mix\"],\n[\"\u0448\u0438\", null],\n[\"\u0432\u0430\u0442\u044c\", null],\n[\"\u2581\u0435\u0433\u043e\", null],\n[\"\u2581\u0441\", null],\n[\"\u2581\u043a\u043e\u043c\u043c\u0443\", \"communism\"],\n[\"\u043d\u0438\u0437\", null],\n[\"\u043c\u043e\u043c\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u041d\u0430 \u0442\u0440\u0435\u0442\u0438\u0439 \u0434\u0435\u043d\u044c \u041d\u0438\u043a\u043e\u043b\u0430\u0439 \u0432\u044b\u0437\u0432\u0430\u043b \u0431\u0440\u0430\u0442\u0430 \u0432\u044b\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u043e\u043f\u044f\u0442\u044c \u0435\u043c\u0443 \u0441\u0432\u043e\u0439 \u043f\u043b\u0430\u043d \u0438 \u0441\u0442\u0430\u043b \u043d\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0441\u0443\u0436\u0434\u0430\u0442\u044c \u0435\u0433\u043e, \u043d\u043e \u0441\u0442\u0430\u043b \u0443\u043c\u044b\u0448\u043b\u0435\u043d\u043d\u043e \u0441\u043c\u0435\u0448\u0438\u0432\u0430\u0442\u044c \u0435\u0433\u043e \u0441 \u043a\u043e\u043c\u043c\u0443\u043d\u0438\u0437\u043c\u043e\u043c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u041d\u0430', '\u2581\u0442\u0440\u0435\u0442\u0438\u0439', '\u2581\u0434\u0435\u043d\u044c', '\u2581\u041d\u0438\u043a\u043e\u043b\u0430\u0439', '\u2581\u0432\u044b\u0437', '\u0432\u0430\u043b', '\u2581\u0431\u0440\u0430', '\u0442\u0430', '\u2581\u0432\u044b', '\u0441\u043a\u0430\u0437\u0430', '\u0442\u044c', '\u2581\u043e\u043f\u044f\u0442\u044c', '\u2581\u0435\u043c\u0443', '\u2581\u0441\u0432\u043e\u0439', '\u2581\u043f\u043b\u0430\u043d', '\u2581\u0438', '\u2581\u0441\u0442\u0430\u043b', '\u2581\u043d\u0435', '\u2581\u0442\u043e\u043b\u044c\u043a\u043e', '\u2581\u043e\u0441\u0443', '\u0436\u0434\u0430\u0442\u044c', '\u2581\u0435\u0433\u043e', ',', '\u2581\u043d\u043e', '\u2581\u0441\u0442\u0430\u043b', '\u2581\u0443', '\u043c\u044b\u0448\u043b\u0435\u043d', '\u043d\u043e', '\u2581\u0441\u043c\u0435', '\u0448\u0438', '\u0432\u0430\u0442\u044c', '\u2581\u0435\u0433\u043e', '\u2581\u0441', '\u2581\u043a\u043e\u043c\u043c\u0443', '\u043d\u0438\u0437', '\u043c\u043e\u043c', '.']"}], "[\n    [\"\u041d\u0430\", null],\n    [\"\u2581\u0442\u0440\u0435\u0442\u0438\u0439\", \"troisi\u00e8me\"],\n    [\"\u2581\u0434\u0435\u043d\u044c\", \"jour\"],\n    [\"\u2581\u041d\u0438\u043a\u043e\u043b\u0430\u0439\", \"Nicolas\"],\n    [\"\u2581\u0432\u044b\u0437\", \"appeler\"],\n    [\"\u0432\u0430\u043b\", null],\n    [\"\u2581\u0431\u0440\u0430\", \"fr\u00e8re\"],\n    [\"\u0442\u0430\", null],\n    [\"\u2581\u0432\u044b\", null],\n    [\"\u0441\u043a\u0430\u0437\u0430\", \"exprimer\"],\n    [\"\u0442\u044c\", null],\n    [\"\u2581\u043e\u043f\u044f\u0442\u044c\", \"encore\"],\n    [\"\u2581\u0435\u043c\u0443\", null],\n    [\"\u2581\u0441\u0432\u043e\u0439\", \"son\"],\n    [\"\u2581\u043f\u043b\u0430\u043d\", \"plan\"],\n    [\"\u2581\u0438\", null],\n    [\"\u2581\u0441\u0442\u0430\u043b\", \"commen\u00e7a\"],\n    [\"\u2581\u043d\u0435\", null],\n    [\"\u2581\u0442\u043e\u043b\u044c\u043a\u043e\", \"seulement\"],\n    [\"\u2581\u043e\u0441\u0443\", \"condamner\"],\n    [\"\u0436\u0434\u0430\u0442\u044c\", null],\n    [\"\u2581\u0435\u0433\u043e\", null],\n    [\",\", null],\n    [\"\u2581\u043d\u043e\", null],\n    [\"\u2581\u0441\u0442\u0430\u043b\", \"commen\u00e7a\"],\n    [\"\u2581\u0443\", null],\n    [\"\u043c\u044b\u0448\u043b\u0435\u043d\", \"d\u00e9lib\u00e9r\u00e9ment\"],\n    [\"\u043d\u043e\", null],\n    [\"\u2581\u0441\u043c\u0435\", \"m\u00e9langer\"],\n    [\"\u0448\u0438\", null],\n    [\"\u0432\u0430\u0442\u044c\", null],\n    [\"\u2581\u0435\u0433\u043e\", null],\n    [\"\u2581\u0441\", null],\n    [\"\u2581\u043a\u043e\u043c\u043c\u0443\", \"communisme\"],\n    [\"\u043d\u0438\u0437\", null],\n    [\"\u043c\u043e\u043c\", null],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041e\u043d \u0441\u0435\u043b, \u043e\u0431\u0432\u0435\u0440\u043d\u0443\u043b \u0441\u0435\u0431\u0435 \u043d\u043e\u0433\u0438 \u0442\u0438\u0433\u0440\u043e\u0432\u044b\u043c \u043f\u043b\u0435\u0434\u043e\u043c \u0438 \u0437\u0430\u043a\u0443\u0440\u0438\u043b \u0441\u0438\u0433\u0430\u0440\u0443. -- \u041a\u0430\u043a \u044d\u0442\u043e \u0442\u044b \u043d\u0435 \u043a\u0443\u0440\u0438\u0448\u044c!\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u041e\u043d', '\u2581\u0441\u0435\u043b', ',', '\u2581\u043e\u0431', '\u0432\u0435\u0440', '\u043d\u0443\u043b', '\u2581\u0441\u0435\u0431\u0435', '\u2581\u043d\u043e\u0433\u0438', '\u2581\u0442\u0438', '\u0433\u0440\u043e', '\u0432\u044b\u043c', '\u2581\u043f\u043b\u0435', '\u0434\u043e\u043c', '\u2581\u0438', '\u2581\u0437\u0430\u043a\u0443', '\u0440\u0438\u043b', '\u2581\u0441\u0438', '\u0433\u0430', '\u0440\u0443', '.', '\u2581--', '\u2581\u041a\u0430\u043a', '\u2581\u044d\u0442\u043e', '\u2581\u0442\u044b', '\u2581\u043d\u0435', '\u2581\u043a\u0443\u0440\u0438', '\u0448\u044c', '!']"}], "[\n  [\"\u041e\u043d\", \"He\"],\n  [\"\u2581\u0441\u0435\u043b\", \"sat\"],\n  [\",\", null],\n  [\"\u2581\u043e\u0431\", \"wrapped\"],\n  [\"\u0432\u0435\u0440\", null],\n  [\"\u043d\u0443\u043b\", null],\n  [\"\u2581\u0441\u0435\u0431\u0435\", \"himself\"],\n  [\"\u2581\u043d\u043e\u0433\u0438\", \"legs\"],\n  [\"\u2581\u0442\u0438\", \"tiger\"],\n  [\"\u0433\u0440\u043e\", null],\n  [\"\u0432\u044b\u043c\", null],\n  [\"\u2581\u043f\u043b\u0435\", \"blanket\"],\n  [\"\u0434\u043e\u043c\", null],\n  [\"\u2581\u0438\", null],\n  [\"\u2581\u0437\u0430\u043a\u0443\", \"lit\"],\n  [\"\u0440\u0438\u043b\", null],\n  [\"\u2581\u0441\u0438\", \"cigar\"],\n  [\"\u0433\u0430\", null],\n  [\"\u0440\u0443\", null],\n  [\".\", null],\n  [\"\u2581--\", null],\n  [\"\u2581\u041a\u0430\u043a\", \"How\"],\n  [\"\u2581\u044d\u0442\u043e\", null],\n  [\"\u2581\u0442\u044b\", \"you\"],\n  [\"\u2581\u043d\u0435\", \"don't\"],\n  [\"\u2581\u043a\u0443\u0440\u0438\", \"smoke\"],\n  [\"\u0448\u044c\", null],\n  [\"!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u041e\u043d \u0441\u0435\u043b, \u043e\u0431\u0432\u0435\u0440\u043d\u0443\u043b \u0441\u0435\u0431\u0435 \u043d\u043e\u0433\u0438 \u0442\u0438\u0433\u0440\u043e\u0432\u044b\u043c \u043f\u043b\u0435\u0434\u043e\u043c \u0438 \u0437\u0430\u043a\u0443\u0440\u0438\u043b \u0441\u0438\u0433\u0430\u0440\u0443. -- \u041a\u0430\u043a \u044d\u0442\u043e \u0442\u044b \u043d\u0435 \u043a\u0443\u0440\u0438\u0448\u044c!\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u041e\u043d', '\u2581\u0441\u0435\u043b', ',', '\u2581\u043e\u0431', '\u0432\u0435\u0440', '\u043d\u0443\u043b', '\u2581\u0441\u0435\u0431\u0435', '\u2581\u043d\u043e\u0433\u0438', '\u2581\u0442\u0438', '\u0433\u0440\u043e', '\u0432\u044b\u043c', '\u2581\u043f\u043b\u0435', '\u0434\u043e\u043c', '\u2581\u0438', '\u2581\u0437\u0430\u043a\u0443', '\u0440\u0438\u043b', '\u2581\u0441\u0438', '\u0433\u0430', '\u0440\u0443', '.', '\u2581--', '\u2581\u041a\u0430\u043a', '\u2581\u044d\u0442\u043e', '\u2581\u0442\u044b', '\u2581\u043d\u0435', '\u2581\u043a\u0443\u0440\u0438', '\u0448\u044c', '!']"}], "[\n  [\"\u041e\u043d\", \"Il\"],\n  [\"\u2581\u0441\u0435\u043b\", \"s'est assis\"],\n  [\",\", null],\n  [\"\u2581\u043e\u0431\", \"envelopp\u00e9\"],\n  [\"\u0432\u0435\u0440\", null],\n  [\"\u043d\u0443\u043b\", null],\n  [\"\u2581\u0441\u0435\u0431\u0435\", \"ses\"],\n  [\"\u2581\u043d\u043e\u0433\u0438\", \"jambes\"],\n  [\"\u2581\u0442\u0438\", \"tigre\"],\n  [\"\u0433\u0440\u043e\", null],\n  [\"\u0432\u044b\u043c\", null],\n  [\"\u2581\u043f\u043b\u0435\", \"plaid\"],\n  [\"\u0434\u043e\u043c\", null],\n  [\"\u2581\u0438\", \"et\"],\n  [\"\u2581\u0437\u0430\u043a\u0443\", \"allum\u00e9\"],\n  [\"\u0440\u0438\u043b\", null],\n  [\"\u2581\u0441\u0438\", \"cigare\"],\n  [\"\u0433\u0430\", null],\n  [\"\u0440\u0443\", null],\n  [\".\", null],\n  [\"\u2581--\", null],\n  [\"\u2581\u041a\u0430\u043a\", \"Comment\"],\n  [\"\u2581\u044d\u0442\u043e\", null],\n  [\"\u2581\u0442\u044b\", \"tu\"],\n  [\"\u2581\u043d\u0435\", \"ne\"],\n  [\"\u2581\u043a\u0443\u0440\u0438\", \"fumes\"],\n  [\"\u0448\u044c\", null],\n  [\"!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041a\u043d\u044f\u0433\u0438\u043d\u044f \u041c\u044f\u0433\u043a\u0430\u044f \u0441\u0438\u0434\u0435\u043b\u0430 \u043f\u043e\u0441\u0435\u0440\u0435\u0434\u0438\u043d\u0435 \u043c\u0435\u0436\u0434\u0443 \u043e\u0431\u043e\u0438\u043c\u0438 \u043a\u0440\u0443\u0436\u043a\u0430\u043c\u0438 \u0438, \u043f\u0440\u0438\u0441\u043b\u0443\u0448\u0438\u0432\u0430\u044f\u0441\u044c, \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u043b\u0430 \u0443\u0447\u0430\u0441\u0442\u0438\u0435 \u0442\u043e \u0432 \u0442\u043e\u043c, \u0442\u043e \u0432 \u0434\u0440\u0443\u0433\u043e\u043c. -- \u041c\u043d\u0435 \u043d\u044b\u043d\u0447\u0435 \u0442\u0440\u0438 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430 \u0441\u043a\u0430\u0437\u0430\u043b\u0438 \u044d\u0442\u0443 \u0441\u0430\u043c\u0443\u044e \u0444\u0440\u0430\u0437\u0443 \u043f\u0440\u043e \u041a\u0430\u0443\u043b\u044c\u0431\u0430\u0445\u0430, \u0442\u043e\u0447\u043d\u043e \u0441\u0433\u043e\u0432\u043e\u0440\u0438\u043b\u0438\u0441\u044c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u041a', '\u043d\u044f', '\u0433\u0438', '\u043d\u044f', '\u2581\u041c', '\u044f\u0433', '\u043a\u0430\u044f', '\u2581\u0441\u0438', '\u0434\u0435\u043b\u0430', '\u2581\u043f\u043e\u0441\u0435', '\u0440\u0435\u0434\u0438', '\u043d\u0435', '\u2581\u043c\u0435\u0436\u0434\u0443', '\u2581\u043e\u0431\u043e\u0438', '\u043c\u0438', '\u2581\u043a\u0440\u0443', '\u0436', '\u043a\u0430\u043c\u0438', '\u2581\u0438', ',', '\u2581\u043f\u0440\u0438', '\u0441\u043b\u0443', '\u0448\u0438', '\u0432\u0430\u044f\u0441\u044c', ',', '\u2581\u043f\u0440\u0438\u043d\u0438\u043c\u0430', '\u043b\u0430', '\u2581\u0443\u0447\u0430\u0441\u0442\u0438\u0435', '\u2581\u0442\u043e', '\u2581\u0432', '\u2581\u0442\u043e\u043c', ',', '\u2581\u0442\u043e', '\u2581\u0432', '\u2581\u0434\u0440\u0443\u0433\u043e\u043c', '.', '\u2581--', '\u2581\u041c\u043d\u0435', '\u2581\u043d\u044b', '\u043d', '\u0447\u0435', '\u2581\u0442\u0440\u0438', '\u2581\u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430', '\u2581\u0441\u043a\u0430\u0437\u0430\u043b\u0438', '\u2581\u044d\u0442\u0443', '\u2581\u0441\u0430', '\u043c\u0443\u044e', '\u2581\u0444', '\u0440\u0430\u0437\u0443', '\u2581\u043f\u0440\u043e', '\u2581\u041a\u0430', '\u0443', '\u043b\u044c', '\u0431\u0430', '\u0445\u0430', ',', '\u2581\u0442\u043e\u0447\u043d\u043e', '\u2581\u0441', '\u0433\u043e\u0432\u043e\u0440\u0438', '\u043b\u0438\u0441\u044c', '.']"}], "[\n[\"\u041a\", \"Princess\"],\n[\"\u043d\u044f\", null],\n[\"\u0433\u0438\", null],\n[\"\u043d\u044f\", null],\n[\"\u2581\u041c\", \"Myagkaya\"],\n[\"\u044f\u0433\", null],\n[\"\u043a\u0430\u044f\", null],\n[\"\u2581\u0441\u0438\", \"sat\"],\n[\"\u0434\u0435\u043b\u0430\", null],\n[\"\u2581\u043f\u043e\u0441\u0435\", \"in\"],\n[\"\u0440\u0435\u0434\u0438\", null],\n[\"\u043d\u0435\", null],\n[\"\u2581\u043c\u0435\u0436\u0434\u0443\", \"between\"],\n[\"\u2581\u043e\u0431\u043e\u0438\", \"both\"],\n[\"\u043c\u0438\", null],\n[\"\u2581\u043a\u0440\u0443\", \"circles\"],\n[\"\u0436\", null],\n[\"\u043a\u0430\u043c\u0438\", null],\n[\"\u2581\u0438\", null],\n[\",\", null],\n[\"\u2581\u043f\u0440\u0438\", \"listening\"],\n[\"\u0441\u043b\u0443\", null],\n[\"\u0448\u0438\", null],\n[\"\u0432\u0430\u044f\u0441\u044c\", null],\n[\",\", null],\n[\"\u2581\u043f\u0440\u0438\u043d\u0438\u043c\u0430\", \"took\"],\n[\"\u043b\u0430\", null],\n[\"\u2581\u0443\u0447\u0430\u0441\u0442\u0438\u0435\", \"part\"],\n[\"\u2581\u0442\u043e\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0442\u043e\u043c\", \"one\"],\n[\",\", null],\n[\"\u2581\u0442\u043e\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0434\u0440\u0443\u0433\u043e\u043c\", \"another\"],\n[\".\", null],\n[\"\u2581--\", null],\n[\"\u2581\u041c\u043d\u0435\", \"Today\"],\n[\"\u2581\u043d\u044b\", null],\n[\"\u043d\", null],\n[\"\u0447\u0435\", null],\n[\"\u2581\u0442\u0440\u0438\", \"three\"],\n[\"\u2581\u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430\", \"people\"],\n[\"\u2581\u0441\u043a\u0430\u0437\u0430\u043b\u0438\", \"said\"],\n[\"\u2581\u044d\u0442\u0443\", \"this\"],\n[\"\u2581\u0441\u0430\", \"very\"],\n[\"\u043c\u0443\u044e\", null],\n[\"\u2581\u0444\", \"phrase\"],\n[\"\u0440\u0430\u0437\u0443\", null],\n[\"\u2581\u043f\u0440\u043e\", \"about\"],\n[\"\u2581\u041a\u0430\", \"Kaulbach\"],\n[\"\u0443\", null],\n[\"\u043b\u044c\", null],\n[\"\u0431\u0430\", null],\n[\"\u0445\u0430\", null],\n[\",\", null],\n[\"\u2581\u0442\u043e\u0447\u043d\u043e\", \"as if\"],\n[\"\u2581\u0441\", \"they\"],\n[\"\u0433\u043e\u0432\u043e\u0440\u0438\", \"had\"],\n[\"\u043b\u0438\u0441\u044c\", \"agreed\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u041a\u043d\u044f\u0433\u0438\u043d\u044f \u041c\u044f\u0433\u043a\u0430\u044f \u0441\u0438\u0434\u0435\u043b\u0430 \u043f\u043e\u0441\u0435\u0440\u0435\u0434\u0438\u043d\u0435 \u043c\u0435\u0436\u0434\u0443 \u043e\u0431\u043e\u0438\u043c\u0438 \u043a\u0440\u0443\u0436\u043a\u0430\u043c\u0438 \u0438, \u043f\u0440\u0438\u0441\u043b\u0443\u0448\u0438\u0432\u0430\u044f\u0441\u044c, \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u043b\u0430 \u0443\u0447\u0430\u0441\u0442\u0438\u0435 \u0442\u043e \u0432 \u0442\u043e\u043c, \u0442\u043e \u0432 \u0434\u0440\u0443\u0433\u043e\u043c. -- \u041c\u043d\u0435 \u043d\u044b\u043d\u0447\u0435 \u0442\u0440\u0438 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430 \u0441\u043a\u0430\u0437\u0430\u043b\u0438 \u044d\u0442\u0443 \u0441\u0430\u043c\u0443\u044e \u0444\u0440\u0430\u0437\u0443 \u043f\u0440\u043e \u041a\u0430\u0443\u043b\u044c\u0431\u0430\u0445\u0430, \u0442\u043e\u0447\u043d\u043e \u0441\u0433\u043e\u0432\u043e\u0440\u0438\u043b\u0438\u0441\u044c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u041a', '\u043d\u044f', '\u0433\u0438', '\u043d\u044f', '\u2581\u041c', '\u044f\u0433', '\u043a\u0430\u044f', '\u2581\u0441\u0438', '\u0434\u0435\u043b\u0430', '\u2581\u043f\u043e\u0441\u0435', '\u0440\u0435\u0434\u0438', '\u043d\u0435', '\u2581\u043c\u0435\u0436\u0434\u0443', '\u2581\u043e\u0431\u043e\u0438', '\u043c\u0438', '\u2581\u043a\u0440\u0443', '\u0436', '\u043a\u0430\u043c\u0438', '\u2581\u0438', ',', '\u2581\u043f\u0440\u0438', '\u0441\u043b\u0443', '\u0448\u0438', '\u0432\u0430\u044f\u0441\u044c', ',', '\u2581\u043f\u0440\u0438\u043d\u0438\u043c\u0430', '\u043b\u0430', '\u2581\u0443\u0447\u0430\u0441\u0442\u0438\u0435', '\u2581\u0442\u043e', '\u2581\u0432', '\u2581\u0442\u043e\u043c', ',', '\u2581\u0442\u043e', '\u2581\u0432', '\u2581\u0434\u0440\u0443\u0433\u043e\u043c', '.', '\u2581--', '\u2581\u041c\u043d\u0435', '\u2581\u043d\u044b', '\u043d', '\u0447\u0435', '\u2581\u0442\u0440\u0438', '\u2581\u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430', '\u2581\u0441\u043a\u0430\u0437\u0430\u043b\u0438', '\u2581\u044d\u0442\u0443', '\u2581\u0441\u0430', '\u043c\u0443\u044e', '\u2581\u0444', '\u0440\u0430\u0437\u0443', '\u2581\u043f\u0440\u043e', '\u2581\u041a\u0430', '\u0443', '\u043b\u044c', '\u0431\u0430', '\u0445\u0430', ',', '\u2581\u0442\u043e\u0447\u043d\u043e', '\u2581\u0441', '\u0433\u043e\u0432\u043e\u0440\u0438', '\u043b\u0438\u0441\u044c', '.']"}], "[\n[\"\u041a\", null],\n[\"\u043d\u044f\", null],\n[\"\u0433\u0438\", null],\n[\"\u043d\u044f\", \"princesse\"],\n[\"\u2581\u041c\", null],\n[\"\u044f\u0433\", null],\n[\"\u043a\u0430\u044f\", \"Miagka\u00efa\"],\n[\"\u2581\u0441\u0438\", null],\n[\"\u0434\u0435\u043b\u0430\", \"\u00e9tait assise\"],\n[\"\u2581\u043f\u043e\u0441\u0435\", null],\n[\"\u0440\u0435\u0434\u0438\", null],\n[\"\u043d\u0435\", \"au milieu\"],\n[\"\u2581\u043c\u0435\u0436\u0434\u0443\", \"entre\"],\n[\"\u2581\u043e\u0431\u043e\u0438\", null],\n[\"\u043c\u0438\", \"les deux\"],\n[\"\u2581\u043a\u0440\u0443\", null],\n[\"\u0436\", null],\n[\"\u043a\u0430\u043c\u0438\", \"groupes\"],\n[\"\u2581\u0438\", \"et\"],\n[\",\", null],\n[\"\u2581\u043f\u0440\u0438\", null],\n[\"\u0441\u043b\u0443\", null],\n[\"\u0448\u0438\", null],\n[\"\u0432\u0430\u044f\u0441\u044c\", \"\u00e9coutant\"],\n[\",\", null],\n[\"\u2581\u043f\u0440\u0438\u043d\u0438\u043c\u0430\", null],\n[\"\u043b\u0430\", \"participait\"],\n[\"\u2581\u0443\u0447\u0430\u0441\u0442\u0438\u0435\", \"alternativement\"],\n[\"\u2581\u0442\u043e\", \"tant\u00f4t\"],\n[\"\u2581\u0432\", \"\u00e0\"],\n[\"\u2581\u0442\u043e\u043c\", \"l'un\"],\n[\",\", null],\n[\"\u2581\u0442\u043e\", \"tant\u00f4t\"],\n[\"\u2581\u0432\", \"\u00e0\"],\n[\"\u2581\u0434\u0440\u0443\u0433\u043e\u043c\", \"l'autre\"],\n[\".\", null],\n[\"\u2581--\", null],\n[\"\u2581\u041c\u043d\u0435\", \"Moi\"],\n[\"\u2581\u043d\u044b\", null],\n[\"\u043d\", null],\n[\"\u0447\u0435\", \"aujourd'hui\"],\n[\"\u2581\u0442\u0440\u0438\", \"trois\"],\n[\"\u2581\u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430\", \"personnes\"],\n[\"\u2581\u0441\u043a\u0430\u0437\u0430\u043b\u0438\", \"ont dit\"],\n[\"\u2581\u044d\u0442\u0443\", \"cette\"],\n[\"\u2581\u0441\u0430\", null],\n[\"\u043c\u0443\u044e\", \"m\u00eame\"],\n[\"\u2581\u0444\", null],\n[\"\u0440\u0430\u0437\u0443\", \"phrase\"],\n[\"\u2581\u043f\u0440\u043e\", \"sur\"],\n[\"\u2581\u041a\u0430\", null],\n[\"\u0443\", null],\n[\"\u043b\u044c\", null],\n[\"\u0431\u0430\", null],\n[\"\u0445\u0430\", \"Kaulbach\"],\n[\",\", null],\n[\"\u2581\u0442\u043e\u0447\u043d\u043e\", \"comme si\"],\n[\"\u2581\u0441\", null],\n[\"\u0433\u043e\u0432\u043e\u0440\u0438\", null],\n[\"\u043b\u0438\u0441\u044c\", \"elles s'\u00e9taient concert\u00e9es\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0422\u043e\u0447\u043d\u043e \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043a\u0430\u043a\u043e\u0435-\u043d\u0438\u0431\u0443\u0434\u044c \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0438 \u0431\u043e\u0440\u044c\u0431\u0430 \u0432 \u0431\u0435\u0441\u043a\u043e\u043d\u0435\u0447\u043d\u043e\u043c!\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0422\u043e', '\u0447\u043d\u043e', '\u2581\u043c\u043e\u0436\u0435\u0442', '\u2581\u0431\u044b\u0442\u044c', '\u2581\u043a\u0430\u043a\u043e\u0435', '-', '\u043d\u0438\u0431\u0443\u0434\u044c', '\u2581\u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435', '\u2581\u0438', '\u2581\u0431\u043e\u0440\u044c', '\u0431\u0430', '\u2581\u0432', '\u2581\u0431\u0435\u0441\u043a\u043e', '\u043d\u0435', '\u0447\u043d\u043e\u043c', '!']"}], "[\n  [\"\u0422\u043e\", \"Surely\"],\n  [\"\u0447\u043d\u043e\", null],\n  [\"\u2581\u043c\u043e\u0436\u0435\u0442\", \"can\"],\n  [\"\u2581\u0431\u044b\u0442\u044c\", \"be\"],\n  [\"\u2581\u043a\u0430\u043a\u043e\u0435\", \"some\"],\n  [\"-\", null],\n  [\"\u043d\u0438\u0431\u0443\u0434\u044c\", null],\n  [\"\u2581\u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435\", \"direction\"],\n  [\"\u2581\u0438\", null],\n  [\"\u2581\u0431\u043e\u0440\u044c\", \"struggle\"],\n  [\"\u0431\u0430\", null],\n  [\"\u2581\u0432\", null],\n  [\"\u2581\u0431\u0435\u0441\u043a\u043e\", \"infinite\"],\n  [\"\u043d\u0435\", null],\n  [\"\u0447\u043d\u043e\u043c\", null],\n  [\"!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u0422\u043e\u0447\u043d\u043e \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043a\u0430\u043a\u043e\u0435-\u043d\u0438\u0431\u0443\u0434\u044c \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0438 \u0431\u043e\u0440\u044c\u0431\u0430 \u0432 \u0431\u0435\u0441\u043a\u043e\u043d\u0435\u0447\u043d\u043e\u043c!\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0422\u043e', '\u0447\u043d\u043e', '\u2581\u043c\u043e\u0436\u0435\u0442', '\u2581\u0431\u044b\u0442\u044c', '\u2581\u043a\u0430\u043a\u043e\u0435', '-', '\u043d\u0438\u0431\u0443\u0434\u044c', '\u2581\u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435', '\u2581\u0438', '\u2581\u0431\u043e\u0440\u044c', '\u0431\u0430', '\u2581\u0432', '\u2581\u0431\u0435\u0441\u043a\u043e', '\u043d\u0435', '\u0447\u043d\u043e\u043c', '!']"}], "[\n    [\"\u0422\u043e\", \"Certainement\"],\n    [\"\u0447\u043d\u043e\", null],\n    [\"\u2581\u043c\u043e\u0436\u0435\u0442\", \"peut\"],\n    [\"\u2581\u0431\u044b\u0442\u044c\", \"\u00eatre\"],\n    [\"\u2581\u043a\u0430\u043a\u043e\u0435\", \"quelque\"],\n    [\"-\", null],\n    [\"\u043d\u0438\u0431\u0443\u0434\u044c\", null],\n    [\"\u2581\u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435\", \"direction\"],\n    [\"\u2581\u0438\", null],\n    [\"\u2581\u0431\u043e\u0440\u044c\", \"lutte\"],\n    [\"\u0431\u0430\", null],\n    [\"\u2581\u0432\", null],\n    [\"\u2581\u0431\u0435\u0441\u043a\u043e\", \"infini\"],\n    [\"\u043d\u0435\", null],\n    [\"\u0447\u043d\u043e\u043c\", null],\n    [\"!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n-- \u041a\u043e\u0441\u0442\u044f, \u0442\u044b \u0437\u043d\u0430\u0435\u0448\u044c, \u0447\u0442\u043e \u0443 \u043c\u0435\u043d\u044f \u0443\u0436 \u043e\u0441\u0442\u0430\u0435\u0442\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u044f\u0442\u044c\u0434\u0435\u0441\u044f\u0442 \u0440\u0443\u0431\u043b\u0435\u0439.\n\n\n---=== TOKENIZED TEXT ===---\n\n['--', '\u2581\u041a\u043e', '\u0441\u0442\u044f', ',', '\u2581\u0442\u044b', '\u2581\u0437\u043d\u0430\u0435\u0448\u044c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0443', '\u2581\u043c\u0435\u043d\u044f', '\u2581\u0443\u0436', '\u2581\u043e\u0441\u0442\u0430\u0435\u0442\u0441\u044f', '\u2581\u0442\u043e\u043b\u044c\u043a\u043e', '\u2581\u043f\u044f\u0442\u044c', '\u0434\u0435', '\u0441\u044f\u0442', '\u2581\u0440\u0443\u0431\u043b\u0435\u0439', '.']"}], "[\n  [\"--\", null],\n  [\"\u2581\u041a\u043e\", \"Kostya\"],\n  [\"\u0441\u0442\u044f\", null],\n  [\",\", null],\n  [\"\u2581\u0442\u044b\", \"you\"],\n  [\"\u2581\u0437\u043d\u0430\u0435\u0448\u044c\", \"know\"],\n  [\",\", null],\n  [\"\u2581\u0447\u0442\u043e\", \"that\"],\n  [\"\u2581\u0443\", null],\n  [\"\u2581\u043c\u0435\u043d\u044f\", \"I\"],\n  [\"\u2581\u0443\u0436\", \"already\"],\n  [\"\u2581\u043e\u0441\u0442\u0430\u0435\u0442\u0441\u044f\", \"have left\"],\n  [\"\u2581\u0442\u043e\u043b\u044c\u043a\u043e\", \"only\"],\n  [\"\u2581\u043f\u044f\u0442\u044c\", \"fifty\"],\n  [\"\u0434\u0435\", null],\n  [\"\u0441\u044f\u0442\", null],\n  [\"\u2581\u0440\u0443\u0431\u043b\u0435\u0439\", \"rubles\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n-- \u041a\u043e\u0441\u0442\u044f, \u0442\u044b \u0437\u043d\u0430\u0435\u0448\u044c, \u0447\u0442\u043e \u0443 \u043c\u0435\u043d\u044f \u0443\u0436 \u043e\u0441\u0442\u0430\u0435\u0442\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u044f\u0442\u044c\u0434\u0435\u0441\u044f\u0442 \u0440\u0443\u0431\u043b\u0435\u0439.\n\n\n---=== TOKENIZED TEXT ===---\n\n['--', '\u2581\u041a\u043e', '\u0441\u0442\u044f', ',', '\u2581\u0442\u044b', '\u2581\u0437\u043d\u0430\u0435\u0448\u044c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0443', '\u2581\u043c\u0435\u043d\u044f', '\u2581\u0443\u0436', '\u2581\u043e\u0441\u0442\u0430\u0435\u0442\u0441\u044f', '\u2581\u0442\u043e\u043b\u044c\u043a\u043e', '\u2581\u043f\u044f\u0442\u044c', '\u0434\u0435', '\u0441\u044f\u0442', '\u2581\u0440\u0443\u0431\u043b\u0435\u0439', '.']"}], "[\n    [\"--\", null],\n    [\"\u2581\u041a\u043e\", \"Kostia\"],\n    [\"\u0441\u0442\u044f\", null],\n    [\",\", null],\n    [\"\u2581\u0442\u044b\", \"tu\"],\n    [\"\u2581\u0437\u043d\u0430\u0435\u0448\u044c\", \"sais\"],\n    [\",\", null],\n    [\"\u2581\u0447\u0442\u043e\", \"que\"],\n    [\"\u2581\u0443\", null],\n    [\"\u2581\u043c\u0435\u043d\u044f\", \"je\"],\n    [\"\u2581\u0443\u0436\", \"ne\"],\n    [\"\u2581\u043e\u0441\u0442\u0430\u0435\u0442\u0441\u044f\", \"reste\"],\n    [\"\u2581\u0442\u043e\u043b\u044c\u043a\u043e\", \"que\"],\n    [\"\u2581\u043f\u044f\u0442\u044c\", \"cinquante\"],\n    [\"\u0434\u0435\", null],\n    [\"\u0441\u044f\u0442\", null],\n    [\"\u2581\u0440\u0443\u0431\u043b\u0435\u0439\", \"roubles\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nBut this is the way we manage it; we collect a crowd like this one here, then each personin turn passes his head through a hole, and makes a grimace at the rest; time one who makes the ugliest, is elected pope by general acclamation; that's the way it is.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581But', '\u2581this', '\u2581is', '\u2581the', '\u2581way', '\u2581we', '\u2581manage', '\u2581it', ';', '\u2581we', '\u2581collect', '\u2581a', '\u2581crowd', '\u2581like', '\u2581this', '\u2581one', '\u2581here', ',', '\u2581then', '\u2581each', '\u2581person', 'in', '\u2581turn', '\u2581passes', '\u2581his', '\u2581head', '\u2581through', '\u2581a', '\u2581hole', ',', '\u2581and', '\u2581makes', '\u2581a', '\u2581gr', 'im', 'ace', '\u2581at', '\u2581the', '\u2581rest', ';', '\u2581time', '\u2581one', '\u2581who', '\u2581makes', '\u2581the', '\u2581u', 'g', 'liest', ',', '\u2581is', '\u2581elected', '\u2581po', 'pe', '\u2581by', '\u2581general', '\u2581acc', 'lam', 'ation', ';', '\u2581that', \"'\", 's', '\u2581the', '\u2581way', '\u2581it', '\u2581is', '.']"}], "[\n[\"\u2581But\", \"Mais\"],\n[\"\u2581this\", \"ceci\"],\n[\"\u2581is\", null],\n[\"\u2581the\", null],\n[\"\u2581way\", \"fa\u00e7on\"],\n[\"\u2581we\", \"nous\"],\n[\"\u2581manage\", \"g\u00e9rons\"],\n[\"\u2581it\", null],\n[\";\", null],\n[\"\u2581we\", \"nous\"],\n[\"\u2581collect\", \"rassemblons\"],\n[\"\u2581a\", null],\n[\"\u2581crowd\", \"foule\"],\n[\"\u2581like\", \"comme\"],\n[\"\u2581this\", \"celle\"],\n[\"\u2581one\", null],\n[\"\u2581here\", \"ici\"],\n[\",\", null],\n[\"\u2581then\", \"puis\"],\n[\"\u2581each\", \"chaque\"],\n[\"\u2581person\", \"personne\"],\n[\"in\", null],\n[\"\u2581turn\", \"tour\"],\n[\"\u2581passes\", \"passe\"],\n[\"\u2581his\", \"sa\"],\n[\"\u2581head\", \"t\u00eate\"],\n[\"\u2581through\", \"\u00e0 travers\"],\n[\"\u2581a\", null],\n[\"\u2581hole\", \"trou\"],\n[\",\", null],\n[\"\u2581and\", \"et\"],\n[\"\u2581makes\", \"fait\"],\n[\"\u2581a\", null],\n[\"\u2581gr\", \"grimace\"],\n[\"im\", null],\n[\"ace\", null],\n[\"\u2581at\", \"\u00e0\"],\n[\"\u2581the\", null],\n[\"\u2581rest\", \"reste\"],\n[\";\", null],\n[\"\u2581time\", \"celui\"],\n[\"\u2581one\", null],\n[\"\u2581who\", \"qui\"],\n[\"\u2581makes\", \"fait\"],\n[\"\u2581the\", null],\n[\"\u2581u\", \"plus laide\"],\n[\"g\", null],\n[\"liest\", null],\n[\",\", null],\n[\"\u2581is\", \"est\"],\n[\"\u2581elected\", \"\u00e9lu\"],\n[\"\u2581po\", \"pape\"],\n[\"pe\", null],\n[\"\u2581by\", \"par\"],\n[\"\u2581general\", \"acclamation\"],\n[\"\u2581acc\", null],\n[\"lam\", null],\n[\"ation\", null],\n[\";\", null],\n[\"\u2581that\", \"c'est\"],\n[\"'\", null],\n[\"s\", null],\n[\"\u2581the\", null],\n[\"\u2581way\", \"fa\u00e7on\"],\n[\"\u2581it\", null],\n[\"\u2581is\", \"dont\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nBut this is the way we manage it; we collect a crowd like this one here, then each personin turn passes his head through a hole, and makes a grimace at the rest; time one who makes the ugliest, is elected pope by general acclamation; that's the way it is.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581But', '\u2581this', '\u2581is', '\u2581the', '\u2581way', '\u2581we', '\u2581manage', '\u2581it', ';', '\u2581we', '\u2581collect', '\u2581a', '\u2581crowd', '\u2581like', '\u2581this', '\u2581one', '\u2581here', ',', '\u2581then', '\u2581each', '\u2581person', 'in', '\u2581turn', '\u2581passes', '\u2581his', '\u2581head', '\u2581through', '\u2581a', '\u2581hole', ',', '\u2581and', '\u2581makes', '\u2581a', '\u2581gr', 'im', 'ace', '\u2581at', '\u2581the', '\u2581rest', ';', '\u2581time', '\u2581one', '\u2581who', '\u2581makes', '\u2581the', '\u2581u', 'g', 'liest', ',', '\u2581is', '\u2581elected', '\u2581po', 'pe', '\u2581by', '\u2581general', '\u2581acc', 'lam', 'ation', ';', '\u2581that', \"'\", 's', '\u2581the', '\u2581way', '\u2581it', '\u2581is', '.']"}], "[\n[\"\u2581But\", \"\u041d\u043e\"],\n[\"\u2581this\", \"\u044d\u0442\u043e\"],\n[\"\u2581is\", null],\n[\"\u2581the\", null],\n[\"\u2581way\", \"\u0441\u043f\u043e\u0441\u043e\u0431\"],\n[\"\u2581we\", \"\u043c\u044b\"],\n[\"\u2581manage\", \"\u0443\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u043c\"],\n[\"\u2581it\", \"\u044d\u0442\u0438\u043c\"],\n[\";\", null],\n[\"\u2581we\", \"\u043c\u044b\"],\n[\"\u2581collect\", \"\u0441\u043e\u0431\u0438\u0440\u0430\u0435\u043c\"],\n[\"\u2581a\", null],\n[\"\u2581crowd\", \"\u0442\u043e\u043b\u043f\u0443\"],\n[\"\u2581like\", \"\u043a\u0430\u043a\"],\n[\"\u2581this\", \"\u044d\u0442\u0443\"],\n[\"\u2581one\", \"\u043e\u0434\u043d\u0443\"],\n[\"\u2581here\", \"\u0437\u0434\u0435\u0441\u044c\"],\n[\",\", null],\n[\"\u2581then\", \"\u0437\u0430\u0442\u0435\u043c\"],\n[\"\u2581each\", \"\u043a\u0430\u0436\u0434\u044b\u0439\"],\n[\"\u2581person\", \"\u0447\u0435\u043b\u043e\u0432\u0435\u043a\"],\n[\"in\", null],\n[\"\u2581turn\", \"\u043f\u043e \u043e\u0447\u0435\u0440\u0435\u0434\u0438\"],\n[\"\u2581passes\", \"\u043f\u0440\u043e\u0441\u043e\u0432\u044b\u0432\u0430\u0435\u0442\"],\n[\"\u2581his\", \"\u0441\u0432\u043e\u044e\"],\n[\"\u2581head\", \"\u0433\u043e\u043b\u043e\u0432\u0443\"],\n[\"\u2581through\", \"\u0447\u0435\u0440\u0435\u0437\"],\n[\"\u2581a\", null],\n[\"\u2581hole\", \"\u0434\u044b\u0440\u0443\"],\n[\",\", null],\n[\"\u2581and\", \"\u0438\"],\n[\"\u2581makes\", \"\u043a\u043e\u0440\u0447\u0438\u0442\"],\n[\"\u2581a\", null],\n[\"\u2581gr\", \"\u0433\u0440\u0438\u043c\u0430\u0441\u0443\"],\n[\"im\", null],\n[\"ace\", null],\n[\"\u2581at\", null],\n[\"\u2581the\", null],\n[\"\u2581rest\", \"\u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u043c\"],\n[\";\", null],\n[\"\u2581time\", \"\u0442\u043e\u0442\"],\n[\"\u2581one\", null],\n[\"\u2581who\", \"\u043a\u0442\u043e\"],\n[\"\u2581makes\", \"\u0441\u0434\u0435\u043b\u0430\u0435\u0442\"],\n[\"\u2581the\", null],\n[\"\u2581u\", \"\u0441\u0430\u043c\u0443\u044e\"],\n[\"g\", null],\n[\"liest\", \"\u0443\u0440\u043e\u0434\u043b\u0438\u0432\u0443\u044e\"],\n[\",\", null],\n[\"\u2581is\", null],\n[\"\u2581elected\", \"\u0438\u0437\u0431\u0438\u0440\u0430\u0435\u0442\u0441\u044f\"],\n[\"\u2581po\", \"\u043f\u0430\u043f\u043e\u0439\"],\n[\"pe\", null],\n[\"\u2581by\", null],\n[\"\u2581general\", \"\u0432\u0441\u0435\u043e\u0431\u0449\u0438\u043c\"],\n[\"\u2581acc\", \"\u043e\u0434\u043e\u0431\u0440\u0435\u043d\u0438\u0435\u043c\"],\n[\"lam\", null],\n[\"ation\", null],\n[\";\", null],\n[\"\u2581that\", \"\u0432\u043e\u0442\"],\n[\"'\", null],\n[\"s\", null],\n[\"\u2581the\", null],\n[\"\u2581way\", \"\u043a\u0430\u043a\"],\n[\"\u2581it\", \"\u044d\u0442\u043e\"],\n[\"\u2581is\", \"\u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nHe too was silent for a while and then again began in a less squeaky voice, coldly emphasizing certain chance words that had no special importance.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581He', '\u2581too', '\u2581was', '\u2581silent', '\u2581for', '\u2581a', '\u2581while', '\u2581and', '\u2581then', '\u2581again', '\u2581began', '\u2581in', '\u2581a', '\u2581less', '\u2581s', 'que', 'ak', 'y', '\u2581voice', ',', '\u2581cold', 'ly', '\u2581emphas', 'izing', '\u2581certain', '\u2581chance', '\u2581words', '\u2581that', '\u2581had', '\u2581no', '\u2581special', '\u2581importance', '.']"}], "[\n[\"\u2581He\", \"Il\"],\n[\"\u2581too\", \"aussi\"],\n[\"\u2581was\", null],\n[\"\u2581silent\", \"silencieux\"],\n[\"\u2581for\", null],\n[\"\u2581a\", null],\n[\"\u2581while\", \"moment\"],\n[\"\u2581and\", null],\n[\"\u2581then\", \"puis\"],\n[\"\u2581again\", null],\n[\"\u2581began\", \"commen\u00e7a\"],\n[\"\u2581in\", null],\n[\"\u2581a\", null],\n[\"\u2581less\", \"moins\"],\n[\"\u2581s\", \"grin\u00e7ante\"],\n[\"que\", null],\n[\"ak\", null],\n[\"y\", null],\n[\"\u2581voice\", \"voix\"],\n[\",\", null],\n[\"\u2581cold\", \"froidement\"],\n[\"ly\", null],\n[\"\u2581emphas\", \"soulignant\"],\n[\"izing\", null],\n[\"\u2581certain\", \"certains\"],\n[\"\u2581chance\", \"hasard\"],\n[\"\u2581words\", \"mots\"],\n[\"\u2581that\", null],\n[\"\u2581had\", null],\n[\"\u2581no\", null],\n[\"\u2581special\", \"particuli\u00e8re\"],\n[\"\u2581importance\", \"importance\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nHe too was silent for a while and then again began in a less squeaky voice, coldly emphasizing certain chance words that had no special importance.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581He', '\u2581too', '\u2581was', '\u2581silent', '\u2581for', '\u2581a', '\u2581while', '\u2581and', '\u2581then', '\u2581again', '\u2581began', '\u2581in', '\u2581a', '\u2581less', '\u2581s', 'que', 'ak', 'y', '\u2581voice', ',', '\u2581cold', 'ly', '\u2581emphas', 'izing', '\u2581certain', '\u2581chance', '\u2581words', '\u2581that', '\u2581had', '\u2581no', '\u2581special', '\u2581importance', '.']"}], "[\n  [\"\u2581He\", \"\u041e\u043d\"],\n  [\"\u2581too\", null],\n  [\"\u2581was\", null],\n  [\"\u2581silent\", \"\u043c\u043e\u043b\u0447\u0430\u043b\"],\n  [\"\u2581for\", null],\n  [\"\u2581a\", null],\n  [\"\u2581while\", \"\u043d\u0435\u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u0432\u0440\u0435\u043c\u044f\"],\n  [\"\u2581and\", null],\n  [\"\u2581then\", \"\u0437\u0430\u0442\u0435\u043c\"],\n  [\"\u2581again\", \"\u0441\u043d\u043e\u0432\u0430\"],\n  [\"\u2581began\", \"\u043d\u0430\u0447\u0430\u043b\"],\n  [\"\u2581in\", null],\n  [\"\u2581a\", null],\n  [\"\u2581less\", \"\u043c\u0435\u043d\u0435\u0435\"],\n  [\"\u2581s\", \"\u043f\u0438\u0441\u043a\u043b\u044f\u0432\u044b\u043c\"],\n  [\"que\", null],\n  [\"ak\", null],\n  [\"y\", null],\n  [\"\u2581voice\", \"\u0433\u043e\u043b\u043e\u0441\u043e\u043c\"],\n  [\",\", null],\n  [\"\u2581cold\", \"\u0445\u043e\u043b\u043e\u0434\u043d\u043e\"],\n  [\"ly\", null],\n  [\"\u2581emphas\", \"\u043f\u043e\u0434\u0447\u0435\u0440\u043a\u0438\u0432\u0430\u044f\"],\n  [\"izing\", null],\n  [\"\u2581certain\", \"\u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435\"],\n  [\"\u2581chance\", \"\u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0435\"],\n  [\"\u2581words\", \"\u0441\u043b\u043e\u0432\u0430\"],\n  [\"\u2581that\", null],\n  [\"\u2581had\", null],\n  [\"\u2581no\", null],\n  [\"\u2581special\", \"\u043e\u0441\u043e\u0431\u043e\u0433\u043e\"],\n  [\"\u2581importance\", \"\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nThe manor-house of Ferndean was a building of considerable antiquity, moderate size, and no architectural pretensions, deep buried in a wood.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581man', 'or', '-', 'house', '\u2581of', '\u2581Fer', 'nde', 'an', '\u2581was', '\u2581a', '\u2581building', '\u2581of', '\u2581considerable', '\u2581anti', 'qu', 'ity', ',', '\u2581moder', 'ate', '\u2581size', ',', '\u2581and', '\u2581no', '\u2581architect', 'ural', '\u2581pret', 'ensions', ',', '\u2581deep', '\u2581buried', '\u2581in', '\u2581a', '\u2581wood', '.']"}], "[\n  [\"\u2581The\", \"Le\"],\n  [\"\u2581man\", \"manoir\"],\n  [\"or\", null],\n  [\"-\", null],\n  [\"house\", null],\n  [\"\u2581of\", null],\n  [\"\u2581Fer\", \"Ferndean\"],\n  [\"nde\", null],\n  [\"an\", null],\n  [\"\u2581was\", null],\n  [\"\u2581a\", null],\n  [\"\u2581building\", \"b\u00e2timent\"],\n  [\"\u2581of\", null],\n  [\"\u2581considerable\", \"consid\u00e9rable\"],\n  [\"\u2581anti\", \"antiquit\u00e9\"],\n  [\"qu\", null],\n  [\"ity\", null],\n  [\",\", null],\n  [\"\u2581moder\", \"mod\u00e9r\u00e9e\"],\n  [\"ate\", null],\n  [\"\u2581size\", \"taille\"],\n  [\",\", null],\n  [\"\u2581and\", null],\n  [\"\u2581no\", null],\n  [\"\u2581architect\", \"architecturale\"],\n  [\"ural\", null],\n  [\"\u2581pret\", \"pr\u00e9tentions\"],\n  [\"ensions\", null],\n  [\",\", null],\n  [\"\u2581deep\", \"profond\u00e9ment\"],\n  [\"\u2581buried\", \"enfoui\"],\n  [\"\u2581in\", null],\n  [\"\u2581a\", null],\n  [\"\u2581wood\", \"bois\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThe manor-house of Ferndean was a building of considerable antiquity, moderate size, and no architectural pretensions, deep buried in a wood.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581man', 'or', '-', 'house', '\u2581of', '\u2581Fer', 'nde', 'an', '\u2581was', '\u2581a', '\u2581building', '\u2581of', '\u2581considerable', '\u2581anti', 'qu', 'ity', ',', '\u2581moder', 'ate', '\u2581size', ',', '\u2581and', '\u2581no', '\u2581architect', 'ural', '\u2581pret', 'ensions', ',', '\u2581deep', '\u2581buried', '\u2581in', '\u2581a', '\u2581wood', '.']"}], "[\n[\"\u2581The\", \"\u041f\u043e\u043c\u0435\u0441\u0442\u044c\u0435\"],\n[\"\u2581man\", null],\n[\"or\", null],\n[\"-\", null],\n[\"house\", null],\n[\"\u2581of\", null],\n[\"\u2581Fer\", \"\u0424\u0435\u0440\u043d\u0434\u0438\u043d\"],\n[\"nde\", null],\n[\"an\", null],\n[\"\u2581was\", null],\n[\"\u2581a\", null],\n[\"\u2581building\", \"\u0437\u0434\u0430\u043d\u0438\u0435\"],\n[\"\u2581of\", null],\n[\"\u2581considerable\", \"\u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439\"],\n[\"\u2581anti\", \"\u0434\u0440\u0435\u0432\u043d\u043e\u0441\u0442\u0438\"],\n[\"qu\", null],\n[\"ity\", null],\n[\",\", null],\n[\"\u2581moder\", \"\u0443\u043c\u0435\u0440\u0435\u043d\u043d\u044b\u0445\"],\n[\"ate\", null],\n[\"\u2581size\", \"\u0440\u0430\u0437\u043c\u0435\u0440\u043e\u0432\"],\n[\",\", null],\n[\"\u2581and\", null],\n[\"\u2581no\", null],\n[\"\u2581architect\", \"\u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u043d\u044b\u0445\"],\n[\"ural\", null],\n[\"\u2581pret\", \"\u043f\u0440\u0435\u0442\u0435\u043d\u0437\u0438\u0439\"],\n[\"ensions\", null],\n[\",\", null],\n[\"\u2581deep\", \"\u0433\u043b\u0443\u0431\u043e\u043a\u043e\"],\n[\"\u2581buried\", \"\u043f\u043e\u0433\u0440\u0443\u0436\u0435\u043d\u043d\u043e\u0435\"],\n[\"\u2581in\", null],\n[\"\u2581a\", null],\n[\"\u2581wood\", \"\u043b\u0435\u0441\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nThe fear of failure in these points harassed me worse than the physical hardships of my lot; though these were no trifles.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581fear', '\u2581of', '\u2581failure', '\u2581in', '\u2581these', '\u2581points', '\u2581har', 'ass', 'ed', '\u2581me', '\u2581worse', '\u2581than', '\u2581the', '\u2581physical', '\u2581hard', 'ships', '\u2581of', '\u2581my', '\u2581lot', ';', '\u2581though', '\u2581these', '\u2581were', '\u2581no', '\u2581trif', 'les', '.']"}], "[\n[\"\u2581The\", null],\n[\"\u2581fear\", \"peur\"],\n[\"\u2581of\", null],\n[\"\u2581failure\", \"\u00e9chec\"],\n[\"\u2581in\", null],\n[\"\u2581these\", null],\n[\"\u2581points\", \"points\"],\n[\"\u2581har\", \"harcelait\"],\n[\"ass\", null],\n[\"ed\", null],\n[\"\u2581me\", null],\n[\"\u2581worse\", \"pire\"],\n[\"\u2581than\", null],\n[\"\u2581the\", null],\n[\"\u2581physical\", \"physiques\"],\n[\"\u2581hard\", \"difficult\u00e9s\"],\n[\"ships\", null],\n[\"\u2581of\", null],\n[\"\u2581my\", null],\n[\"\u2581lot\", \"sort\"],\n[\";\", null],\n[\"\u2581though\", \"bien que\"],\n[\"\u2581these\", null],\n[\"\u2581were\", null],\n[\"\u2581no\", null],\n[\"\u2581trif\", \"bagatelles\"],\n[\"les\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThe fear of failure in these points harassed me worse than the physical hardships of my lot; though these were no trifles.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581fear', '\u2581of', '\u2581failure', '\u2581in', '\u2581these', '\u2581points', '\u2581har', 'ass', 'ed', '\u2581me', '\u2581worse', '\u2581than', '\u2581the', '\u2581physical', '\u2581hard', 'ships', '\u2581of', '\u2581my', '\u2581lot', ';', '\u2581though', '\u2581these', '\u2581were', '\u2581no', '\u2581trif', 'les', '.']"}], "[\n[\"\u2581The\", null],\n[\"\u2581fear\", \"\u0421\u0442\u0440\u0430\u0445\"],\n[\"\u2581of\", null],\n[\"\u2581failure\", \"\u043d\u0435\u0443\u0434\u0430\u0447\u0438\"],\n[\"\u2581in\", null],\n[\"\u2581these\", \"\u044d\u0442\u0438\u0445\"],\n[\"\u2581points\", \"\u043c\u043e\u043c\u0435\u043d\u0442\u0430\u0445\"],\n[\"\u2581har\", \"\u0431\u0435\u0441\u043f\u043e\u043a\u043e\u0438\u043b\"],\n[\"ass\", null],\n[\"ed\", null],\n[\"\u2581me\", \"\u043c\u0435\u043d\u044f\"],\n[\"\u2581worse\", \"\u0441\u0438\u043b\u044c\u043d\u0435\u0435\"],\n[\"\u2581than\", null],\n[\"\u2581the\", null],\n[\"\u2581physical\", \"\u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0438\u0445\"],\n[\"\u2581hard\", \"\u0442\u0440\u0443\u0434\u043d\u043e\u0441\u0442\u0435\u0439\"],\n[\"ships\", null],\n[\"\u2581of\", null],\n[\"\u2581my\", \"\u043c\u043e\u0435\u0439\"],\n[\"\u2581lot\", \"\u0443\u0447\u0430\u0441\u0442\u0438\"],\n[\";\", null],\n[\"\u2581though\", \"\u0445\u043e\u0442\u044f\"],\n[\"\u2581these\", \"\u043e\u043d\u0438\"],\n[\"\u2581were\", null],\n[\"\u2581no\", null],\n[\"\u2581trif\", \"\u043f\u0443\u0441\u0442\u044f\u043a\u0430\u043c\u0438\"],\n[\"les\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nThe archdeacon maintained silence for several minutes, during which Gringoire had time to observe him.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581arch', 'de', 'acon', '\u2581maintained', '\u2581silence', '\u2581for', '\u2581several', '\u2581minutes', ',', '\u2581during', '\u2581which', '\u2581Gr', 'ing', 'oire', '\u2581had', '\u2581time', '\u2581to', '\u2581observe', '\u2581him', '.']"}], "[\n  [\"\u2581The\", null],\n  [\"\u2581arch\", \"archidiacre\"],\n  [\"de\", null],\n  [\"acon\", null],\n  [\"\u2581maintained\", \"garda\"],\n  [\"\u2581silence\", \"silence\"],\n  [\"\u2581for\", null],\n  [\"\u2581several\", \"plusieurs\"],\n  [\"\u2581minutes\", \"minutes\"],\n  [\",\", null],\n  [\"\u2581during\", \"pendant\"],\n  [\"\u2581which\", null],\n  [\"\u2581Gr\", \"Gringoire\"],\n  [\"ing\", null],\n  [\"oire\", null],\n  [\"\u2581had\", \"eut\"],\n  [\"\u2581time\", \"temps\"],\n  [\"\u2581to\", null],\n  [\"\u2581observe\", \"observer\"],\n  [\"\u2581him\", \"le\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThe archdeacon maintained silence for several minutes, during which Gringoire had time to observe him.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581arch', 'de', 'acon', '\u2581maintained', '\u2581silence', '\u2581for', '\u2581several', '\u2581minutes', ',', '\u2581during', '\u2581which', '\u2581Gr', 'ing', 'oire', '\u2581had', '\u2581time', '\u2581to', '\u2581observe', '\u2581him', '.']"}], "[\n  [\"\u2581The\", null],\n  [\"\u2581arch\", \"\u0410\u0440\u0445\u0438\u0434\u044c\u044f\u043a\u043e\u043d\"],\n  [\"de\", null],\n  [\"acon\", null],\n  [\"\u2581maintained\", \"\u0441\u043e\u0445\u0440\u0430\u043d\u044f\u043b\"],\n  [\"\u2581silence\", \"\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u0435\"],\n  [\"\u2581for\", null],\n  [\"\u2581several\", \"\u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e\"],\n  [\"\u2581minutes\", \"\u043c\u0438\u043d\u0443\u0442\"],\n  [\",\", null],\n  [\"\u2581during\", \"\u0432 \u0442\u0435\u0447\u0435\u043d\u0438\u0435\"],\n  [\"\u2581which\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u0445\"],\n  [\"\u2581Gr\", \"\u0413\u0440\u0435\u043d\u0433\u0443\u0430\u0440\"],\n  [\"ing\", null],\n  [\"oire\", null],\n  [\"\u2581had\", \"\u0438\u043c\u0435\u043b\"],\n  [\"\u2581time\", \"\u0432\u0440\u0435\u043c\u044f\"],\n  [\"\u2581to\", null],\n  [\"\u2581observe\", \"\u043d\u0430\u0431\u043b\u044e\u0434\u0430\u0442\u044c\"],\n  [\"\u2581him\", \"\u0437\u0430 \u043d\u0438\u043c\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nEach of those faces that with so much searching, so many faults and corrections, he had evolved with its own character, each representing so much pain and pleasure, and all of them so often placed and replaced to obtain harmony; all the shades of colour and tone elaborated with such effort all this, regarded as a whole from those others' point of view, now seemed trivialities a thousand times repeated.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Each', '\u2581of', '\u2581those', '\u2581faces', '\u2581that', '\u2581with', '\u2581so', '\u2581much', '\u2581searching', ',', '\u2581so', '\u2581many', '\u2581fault', 's', '\u2581and', '\u2581corre', 'ctions', ',', '\u2581he', '\u2581had', '\u2581evol', 'ved', '\u2581with', '\u2581its', '\u2581own', '\u2581character', ',', '\u2581each', '\u2581representing', '\u2581so', '\u2581much', '\u2581pain', '\u2581and', '\u2581pleasure', ',', '\u2581and', '\u2581all', '\u2581of', '\u2581them', '\u2581so', '\u2581often', '\u2581placed', '\u2581and', '\u2581replaced', '\u2581to', '\u2581obtain', '\u2581harm', 'ony', ';', '\u2581all', '\u2581the', '\u2581sh', 'ades', '\u2581of', '\u2581colour', '\u2581and', '\u2581tone', '\u2581elabor', 'ated', '\u2581with', '\u2581such', '\u2581effort', '\u2581all', '\u2581this', ',', '\u2581regarded', '\u2581as', '\u2581a', '\u2581whole', '\u2581from', '\u2581those', '\u2581others', \"'\", '\u2581point', '\u2581of', '\u2581view', ',', '\u2581now', '\u2581seemed', '\u2581trivial', 'ities', '\u2581a', '\u2581thousand', '\u2581times', '\u2581repeated', '.']"}], "[\n  [\"\u2581Each\", \"Chacun\"],\n  [\"\u2581of\", null],\n  [\"\u2581those\", \"ces\"],\n  [\"\u2581faces\", \"visages\"],\n  [\"\u2581that\", \"que\"],\n  [\"\u2581with\", \"avec\"],\n  [\"\u2581so\", \"tellement\"],\n  [\"\u2581much\", \"de\"],\n  [\"\u2581searching\", \"recherche\"],\n  [\",\", null],\n  [\"\u2581so\", \"tellement\"],\n  [\"\u2581many\", \"de\"],\n  [\"\u2581fault\", \"d\u00e9fauts\"],\n  [\"s\", null],\n  [\"\u2581and\", \"et\"],\n  [\"\u2581corre\", \"corrections\"],\n  [\"ctions\", null],\n  [\",\", null],\n  [\"\u2581he\", \"il\"],\n  [\"\u2581had\", \"avait\"],\n  [\"\u2581evol\", \"d\u00e9velopp\u00e9\"],\n  [\"ved\", null],\n  [\"\u2581with\", \"avec\"],\n  [\"\u2581its\", \"son\"],\n  [\"\u2581own\", \"propre\"],\n  [\"\u2581character\", \"caract\u00e8re\"],\n  [\",\", null],\n  [\"\u2581each\", \"chacun\"],\n  [\"\u2581representing\", \"repr\u00e9sentant\"],\n  [\"\u2581so\", \"tellement\"],\n  [\"\u2581much\", \"de\"],\n  [\"\u2581pain\", \"douleur\"],\n  [\"\u2581and\", \"et\"],\n  [\"\u2581pleasure\", \"plaisir\"],\n  [\",\", null],\n  [\"\u2581and\", \"et\"],\n  [\"\u2581all\", \"tous\"],\n  [\"\u2581of\", null],\n  [\"\u2581them\", \"eux\"],\n  [\"\u2581so\", \"si\"],\n  [\"\u2581often\", \"souvent\"],\n  [\"\u2581placed\", \"plac\u00e9s\"],\n  [\"\u2581and\", \"et\"],\n  [\"\u2581replaced\", \"replac\u00e9s\"],\n  [\"\u2581to\", \"pour\"],\n  [\"\u2581obtain\", \"obtenir\"],\n  [\"\u2581harm\", \"harmonie\"],\n  [\"ony\", null],\n  [\";\", null],\n  [\"\u2581all\", \"toutes\"],\n  [\"\u2581the\", \"les\"],\n  [\"\u2581sh\", \"nuances\"],\n  [\"ades\", null],\n  [\"\u2581of\", \"de\"],\n  [\"\u2581colour\", \"couleur\"],\n  [\"\u2581and\", \"et\"],\n  [\"\u2581tone\", \"ton\"],\n  [\"\u2581elabor\", \"\u00e9labor\u00e9es\"],\n  [\"ated\", null],\n  [\"\u2581with\", \"avec\"],\n  [\"\u2581such\", \"tant\"],\n  [\"\u2581effort\", \"d'effort\"],\n  [\"\u2581all\", \"tout\"],\n  [\"\u2581this\", \"cela\"],\n  [\",\", null],\n  [\"\u2581regarded\", \"consid\u00e9r\u00e9\"],\n  [\"\u2581as\", \"comme\"],\n  [\"\u2581a\", \"un\"],\n  [\"\u2581whole\", \"ensemble\"],\n  [\"\u2581from\", \"du\"],\n  [\"\u2581those\", \"ces\"],\n  [\"\u2581others\", \"autres\"],\n  [\"'\", null],\n  [\"\u2581point\", \"point\"],\n  [\"\u2581of\", \"de\"],\n  [\"\u2581view\", \"vue\"],\n  [\",\", null],\n  [\"\u2581now\", \"maintenant\"],\n  [\"\u2581seemed\", \"semblait\"],\n  [\"\u2581trivial\", \"banalit\u00e9s\"],\n  [\"ities\", null],\n  [\"\u2581a\", \"mille\"],\n  [\"\u2581thousand\", null],\n  [\"\u2581times\", \"fois\"],\n  [\"\u2581repeated\", \"r\u00e9p\u00e9t\u00e9es\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nEach of those faces that with so much searching, so many faults and corrections, he had evolved with its own character, each representing so much pain and pleasure, and all of them so often placed and replaced to obtain harmony; all the shades of colour and tone elaborated with such effort all this, regarded as a whole from those others' point of view, now seemed trivialities a thousand times repeated.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Each', '\u2581of', '\u2581those', '\u2581faces', '\u2581that', '\u2581with', '\u2581so', '\u2581much', '\u2581searching', ',', '\u2581so', '\u2581many', '\u2581fault', 's', '\u2581and', '\u2581corre', 'ctions', ',', '\u2581he', '\u2581had', '\u2581evol', 'ved', '\u2581with', '\u2581its', '\u2581own', '\u2581character', ',', '\u2581each', '\u2581representing', '\u2581so', '\u2581much', '\u2581pain', '\u2581and', '\u2581pleasure', ',', '\u2581and', '\u2581all', '\u2581of', '\u2581them', '\u2581so', '\u2581often', '\u2581placed', '\u2581and', '\u2581replaced', '\u2581to', '\u2581obtain', '\u2581harm', 'ony', ';', '\u2581all', '\u2581the', '\u2581sh', 'ades', '\u2581of', '\u2581colour', '\u2581and', '\u2581tone', '\u2581elabor', 'ated', '\u2581with', '\u2581such', '\u2581effort', '\u2581all', '\u2581this', ',', '\u2581regarded', '\u2581as', '\u2581a', '\u2581whole', '\u2581from', '\u2581those', '\u2581others', \"'\", '\u2581point', '\u2581of', '\u2581view', ',', '\u2581now', '\u2581seemed', '\u2581trivial', 'ities', '\u2581a', '\u2581thousand', '\u2581times', '\u2581repeated', '.']"}], "[\n[\"\u2581Each\", \"\u041a\u0430\u0436\u0434\u043e\u0435\"],\n[\"\u2581of\", null],\n[\"\u2581those\", \"\u0442\u0435\u0445\"],\n[\"\u2581faces\", \"\u043b\u0438\u0446\"],\n[\"\u2581that\", null],\n[\"\u2581with\", null],\n[\"\u2581so\", null],\n[\"\u2581much\", null],\n[\"\u2581searching\", \"\u043f\u043e\u0438\u0441\u043a\u0430\"],\n[\",\", null],\n[\"\u2581so\", null],\n[\"\u2581many\", \"\u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u0430\"],\n[\"\u2581fault\", \"\u043d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043a\u043e\u0432\"],\n[\"s\", null],\n[\"\u2581and\", null],\n[\"\u2581corre\", \"\u0438\u0441\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0439\"],\n[\"ctions\", null],\n[\",\", null],\n[\"\u2581he\", \"\u043e\u043d\"],\n[\"\u2581had\", null],\n[\"\u2581evol\", \"\u0440\u0430\u0437\u0432\u0438\u043b\"],\n[\"ved\", null],\n[\"\u2581with\", null],\n[\"\u2581its\", \"\u0441\u0432\u043e\u0438\u043c\"],\n[\"\u2581own\", \"\u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u043c\"],\n[\"\u2581character\", \"\u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u043e\u043c\"],\n[\",\", null],\n[\"\u2581each\", \"\u043a\u0430\u0436\u0434\u043e\u0435\"],\n[\"\u2581representing\", \"\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0449\u0435\u0435\"],\n[\"\u2581so\", null],\n[\"\u2581much\", null],\n[\"\u2581pain\", \"\u0431\u043e\u043b\u044c\"],\n[\"\u2581and\", null],\n[\"\u2581pleasure\", \"\u0443\u0434\u043e\u0432\u043e\u043b\u044c\u0441\u0442\u0432\u0438\u0435\"],\n[\",\", null],\n[\"\u2581and\", null],\n[\"\u2581all\", \"\u0432\u0441\u0435\"],\n[\"\u2581of\", null],\n[\"\u2581them\", \"\u043e\u043d\u0438\"],\n[\"\u2581so\", null],\n[\"\u2581often\", \"\u0447\u0430\u0441\u0442\u043e\"],\n[\"\u2581placed\", \"\u0440\u0430\u0437\u043c\u0435\u0449\u0430\u043b\u0438\u0441\u044c\"],\n[\"\u2581and\", null],\n[\"\u2581replaced\", \"\u043f\u0435\u0440\u0435\u043c\u0435\u0449\u0430\u043b\u0438\u0441\u044c\"],\n[\"\u2581to\", null],\n[\"\u2581obtain\", \"\u0434\u043e\u0441\u0442\u0438\u0447\u044c\"],\n[\"\u2581harm\", \"\u0433\u0430\u0440\u043c\u043e\u043d\u0438\u0438\"],\n[\"ony\", null],\n[\";\", null],\n[\"\u2581all\", \"\u0432\u0441\u0435\"],\n[\"\u2581the\", null],\n[\"\u2581sh\", \"\u043e\u0442\u0442\u0435\u043d\u043a\u0438\"],\n[\"ades\", null],\n[\"\u2581of\", null],\n[\"\u2581colour\", \"\u0446\u0432\u0435\u0442\u0430\"],\n[\"\u2581and\", null],\n[\"\u2581tone\", \"\u0442\u043e\u043d\u0430\"],\n[\"\u2581elabor\", \"\u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u044b\u0435\"],\n[\"ated\", null],\n[\"\u2581with\", null],\n[\"\u2581such\", \"\u0442\u0430\u043a\u0438\u043c\"],\n[\"\u2581effort\", \"\u0443\u0441\u0438\u043b\u0438\u0435\u043c\"],\n[\"\u2581all\", \"\u0432\u0441\u0435\"],\n[\"\u2581this\", \"\u044d\u0442\u043e\"],\n[\",\", null],\n[\"\u2581regarded\", \"\u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0435\u043c\u043e\u0435\"],\n[\"\u2581as\", null],\n[\"\u2581a\", null],\n[\"\u2581whole\", \"\u0446\u0435\u043b\u043e\u0435\"],\n[\"\u2581from\", null],\n[\"\u2581those\", \"\u0442\u0435\u0445\"],\n[\"\u2581others\", \"\u0434\u0440\u0443\u0433\u0438\u0445\"],\n[\"'\", null],\n[\"\u2581point\", \"\u0442\u043e\u0447\u043a\u0438\"],\n[\"\u2581of\", null],\n[\"\u2581view\", \"\u0437\u0440\u0435\u043d\u0438\u044f\"],\n[\",\", null],\n[\"\u2581now\", \"\u0442\u0435\u043f\u0435\u0440\u044c\"],\n[\"\u2581seemed\", \"\u043a\u0430\u0437\u0430\u043b\u043e\u0441\u044c\"],\n[\"\u2581trivial\", \"\u0431\u0430\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044f\u043c\u0438\"],\n[\"ities\", null],\n[\"\u2581a\", null],\n[\"\u2581thousand\", \"\u0442\u044b\u0441\u044f\u0447\u0443\"],\n[\"\u2581times\", \"\u0440\u0430\u0437\"],\n[\"\u2581repeated\", \"\u043f\u043e\u0432\u0442\u043e\u0440\u0435\u043d\u043d\u044b\u043c\u0438\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nFor instance, it ought to consider the education of women injurious, yet it established courses of lectures and universities for women.'\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581For', '\u2581instance', ',', '\u2581it', '\u2581ought', '\u2581to', '\u2581consider', '\u2581the', '\u2581education', '\u2581of', '\u2581women', '\u2581inj', 'ur', 'ious', ',', '\u2581yet', '\u2581it', '\u2581established', '\u2581courses', '\u2581of', '\u2581lect', 'ures', '\u2581and', '\u2581univers', 'ities', '\u2581for', '\u2581women', \".'\"]"}], "[\n  [\"\u2581For\", \"Par\"],\n  [\"\u2581instance\", \"exemple\"],\n  [\",\", null],\n  [\"\u2581it\", null],\n  [\"\u2581ought\", \"devrait\"],\n  [\"\u2581to\", null],\n  [\"\u2581consider\", \"consid\u00e9rer\"],\n  [\"\u2581the\", null],\n  [\"\u2581education\", \"\u00e9ducation\"],\n  [\"\u2581of\", null],\n  [\"\u2581women\", \"femmes\"],\n  [\"\u2581inj\", \"nuisible\"],\n  [\"ur\", null],\n  [\"ious\", null],\n  [\",\", null],\n  [\"\u2581yet\", \"pourtant\"],\n  [\"\u2581it\", null],\n  [\"\u2581established\", \"\u00e9tabli\"],\n  [\"\u2581courses\", \"cours\"],\n  [\"\u2581of\", null],\n  [\"\u2581lect\", \"conf\u00e9rences\"],\n  [\"ures\", null],\n  [\"\u2581and\", null],\n  [\"\u2581univers\", \"universit\u00e9s\"],\n  [\"ities\", null],\n  [\"\u2581for\", \"pour\"],\n  [\"\u2581women\", \"femmes\"],\n  [\".'\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nFor instance, it ought to consider the education of women injurious, yet it established courses of lectures and universities for women.'\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581For', '\u2581instance', ',', '\u2581it', '\u2581ought', '\u2581to', '\u2581consider', '\u2581the', '\u2581education', '\u2581of', '\u2581women', '\u2581inj', 'ur', 'ious', ',', '\u2581yet', '\u2581it', '\u2581established', '\u2581courses', '\u2581of', '\u2581lect', 'ures', '\u2581and', '\u2581univers', 'ities', '\u2581for', '\u2581women', \".'\"]"}], "[\n[\"\u2581For\", \"\u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440\"],\n[\"\u2581instance\", null],\n[\",\", null],\n[\"\u2581it\", null],\n[\"\u2581ought\", \"\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043b\u043e\"],\n[\"\u2581to\", null],\n[\"\u2581consider\", \"\u0441\u0447\u0438\u0442\u0430\u0442\u044c\"],\n[\"\u2581the\", null],\n[\"\u2581education\", \"\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435\"],\n[\"\u2581of\", null],\n[\"\u2581women\", \"\u0436\u0435\u043d\u0449\u0438\u043d\"],\n[\"\u2581inj\", \"\u0432\u0440\u0435\u0434\u043d\u044b\u043c\"],\n[\"ur\", null],\n[\"ious\", null],\n[\",\", null],\n[\"\u2581yet\", \"\u043e\u0434\u043d\u0430\u043a\u043e\"],\n[\"\u2581it\", null],\n[\"\u2581established\", \"\u0443\u0447\u0440\u0435\u0434\u0438\u043b\u043e\"],\n[\"\u2581courses\", \"\u043a\u0443\u0440\u0441\u044b\"],\n[\"\u2581of\", null],\n[\"\u2581lect\", \"\u043b\u0435\u043a\u0446\u0438\u0439\"],\n[\"ures\", null],\n[\"\u2581and\", \"\u0438\"],\n[\"\u2581univers\", \"\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442\u044b\"],\n[\"ities\", null],\n[\"\u2581for\", \"\u0434\u043b\u044f\"],\n[\"\u2581women\", \"\u0436\u0435\u043d\u0449\u0438\u043d\"],\n[\".'\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\n'Oh, how sweet he is \u2013 don't frighten him!' exclaimed Kitty unexpectedly, looking at a sparrow that had settled on the railing, turned a raspberry stalk over, and was pecking at it.\n\n\n---=== TOKENIZED TEXT ===---\n\n[\"\u2581'\", 'Oh', ',', '\u2581how', '\u2581sweet', '\u2581he', '\u2581is', '\u2581\u2013', '\u2581don', \"'\", 't', '\u2581fright', 'en', '\u2581him', \"!'\", '\u2581exclaimed', '\u2581K', 'itt', 'y', '\u2581unexpected', 'ly', ',', '\u2581looking', '\u2581at', '\u2581a', '\u2581sp', 'arrow', '\u2581that', '\u2581had', '\u2581settled', '\u2581on', '\u2581the', '\u2581ra', 'iling', ',', '\u2581turned', '\u2581a', '\u2581r', 'asp', 'berry', '\u2581st', 'alk', '\u2581over', ',', '\u2581and', '\u2581was', '\u2581pe', 'ck', 'ing', '\u2581at', '\u2581it', '.']"}], "[\n  [\"\u2581'\", null],\n  [\"Oh\", \"Oh\"],\n  [\",\", null],\n  [\"\u2581how\", \"comment\"],\n  [\"\u2581sweet\", \"mignon\"],\n  [\"\u2581he\", \"il\"],\n  [\"\u2581is\", \"est\"],\n  [\"\u2581\u2013\", null],\n  [\"\u2581don\", \"ne\"],\n  [\"'\", null],\n  [\"t\", null],\n  [\"\u2581fright\", \"effrayez\"],\n  [\"en\", null],\n  [\"\u2581him\", \"pas\"],\n  [\"!'\", null],\n  [\"\u2581exclaimed\", \"s'exclama\"],\n  [\"\u2581K\", \"Kitty\"],\n  [\"itt\", null],\n  [\"y\", null],\n  [\"\u2581unexpected\", \"soudainement\"],\n  [\"ly\", null],\n  [\",\", null],\n  [\"\u2581looking\", \"regardant\"],\n  [\"\u2581at\", null],\n  [\"\u2581a\", \"un\"],\n  [\"\u2581sp\", \"moineau\"],\n  [\"arrow\", null],\n  [\"\u2581that\", \"qui\"],\n  [\"\u2581had\", \"s'\u00e9tait\"],\n  [\"\u2581settled\", \"pos\u00e9\"],\n  [\"\u2581on\", \"sur\"],\n  [\"\u2581the\", \"la\"],\n  [\"\u2581ra\", \"balustrade\"],\n  [\"iling\", null],\n  [\",\", null],\n  [\"\u2581turned\", \"retournait\"],\n  [\"\u2581a\", \"une\"],\n  [\"\u2581r\", \"tige\"],\n  [\"asp\", null],\n  [\"berry\", null],\n  [\"\u2581st\", null],\n  [\"alk\", null],\n  [\"\u2581over\", null],\n  [\",\", null],\n  [\"\u2581and\", \"et\"],\n  [\"\u2581was\", null],\n  [\"\u2581pe\", \"picorait\"],\n  [\"ck\", null],\n  [\"ing\", null],\n  [\"\u2581at\", null],\n  [\"\u2581it\", \"la\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\n'Oh, how sweet he is \u2013 don't frighten him!' exclaimed Kitty unexpectedly, looking at a sparrow that had settled on the railing, turned a raspberry stalk over, and was pecking at it.\n\n\n---=== TOKENIZED TEXT ===---\n\n[\"\u2581'\", 'Oh', ',', '\u2581how', '\u2581sweet', '\u2581he', '\u2581is', '\u2581\u2013', '\u2581don', \"'\", 't', '\u2581fright', 'en', '\u2581him', \"!'\", '\u2581exclaimed', '\u2581K', 'itt', 'y', '\u2581unexpected', 'ly', ',', '\u2581looking', '\u2581at', '\u2581a', '\u2581sp', 'arrow', '\u2581that', '\u2581had', '\u2581settled', '\u2581on', '\u2581the', '\u2581ra', 'iling', ',', '\u2581turned', '\u2581a', '\u2581r', 'asp', 'berry', '\u2581st', 'alk', '\u2581over', ',', '\u2581and', '\u2581was', '\u2581pe', 'ck', 'ing', '\u2581at', '\u2581it', '.']"}], "[\n  [\"\u2581'\", null],\n  [\"Oh\", \"\u041e\"],\n  [\",\", null],\n  [\"\u2581how\", \"\u043a\u0430\u043a\"],\n  [\"\u2581sweet\", \"\u043c\u0438\u043b\"],\n  [\"\u2581he\", \"\u043e\u043d\"],\n  [\"\u2581is\", \"\u0435\u0441\u0442\u044c\"],\n  [\"\u2581\u2013\", null],\n  [\"\u2581don\", \"\u043d\u0435\"],\n  [\"'\", null],\n  [\"t\", null],\n  [\"\u2581fright\", \"\u043f\u0443\u0433\u0430\u0439\"],\n  [\"en\", null],\n  [\"\u2581him\", \"\u0435\u0433\u043e\"],\n  [\"!'\", null],\n  [\"\u2581exclaimed\", \"\u0432\u043e\u0441\u043a\u043b\u0438\u043a\u043d\u0443\u043b\u0430\"],\n  [\"\u2581K\", \"\u041a\u0438\u0442\u0442\u0438\"],\n  [\"itt\", null],\n  [\"y\", null],\n  [\"\u2581unexpected\", \"\u043d\u0435\u043e\u0436\u0438\u0434\u0430\u043d\u043d\u043e\"],\n  [\"ly\", null],\n  [\",\", null],\n  [\"\u2581looking\", \"\u0433\u043b\u044f\u0434\u044f\"],\n  [\"\u2581at\", \"\u043d\u0430\"],\n  [\"\u2581a\", null],\n  [\"\u2581sp\", \"\u0432\u043e\u0440\u043e\u0431\u044c\u044f\"],\n  [\"arrow\", null],\n  [\"\u2581that\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u0439\"],\n  [\"\u2581had\", null],\n  [\"\u2581settled\", \"\u0441\u0435\u043b\"],\n  [\"\u2581on\", \"\u043d\u0430\"],\n  [\"\u2581the\", null],\n  [\"\u2581ra\", \"\u043f\u0435\u0440\u0438\u043b\u0430\"],\n  [\"iling\", null],\n  [\",\", null],\n  [\"\u2581turned\", \"\u043f\u0435\u0440\u0435\u0432\u0435\u0440\u043d\u0443\u043b\"],\n  [\"\u2581a\", null],\n  [\"\u2581r\", \"\u043c\u0430\u043b\u0438\u043d\u043e\u0432\u044b\u0439\"],\n  [\"asp\", null],\n  [\"berry\", null],\n  [\"\u2581st\", \"\u0441\u0442\u0435\u0431\u0435\u043b\u044c\"],\n  [\"alk\", null],\n  [\"\u2581over\", null],\n  [\",\", null],\n  [\"\u2581and\", \"\u0438\"],\n  [\"\u2581was\", null],\n  [\"\u2581pe\", \"\u043a\u043b\u0435\u0432\u0430\u043b\"],\n  [\"ck\", null],\n  [\"ing\", null],\n  [\"\u2581at\", null],\n  [\"\u2581it\", \"\u0435\u0433\u043e\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\n'There is one thing I want to say...' the Princess began, and from her serious look Kitty guessed what was coming.\n\n\n---=== TOKENIZED TEXT ===---\n\n[\"\u2581'\", 'There', '\u2581is', '\u2581one', '\u2581thing', '\u2581I', '\u2581want', '\u2581to', '\u2581say', '...', \"'\", '\u2581the', '\u2581Princess', '\u2581began', ',', '\u2581and', '\u2581from', '\u2581her', '\u2581serious', '\u2581look', '\u2581K', 'itt', 'y', '\u2581guess', 'ed', '\u2581what', '\u2581was', '\u2581coming', '.']"}], "[\n  [\"\u2581'\", null],\n  [\"There\", \"Il\"],\n  [\"\u2581is\", null],\n  [\"\u2581one\", \"une\"],\n  [\"\u2581thing\", \"chose\"],\n  [\"\u2581I\", \"je\"],\n  [\"\u2581want\", \"veux\"],\n  [\"\u2581to\", null],\n  [\"\u2581say\", \"dire\"],\n  [\"...\", null],\n  [\"'\", null],\n  [\"\u2581the\", null],\n  [\"\u2581Princess\", \"Princesse\"],\n  [\"\u2581began\", \"commen\u00e7a\"],\n  [\",\", null],\n  [\"\u2581and\", null],\n  [\"\u2581from\", \"\u00e0\"],\n  [\"\u2581her\", \"son\"],\n  [\"\u2581serious\", \"s\u00e9rieux\"],\n  [\"\u2581look\", \"regard\"],\n  [\"\u2581K\", \"Kitty\"],\n  [\"itt\", null],\n  [\"y\", null],\n  [\"\u2581guess\", \"devina\"],\n  [\"ed\", null],\n  [\"\u2581what\", \"ce\"],\n  [\"\u2581was\", null],\n  [\"\u2581coming\", \"qui allait suivre\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\n'There is one thing I want to say...' the Princess began, and from her serious look Kitty guessed what was coming.\n\n\n---=== TOKENIZED TEXT ===---\n\n[\"\u2581'\", 'There', '\u2581is', '\u2581one', '\u2581thing', '\u2581I', '\u2581want', '\u2581to', '\u2581say', '...', \"'\", '\u2581the', '\u2581Princess', '\u2581began', ',', '\u2581and', '\u2581from', '\u2581her', '\u2581serious', '\u2581look', '\u2581K', 'itt', 'y', '\u2581guess', 'ed', '\u2581what', '\u2581was', '\u2581coming', '.']"}], "[\n[\"\u2581'\", null],\n[\"There\", null],\n[\"\u2581is\", null],\n[\"\u2581one\", \"\u043e\u0434\u043d\u0443\"],\n[\"\u2581thing\", \"\u0432\u0435\u0449\u044c\"],\n[\"\u2581I\", null],\n[\"\u2581want\", \"\u0445\u043e\u0447\u0443\"],\n[\"\u2581to\", null],\n[\"\u2581say\", \"\u0441\u043a\u0430\u0437\u0430\u0442\u044c\"],\n[\"...\", null],\n[\"'\", null],\n[\"\u2581the\", null],\n[\"\u2581Princess\", \"\u041a\u043d\u044f\u0433\u0438\u043d\u044f\"],\n[\"\u2581began\", \"\u043d\u0430\u0447\u0430\u043b\u0430\"],\n[\",\", null],\n[\"\u2581and\", null],\n[\"\u2581from\", null],\n[\"\u2581her\", null],\n[\"\u2581serious\", \"\u0441\u0435\u0440\u044c\u0435\u0437\u043d\u043e\u0433\u043e\"],\n[\"\u2581look\", \"\u0432\u0437\u0433\u043b\u044f\u0434\u0430\"],\n[\"\u2581K\", \"\u041a\u0438\u0442\u0438\"],\n[\"itt\", null],\n[\"y\", null],\n[\"\u2581guess\", \"\u0434\u043e\u0433\u0430\u0434\u0430\u043b\u0430\u0441\u044c\"],\n[\"ed\", null],\n[\"\u2581what\", null],\n[\"\u2581was\", null],\n[\"\u2581coming\", \"\u0441\u043b\u0435\u0434\u0443\u0435\u0442\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nCharles remained alone the whole afternoon; they had taken Berthe to Madame Homais'; Felicite was in the room upstairs with Madame Lefrancois.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Charles', '\u2581remained', '\u2581alone', '\u2581the', '\u2581whole', '\u2581afternoon', ';', '\u2581they', '\u2581had', '\u2581taken', '\u2581Bert', 'he', '\u2581to', '\u2581Madame', '\u2581Hom', 'ais', \"';\", '\u2581Fel', 'ic', 'ite', '\u2581was', '\u2581in', '\u2581the', '\u2581room', '\u2581up', 'st', 'airs', '\u2581with', '\u2581Madame', '\u2581Le', 'fran', 'co', 'is', '.']"}], "[\n[\"\u2581Charles\", \"Charles\"],\n[\"\u2581remained\", \"restait\"],\n[\"\u2581alone\", \"seul\"],\n[\"\u2581the\", null],\n[\"\u2581whole\", \"tout\"],\n[\"\u2581afternoon\", \"l'apr\u00e8s-midi\"],\n[\";\", null],\n[\"\u2581they\", \"ils\"],\n[\"\u2581had\", \"avaient\"],\n[\"\u2581taken\", \"emmen\u00e9\"],\n[\"\u2581Bert\", \"Berthe\"],\n[\"he\", null],\n[\"\u2581to\", \"chez\"],\n[\"\u2581Madame\", \"Madame\"],\n[\"\u2581Hom\", \"Homais\"],\n[\"ais\", null],\n[\"'\", null],\n[\";\", null],\n[\"\u2581Fel\", \"F\u00e9licit\u00e9\"],\n[\"ic\", null],\n[\"ite\", null],\n[\"\u2581was\", \"\u00e9tait\"],\n[\"\u2581in\", \"dans\"],\n[\"\u2581the\", null],\n[\"\u2581room\", \"chambre\"],\n[\"\u2581up\", \"en\"],\n[\"st\", null],\n[\"airs\", null],\n[\"\u2581with\", \"avec\"],\n[\"\u2581Madame\", \"Madame\"],\n[\"\u2581Le\", \"Lefran\u00e7ois\"],\n[\"fran\", null],\n[\"co\", null],\n[\"is\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nCharles remained alone the whole afternoon; they had taken Berthe to Madame Homais'; Felicite was in the room upstairs with Madame Lefrancois.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Charles', '\u2581remained', '\u2581alone', '\u2581the', '\u2581whole', '\u2581afternoon', ';', '\u2581they', '\u2581had', '\u2581taken', '\u2581Bert', 'he', '\u2581to', '\u2581Madame', '\u2581Hom', 'ais', \"';\", '\u2581Fel', 'ic', 'ite', '\u2581was', '\u2581in', '\u2581the', '\u2581room', '\u2581up', 'st', 'airs', '\u2581with', '\u2581Madame', '\u2581Le', 'fran', 'co', 'is', '.']"}], "[\n[\"\u2581Charles\", \"\u0428\u0430\u0440\u043b\u044c\"],\n[\"\u2581remained\", \"\u043e\u0441\u0442\u0430\u0432\u0430\u043b\u0441\u044f\"],\n[\"\u2581alone\", \"\u043e\u0434\u0438\u043d\"],\n[\"\u2581the\", null],\n[\"\u2581whole\", \"\u0432\u0435\u0441\u044c\"],\n[\"\u2581afternoon\", \"\u0434\u0435\u043d\u044c\"],\n[\";\", null],\n[\"\u2581they\", \"\u043e\u043d\u0438\"],\n[\"\u2581had\", null],\n[\"\u2581taken\", \"\u043e\u0442\u0432\u0435\u043b\u0438\"],\n[\"\u2581Bert\", \"\u0411\u0435\u0440\u0442\u0443\"],\n[\"he\", null],\n[\"\u2581to\", null],\n[\"\u2581Madame\", \"\u043c\u0430\u0434\u0430\u043c\"],\n[\"\u2581Hom\", \"\u041e\u043c\"],\n[\"ais\", null],\n[\"';\", null],\n[\"\u2581Fel\", \"\u0424\u0435\u043b\u0438\"],\n[\"ic\", null],\n[\"ite\", null],\n[\"\u2581was\", null],\n[\"\u2581in\", \"\u0432\"],\n[\"\u2581the\", null],\n[\"\u2581room\", \"\u043a\u043e\u043c\u043d\u0430\u0442\u0435\"],\n[\"\u2581up\", \"\u043d\u0430\"],\n[\"st\", null],\n[\"airs\", null],\n[\"\u2581with\", \"\u0441\"],\n[\"\u2581Madame\", \"\u043c\u0430\u0434\u0430\u043c\"],\n[\"\u2581Le\", \"\u041b\u0435\"],\n[\"fran\", null],\n[\"co\", null],\n[\"is\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\nA splendid Midsummer shone over England: skies so pure, suns so radiant as were then seen in long succession, seldom favour even singly, our wave- girt land.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581A', '\u2581splendid', '\u2581Mid', 'sum', 'mer', '\u2581sh', 'one', '\u2581over', '\u2581England', ':', '\u2581sk', 'ies', '\u2581so', '\u2581pure', ',', '\u2581sun', 's', '\u2581so', '\u2581radi', 'ant', '\u2581as', '\u2581were', '\u2581then', '\u2581seen', '\u2581in', '\u2581long', '\u2581succession', ',', '\u2581s', 'eld', 'om', '\u2581favour', '\u2581even', '\u2581sing', 'ly', ',', '\u2581our', '\u2581wave', '-', '\u2581g', 'irt', '\u2581land', '.']"}], "[\n[\"\u2581A\", null],\n[\"\u2581splendid\", \"splendide\"],\n[\"\u2581Mid\", \"\u00c9t\u00e9\"],\n[\"sum\", null],\n[\"mer\", null],\n[\"\u2581sh\", \"brillait\"],\n[\"one\", null],\n[\"\u2581over\", \"sur\"],\n[\"\u2581England\", \"l'Angleterre\"],\n[\":\", null],\n[\"\u2581sk\", \"cieux\"],\n[\"ies\", null],\n[\"\u2581so\", \"si\"],\n[\"\u2581pure\", \"purs\"],\n[\",\", null],\n[\"\u2581sun\", \"soleils\"],\n[\"s\", null],\n[\"\u2581so\", \"si\"],\n[\"\u2581radi\", \"radieux\"],\n[\"ant\", null],\n[\"\u2581as\", \"comme\"],\n[\"\u2581were\", \"\u00e9taient\"],\n[\"\u2581then\", \"alors\"],\n[\"\u2581seen\", \"vus\"],\n[\"\u2581in\", \"en\"],\n[\"\u2581long\", \"longue\"],\n[\"\u2581succession\", \"succession\"],\n[\",\", null],\n[\"\u2581s\", \"rarement\"],\n[\"eld\", null],\n[\"om\", null],\n[\"\u2581favour\", \"favorisent\"],\n[\"\u2581even\", \"m\u00eame\"],\n[\"\u2581sing\", \"individuellement\"],\n[\"ly\", null],\n[\",\", null],\n[\"\u2581our\", \"notre\"],\n[\"\u2581wave\", \"ceinte\"],\n[\"-\", null],\n[\"\u2581g\", \"de\"],\n[\"irt\", null],\n[\"\u2581land\", \"terre\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nA splendid Midsummer shone over England: skies so pure, suns so radiant as were then seen in long succession, seldom favour even singly, our wave- girt land.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581A', '\u2581splendid', '\u2581Mid', 'sum', 'mer', '\u2581sh', 'one', '\u2581over', '\u2581England', ':', '\u2581sk', 'ies', '\u2581so', '\u2581pure', ',', '\u2581sun', 's', '\u2581so', '\u2581radi', 'ant', '\u2581as', '\u2581were', '\u2581then', '\u2581seen', '\u2581in', '\u2581long', '\u2581succession', ',', '\u2581s', 'eld', 'om', '\u2581favour', '\u2581even', '\u2581sing', 'ly', ',', '\u2581our', '\u2581wave', '-', '\u2581g', 'irt', '\u2581land', '.']"}], "[\n  [\"\u2581A\", null],\n  [\"\u2581splendid\", \"\u0412\u0435\u043b\u0438\u043a\u043e\u043b\u0435\u043f\u043d\u043e\u0435\"],\n  [\"\u2581Mid\", \"\u0421\u0435\u0440\u0435\u0434\u0438\u043d\u0430\"],\n  [\"sum\", null],\n  [\"mer\", null],\n  [\"\u2581sh\", \"\u0441\u0438\u044f\u043b\u043e\"],\n  [\"one\", null],\n  [\"\u2581over\", \"\u043d\u0430\u0434\"],\n  [\"\u2581England\", \"\u0410\u043d\u0433\u043b\u0438\u0435\u0439\"],\n  [\":\", null],\n  [\"\u2581sk\", \"\u043d\u0435\u0431\u0435\u0441\u0430\"],\n  [\"ies\", null],\n  [\"\u2581so\", \"\u0442\u0430\u043a\u0438\u0435\"],\n  [\"\u2581pure\", \"\u0447\u0438\u0441\u0442\u044b\u0435\"],\n  [\",\", null],\n  [\"\u2581sun\", \"\u0441\u043e\u043b\u043d\u0446\u0430\"],\n  [\"s\", null],\n  [\"\u2581so\", \"\u0442\u0430\u043a\u0438\u0435\"],\n  [\"\u2581radi\", \"\u0441\u0438\u044f\u044e\u0449\u0438\u0435\"],\n  [\"ant\", null],\n  [\"\u2581as\", \"\u043a\u0430\u043a\u0438\u0435\"],\n  [\"\u2581were\", \"\u0431\u044b\u043b\u0438\"],\n  [\"\u2581then\", \"\u0442\u043e\u0433\u0434\u0430\"],\n  [\"\u2581seen\", \"\u0432\u0438\u0434\u043d\u044b\"],\n  [\"\u2581in\", \"\u0432\"],\n  [\"\u2581long\", \"\u0434\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439\"],\n  [\"\u2581succession\", \"\u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438\"],\n  [\",\", null],\n  [\"\u2581s\", \"\u0440\u0435\u0434\u043a\u043e\"],\n  [\"eld\", null],\n  [\"om\", null],\n  [\"\u2581favour\", \"\u0431\u043b\u0430\u0433\u043e\u043f\u0440\u0438\u044f\u0442\u0441\u0442\u0432\u0443\u044e\u0442\"],\n  [\"\u2581even\", \"\u0434\u0430\u0436\u0435\"],\n  [\"\u2581sing\", \"\u043f\u043e \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438\"],\n  [\"ly\", null],\n  [\",\", null],\n  [\"\u2581our\", \"\u043d\u0430\u0448\u0435\u0439\"],\n  [\"\u2581wave\", \"\u043e\u043a\u0440\u0443\u0436\u0435\u043d\u043d\u043e\u0439\"],\n  [\"-\", null],\n  [\"\u2581g\", \"\u0432\u043e\u043b\u043d\u0430\u043c\u0438\"],\n  [\"irt\", null],\n  [\"\u2581land\", \"\u0437\u0435\u043c\u043b\u0435\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to French.\n\n---=== TEXT ===---\n\n\"You damned ... ,\" in the first gurglings of his excitement his words could hardly be understood, K. was startled even though he had been expecting something of the sort and ran to his uncle with the intention, no doubt, of closing his mouth with both his hands.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\"', 'You', '\u2581dam', 'ned', '\u2581...', '\u2581', ',\"', '\u2581in', '\u2581the', '\u2581first', '\u2581g', 'urg', 'lings', '\u2581of', '\u2581his', '\u2581excitement', '\u2581his', '\u2581words', '\u2581could', '\u2581hardly', '\u2581be', '\u2581understood', ',', '\u2581K', '.', '\u2581was', '\u2581start', 'led', '\u2581even', '\u2581though', '\u2581he', '\u2581had', '\u2581been', '\u2581expecting', '\u2581something', '\u2581of', '\u2581the', '\u2581sort', '\u2581and', '\u2581ran', '\u2581to', '\u2581his', '\u2581uncle', '\u2581with', '\u2581the', '\u2581intention', ',', '\u2581no', '\u2581doubt', ',', '\u2581of', '\u2581closing', '\u2581his', '\u2581mouth', '\u2581with', '\u2581both', '\u2581his', '\u2581hands', '.']"}], "[\n  [\"\u2581\\\"\", null],\n  [\"You\", \"Vous\"],\n  [\"\u2581dam\", \"maudit\"],\n  [\"ned\", null],\n  [\"\u2581...\", null],\n  [\"\u2581\", null],\n  [\",\\\"\", null],\n  [\"\u2581in\", null],\n  [\"\u2581the\", null],\n  [\"\u2581first\", \"premiers\"],\n  [\"\u2581g\", \"gargouillements\"],\n  [\"urg\", null],\n  [\"lings\", null],\n  [\"\u2581of\", null],\n  [\"\u2581his\", null],\n  [\"\u2581excitement\", \"excitation\"],\n  [\"\u2581his\", \"ses\"],\n  [\"\u2581words\", \"mots\"],\n  [\"\u2581could\", \"pouvaient\"],\n  [\"\u2581hardly\", \"\u00e0 peine\"],\n  [\"\u2581be\", \"\u00eatre\"],\n  [\"\u2581understood\", \"compris\"],\n  [\",\", null],\n  [\"\u2581K\", \"K\"],\n  [\".\", null],\n  [\"\u2581was\", null],\n  [\"\u2581start\", \"surpris\"],\n  [\"led\", null],\n  [\"\u2581even\", \"m\u00eame\"],\n  [\"\u2581though\", \"si\"],\n  [\"\u2581he\", \"il\"],\n  [\"\u2581had\", null],\n  [\"\u2581been\", null],\n  [\"\u2581expecting\", \"s'attendait\"],\n  [\"\u2581something\", \"quelque chose\"],\n  [\"\u2581of\", null],\n  [\"\u2581the\", null],\n  [\"\u2581sort\", \"genre\"],\n  [\"\u2581and\", \"et\"],\n  [\"\u2581ran\", \"courut\"],\n  [\"\u2581to\", \"vers\"],\n  [\"\u2581his\", \"son\"],\n  [\"\u2581uncle\", \"oncle\"],\n  [\"\u2581with\", \"avec\"],\n  [\"\u2581the\", null],\n  [\"\u2581intention\", \"intention\"],\n  [\",\", null],\n  [\"\u2581no\", \"sans\"],\n  [\"\u2581doubt\", \"doute\"],\n  [\",\", null],\n  [\"\u2581of\", null],\n  [\"\u2581closing\", \"fermer\"],\n  [\"\u2581his\", \"sa\"],\n  [\"\u2581mouth\", \"bouche\"],\n  [\"\u2581with\", \"avec\"],\n  [\"\u2581both\", \"ses deux\"],\n  [\"\u2581his\", null],\n  [\"\u2581hands\", \"mains\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\n\"You damned ... ,\" in the first gurglings of his excitement his words could hardly be understood, K. was startled even though he had been expecting something of the sort and ran to his uncle with the intention, no doubt, of closing his mouth with both his hands.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\"', 'You', '\u2581dam', 'ned', '\u2581...', '\u2581', ',\"', '\u2581in', '\u2581the', '\u2581first', '\u2581g', 'urg', 'lings', '\u2581of', '\u2581his', '\u2581excitement', '\u2581his', '\u2581words', '\u2581could', '\u2581hardly', '\u2581be', '\u2581understood', ',', '\u2581K', '.', '\u2581was', '\u2581start', 'led', '\u2581even', '\u2581though', '\u2581he', '\u2581had', '\u2581been', '\u2581expecting', '\u2581something', '\u2581of', '\u2581the', '\u2581sort', '\u2581and', '\u2581ran', '\u2581to', '\u2581his', '\u2581uncle', '\u2581with', '\u2581the', '\u2581intention', ',', '\u2581no', '\u2581doubt', ',', '\u2581of', '\u2581closing', '\u2581his', '\u2581mouth', '\u2581with', '\u2581both', '\u2581his', '\u2581hands', '.']"}], "[\n[\"\u2581\\\"\", null],\n[\"You\", \"\u0422\u044b\"],\n[\"\u2581dam\", \"\u043f\u0440\u043e\u043a\u043b\u044f\u0442\u044b\u0439\"],\n[\"ned\", null],\n[\"\u2581...\", null],\n[\"\u2581\", null],\n[\",\\\"\", null],\n[\"\u2581in\", null],\n[\"\u2581the\", null],\n[\"\u2581first\", \"\u043f\u0435\u0440\u0432\u044b\u0435\"],\n[\"\u2581g\", \"\u0431\u0443\u043b\u044c\u043a\u0430\u043d\u044c\u044f\"],\n[\"urg\", null],\n[\"lings\", null],\n[\"\u2581of\", null],\n[\"\u2581his\", null],\n[\"\u2581excitement\", \"\u0432\u043e\u0437\u0431\u0443\u0436\u0434\u0435\u043d\u0438\u044f\"],\n[\"\u2581his\", \"\u0435\u0433\u043e\"],\n[\"\u2581words\", \"\u0441\u043b\u043e\u0432\u0430\"],\n[\"\u2581could\", \"\u043c\u043e\u0433\u043b\u0438\"],\n[\"\u2581hardly\", \"\u0435\u0434\u0432\u0430\"],\n[\"\u2581be\", \"\u0431\u044b\u0442\u044c\"],\n[\"\u2581understood\", \"\u043f\u043e\u043d\u044f\u0442\u044b\"],\n[\",\", null],\n[\"\u2581K\", \"\u041a\"],\n[\".\", null],\n[\"\u2581was\", null],\n[\"\u2581start\", \"\u0438\u0441\u043f\u0443\u0433\u0430\u043b\u0441\u044f\"],\n[\"led\", null],\n[\"\u2581even\", \"\u0434\u0430\u0436\u0435\"],\n[\"\u2581though\", \"\u0445\u043e\u0442\u044f\"],\n[\"\u2581he\", \"\u043e\u043d\"],\n[\"\u2581had\", null],\n[\"\u2581been\", null],\n[\"\u2581expecting\", \"\u043e\u0436\u0438\u0434\u0430\u043b\"],\n[\"\u2581something\", \"\u0447\u0435\u0433\u043e-\u0442\u043e\"],\n[\"\u2581of\", null],\n[\"\u2581the\", null],\n[\"\u2581sort\", \"\u043f\u043e\u0434\u043e\u0431\u043d\u043e\u0433\u043e\"],\n[\"\u2581and\", \"\u0438\"],\n[\"\u2581ran\", \"\u043f\u043e\u0431\u0435\u0436\u0430\u043b\"],\n[\"\u2581to\", \"\u043a\"],\n[\"\u2581his\", \"\u0441\u0432\u043e\u0435\u043c\u0443\"],\n[\"\u2581uncle\", \"\u0434\u044f\u0434\u0435\"],\n[\"\u2581with\", \"\u0441\"],\n[\"\u2581the\", null],\n[\"\u2581intention\", \"\u043d\u0430\u043c\u0435\u0440\u0435\u043d\u0438\u0435\u043c\"],\n[\",\", null],\n[\"\u2581no\", \"\u043d\u0435\u0441\u043e\u043c\u043d\u0435\u043d\u043d\u043e\"],\n[\"\u2581doubt\", null],\n[\",\", null],\n[\"\u2581of\", null],\n[\"\u2581closing\", \"\u0437\u0430\u043a\u0440\u044b\u0442\u044c\"],\n[\"\u2581his\", \"\u0435\u0433\u043e\"],\n[\"\u2581mouth\", \"\u0440\u043e\u0442\"],\n[\"\u2581with\", null],\n[\"\u2581both\", \"\u043e\u0431\u0435\u0438\u043c\u0438\"],\n[\"\u2581his\", \"\u0441\u0432\u043e\u0438\u043c\u0438\"],\n[\"\u2581hands\", \"\u0440\u0443\u043a\u0430\u043c\u0438\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nVoil\u00e0 tout. \u00bb Un soldat montra au roi une serpe qu\u2019on avait saisie sur le truand.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Vo', 'il', '\u00e0', '\u2581tout', '.', '\u2581\u00bb', '\u2581Un', '\u2581sol', 'dat', '\u2581mont', 'ra', '\u2581au', '\u2581roi', '\u2581une', '\u2581ser', 'pe', '\u2581qu', '\u2019', 'on', '\u2581avait', '\u2581sa', 'isie', '\u2581sur', '\u2581le', '\u2581tr', 'u', 'and', '.']"}], "[\n  [\"\u2581Vo\", \"That\"],\n  [\"il\", null],\n  [\"\u00e0\", null],\n  [\"\u2581tout\", \"all\"],\n  [\".\", null],\n  [\"\u2581\u00bb\", null],\n  [\"\u2581Un\", \"A\"],\n  [\"\u2581sol\", \"soldier\"],\n  [\"dat\", null],\n  [\"\u2581mont\", \"showed\"],\n  [\"ra\", null],\n  [\"\u2581au\", null],\n  [\"\u2581roi\", \"king\"],\n  [\"\u2581une\", \"a\"],\n  [\"\u2581ser\", \"sickle\"],\n  [\"pe\", null],\n  [\"\u2581qu\", \"that\"],\n  [\"'\", null],\n  [\"on\", null],\n  [\"\u2581avait\", \"had\"],\n  [\"\u2581sa\", \"seized\"],\n  [\"isie\", null],\n  [\"\u2581sur\", \"from\"],\n  [\"\u2581le\", null],\n  [\"\u2581tr\", \"vagabond\"],\n  [\"u\", null],\n  [\"and\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nVoil\u00e0 tout. \u00bb Un soldat montra au roi une serpe qu\u2019on avait saisie sur le truand.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Vo', 'il', '\u00e0', '\u2581tout', '.', '\u2581\u00bb', '\u2581Un', '\u2581sol', 'dat', '\u2581mont', 'ra', '\u2581au', '\u2581roi', '\u2581une', '\u2581ser', 'pe', '\u2581qu', '\u2019', 'on', '\u2581avait', '\u2581sa', 'isie', '\u2581sur', '\u2581le', '\u2581tr', 'u', 'and', '.']"}], "[\n  [\"\u2581Vo\", \"\u0412\u043e\u0442\"],\n  [\"il\", null],\n  [\"\u00e0\", null],\n  [\"\u2581tout\", \"\u0432\u0441\u0435\"],\n  [\".\", null],\n  [\"\u2581\u00bb\", null],\n  [\"\u2581Un\", \"\u041e\u0434\u0438\u043d\"],\n  [\"\u2581sol\", \"\u0441\u043e\u043b\u0434\u0430\u0442\"],\n  [\"dat\", null],\n  [\"\u2581mont\", \"\u043f\u043e\u043a\u0430\u0437\u0430\u043b\"],\n  [\"ra\", null],\n  [\"\u2581au\", null],\n  [\"\u2581roi\", \"\u043a\u043e\u0440\u043e\u043b\u044e\"],\n  [\"\u2581une\", null],\n  [\"\u2581ser\", \"\u0441\u0435\u0440\u043f\"],\n  [\"pe\", null],\n  [\"\u2581qu\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u0439\"],\n  [\"'\", null],\n  [\"on\", null],\n  [\"\u2581avait\", \"\u0431\u044b\u043b\"],\n  [\"\u2581sa\", \"\u0438\u0437\u044a\u044f\u0442\"],\n  [\"isie\", null],\n  [\"\u2581sur\", \"\u0443\"],\n  [\"\u2581le\", null],\n  [\"\u2581tr\", \"\u0431\u0440\u043e\u0434\u044f\u0433\u0438\"],\n  [\"u\", null],\n  [\"and\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nL\u2019autre eut un geste d\u00e9daigneux plein de cordialit\u00e9, et, prenant son chapeau:\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581L', '\u2019', 'autre', '\u2581e', 'ut', '\u2581un', '\u2581g', 'este', '\u2581d\u00e9', 'da', 'igne', 'ux', '\u2581ple', 'in', '\u2581de', '\u2581cord', 'ial', 'it\u00e9', ',', '\u2581et', ',', '\u2581pr', 'enant', '\u2581son', '\u2581ch', 'ape', 'au', ':']"}], "[\n  [\"\u2581L\", null],\n  [\"'\", null],\n  [\"autre\", \"other\"],\n  [\"\u2581e\", null],\n  [\"ut\", null],\n  [\"\u2581un\", null],\n  [\"\u2581g\", null],\n  [\"este\", \"gesture\"],\n  [\"\u2581d\u00e9\", null],\n  [\"da\", null],\n  [\"igne\", null],\n  [\"ux\", null],\n  [\"\u2581ple\", null],\n  [\"in\", null],\n  [\"\u2581de\", null],\n  [\"\u2581cord\", null],\n  [\"ial\", null],\n  [\"it\u00e9\", \"cordiality\"],\n  [\",\", null],\n  [\"\u2581et\", null],\n  [\",\", null],\n  [\"\u2581pr\", null],\n  [\"enant\", \"taking\"],\n  [\"\u2581son\", null],\n  [\"\u2581ch\", null],\n  [\"ape\", null],\n  [\"au\", \"hat\"],\n  [\":\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nL\u2019autre eut un geste d\u00e9daigneux plein de cordialit\u00e9, et, prenant son chapeau:\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581L', '\u2019', 'autre', '\u2581e', 'ut', '\u2581un', '\u2581g', 'este', '\u2581d\u00e9', 'da', 'igne', 'ux', '\u2581ple', 'in', '\u2581de', '\u2581cord', 'ial', 'it\u00e9', ',', '\u2581et', ',', '\u2581pr', 'enant', '\u2581son', '\u2581ch', 'ape', 'au', ':']"}], "[\n  [\"\u2581L\", null],\n  [\"'\", null],\n  [\"autre\", \"\u0414\u0440\u0443\u0433\u043e\u0439\"],\n  [\"\u2581e\", null],\n  [\"ut\", null],\n  [\"\u2581un\", null],\n  [\"\u2581g\", null],\n  [\"este\", \"\u0436\u0435\u0441\u0442\"],\n  [\"\u2581d\u00e9\", null],\n  [\"da\", null],\n  [\"igne\", null],\n  [\"ux\", null],\n  [\"\u2581ple\", null],\n  [\"in\", null],\n  [\"\u2581de\", null],\n  [\"\u2581cord\", null],\n  [\"ial\", null],\n  [\"it\u00e9\", \"\u0441\u0435\u0440\u0434\u0435\u0447\u043d\u043e\u0441\u0442\u044c\"],\n  [\",\", null],\n  [\"\u2581et\", null],\n  [\",\", null],\n  [\"\u2581pr\", null],\n  [\"enant\", \"\u0432\u0437\u044f\u0432\"],\n  [\"\u2581son\", null],\n  [\"\u2581ch\", null],\n  [\"ape\", null],\n  [\"au\", \"\u0448\u043b\u044f\u043f\u0443\"],\n  [\":\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nJe vous dirai sur la voyageuse que vous avez recueillie tout ce que je puis dire sans compromettre la paix de mon esprit, ma propre s\u00e9curit\u00e9 morale et physique, et surtout celle des autres.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Je', '\u2581vous', '\u2581di', 'rai', '\u2581sur', '\u2581la', '\u2581voyage', 'use', '\u2581que', '\u2581vous', '\u2581ave', 'z', '\u2581rec', 'ue', 'ill', 'ie', '\u2581tout', '\u2581ce', '\u2581que', '\u2581je', '\u2581puis', '\u2581dire', '\u2581sans', '\u2581comprom', 'ett', 're', '\u2581la', '\u2581pa', 'ix', '\u2581de', '\u2581mon', '\u2581es', 'prit', ',', '\u2581ma', '\u2581propre', '\u2581s\u00e9', 'cur', 'it\u00e9', '\u2581mor', 'ale', '\u2581et', '\u2581phys', 'ique', ',', '\u2581et', '\u2581surtout', '\u2581celle', '\u2581des', '\u2581autres', '.']"}], "[\n  [\"\u2581Je\", \"I\"],\n  [\"\u2581vous\", null],\n  [\"\u2581di\", \"tell\"],\n  [\"rai\", null],\n  [\"\u2581sur\", null],\n  [\"\u2581la\", null],\n  [\"\u2581voyage\", \"traveler\"],\n  [\"use\", null],\n  [\"\u2581que\", null],\n  [\"\u2581vous\", null],\n  [\"\u2581ave\", \"have\"],\n  [\"z\", null],\n  [\"\u2581rec\", \"taken\"],\n  [\"ue\", null],\n  [\"ill\", null],\n  [\"ie\", null],\n  [\"\u2581tout\", \"everything\"],\n  [\"\u2581ce\", null],\n  [\"\u2581que\", null],\n  [\"\u2581je\", null],\n  [\"\u2581puis\", \"can\"],\n  [\"\u2581dire\", \"say\"],\n  [\"\u2581sans\", \"without\"],\n  [\"\u2581comprom\", \"compromising\"],\n  [\"ett\", null],\n  [\"re\", null],\n  [\"\u2581la\", null],\n  [\"\u2581pa\", \"peace\"],\n  [\"ix\", null],\n  [\"\u2581de\", null],\n  [\"\u2581mon\", null],\n  [\"\u2581es\", \"mind\"],\n  [\"prit\", null],\n  [\",\", null],\n  [\"\u2581ma\", null],\n  [\"\u2581propre\", \"own\"],\n  [\"\u2581s\u00e9\", \"security\"],\n  [\"cur\", null],\n  [\"it\u00e9\", null],\n  [\"\u2581mor\", \"moral\"],\n  [\"ale\", null],\n  [\"\u2581et\", null],\n  [\"\u2581phys\", \"physical\"],\n  [\"ique\", null],\n  [\",\", null],\n  [\"\u2581et\", null],\n  [\"\u2581surtout\", \"especially\"],\n  [\"\u2581celle\", \"that\"],\n  [\"\u2581des\", null],\n  [\"\u2581autres\", \"others\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nJe vous dirai sur la voyageuse que vous avez recueillie tout ce que je puis dire sans compromettre la paix de mon esprit, ma propre s\u00e9curit\u00e9 morale et physique, et surtout celle des autres.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Je', '\u2581vous', '\u2581di', 'rai', '\u2581sur', '\u2581la', '\u2581voyage', 'use', '\u2581que', '\u2581vous', '\u2581ave', 'z', '\u2581rec', 'ue', 'ill', 'ie', '\u2581tout', '\u2581ce', '\u2581que', '\u2581je', '\u2581puis', '\u2581dire', '\u2581sans', '\u2581comprom', 'ett', 're', '\u2581la', '\u2581pa', 'ix', '\u2581de', '\u2581mon', '\u2581es', 'prit', ',', '\u2581ma', '\u2581propre', '\u2581s\u00e9', 'cur', 'it\u00e9', '\u2581mor', 'ale', '\u2581et', '\u2581phys', 'ique', ',', '\u2581et', '\u2581surtout', '\u2581celle', '\u2581des', '\u2581autres', '.']"}], "[\n[\"\u2581Je\", \"\u042f\"],\n[\"\u2581vous\", null],\n[\"\u2581di\", \"\u0441\u043a\u0430\u0436\u0443\"],\n[\"rai\", null],\n[\"\u2581sur\", \"\u043e\"],\n[\"\u2581la\", null],\n[\"\u2581voyage\", \"\u043f\u0443\u0442\u0435\u0448\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u0438\u0446\u0435\"],\n[\"use\", null],\n[\"\u2581que\", null],\n[\"\u2581vous\", null],\n[\"\u2581ave\", \"\u043f\u0440\u0438\u044e\u0442\u0438\u043b\u0438\"],\n[\"z\", null],\n[\"\u2581rec\", null],\n[\"ue\", null],\n[\"ill\", null],\n[\"ie\", null],\n[\"\u2581tout\", \"\u0432\u0441\u0435\"],\n[\"\u2581ce\", null],\n[\"\u2581que\", null],\n[\"\u2581je\", null],\n[\"\u2581puis\", \"\u043c\u043e\u0433\u0443\"],\n[\"\u2581dire\", \"\u0441\u043a\u0430\u0437\u0430\u0442\u044c\"],\n[\"\u2581sans\", \"\u043d\u0435\"],\n[\"\u2581comprom\", \"\u043f\u043e\u0434\u0432\u0435\u0440\u0433\u0430\u044f\"],\n[\"ett\", null],\n[\"re\", null],\n[\"\u2581la\", null],\n[\"\u2581pa\", \"\u0441\u043f\u043e\u043a\u043e\u0439\u0441\u0442\u0432\u0438\u0435\"],\n[\"ix\", null],\n[\"\u2581de\", null],\n[\"\u2581mon\", \"\u043c\u043e\u0435\u0433\u043e\"],\n[\"\u2581es\", \"\u0440\u0430\u0437\u0443\u043c\u0430\"],\n[\"prit\", null],\n[\",\", null],\n[\"\u2581ma\", \"\u043c\u043e\u044e\"],\n[\"\u2581propre\", \"\u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u0443\u044e\"],\n[\"\u2581s\u00e9\", \"\u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u044c\"],\n[\"cur\", null],\n[\"it\u00e9\", null],\n[\"\u2581mor\", \"\u043c\u043e\u0440\u0430\u043b\u044c\u043d\u0443\u044e\"],\n[\"ale\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581phys\", \"\u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0443\u044e\"],\n[\"ique\", null],\n[\",\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581surtout\", \"\u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\"],\n[\"\u2581celle\", \"\u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u044c\"],\n[\"\u2581des\", null],\n[\"\u2581autres\", \"\u0434\u0440\u0443\u0433\u0438\u0445\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n\u00c7\u00e0 et l\u00e0 \u00e9tincelaient des points lumineux, qui se croisaient dans tous les sens, s\u2019\u00e9teignaient et se rallumaient tour \u00e0 tour.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u00c7', '\u00e0', '\u2581et', '\u2581l\u00e0', '\u2581\u00e9t', 'in', 'cel', 'aient', '\u2581des', '\u2581points', '\u2581lum', 'ine', 'ux', ',', '\u2581qui', '\u2581se', '\u2581cro', 'isa', 'ient', '\u2581dans', '\u2581tous', '\u2581les', '\u2581sens', ',', '\u2581s', '\u2019', '\u00e9', 'te', 'ign', 'aient', '\u2581et', '\u2581se', '\u2581r', 'all', 'uma', 'ient', '\u2581tour', '\u2581\u00e0', '\u2581tour', '.']"}], "[\n[\"\u2581\", null],\n[\"\u00c7\", \"Here\"],\n[\"\u00e0\", null],\n[\"\u2581et\", null],\n[\"\u2581l\u00e0\", \"there\"],\n[\"\u2581\u00e9t\", \"sparkled\"],\n[\"in\", null],\n[\"cel\", null],\n[\"aient\", null],\n[\"\u2581des\", null],\n[\"\u2581points\", \"points\"],\n[\"\u2581lum\", \"bright\"],\n[\"ine\", null],\n[\"ux\", null],\n[\",\" , null],\n[\"\u2581qui\", \"which\"],\n[\"\u2581se\", null],\n[\"\u2581cro\", \"crossed\"],\n[\"isa\", null],\n[\"ient\", null],\n[\"\u2581dans\", \"in\"],\n[\"\u2581tous\", \"all\"],\n[\"\u2581les\", null],\n[\"\u2581sens\", \"directions\"],\n[\",\" , null],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"\u00e9\", \"extinguished\"],\n[\"te\", null],\n[\"ign\", null],\n[\"aient\", null],\n[\"\u2581et\", \"and\"],\n[\"\u2581se\", null],\n[\"\u2581r\", \"relit\"],\n[\"all\", null],\n[\"uma\", null],\n[\"ient\", null],\n[\"\u2581tour\", \"turn\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581tour\", \"by turn\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n\u00c7\u00e0 et l\u00e0 \u00e9tincelaient des points lumineux, qui se croisaient dans tous les sens, s\u2019\u00e9teignaient et se rallumaient tour \u00e0 tour.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u00c7', '\u00e0', '\u2581et', '\u2581l\u00e0', '\u2581\u00e9t', 'in', 'cel', 'aient', '\u2581des', '\u2581points', '\u2581lum', 'ine', 'ux', ',', '\u2581qui', '\u2581se', '\u2581cro', 'isa', 'ient', '\u2581dans', '\u2581tous', '\u2581les', '\u2581sens', ',', '\u2581s', '\u2019', '\u00e9', 'te', 'ign', 'aient', '\u2581et', '\u2581se', '\u2581r', 'all', 'uma', 'ient', '\u2581tour', '\u2581\u00e0', '\u2581tour', '.']"}], "[\n  [\"\u2581\", null],\n  [\"\u00c7\", \"\u0422\u0430\u043c\"],\n  [\"\u00e0\", null],\n  [\"\u2581et\", null],\n  [\"\u2581l\u00e0\", null],\n  [\"\u2581\u00e9t\", \"\u0441\u0432\u0435\u0440\u043a\u0430\u043b\u0438\"],\n  [\"in\", null],\n  [\"cel\", null],\n  [\"aient\", null],\n  [\"\u2581des\", null],\n  [\"\u2581points\", \"\u0442\u043e\u0447\u043a\u0438\"],\n  [\"\u2581lum\", \"\u0441\u0432\u0435\u0442\u044f\u0449\u0438\u0435\u0441\u044f\"],\n  [\"ine\", null],\n  [\"ux\", null],\n  [\",\", null],\n  [\"\u2581qui\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u0435\"],\n  [\"\u2581se\", null],\n  [\"\u2581cro\", \"\u043f\u0435\u0440\u0435\u0441\u0435\u043a\u0430\u043b\u0438\u0441\u044c\"],\n  [\"isa\", null],\n  [\"ient\", null],\n  [\"\u2581dans\", \"\u0432\u043e\"],\n  [\"\u2581tous\", \"\u0432\u0441\u0435\u0445\"],\n  [\"\u2581les\", null],\n  [\"\u2581sens\", \"\u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f\u0445\"],\n  [\",\", null],\n  [\"\u2581s\", null],\n  [\"'\", null],\n  [\"\u00e9\", \"\u0433\u0430\u0441\u043b\u0438\"],\n  [\"te\", null],\n  [\"ign\", null],\n  [\"aient\", null],\n  [\"\u2581et\", \"\u0438\"],\n  [\"\u2581se\", null],\n  [\"\u2581r\", \"\u0437\u0430\u0436\u0438\u0433\u0430\u043b\u0438\u0441\u044c\"],\n  [\"all\", null],\n  [\"uma\", null],\n  [\"ient\", null],\n  [\"\u2581tour\", \"\u043f\u043e\u043e\u0447\u0435\u0440\u0435\u0434\u043d\u043e\"],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581tour\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n\u00c0 ces hardies paroles, lord Glenarvan avait tendu les bras \u00e0 sa jeune femme; il souriait, il la pressait sur son coeur, tandis que Mary et Robert lui baisaient les mains.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00c0', '\u2581ces', '\u2581hard', 'ies', '\u2581par', 'oles', ',', '\u2581lord', '\u2581Glen', 'ar', 'van', '\u2581avait', '\u2581tend', 'u', '\u2581les', '\u2581bras', '\u2581\u00e0', '\u2581sa', '\u2581jeune', '\u2581femme', ';', '\u2581il', '\u2581sou', 'ri', 'ait', ',', '\u2581il', '\u2581la', '\u2581press', 'ait', '\u2581sur', '\u2581son', '\u2581co', 'eur', ',', '\u2581tandis', '\u2581que', '\u2581Mary', '\u2581et', '\u2581Robert', '\u2581lui', '\u2581b', 'ais', 'aient', '\u2581les', '\u2581main', 's', '.']"}], "[\n  [\"\u2581\u00c0\", \"At\"],\n  [\"\u2581ces\", null],\n  [\"\u2581hard\", \"bold\"],\n  [\"ies\", null],\n  [\"\u2581par\", \"words\"],\n  [\"oles\", null],\n  [\",\", null],\n  [\"\u2581lord\", \"Lord\"],\n  [\"\u2581Glen\", \"Glenarvan\"],\n  [\"ar\", null],\n  [\"van\", null],\n  [\"\u2581avait\", \"had\"],\n  [\"\u2581tend\", \"extended\"],\n  [\"u\", null],\n  [\"\u2581les\", null],\n  [\"\u2581bras\", \"arms\"],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581sa\", null],\n  [\"\u2581jeune\", \"young\"],\n  [\"\u2581femme\", \"wife\"],\n  [\";\", null],\n  [\"\u2581il\", \"he\"],\n  [\"\u2581sou\", \"smiled\"],\n  [\"ri\", null],\n  [\"ait\", null],\n  [\",\", null],\n  [\"\u2581il\", \"he\"],\n  [\"\u2581la\", null],\n  [\"\u2581press\", \"pressed\"],\n  [\"ait\", null],\n  [\"\u2581sur\", \"against\"],\n  [\"\u2581son\", \"his\"],\n  [\"\u2581co\", \"heart\"],\n  [\"eur\", null],\n  [\",\", null],\n  [\"\u2581tandis\", \"while\"],\n  [\"\u2581que\", null],\n  [\"\u2581Mary\", \"Mary\"],\n  [\"\u2581et\", \"and\"],\n  [\"\u2581Robert\", \"Robert\"],\n  [\"\u2581lui\", null],\n  [\"\u2581b\", \"kissed\"],\n  [\"ais\", null],\n  [\"aient\", null],\n  [\"\u2581les\", null],\n  [\"\u2581main\", \"hands\"],\n  [\"s\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n\u00c0 ces hardies paroles, lord Glenarvan avait tendu les bras \u00e0 sa jeune femme; il souriait, il la pressait sur son coeur, tandis que Mary et Robert lui baisaient les mains.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00c0', '\u2581ces', '\u2581hard', 'ies', '\u2581par', 'oles', ',', '\u2581lord', '\u2581Glen', 'ar', 'van', '\u2581avait', '\u2581tend', 'u', '\u2581les', '\u2581bras', '\u2581\u00e0', '\u2581sa', '\u2581jeune', '\u2581femme', ';', '\u2581il', '\u2581sou', 'ri', 'ait', ',', '\u2581il', '\u2581la', '\u2581press', 'ait', '\u2581sur', '\u2581son', '\u2581co', 'eur', ',', '\u2581tandis', '\u2581que', '\u2581Mary', '\u2581et', '\u2581Robert', '\u2581lui', '\u2581b', 'ais', 'aient', '\u2581les', '\u2581main', 's', '.']"}], "[\n[\"\u2581\u00c0\", \"\u041d\u0430\"],\n[\"\u2581ces\", null],\n[\"\u2581hard\", \"\u0441\u043c\u0435\u043b\u044b\u0435\"],\n[\"ies\", null],\n[\"\u2581par\", \"\u0441\u043b\u043e\u0432\u0430\"],\n[\"oles\", null],\n[\",\", null],\n[\"\u2581lord\", \"\u043b\u043e\u0440\u0434\"],\n[\"\u2581Glen\", \"\u0413\u043b\u0435\u043d\"],\n[\"ar\", null],\n[\"van\", null],\n[\"\u2581avait\", \"\u043f\u0440\u043e\u0442\u044f\u043d\u0443\u043b\"],\n[\"\u2581tend\", \"\u0440\u0443\u043a\u0438\"],\n[\"u\", null],\n[\"\u2581les\", null],\n[\"\u2581bras\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581sa\", null],\n[\"\u2581jeune\", \"\u043c\u043e\u043b\u043e\u0434\u043e\u0439\"],\n[\"\u2581femme\", \"\u0436\u0435\u043d\u0435\"],\n[\";\", null],\n[\"\u2581il\", \"\u043e\u043d\"],\n[\"\u2581sou\", \"\u0443\u043b\u044b\u0431\u0430\u043b\u0441\u044f\"],\n[\"ri\", null],\n[\"ait\", null],\n[\",\", null],\n[\"\u2581il\", \"\u043e\u043d\"],\n[\"\u2581la\", null],\n[\"\u2581press\", \"\u043f\u0440\u0438\u0436\u0438\u043c\u0430\u043b\"],\n[\"ait\", null],\n[\"\u2581sur\", \"\u043a\"],\n[\"\u2581son\", \"\u0441\u0432\u043e\u0435\u043c\u0443\"],\n[\"\u2581co\", \"\u0441\u0435\u0440\u0434\u0446\u0443\"],\n[\"eur\", null],\n[\",\", null],\n[\"\u2581tandis\", \"\u043f\u043e\u043a\u0430\"],\n[\"\u2581que\", null],\n[\"\u2581Mary\", \"\u041c\u044d\u0440\u0438\"],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581Robert\", \"\u0420\u043e\u0431\u0435\u0440\u0442\"],\n[\"\u2581lui\", null],\n[\"\u2581b\", \"\u0446\u0435\u043b\u043e\u0432\u0430\u043b\u0438\"],\n[\"ais\", null],\n[\"aient\", null],\n[\"\u2581les\", null],\n[\"\u2581main\", \"\u0440\u0443\u043a\u0438\"],\n[\"s\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nCette force du sexe et du sang de l\u2019homme que, fol adolescent, j\u2019avais cru \u00e9touffer pour la vie, avait plus d\u2019une fois soulev\u00e9 convulsivement la cha\u00eene des v\u0153ux de fer qui me scellent, mis\u00e9rable, aux froides pierres de l\u2019autel.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Cette', '\u2581force', '\u2581du', '\u2581sex', 'e', '\u2581et', '\u2581du', '\u2581sang', '\u2581de', '\u2581l', '\u2019', 'homme', '\u2581que', ',', '\u2581fol', '\u2581ad', 'oles', 'cent', ',', '\u2581j', '\u2019', 'ava', 'is', '\u2581cru', '\u2581\u00e9t', 'ou', 'f', 'fer', '\u2581pour', '\u2581la', '\u2581vie', ',', '\u2581avait', '\u2581plus', '\u2581d', '\u2019', 'une', '\u2581fois', '\u2581sou', 'lev', '\u00e9', '\u2581conv', 'uls', 'ivement', '\u2581la', '\u2581cha', '\u00eene', '\u2581des', '\u2581v', '\u0153', 'ux', '\u2581de', '\u2581fer', '\u2581qui', '\u2581me', '\u2581sc', 'ell', 'ent', ',', '\u2581mis', '\u00e9ra', 'ble', ',', '\u2581aux', '\u2581f', 'roid', 'es', '\u2581pier', 'res', '\u2581de', '\u2581l', '\u2019', 'aut', 'el', '.']"}], "[\n[\"\u2581Cette\", \"This\"],\n[\"\u2581force\", \"strength\"],\n[\"\u2581du\", null],\n[\"\u2581sex\", \"sex\"],\n[\"e\", null],\n[\"\u2581et\", null],\n[\"\u2581du\", null],\n[\"\u2581sang\", \"blood\"],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"homme\", \"man\"],\n[\"\u2581que\", null],\n[\",\", null],\n[\"\u2581fol\", \"foolish\"],\n[\"\u2581ad\", \"adolescent\"],\n[\"oles\", null],\n[\"cent\", null],\n[\",\", null],\n[\"\u2581j\", \"I\"],\n[\"'\", null],\n[\"ava\", \"had\"],\n[\"is\", null],\n[\"\u2581cru\", \"believed\"],\n[\"\u2581\u00e9t\", \"stifle\"],\n[\"ou\", null],\n[\"f\", null],\n[\"fer\", null],\n[\"\u2581pour\", null],\n[\"\u2581la\", null],\n[\"\u2581vie\", \"life\"],\n[\",\", null],\n[\"\u2581avait\", \"had\"],\n[\"\u2581plus\", \"more\"],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"une\", \"one\"],\n[\"\u2581fois\", \"time\"],\n[\"\u2581sou\", \"lifted\"],\n[\"lev\", null],\n[\"\u00e9\", null],\n[\"\u2581conv\", \"convulsively\"],\n[\"uls\", null],\n[\"ivement\", null],\n[\"\u2581la\", null],\n[\"\u2581cha\", \"chain\"],\n[\"\u00eene\", null],\n[\"\u2581des\", null],\n[\"\u2581v\", \"vows\"],\n[\"\u0153\", null],\n[\"ux\", null],\n[\"\u2581de\", null],\n[\"\u2581fer\", \"iron\"],\n[\"\u2581qui\", null],\n[\"\u2581me\", null],\n[\"\u2581sc\", \"seal\"],\n[\"ell\", null],\n[\"ent\", null],\n[\",\", null],\n[\"\u2581mis\", \"wretched\"],\n[\"\u00e9ra\", null],\n[\"ble\", null],\n[\",\", null],\n[\"\u2581aux\", null],\n[\"\u2581f\", \"cold\"],\n[\"roid\", null],\n[\"es\", null],\n[\"\u2581pier\", \"stones\"],\n[\"res\", null],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"aut\", \"altar\"],\n[\"el\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nCette force du sexe et du sang de l\u2019homme que, fol adolescent, j\u2019avais cru \u00e9touffer pour la vie, avait plus d\u2019une fois soulev\u00e9 convulsivement la cha\u00eene des v\u0153ux de fer qui me scellent, mis\u00e9rable, aux froides pierres de l\u2019autel.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Cette', '\u2581force', '\u2581du', '\u2581sex', 'e', '\u2581et', '\u2581du', '\u2581sang', '\u2581de', '\u2581l', '\u2019', 'homme', '\u2581que', ',', '\u2581fol', '\u2581ad', 'oles', 'cent', ',', '\u2581j', '\u2019', 'ava', 'is', '\u2581cru', '\u2581\u00e9t', 'ou', 'f', 'fer', '\u2581pour', '\u2581la', '\u2581vie', ',', '\u2581avait', '\u2581plus', '\u2581d', '\u2019', 'une', '\u2581fois', '\u2581sou', 'lev', '\u00e9', '\u2581conv', 'uls', 'ivement', '\u2581la', '\u2581cha', '\u00eene', '\u2581des', '\u2581v', '\u0153', 'ux', '\u2581de', '\u2581fer', '\u2581qui', '\u2581me', '\u2581sc', 'ell', 'ent', ',', '\u2581mis', '\u00e9ra', 'ble', ',', '\u2581aux', '\u2581f', 'roid', 'es', '\u2581pier', 'res', '\u2581de', '\u2581l', '\u2019', 'aut', 'el', '.']"}], "[\n[\"\u2581Cette\", \"\u042d\u0442\u0430\"],\n[\"\u2581force\", \"\u0441\u0438\u043b\u0430\"],\n[\"\u2581du\", null],\n[\"\u2581sex\", \"\u043f\u043e\u043b\"],\n[\"e\", null],\n[\"\u2581et\", null],\n[\"\u2581du\", null],\n[\"\u2581sang\", \"\u043a\u0440\u043e\u0432\u0438\"],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"homme\", \"\u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430\"],\n[\"\u2581que\", null],\n[\",\", null],\n[\"\u2581fol\", \"\u0431\u0435\u0437\u0443\u043c\u043d\u044b\u0439\"],\n[\"\u2581ad\", \"\u044e\u043d\u043e\u0448\"],\n[\"oles\", null],\n[\"cent\", null],\n[\",\", null],\n[\"\u2581j\", null],\n[\"'\", null],\n[\"ava\", \"\u0441\u0447\u0438\u0442\"],\n[\"is\", null],\n[\"\u2581cru\", \"\u0430\u043b\"],\n[\"\u2581\u00e9t\", \"\u043f\u043e\u0434\u0430\u0432\"],\n[\"ou\", null],\n[\"f\", null],\n[\"fer\", \"\u0438\u0442\u044c\"],\n[\"\u2581pour\", null],\n[\"\u2581la\", null],\n[\"\u2581vie\", \"\u0436\u0438\u0437\u043d\u044c\"],\n[\",\", null],\n[\"\u2581avait\", \"\u0438\u043c\u0435\u043b\u0430\"],\n[\"\u2581plus\", \"\u0431\u043e\u043b\u044c\u0448\u0435\"],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"une\", \"\u043e\u0434\u043d\u043e\u0433\u043e\"],\n[\"\u2581fois\", \"\u0440\u0430\u0437\u0430\"],\n[\"\u2581sou\", \"\u043f\u043e\u0434\u043d\u0438\u043c\"],\n[\"lev\", null],\n[\"\u00e9\", null],\n[\"\u2581conv\", \"\u0441\u0443\u0434\u043e\u0440\u043e\u0436\"],\n[\"uls\", null],\n[\"ivement\", \"\u043d\u043e\"],\n[\"\u2581la\", null],\n[\"\u2581cha\", \"\u0446\u0435\u043f\u044c\"],\n[\"\u00eene\", null],\n[\"\u2581des\", null],\n[\"\u2581v\", \"\u043e\u0431\u0435\u0442\"],\n[\"\u0153\", null],\n[\"ux\", null],\n[\"\u2581de\", null],\n[\"\u2581fer\", \"\u0436\u0435\u043b\u0435\u0437\u043d\u044b\u0445\"],\n[\"\u2581qui\", null],\n[\"\u2581me\", \"\u043c\u0435\u043d\u044f\"],\n[\"\u2581sc\", \"\u043f\u0440\u0438\u043a\"],\n[\"ell\", \"\u043e\u0432\u044b\u0432\u0430\"],\n[\"ent\", \"\u044e\u0442\"],\n[\",\", null],\n[\"\u2581mis\", \"\u0436\u0430\u043b\u043a\"],\n[\"\u00e9ra\", null],\n[\"ble\", \"\u043e\u0433\u043e\"],\n[\",\", null],\n[\"\u2581aux\", null],\n[\"\u2581f\", \"\u0445\u043e\u043b\u043e\u0434\u043d\"],\n[\"roid\", null],\n[\"es\", \"\u044b\u043c\"],\n[\"\u2581pier\", \"\u043a\u0430\u043c\u043d\"],\n[\"res\", \"\u044f\u043c\"],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"aut\", \"\u0430\u043b\u0442\u0430\u0440\"],\n[\"el\", \"\u044f\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nEnfin on disait, horreur dans un enfant de seize ans, que ses d\u00e9bordements allaient souventes fois jusqu\u2019\u00e0 la rue de Glatigny.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581En', 'fin', '\u2581on', '\u2581dis', 'ait', ',', '\u2581hor', 're', 'ur', '\u2581dans', '\u2581un', '\u2581enf', 'ant', '\u2581de', '\u2581se', 'ize', '\u2581ans', ',', '\u2581que', '\u2581ses', '\u2581d\u00e9', 'bor', 'de', 'ments', '\u2581alla', 'ient', '\u2581souvent', 'es', '\u2581fois', '\u2581jusqu', '\u2019', '\u00e0', '\u2581la', '\u2581rue', '\u2581de', '\u2581G', 'lat', 'igny', '.']"}], "[\n    [\"\u2581En\", \"Finally\"],\n    [\"fin\", null],\n    [\"\u2581on\", null],\n    [\"\u2581dis\", \"said\"],\n    [\"ait\", null],\n    [\",\", null],\n    [\"\u2581hor\", \"horror\"],\n    [\"re\", null],\n    [\"ur\", null],\n    [\"\u2581dans\", null],\n    [\"\u2581un\", null],\n    [\"\u2581enf\", \"child\"],\n    [\"ant\", null],\n    [\"\u2581de\", null],\n    [\"\u2581se\", \"sixteen\"],\n    [\"ize\", null],\n    [\"\u2581ans\", \"years\"],\n    [\",\", null],\n    [\"\u2581que\", null],\n    [\"\u2581ses\", null],\n    [\"\u2581d\u00e9\", \"excesses\"],\n    [\"bor\", null],\n    [\"de\", null],\n    [\"ments\", null],\n    [\"\u2581alla\", \"went\"],\n    [\"ient\", null],\n    [\"\u2581souvent\", \"often\"],\n    [\"es\", null],\n    [\"\u2581fois\", null],\n    [\"\u2581jusqu\", \"as far as\"],\n    [\"'\", null],\n    [\"\u00e0\", null],\n    [\"\u2581la\", null],\n    [\"\u2581rue\", \"street\"],\n    [\"\u2581de\", null],\n    [\"\u2581G\", \"Glatigny\"],\n    [\"lat\", null],\n    [\"igny\", null],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nEnfin on disait, horreur dans un enfant de seize ans, que ses d\u00e9bordements allaient souventes fois jusqu\u2019\u00e0 la rue de Glatigny.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581En', 'fin', '\u2581on', '\u2581dis', 'ait', ',', '\u2581hor', 're', 'ur', '\u2581dans', '\u2581un', '\u2581enf', 'ant', '\u2581de', '\u2581se', 'ize', '\u2581ans', ',', '\u2581que', '\u2581ses', '\u2581d\u00e9', 'bor', 'de', 'ments', '\u2581alla', 'ient', '\u2581souvent', 'es', '\u2581fois', '\u2581jusqu', '\u2019', '\u00e0', '\u2581la', '\u2581rue', '\u2581de', '\u2581G', 'lat', 'igny', '.']"}], "[\n[\"\u2581En\", \"\u041d\u0430\u043a\u043e\u043d\u0435\u0446\"],\n[\"fin\", null],\n[\"\u2581on\", null],\n[\"\u2581dis\", \"\u0433\u043e\u0432\u043e\u0440\u0438\u043b\u0438\"],\n[\"ait\", null],\n[\",\", null],\n[\"\u2581hor\", \"\u0443\u0436\u0430\u0441\"],\n[\"re\", null],\n[\"ur\", null],\n[\"\u2581dans\", null],\n[\"\u2581un\", null],\n[\"\u2581enf\", \"\u0440\u0435\u0431\u0435\u043d\u043e\u043a\"],\n[\"ant\", null],\n[\"\u2581de\", null],\n[\"\u2581se\", \"\u0448\u0435\u0441\u0442\u043d\u0430\u0434\u0446\u0430\u0442\u0438\"],\n[\"ize\", null],\n[\"\u2581ans\", \"\u043b\u0435\u0442\"],\n[\",\", null],\n[\"\u2581que\", \"\u0447\u0442\u043e\"],\n[\"\u2581ses\", \"\u0435\u0433\u043e\"],\n[\"\u2581d\u00e9\", \"\u0432\u044b\u0445\u043e\u0434\u043a\u0438\"],\n[\"bor\", null],\n[\"de\", null],\n[\"ments\", null],\n[\"\u2581alla\", \"\u0434\u043e\u0445\u043e\u0434\u0438\u043b\u0438\"],\n[\"ient\", null],\n[\"\u2581souvent\", \"\u0447\u0430\u0441\u0442\u043e\"],\n[\"es\", null],\n[\"\u2581fois\", null],\n[\"\u2581jusqu\", \"\u0434\u043e\"],\n[\"'\", null],\n[\"\u00e0\", null],\n[\"\u2581la\", null],\n[\"\u2581rue\", \"\u0443\u043b\u0438\u0446\u044b\"],\n[\"\u2581de\", null],\n[\"\u2581G\", \"\u0413\u043b\u0430\u0442\u0438\u043d\u044c\u0438\"],\n[\"lat\", null],\n[\"igny\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nQui e\u00fbt pu voir en ce moment la figure du malheureux coll\u00e9e aux barreaux vermoulus e\u00fbt cru voir une face de tigre regardant du fond d\u2019une cage quelque chacal qui d\u00e9vore une gazelle.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Qu', 'i', '\u2581e', '\u00fbt', '\u2581pu', '\u2581voir', '\u2581en', '\u2581ce', '\u2581moment', '\u2581la', '\u2581figure', '\u2581du', '\u2581mal', 'heure', 'ux', '\u2581coll', '\u00e9e', '\u2581aux', '\u2581bar', 're', 'aux', '\u2581verm', 'oul', 'us', '\u2581e', '\u00fbt', '\u2581cru', '\u2581voir', '\u2581une', '\u2581face', '\u2581de', '\u2581t', 'ig', 're', '\u2581regard', 'ant', '\u2581du', '\u2581fond', '\u2581d', '\u2019', 'une', '\u2581c', 'age', '\u2581quelque', '\u2581ch', 'ac', 'al', '\u2581qui', '\u2581d\u00e9', 'v', 'ore', '\u2581une', '\u2581gaz', 'elle', '.']"}], "[\n[\"\u2581Qu\", \"Who\"],\n[\"i\", null],\n[\"\u2581e\", null],\n[\"\u00fbt\", null],\n[\"\u2581pu\", \"could\"],\n[\"\u2581voir\", \"see\"],\n[\"\u2581en\", \"at\"],\n[\"\u2581ce\", \"this\"],\n[\"\u2581moment\", \"moment\"],\n[\"\u2581la\", null],\n[\"\u2581figure\", \"face\"],\n[\"\u2581du\", null],\n[\"\u2581mal\", \"unfortunate\"],\n[\"heure\", null],\n[\"ux\", null],\n[\"\u2581coll\", \"pressed\"],\n[\"\u00e9e\", null],\n[\"\u2581aux\", \"against\"],\n[\"\u2581bar\", \"bars\"],\n[\"re\", null],\n[\"aux\", null],\n[\"\u2581verm\", \"worm-eaten\"],\n[\"oul\", null],\n[\"us\", null],\n[\"\u2581e\", null],\n[\"\u00fbt\", null],\n[\"\u2581cru\", \"believed\"],\n[\"\u2581voir\", \"seeing\"],\n[\"\u2581une\", null],\n[\"\u2581face\", \"face\"],\n[\"\u2581de\", null],\n[\"\u2581t\", \"tiger\"],\n[\"ig\", null],\n[\"re\", null],\n[\"\u2581regard\", \"looking\"],\n[\"ant\", null],\n[\"\u2581du\", \"from\"],\n[\"\u2581fond\", \"bottom\"],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"une\", null],\n[\"\u2581c\", \"cage\"],\n[\"age\", null],\n[\"\u2581quelque\", \"some\"],\n[\"\u2581ch\", \"jackal\"],\n[\"ac\", null],\n[\"al\", null],\n[\"\u2581qui\", \"that\"],\n[\"\u2581d\u00e9\", \"devours\"],\n[\"v\", null],\n[\"ore\", null],\n[\"\u2581une\", null],\n[\"\u2581gaz\", \"gazelle\"],\n[\"elle\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nQui e\u00fbt pu voir en ce moment la figure du malheureux coll\u00e9e aux barreaux vermoulus e\u00fbt cru voir une face de tigre regardant du fond d\u2019une cage quelque chacal qui d\u00e9vore une gazelle.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Qu', 'i', '\u2581e', '\u00fbt', '\u2581pu', '\u2581voir', '\u2581en', '\u2581ce', '\u2581moment', '\u2581la', '\u2581figure', '\u2581du', '\u2581mal', 'heure', 'ux', '\u2581coll', '\u00e9e', '\u2581aux', '\u2581bar', 're', 'aux', '\u2581verm', 'oul', 'us', '\u2581e', '\u00fbt', '\u2581cru', '\u2581voir', '\u2581une', '\u2581face', '\u2581de', '\u2581t', 'ig', 're', '\u2581regard', 'ant', '\u2581du', '\u2581fond', '\u2581d', '\u2019', 'une', '\u2581c', 'age', '\u2581quelque', '\u2581ch', 'ac', 'al', '\u2581qui', '\u2581d\u00e9', 'v', 'ore', '\u2581une', '\u2581gaz', 'elle', '.']"}], "[\n[\"\u2581Qu\", \"\u041a\u0442\u043e\"],\n[\"i\", null],\n[\"\u2581e\", null],\n[\"\u00fbt\", null],\n[\"\u2581pu\", null],\n[\"\u2581voir\", \"\u0443\u0432\u0438\u0434\u0435\u0442\u044c\"],\n[\"\u2581en\", null],\n[\"\u2581ce\", null],\n[\"\u2581moment\", \"\u043c\u043e\u043c\u0435\u043d\u0442\"],\n[\"\u2581la\", null],\n[\"\u2581figure\", \"\u043b\u0438\u0446\u043e\"],\n[\"\u2581du\", null],\n[\"\u2581mal\", \"\u043d\u0435\u0441\u0447\u0430\u0441\u0442\u043d\u044b\u0439\"],\n[\"heure\", null],\n[\"ux\", null],\n[\"\u2581coll\", \"\u043f\u0440\u0438\u0436\u0430\u0442\u043e\u0435\"],\n[\"\u00e9e\", null],\n[\"\u2581aux\", null],\n[\"\u2581bar\", \"\u0440\u0435\u0448\u0435\u0442\u043a\u0430\u043c\"],\n[\"re\", null],\n[\"aux\", null],\n[\"\u2581verm\", \"\u043f\u0440\u043e\u0433\u043d\u0438\u0432\u0448\u0438\u043c\"],\n[\"oul\", null],\n[\"us\", null],\n[\"\u2581e\", null],\n[\"\u00fbt\", null],\n[\"\u2581cru\", \"\u043f\u043e\u0434\u0443\u043c\u0430\u043b\"],\n[\"\u2581voir\", \"\u0432\u0438\u0434\u0438\u0442\"],\n[\"\u2581une\", null],\n[\"\u2581face\", \"\u043c\u043e\u0440\u0434\u0443\"],\n[\"\u2581de\", null],\n[\"\u2581t\", \"\u0442\u0438\u0433\u0440\u0430\"],\n[\"ig\", null],\n[\"re\", null],\n[\"\u2581regard\", \"\u0441\u043c\u043e\u0442\u0440\u044f\u0449\u0435\u0433\u043e\"],\n[\"ant\", null],\n[\"\u2581du\", null],\n[\"\u2581fond\", \"\u0433\u043b\u0443\u0431\u0438\u043d\u044b\"],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"une\", null],\n[\"\u2581c\", \"\u043a\u043b\u0435\u0442\u043a\u0438\"],\n[\"age\", null],\n[\"\u2581quelque\", \"\u043a\u0430\u043a\u043e\u0433\u043e-\u0442\u043e\"],\n[\"\u2581ch\", \"\u0448\u0430\u043a\u0430\u043b\u0430\"],\n[\"ac\", null],\n[\"al\", null],\n[\"\u2581qui\", null],\n[\"\u2581d\u00e9\", \"\u043f\u043e\u0436\u0438\u0440\u0430\u044e\u0449\u0435\u0433\u043e\"],\n[\"v\", null],\n[\"ore\", null],\n[\"\u2581une\", null],\n[\"\u2581gaz\", \"\u0433\u0430\u0437\u0435\u043b\u044c\"],\n[\"elle\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nGlenarvan garda pour lui le secret de ses angoisses, et ni lady Helena, ni Mary Grant, ni Robert ne soup\u00e7onn\u00e8rent la gravit\u00e9 des p\u00e9rils auxquels ils venaient d\u2019\u00e9chapper.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Glen', 'ar', 'van', '\u2581gard', 'a', '\u2581pour', '\u2581lui', '\u2581le', '\u2581secret', '\u2581de', '\u2581ses', '\u2581an', 'go', 'isses', ',', '\u2581et', '\u2581ni', '\u2581lady', '\u2581Hel', 'ena', ',', '\u2581ni', '\u2581Mary', '\u2581Grant', ',', '\u2581ni', '\u2581Robert', '\u2581ne', '\u2581soup', '\u00e7', 'onn', '\u00e8rent', '\u2581la', '\u2581grav', 'it\u00e9', '\u2581des', '\u2581p', '\u00e9r', 'ils', '\u2581aux', 'qu', 'els', '\u2581ils', '\u2581v', 'ena', 'ient', '\u2581d', '\u2019', '\u00e9ch', 'apper', '.']"}], "[\n  [\"\u2581Glen\", \"Glenarvan\"],\n  [\"ar\", null],\n  [\"van\", null],\n  [\"\u2581gard\", \"kept\"],\n  [\"a\", null],\n  [\"\u2581pour\", \"for\"],\n  [\"\u2581lui\", \"himself\"],\n  [\"\u2581le\", null],\n  [\"\u2581secret\", \"secret\"],\n  [\"\u2581de\", null],\n  [\"\u2581ses\", null],\n  [\"\u2581an\", \"anxieties\"],\n  [\"go\", null],\n  [\"isses\", null],\n  [\",\", null],\n  [\"\u2581et\", null],\n  [\"\u2581ni\", \"neither\"],\n  [\"\u2581lady\", \"Lady\"],\n  [\"\u2581Hel\", \"Helena\"],\n  [\"ena\", null],\n  [\",\", null],\n  [\"\u2581ni\", \"nor\"],\n  [\"\u2581Mary\", \"Mary\"],\n  [\"\u2581Grant\", \"Grant\"],\n  [\",\", null],\n  [\"\u2581ni\", \"nor\"],\n  [\"\u2581Robert\", \"Robert\"],\n  [\"\u2581ne\", null],\n  [\"\u2581soup\", \"suspected\"],\n  [\"\u00e7\", null],\n  [\"onn\", null],\n  [\"\u00e8rent\", null],\n  [\"\u2581la\", null],\n  [\"\u2581grav\", \"seriousness\"],\n  [\"it\u00e9\", null],\n  [\"\u2581des\", null],\n  [\"\u2581p\", \"dangers\"],\n  [\"\u00e9r\", null],\n  [\"ils\", null],\n  [\"\u2581aux\", null],\n  [\"qu\", null],\n  [\"els\", null],\n  [\"\u2581ils\", \"they\"],\n  [\"\u2581v\", \"had\"],\n  [\"ena\", null],\n  [\"ient\", null],\n  [\"\u2581d\", null],\n  [\"'\", null],\n  [\"\u00e9ch\", \"escaped\"],\n  [\"apper\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nGlenarvan garda pour lui le secret de ses angoisses, et ni lady Helena, ni Mary Grant, ni Robert ne soup\u00e7onn\u00e8rent la gravit\u00e9 des p\u00e9rils auxquels ils venaient d\u2019\u00e9chapper.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Glen', 'ar', 'van', '\u2581gard', 'a', '\u2581pour', '\u2581lui', '\u2581le', '\u2581secret', '\u2581de', '\u2581ses', '\u2581an', 'go', 'isses', ',', '\u2581et', '\u2581ni', '\u2581lady', '\u2581Hel', 'ena', ',', '\u2581ni', '\u2581Mary', '\u2581Grant', ',', '\u2581ni', '\u2581Robert', '\u2581ne', '\u2581soup', '\u00e7', 'onn', '\u00e8rent', '\u2581la', '\u2581grav', 'it\u00e9', '\u2581des', '\u2581p', '\u00e9r', 'ils', '\u2581aux', 'qu', 'els', '\u2581ils', '\u2581v', 'ena', 'ient', '\u2581d', '\u2019', '\u00e9ch', 'apper', '.']"}], "[\n  [\"\u2581Glen\", \"\u0413\u043b\u0435\u043d\"],\n  [\"ar\", null],\n  [\"van\", null],\n  [\"\u2581gard\", \"\u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043b\"],\n  [\"a\", null],\n  [\"\u2581pour\", null],\n  [\"\u2581lui\", null],\n  [\"\u2581le\", null],\n  [\"\u2581secret\", \"\u0441\u0435\u043a\u0440\u0435\u0442\"],\n  [\"\u2581de\", null],\n  [\"\u2581ses\", null],\n  [\"\u2581an\", \"\u0442\u0440\u0435\u0432\u043e\u0433\"],\n  [\"go\", null],\n  [\"isses\", null],\n  [\",\", null],\n  [\"\u2581et\", null],\n  [\"\u2581ni\", null],\n  [\"\u2581lady\", \"\u043b\u0435\u0434\u0438\"],\n  [\"\u2581Hel\", \"\u0415\u043b\u0435\u043d\u0430\"],\n  [\"ena\", null],\n  [\",\", null],\n  [\"\u2581ni\", null],\n  [\"\u2581Mary\", \"\u041c\u044d\u0440\u0438\"],\n  [\"\u2581Grant\", \"\u0413\u0440\u0430\u043d\u0442\"],\n  [\",\", null],\n  [\"\u2581ni\", null],\n  [\"\u2581Robert\", \"\u0420\u043e\u0431\u0435\u0440\u0442\"],\n  [\"\u2581ne\", null],\n  [\"\u2581soup\", \"\u043f\u043e\u0434\u043e\u0437\u0440\u0435\u0432\u0430\u043b\u0438\"],\n  [\"\u00e7\", null],\n  [\"onn\", null],\n  [\"\u00e8rent\", null],\n  [\"\u2581la\", null],\n  [\"\u2581grav\", \"\u0441\u0435\u0440\u044c\u0435\u0437\u043d\u043e\u0441\u0442\u044c\"],\n  [\"it\u00e9\", null],\n  [\"\u2581des\", null],\n  [\"\u2581p\", \"\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u0435\u0439\"],\n  [\"\u00e9r\", null],\n  [\"ils\", null],\n  [\"\u2581aux\", null],\n  [\"qu\", null],\n  [\"els\", null],\n  [\"\u2581ils\", null],\n  [\"\u2581v\", \"\u0442\u043e\u043b\u044c\u043a\u043e\"],\n  [\"ena\", null],\n  [\"ient\", null],\n  [\"\u2581d\", null],\n  [\"'\", null],\n  [\"\u00e9ch\", \"\u0438\u0437\u0431\u0435\u0436\u0430\u043b\u0438\"],\n  [\"apper\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nJe commen\u00e7ai \u00e0 comprendre ce qu'elle devait \u00eatre dans un sermon que je l'entendis faire \u00e0 sa propre paroisse de Morton: il n'est pas en mon pouvoir de raconter ce sermon;\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Je', '\u2581comm', 'en\u00e7', 'ai', '\u2581\u00e0', '\u2581compr', 'endre', '\u2581ce', '\u2581qu', \"'\", 'elle', '\u2581dev', 'ait', '\u2581\u00eatre', '\u2581dans', '\u2581un', '\u2581ser', 'mon', '\u2581que', '\u2581je', '\u2581l', \"'\", 'ent', 'end', 'is', '\u2581faire', '\u2581\u00e0', '\u2581sa', '\u2581propre', '\u2581par', 'o', 'isse', '\u2581de', '\u2581Mort', 'on', ':', '\u2581il', '\u2581n', \"'\", 'est', '\u2581pas', '\u2581en', '\u2581mon', '\u2581pouvoir', '\u2581de', '\u2581ra', 'con', 'ter', '\u2581ce', '\u2581ser', 'mon', ';']"}], "[\n[\"\u2581Je\", \"I\"],\n[\"\u2581comm\", \"began\"],\n[\"en\u00e7\", null],\n[\"ai\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581compr\", \"understand\"],\n[\"endre\", null],\n[\"\u2581ce\", \"what\"],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"elle\", \"she\"],\n[\"\u2581dev\", \"must\"],\n[\"ait\", null],\n[\"\u2581\u00eatre\", \"be\"],\n[\"\u2581dans\", \"in\"],\n[\"\u2581un\", \"a\"],\n[\"\u2581ser\", \"sermon\"],\n[\"mon\", null],\n[\"\u2581que\", \"that\"],\n[\"\u2581je\", \"I\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"ent\", \"heard\"],\n[\"end\", null],\n[\"is\", null],\n[\"\u2581faire\", \"give\"],\n[\"\u2581\u00e0\", \"to\"],\n[\"\u2581sa\", \"her\"],\n[\"\u2581propre\", \"own\"],\n[\"\u2581par\", \"parish\"],\n[\"o\", null],\n[\"isse\", null],\n[\"\u2581de\", \"of\"],\n[\"\u2581Mort\", \"Morton\"],\n[\"on\", null],\n[\":\", null],\n[\"\u2581il\", \"it\"],\n[\"\u2581n\", null],\n[\"'\", null],\n[\"est\", \"is\"],\n[\"\u2581pas\", \"not\"],\n[\"\u2581en\", \"in\"],\n[\"\u2581mon\", \"my\"],\n[\"\u2581pouvoir\", \"power\"],\n[\"\u2581de\", \"to\"],\n[\"\u2581ra\", \"recount\"],\n[\"con\", null],\n[\"ter\", null],\n[\"\u2581ce\", \"this\"],\n[\"\u2581ser\", \"sermon\"],\n[\"mon\", null],\n[\";\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nJe commen\u00e7ai \u00e0 comprendre ce qu'elle devait \u00eatre dans un sermon que je l'entendis faire \u00e0 sa propre paroisse de Morton: il n'est pas en mon pouvoir de raconter ce sermon;\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Je', '\u2581comm', 'en\u00e7', 'ai', '\u2581\u00e0', '\u2581compr', 'endre', '\u2581ce', '\u2581qu', \"'\", 'elle', '\u2581dev', 'ait', '\u2581\u00eatre', '\u2581dans', '\u2581un', '\u2581ser', 'mon', '\u2581que', '\u2581je', '\u2581l', \"'\", 'ent', 'end', 'is', '\u2581faire', '\u2581\u00e0', '\u2581sa', '\u2581propre', '\u2581par', 'o', 'isse', '\u2581de', '\u2581Mort', 'on', ':', '\u2581il', '\u2581n', \"'\", 'est', '\u2581pas', '\u2581en', '\u2581mon', '\u2581pouvoir', '\u2581de', '\u2581ra', 'con', 'ter', '\u2581ce', '\u2581ser', 'mon', ';']"}], "[\n[\"\u2581Je\", \"\u042f\"],\n[\"\u2581comm\", \"\u043d\u0430\u0447\u0438\u043d\u0430\u0442\u044c\"],\n[\"en\u00e7\", null],\n[\"ai\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581compr\", \"\u043f\u043e\u043d\u0438\u043c\u0430\u0442\u044c\"],\n[\"endre\", null],\n[\"\u2581ce\", null],\n[\"\u2581qu\", \"\u0447\u0442\u043e\"],\n[\"'\", null],\n[\"elle\", \"\u043e\u043d\u0430\"],\n[\"\u2581dev\", \"\u0434\u043e\u043b\u0436\u043d\u0430\"],\n[\"ait\", null],\n[\"\u2581\u00eatre\", \"\u0431\u044b\u0442\u044c\"],\n[\"\u2581dans\", \"\u0432\"],\n[\"\u2581un\", null],\n[\"\u2581ser\", \"\u043f\u0440\u043e\u043f\u043e\u0432\u0435\u0434\u044c\"],\n[\"mon\", null],\n[\"\u2581que\", \"\u043a\u043e\u0442\u043e\u0440\u0443\u044e\"],\n[\"\u2581je\", \"\u044f\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"ent\", \"\u0441\u043b\u044b\u0448\u0430\u0442\u044c\"],\n[\"end\", null],\n[\"is\", null],\n[\"\u2581faire\", \"\u043f\u0440\u043e\u0438\u0437\u043d\u043e\u0441\u0438\u0442\u044c\"],\n[\"\u2581\u00e0\", \"\u0432\"],\n[\"\u2581sa\", \"\u0441\u0432\u043e\u0451\u043c\"],\n[\"\u2581propre\", \"\u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u043c\"],\n[\"\u2581par\", \"\u043f\u0440\u0438\u0445\u043e\u0434\"],\n[\"o\", null],\n[\"isse\", null],\n[\"\u2581de\", null],\n[\"\u2581Mort\", \"\u041c\u043e\u0440\u0442\u043e\u043d\"],\n[\"on\", null],\n[\":\", null],\n[\"\u2581il\", null],\n[\"\u2581n\", \"\u043d\u0435\"],\n[\"'\", null],\n[\"est\", null],\n[\"\u2581pas\", null],\n[\"\u2581en\", \"\u0432\"],\n[\"\u2581mon\", \"\u043c\u043e\u0438\u0445\"],\n[\"\u2581pouvoir\", \"\u0441\u0438\u043b\u0430\u0445\"],\n[\"\u2581de\", null],\n[\"\u2581ra\", \"\u0440\u0430\u0441\u0441\u043a\u0430\u0437\u0430\u0442\u044c\"],\n[\"con\", null],\n[\"ter\", null],\n[\"\u2581ce\", \"\u044d\u0442\u0443\"],\n[\"\u2581ser\", \"\u043f\u0440\u043e\u043f\u043e\u0432\u0435\u0434\u044c\"],\n[\"mon\", null],\n[\";\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nHuit avirons \u00e0 l\u2019avant le faisaient voler \u00e0 la surface des eaux, pendant qu\u2019un homme, assis \u00e0 l\u2019arri\u00e8re, le dirigeait au moyen d\u2019une pagaie mobile.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581H', 'uit', '\u2581av', 'ir', 'ons', '\u2581\u00e0', '\u2581l', '\u2019', 'avant', '\u2581le', '\u2581fais', 'aient', '\u2581vol', 'er', '\u2581\u00e0', '\u2581la', '\u2581surface', '\u2581des', '\u2581e', 'aux', ',', '\u2581pendant', '\u2581qu', '\u2019', 'un', '\u2581homme', ',', '\u2581ass', 'is', '\u2581\u00e0', '\u2581l', '\u2019', 'ar', 'ri\u00e8re', ',', '\u2581le', '\u2581dirige', 'ait', '\u2581au', '\u2581mo', 'yen', '\u2581d', '\u2019', 'une', '\u2581p', 'aga', 'ie', '\u2581mobile', '.']"}], "[\n  [\"\u2581H\", \"Eight\"],\n  [\"uit\", null],\n  [\"\u2581av\", \"oars\"],\n  [\"ir\", null],\n  [\"ons\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"avant\", \"front\"],\n  [\"\u2581le\", null],\n  [\"\u2581fais\", \"made\"],\n  [\"aient\", null],\n  [\"\u2581vol\", \"fly\"],\n  [\"er\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581la\", null],\n  [\"\u2581surface\", \"surface\"],\n  [\"\u2581des\", null],\n  [\"\u2581e\", \"waters\"],\n  [\"aux\", null],\n  [\",\", null],\n  [\"\u2581pendant\", \"while\"],\n  [\"\u2581qu\", null],\n  [\"'\", null],\n  [\"un\", \"a\"],\n  [\"\u2581homme\", \"man\"],\n  [\",\", null],\n  [\"\u2581ass\", \"seated\"],\n  [\"is\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"ar\", \"stern\"],\n  [\"ri\u00e8re\", null],\n  [\",\", null],\n  [\"\u2581le\", null],\n  [\"\u2581dirige\", \"steered\"],\n  [\"ait\", null],\n  [\"\u2581au\", null],\n  [\"\u2581mo\", \"means\"],\n  [\"yen\", null],\n  [\"\u2581d\", null],\n  [\"'\", null],\n  [\"une\", \"a\"],\n  [\"\u2581p\", \"paddle\"],\n  [\"aga\", null],\n  [\"ie\", null],\n  [\"\u2581mobile\", \"movable\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nHuit avirons \u00e0 l\u2019avant le faisaient voler \u00e0 la surface des eaux, pendant qu\u2019un homme, assis \u00e0 l\u2019arri\u00e8re, le dirigeait au moyen d\u2019une pagaie mobile.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581H', 'uit', '\u2581av', 'ir', 'ons', '\u2581\u00e0', '\u2581l', '\u2019', 'avant', '\u2581le', '\u2581fais', 'aient', '\u2581vol', 'er', '\u2581\u00e0', '\u2581la', '\u2581surface', '\u2581des', '\u2581e', 'aux', ',', '\u2581pendant', '\u2581qu', '\u2019', 'un', '\u2581homme', ',', '\u2581ass', 'is', '\u2581\u00e0', '\u2581l', '\u2019', 'ar', 'ri\u00e8re', ',', '\u2581le', '\u2581dirige', 'ait', '\u2581au', '\u2581mo', 'yen', '\u2581d', '\u2019', 'une', '\u2581p', 'aga', 'ie', '\u2581mobile', '.']"}], "[\n[\"\u2581H\", \"\u0412\u043e\u0441\u0435\u043c\u044c\"],\n[\"uit\", null],\n[\"\u2581av\", \"\u0432\u0435\u0441\u043b\u0430\"],\n[\"ir\", null],\n[\"ons\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"avant\", \"\u0441\u043f\u0435\u0440\u0435\u0434\u0438\"],\n[\"\u2581le\", null],\n[\"\u2581fais\", \"\u0437\u0430\u0441\u0442\u0430\u0432\u043b\u044f\u043b\u0438\"],\n[\"aient\", null],\n[\"\u2581vol\", \"\u043b\u0435\u0442\u0435\u0442\u044c\"],\n[\"er\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581la\", null],\n[\"\u2581surface\", \"\u043f\u043e\u0432\u0435\u0440\u0445\u043d\u043e\u0441\u0442\u044c\"],\n[\"\u2581des\", null],\n[\"\u2581e\", \"\u0432\u043e\u0434\"],\n[\"aux\", null],\n[\",\", null],\n[\"\u2581pendant\", \"\u043f\u043e\u043a\u0430\"],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"un\", null],\n[\"\u2581homme\", \"\u0447\u0435\u043b\u043e\u0432\u0435\u043a\"],\n[\",\", null],\n[\"\u2581ass\", \"\u0441\u0438\u0434\u044f\u0449\u0438\u0439\"],\n[\"is\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"ar\", \"\u043a\u043e\u0440\u043c\u0435\"],\n[\"ri\u00e8re\", null],\n[\",\", null],\n[\"\u2581le\", null],\n[\"\u2581dirige\", \"\u0443\u043f\u0440\u0430\u0432\u043b\u044f\u043b\"],\n[\"ait\", null],\n[\"\u2581au\", null],\n[\"\u2581mo\", \"\u043f\u043e\u043c\u043e\u0449\u044c\u044e\"],\n[\"yen\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"une\", null],\n[\"\u2581p\", \"\u043f\u043e\u0434\u0432\u0438\u0436\u043d\u043e\u0433\u043e\"],\n[\"aga\", null],\n[\"ie\", null],\n[\"\u2581mobile\", \"\u0432\u0435\u0441\u043b\u0430\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nLa r\u00e9ponse co\u00efncidait si peu avec la question, que le fou rire recommen\u00e7a \u00e0 circuler, et que messire Robert s\u2019\u00e9cria, rouge de col\u00e8re :\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581La', '\u2581r\u00e9', 'ponse', '\u2581co', '\u00ef', 'nc', 'id', 'ait', '\u2581si', '\u2581peu', '\u2581avec', '\u2581la', '\u2581question', ',', '\u2581que', '\u2581le', '\u2581fou', '\u2581r', 'ire', '\u2581recom', 'men', '\u00e7a', '\u2581\u00e0', '\u2581circul', 'er', ',', '\u2581et', '\u2581que', '\u2581mess', 'ire', '\u2581Robert', '\u2581s', '\u2019', '\u00e9', 'cri', 'a', ',', '\u2581rou', 'ge', '\u2581de', '\u2581col', '\u00e8re', '\u2581:']"}], "[\n[\"\u2581La\", \"The\"],\n[\"\u2581r\u00e9\", \"answer\"],\n[\"ponse\", null],\n[\"\u2581co\", \"coincided\"],\n[\"\u00ef\", null],\n[\"nc\", null],\n[\"id\", null],\n[\"ait\", null],\n[\"\u2581si\", \"so\"],\n[\"\u2581peu\", \"little\"],\n[\"\u2581avec\", \"with\"],\n[\"\u2581la\", \"the\"],\n[\"\u2581question\", \"question\"],\n[\",\", null],\n[\"\u2581que\", \"that\"],\n[\"\u2581le\", \"the\"],\n[\"\u2581fou\", \"mad\"],\n[\"\u2581r\", \"laughter\"],\n[\"ire\", null],\n[\"\u2581recom\", \"began\"],\n[\"men\", null],\n[\"\u00e7a\", null],\n[\"\u2581\u00e0\", \"to\"],\n[\"\u2581circul\", \"circulate\"],\n[\"er\", null],\n[\",\", null],\n[\"\u2581et\", \"and\"],\n[\"\u2581que\", \"that\"],\n[\"\u2581mess\", \"Sir\"],\n[\"ire\", null],\n[\"\u2581Robert\", \"Robert\"],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"\u00e9\", \"exclaimed\"],\n[\"cri\", null],\n[\"a\", null],\n[\",\", null],\n[\"\u2581rou\", \"red\"],\n[\"ge\", null],\n[\"\u2581de\", \"with\"],\n[\"\u2581col\", \"anger\"],\n[\"\u00e8re\", null],\n[\"\u2581:\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nLa r\u00e9ponse co\u00efncidait si peu avec la question, que le fou rire recommen\u00e7a \u00e0 circuler, et que messire Robert s\u2019\u00e9cria, rouge de col\u00e8re :\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581La', '\u2581r\u00e9', 'ponse', '\u2581co', '\u00ef', 'nc', 'id', 'ait', '\u2581si', '\u2581peu', '\u2581avec', '\u2581la', '\u2581question', ',', '\u2581que', '\u2581le', '\u2581fou', '\u2581r', 'ire', '\u2581recom', 'men', '\u00e7a', '\u2581\u00e0', '\u2581circul', 'er', ',', '\u2581et', '\u2581que', '\u2581mess', 'ire', '\u2581Robert', '\u2581s', '\u2019', '\u00e9', 'cri', 'a', ',', '\u2581rou', 'ge', '\u2581de', '\u2581col', '\u00e8re', '\u2581:']"}], "[\n[\"\u2581La\", \"\u041e\u0442\u0432\u0435\u0442\"],\n[\"\u2581r\u00e9\", null],\n[\"ponse\", null],\n[\"\u2581co\", null],\n[\"\u00ef\", null],\n[\"nc\", null],\n[\"id\", null],\n[\"ait\", null],\n[\"\u2581si\", \"\u0442\u0430\u043a\"],\n[\"\u2581peu\", \"\u043c\u0430\u043b\u043e\"],\n[\"\u2581avec\", \"\u0441\"],\n[\"\u2581la\", null],\n[\"\u2581question\", \"\u0432\u043e\u043f\u0440\u043e\u0441\u043e\u043c\"],\n[\",\", null],\n[\"\u2581que\", \"\u0447\u0442\u043e\"],\n[\"\u2581le\", null],\n[\"\u2581fou\", \"\u0431\u0435\u0437\u0443\u043c\u043d\u044b\u0439\"],\n[\"\u2581r\", null],\n[\"ire\", \"\u0441\u043c\u0435\u0445\"],\n[\"\u2581recom\", null],\n[\"men\", null],\n[\"\u00e7a\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581circul\", \"\u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u044f\u0442\u044c\u0441\u044f\"],\n[\"er\", null],\n[\",\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581que\", \"\u0447\u0442\u043e\"],\n[\"\u2581mess\", null],\n[\"ire\", \"\u043c\u0435\u0441\u0441\u0438\u0440\"],\n[\"\u2581Robert\", \"\u0420\u043e\u0431\u0435\u0440\u0442\"],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"\u00e9\", null],\n[\"cri\", null],\n[\"a\", null],\n[\",\", null],\n[\"\u2581rou\", null],\n[\"ge\", \"\u043a\u0440\u0430\u0441\u043d\u044b\u0439\"],\n[\"\u2581de\", \"\u043e\u0442\"],\n[\"\u2581col\", null],\n[\"\u00e8re\", \"\u0433\u043d\u0435\u0432\u0430\"],\n[\"\u2581:\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nEn 1697, trois b\u00e2timents hollandais de la compagnie des Indes y rel\u00e2ch\u00e8rent, et en d\u00e9termin\u00e8rent les coordonn\u00e9es, laissant au grand astronome Halley le soin de revoir leurs calculs en l\u2019an 1700.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581En', '\u2581', '1', '6', '9', '7', ',', '\u2581trois', '\u2581b\u00e2t', 'iments', '\u2581hol', 'land', 'ais', '\u2581de', '\u2581la', '\u2581compagnie', '\u2581des', '\u2581Ind', 'es', '\u2581y', '\u2581rel', '\u00e2', 'ch', '\u00e8rent', ',', '\u2581et', '\u2581en', '\u2581d\u00e9', 'termin', '\u00e8rent', '\u2581les', '\u2581coord', 'onn\u00e9es', ',', '\u2581la', 'issant', '\u2581au', '\u2581grand', '\u2581astr', 'on', 'ome', '\u2581Hal', 'ley', '\u2581le', '\u2581so', 'in', '\u2581de', '\u2581re', 'voir', '\u2581leurs', '\u2581calcul', 's', '\u2581en', '\u2581l', '\u2019', 'an', '\u2581', '1', '7', '0', '0', '.']"}], "[\n[\"\u2581En\", \"In\"],\n[\"\u2581\", null],\n[\"1\", null],\n[\"6\", null],\n[\"9\", null],\n[\"7\", null],\n[\",\", null],\n[\"\u2581trois\", \"three\"],\n[\"\u2581b\u00e2t\", \"ships\"],\n[\"iments\", null],\n[\"\u2581hol\", \"Dutch\"],\n[\"land\", null],\n[\"ais\", null],\n[\"\u2581de\", null],\n[\"\u2581la\", null],\n[\"\u2581compagnie\", \"company\"],\n[\"\u2581des\", null],\n[\"\u2581Ind\", \"Indies\"],\n[\"es\", null],\n[\"\u2581y\", null],\n[\"\u2581rel\", \"stopped\"],\n[\"\u00e2\", null],\n[\"ch\", null],\n[\"\u00e8rent\", null],\n[\",\", null],\n[\"\u2581et\", \"and\"],\n[\"\u2581en\", null],\n[\"\u2581d\u00e9\", \"determined\"],\n[\"termin\", null],\n[\"\u00e8rent\", null],\n[\"\u2581les\", null],\n[\"\u2581coord\", \"coordinates\"],\n[\"onn\u00e9es\", null],\n[\",\", null],\n[\"\u2581la\", null],\n[\"issant\", \"leaving\"],\n[\"\u2581au\", null],\n[\"\u2581grand\", \"great\"],\n[\"\u2581astr\", \"astronomer\"],\n[\"on\", null],\n[\"ome\", null],\n[\"\u2581Hal\", \"Halley\"],\n[\"ley\", null],\n[\"\u2581le\", null],\n[\"\u2581so\", \"task\"],\n[\"in\", null],\n[\"\u2581de\", null],\n[\"\u2581re\", \"review\"],\n[\"voir\", null],\n[\"\u2581leurs\", \"their\"],\n[\"\u2581calcul\", \"calculations\"],\n[\"s\", null],\n[\"\u2581en\", \"in\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"an\", \"year\"],\n[\"\u2581\", null],\n[\"1\", null],\n[\"7\", null],\n[\"0\", null],\n[\"0\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nEn 1697, trois b\u00e2timents hollandais de la compagnie des Indes y rel\u00e2ch\u00e8rent, et en d\u00e9termin\u00e8rent les coordonn\u00e9es, laissant au grand astronome Halley le soin de revoir leurs calculs en l\u2019an 1700.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581En', '\u2581', '1', '6', '9', '7', ',', '\u2581trois', '\u2581b\u00e2t', 'iments', '\u2581hol', 'land', 'ais', '\u2581de', '\u2581la', '\u2581compagnie', '\u2581des', '\u2581Ind', 'es', '\u2581y', '\u2581rel', '\u00e2', 'ch', '\u00e8rent', ',', '\u2581et', '\u2581en', '\u2581d\u00e9', 'termin', '\u00e8rent', '\u2581les', '\u2581coord', 'onn\u00e9es', ',', '\u2581la', 'issant', '\u2581au', '\u2581grand', '\u2581astr', 'on', 'ome', '\u2581Hal', 'ley', '\u2581le', '\u2581so', 'in', '\u2581de', '\u2581re', 'voir', '\u2581leurs', '\u2581calcul', 's', '\u2581en', '\u2581l', '\u2019', 'an', '\u2581', '1', '7', '0', '0', '.']"}], "[\n[\"\u2581En\", \"\u0412\"],\n[\"\u2581\", null],\n[\"1\", null],\n[\"6\", null],\n[\"9\", null],\n[\"7\", null],\n[\",\", null],\n[\"\u2581trois\", \"\u0442\u0440\u0438\"],\n[\"\u2581b\u00e2t\", \"\u043a\u043e\u0440\u0430\u0431\u043b\u044f\"],\n[\"iments\", null],\n[\"\u2581hol\", \"\u0433\u043e\u043b\u043b\u0430\u043d\u0434\u0441\u043a\u043e\u0439\"],\n[\"land\", null],\n[\"ais\", null],\n[\"\u2581de\", null],\n[\"\u2581la\", null],\n[\"\u2581compagnie\", \"\u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438\"],\n[\"\u2581des\", null],\n[\"\u2581Ind\", \"\u041e\u0441\u0442-\u0418\u043d\u0434\u0441\u043a\u043e\u0439\"],\n[\"es\", null],\n[\"\u2581y\", \"\u0442\u0430\u043c\"],\n[\"\u2581rel\", \"\u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043b\u0438\u0441\u044c\"],\n[\"\u00e2\", null],\n[\"ch\", null],\n[\"\u00e8rent\", null],\n[\",\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581en\", null],\n[\"\u2581d\u00e9\", \"\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043b\u0438\"],\n[\"termin\", null],\n[\"\u00e8rent\", null],\n[\"\u2581les\", null],\n[\"\u2581coord\", \"\u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u044b\"],\n[\"onn\u00e9es\", null],\n[\",\", null],\n[\"\u2581la\", \"\u043e\u0441\u0442\u0430\u0432\u0438\u0432\"],\n[\"issant\", null],\n[\"\u2581au\", null],\n[\"\u2581grand\", \"\u0432\u0435\u043b\u0438\u043a\u043e\u043c\u0443\"],\n[\"\u2581astr\", \"\u0430\u0441\u0442\u0440\u043e\u043d\u043e\u043c\u0443\"],\n[\"on\", null],\n[\"ome\", null],\n[\"\u2581Hal\", \"\u0413\u0430\u043b\u043b\u0435\u044e\"],\n[\"ley\", null],\n[\"\u2581le\", null],\n[\"\u2581so\", \"\u0437\u0430\u0431\u043e\u0442\u0443\"],\n[\"in\", null],\n[\"\u2581de\", null],\n[\"\u2581re\", \"\u043f\u0435\u0440\u0435\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c\"],\n[\"voir\", null],\n[\"\u2581leurs\", \"\u0438\u0445\"],\n[\"\u2581calcul\", \"\u0440\u0430\u0441\u0447\u0435\u0442\u044b\"],\n[\"s\", null],\n[\"\u2581en\", \"\u0432\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"an\", \"\u0433\u043e\u0434\u0443\"],\n[\"\u2581\", null],\n[\"1\", null],\n[\"7\", null],\n[\"0\", null],\n[\"0\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nIl la pla\u00e7a dans l'esp\u00e8ce de charrette qui devait nous conduire; je montai ensuite, et, avant qu'il referm\u00e2t la porti\u00e8re, je lui demandai \u00e0 quelle distance nous \u00e9tions de Thornfield.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Il', '\u2581la', '\u2581pla', '\u00e7a', '\u2581dans', '\u2581l', \"'\", 'esp', '\u00e8ce', '\u2581de', '\u2581char', 'ret', 'te', '\u2581qui', '\u2581dev', 'ait', '\u2581nous', '\u2581condu', 'ire', ';', '\u2581je', '\u2581mont', 'ai', '\u2581ensuite', ',', '\u2581et', ',', '\u2581avant', '\u2581qu', \"'\", 'il', '\u2581refer', 'm', '\u00e2t', '\u2581la', '\u2581port', 'i\u00e8re', ',', '\u2581je', '\u2581lui', '\u2581demand', 'ai', '\u2581\u00e0', '\u2581quelle', '\u2581distance', '\u2581nous', '\u2581\u00e9t', 'ions', '\u2581de', '\u2581Th', 'orn', 'field', '.']"}], "[\n[\"\u2581Il\", \"He\"],\n[\"\u2581la\", null],\n[\"\u2581pla\", \"placed\"],\n[\"\u00e7a\", null],\n[\"\u2581dans\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"esp\", \"kind\"],\n[\"\u00e8ce\", null],\n[\"\u2581de\", null],\n[\"\u2581char\", \"cart\"],\n[\"ret\", null],\n[\"te\", null],\n[\"\u2581qui\", null],\n[\"\u2581dev\", \"was supposed\"],\n[\"ait\", null],\n[\"\u2581nous\", null],\n[\"\u2581condu\", \"take\"],\n[\"ire\", null],\n[\"\u2581;\", null],\n[\"\u2581je\", null],\n[\"\u2581mont\", \"got\"],\n[\"ai\", null],\n[\"\u2581ensuite\", \"then\"],\n[\",\", null],\n[\"\u2581et\", null],\n[\",\", null],\n[\"\u2581avant\", \"before\"],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"il\", null],\n[\"\u2581refer\", \"closed\"],\n[\"m\", null],\n[\"\u00e2t\", null],\n[\"\u2581la\", null],\n[\"\u2581port\", \"door\"],\n[\"i\u00e8re\", null],\n[\",\", null],\n[\"\u2581je\", null],\n[\"\u2581lui\", null],\n[\"\u2581demand\", \"asked\"],\n[\"ai\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581quelle\", \"what\"],\n[\"\u2581distance\", \"distance\"],\n[\"\u2581nous\", null],\n[\"\u2581\u00e9t\", \"were\"],\n[\"ions\", null],\n[\"\u2581de\", null],\n[\"\u2581Th\", \"Thornfield\"],\n[\"orn\", null],\n[\"field\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nIl la pla\u00e7a dans l'esp\u00e8ce de charrette qui devait nous conduire; je montai ensuite, et, avant qu'il referm\u00e2t la porti\u00e8re, je lui demandai \u00e0 quelle distance nous \u00e9tions de Thornfield.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Il', '\u2581la', '\u2581pla', '\u00e7a', '\u2581dans', '\u2581l', \"'\", 'esp', '\u00e8ce', '\u2581de', '\u2581char', 'ret', 'te', '\u2581qui', '\u2581dev', 'ait', '\u2581nous', '\u2581condu', 'ire', ';', '\u2581je', '\u2581mont', 'ai', '\u2581ensuite', ',', '\u2581et', ',', '\u2581avant', '\u2581qu', \"'\", 'il', '\u2581refer', 'm', '\u00e2t', '\u2581la', '\u2581port', 'i\u00e8re', ',', '\u2581je', '\u2581lui', '\u2581demand', 'ai', '\u2581\u00e0', '\u2581quelle', '\u2581distance', '\u2581nous', '\u2581\u00e9t', 'ions', '\u2581de', '\u2581Th', 'orn', 'field', '.']"}], "[\n[\"\u2581Il\", \"\u041e\u043d\"],\n[\"\u2581la\", null],\n[\"\u2581pla\", \"\u043f\u043e\u043c\u0435\u0441\u0442\u0438\u043b\"],\n[\"\u00e7a\", null],\n[\"\u2581dans\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"esp\", \"\u043f\u043e\u0432\u043e\u0437\u043a\u0443\"],\n[\"\u00e8ce\", null],\n[\"\u2581de\", null],\n[\"\u2581char\", null],\n[\"ret\", null],\n[\"te\", null],\n[\"\u2581qui\", \"\u043a\u043e\u0442\u043e\u0440\u0430\u044f\"],\n[\"\u2581dev\", \"\u0434\u043e\u043b\u0436\u043d\u0430\"],\n[\"ait\", null],\n[\"\u2581nous\", \"\u043d\u0430\u0441\"],\n[\"\u2581condu\", \"\u0432\u0435\u0437\u0442\u0438\"],\n[\"ire\", null],\n[\";\", null],\n[\"\u2581je\", \"\u044f\"],\n[\"\u2581mont\", \"\u0441\u0435\u043b\"],\n[\"ai\", null],\n[\"\u2581ensuite\", \"\u0437\u0430\u0442\u0435\u043c\"],\n[\",\", null],\n[\"\u2581et\", \"\u0438\"],\n[\",\", null],\n[\"\u2581avant\", \"\u043f\u0440\u0435\u0436\u0434\u0435\"],\n[\"\u2581qu\", \"\u0447\u0435\u043c\"],\n[\"'\", null],\n[\"il\", \"\u043e\u043d\"],\n[\"\u2581refer\", \"\u0437\u0430\u043a\u0440\u044b\u043b\"],\n[\"m\", null],\n[\"\u00e2t\", null],\n[\"\u2581la\", null],\n[\"\u2581port\", \"\u0434\u0432\u0435\u0440\u0446\u0443\"],\n[\"i\u00e8re\", null],\n[\",\", null],\n[\"\u2581je\", \"\u044f\"],\n[\"\u2581lui\", \"\u0435\u0433\u043e\"],\n[\"\u2581demand\", \"\u0441\u043f\u0440\u043e\u0441\u0438\u043b\"],\n[\"ai\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581quelle\", \"\u043a\u0430\u043a\u043e\u0435\"],\n[\"\u2581distance\", \"\u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0435\"],\n[\"\u2581nous\", \"\u043c\u044b\"],\n[\"\u2581\u00e9t\", \"\u043d\u0430\u0445\u043e\u0434\u0438\u043c\u0441\u044f\"],\n[\"ions\", null],\n[\"\u2581de\", \"\u043e\u0442\"],\n[\"\u2581Th\", \"\u0422\u043e\u0440\u043d\u0444\u0438\u043b\u0434\u0430\"],\n[\"orn\", null],\n[\"field\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nTous ces animaux, achet\u00e9s maigres dans les plaines des montagnes Bleues, allaient s\u2019engraisser au milieu des p\u00e2turages salutaires de l\u2019Australie m\u00e9ridionale, o\u00f9 ils sont revendus avec grand b\u00e9n\u00e9fice. Ainsi, Sam Machell, gagnant deux livres par boeuf et une demi-livre par mouton, devait r\u00e9aliser un b\u00e9n\u00e9fice de cinquante mille francs.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581T', 'ous', '\u2581ces', '\u2581anim', 'aux', ',', '\u2581ach', 'et', '\u00e9s', '\u2581ma', 'ig', 'res', '\u2581dans', '\u2581les', '\u2581pla', 'ines', '\u2581des', '\u2581mont', 'agn', 'es', '\u2581Ble', 'ues', ',', '\u2581alla', 'ient', '\u2581s', '\u2019', 'eng', 'rais', 'ser', '\u2581au', '\u2581milieu', '\u2581des', '\u2581p', '\u00e2t', 'ur', 'ages', '\u2581sal', 'ut', 'aires', '\u2581de', '\u2581l', '\u2019', 'Austral', 'ie', '\u2581m\u00e9r', 'id', 'ionale', ',', '\u2581o\u00f9', '\u2581ils', '\u2581sont', '\u2581rev', 'end', 'us', '\u2581avec', '\u2581grand', '\u2581b', '\u00e9n\u00e9', 'f', 'ice', '.', '\u2581Ain', 'si', ',', '\u2581Sam', '\u2581Mach', 'ell', ',', '\u2581g', 'agn', 'ant', '\u2581deux', '\u2581livres', '\u2581par', '\u2581bo', 'e', 'uf', '\u2581et', '\u2581une', '\u2581demi', '-', 'liv', 're', '\u2581par', '\u2581m', 'out', 'on', ',', '\u2581dev', 'ait', '\u2581r\u00e9alis', 'er', '\u2581un', '\u2581b', '\u00e9n\u00e9', 'f', 'ice', '\u2581de', '\u2581cin', 'qu', 'ante', '\u2581m', 'ille', '\u2581fran', 'cs', '.']"}], "[\n[\"\u2581T\", \"All\"],\n[\"ous\", null],\n[\"\u2581ces\", null],\n[\"\u2581anim\", \"animals\"],\n[\"aux\", null],\n[\",\", null],\n[\"\u2581ach\", \"bought\"],\n[\"et\", null],\n[\"\u00e9s\", null],\n[\"\u2581ma\", \"lean\"],\n[\"ig\", null],\n[\"res\", null],\n[\"\u2581dans\", null],\n[\"\u2581les\", null],\n[\"\u2581pla\", \"plains\"],\n[\"ines\", null],\n[\"\u2581des\", null],\n[\"\u2581mont\", \"Blue\"],\n[\"agn\", null],\n[\"es\", null],\n[\"\u2581Ble\", null],\n[\"ues\", null],\n[\",\", null],\n[\"\u2581alla\", \"were going\"],\n[\"ient\", null],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"eng\", \"fatten\"],\n[\"rais\", null],\n[\"ser\", null],\n[\"\u2581au\", null],\n[\"\u2581milieu\", \"midst\"],\n[\"\u2581des\", null],\n[\"\u2581p\", \"pastures\"],\n[\"\u00e2t\", null],\n[\"ur\", null],\n[\"ages\", null],\n[\"\u2581sal\", \"beneficial\"],\n[\"ut\", null],\n[\"aires\", null],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"Austral\", \"Southern\"],\n[\"ie\", null],\n[\"\u2581m\u00e9r\", null],\n[\"id\", null],\n[\"ionale\", null],\n[\",\", null],\n[\"\u2581o\u00f9\", \"where\"],\n[\"\u2581ils\", null],\n[\"\u2581sont\", null],\n[\"\u2581rev\", \"resold\"],\n[\"end\", null],\n[\"us\", null],\n[\"\u2581avec\", null],\n[\"\u2581grand\", \"great\"],\n[\"\u2581b\", \"profit\"],\n[\"\u00e9n\u00e9\", null],\n[\"f\", null],\n[\"ice\", null],\n[\".\", null],\n[\"\u2581Ain\", \"Thus\"],\n[\"si\", null],\n[\",\", null],\n[\"\u2581Sam\", \"Sam\"],\n[\"\u2581Mach\", \"Machell\"],\n[\"ell\", null],\n[\",\", null],\n[\"\u2581g\", \"earning\"],\n[\"agn\", null],\n[\"ant\", null],\n[\"\u2581deux\", \"two\"],\n[\"\u2581livres\", \"pounds\"],\n[\"\u2581par\", null],\n[\"\u2581bo\", \"ox\"],\n[\"e\", null],\n[\"uf\", null],\n[\"\u2581et\", null],\n[\"\u2581une\", \"half\"],\n[\"\u2581demi\", null],\n[\"-\", null],\n[\"liv\", \"pound\"],\n[\"re\", null],\n[\"\u2581par\", null],\n[\"\u2581m\", \"sheep\"],\n[\"out\", null],\n[\"on\", null],\n[\",\", null],\n[\"\u2581dev\", \"would\"],\n[\"ait\", null],\n[\"\u2581r\u00e9alis\", \"realize\"],\n[\"er\", null],\n[\"\u2581un\", null],\n[\"\u2581b\", \"profit\"],\n[\"\u00e9n\u00e9\", null],\n[\"f\", null],\n[\"ice\", null],\n[\"\u2581de\", null],\n[\"\u2581cin\", \"fifty\"],\n[\"qu\", null],\n[\"ante\", null],\n[\"\u2581m\", \"thousand\"],\n[\"ille\", null],\n[\"\u2581fran\", \"francs\"],\n[\"cs\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nTous ces animaux, achet\u00e9s maigres dans les plaines des montagnes Bleues, allaient s\u2019engraisser au milieu des p\u00e2turages salutaires de l\u2019Australie m\u00e9ridionale, o\u00f9 ils sont revendus avec grand b\u00e9n\u00e9fice. Ainsi, Sam Machell, gagnant deux livres par boeuf et une demi-livre par mouton, devait r\u00e9aliser un b\u00e9n\u00e9fice de cinquante mille francs.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581T', 'ous', '\u2581ces', '\u2581anim', 'aux', ',', '\u2581ach', 'et', '\u00e9s', '\u2581ma', 'ig', 'res', '\u2581dans', '\u2581les', '\u2581pla', 'ines', '\u2581des', '\u2581mont', 'agn', 'es', '\u2581Ble', 'ues', ',', '\u2581alla', 'ient', '\u2581s', '\u2019', 'eng', 'rais', 'ser', '\u2581au', '\u2581milieu', '\u2581des', '\u2581p', '\u00e2t', 'ur', 'ages', '\u2581sal', 'ut', 'aires', '\u2581de', '\u2581l', '\u2019', 'Austral', 'ie', '\u2581m\u00e9r', 'id', 'ionale', ',', '\u2581o\u00f9', '\u2581ils', '\u2581sont', '\u2581rev', 'end', 'us', '\u2581avec', '\u2581grand', '\u2581b', '\u00e9n\u00e9', 'f', 'ice', '.', '\u2581Ain', 'si', ',', '\u2581Sam', '\u2581Mach', 'ell', ',', '\u2581g', 'agn', 'ant', '\u2581deux', '\u2581livres', '\u2581par', '\u2581bo', 'e', 'uf', '\u2581et', '\u2581une', '\u2581demi', '-', 'liv', 're', '\u2581par', '\u2581m', 'out', 'on', ',', '\u2581dev', 'ait', '\u2581r\u00e9alis', 'er', '\u2581un', '\u2581b', '\u00e9n\u00e9', 'f', 'ice', '\u2581de', '\u2581cin', 'qu', 'ante', '\u2581m', 'ille', '\u2581fran', 'cs', '.']"}], "[\n[\"\u2581T\", \"\u0412\u0441\u0435\"],\n[\"ous\", null],\n[\"\u2581ces\", null],\n[\"\u2581anim\", \"\u0436\u0438\u0432\u043e\u0442\u043d\u044b\u0435\"],\n[\"aux\", null],\n[\",\", null],\n[\"\u2581ach\", \"\u043a\u0443\u043f\u043b\u0435\u043d\u043d\u044b\u0435\"],\n[\"et\", null],\n[\"\u00e9s\", null],\n[\"\u2581ma\", \"\u0445\u0443\u0434\u044b\u043c\u0438\"],\n[\"ig\", null],\n[\"res\", null],\n[\"\u2581dans\", null],\n[\"\u2581les\", null],\n[\"\u2581pla\", \"\u0440\u0430\u0432\u043d\u0438\u043d\u0430\u0445\"],\n[\"ines\", null],\n[\"\u2581des\", null],\n[\"\u2581mont\", \"\u0413\u043e\u043b\u0443\u0431\u044b\u0445\"],\n[\"agn\", null],\n[\"es\", null],\n[\"\u2581Ble\", null],\n[\"ues\", null],\n[\",\", null],\n[\"\u2581alla\", \"\u043e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u043b\u0438\u0441\u044c\"],\n[\"ient\", null],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"eng\", \"\u043e\u0442\u043a\u0430\u0440\u043c\u043b\u0438\u0432\u0430\u0442\u044c\u0441\u044f\"],\n[\"rais\", null],\n[\"ser\", null],\n[\"\u2581au\", null],\n[\"\u2581milieu\", \"\u0441\u0440\u0435\u0434\u0438\"],\n[\"\u2581des\", null],\n[\"\u2581p\", \"\u043f\u0430\u0441\u0442\u0431\u0438\u0449\"],\n[\"\u00e2t\", null],\n[\"ur\", null],\n[\"ages\", null],\n[\"\u2581sal\", \"\u0431\u043b\u0430\u0433\u043e\u0442\u0432\u043e\u0440\u043d\u044b\u0445\"],\n[\"ut\", null],\n[\"aires\", null],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"Austral\", \"\u042e\u0436\u043d\u043e\u0439\"],\n[\"ie\", null],\n[\"\u2581m\u00e9r\", \"\u0410\u0432\u0441\u0442\u0440\u0430\u043b\u0438\u0438\"],\n[\"id\", null],\n[\"ionale\", null],\n[\",\", null],\n[\"\u2581o\u00f9\", \"\u0433\u0434\u0435\"],\n[\"\u2581ils\", null],\n[\"\u2581sont\", null],\n[\"\u2581rev\", \"\u043f\u0435\u0440\u0435\u043f\u0440\u043e\u0434\u0430\u0432\u0430\u043b\u0438\u0441\u044c\"],\n[\"end\", null],\n[\"us\", null],\n[\"\u2581avec\", null],\n[\"\u2581grand\", \"\u0431\u043e\u043b\u044c\u0448\u043e\u0439\"],\n[\"\u2581b\", \"\u043f\u0440\u0438\u0431\u044b\u043b\u044c\u044e\"],\n[\"\u00e9n\u00e9\", null],\n[\"f\", null],\n[\"ice\", null],\n[\".\", null],\n[\"\u2581Ain\", \"\u0422\u0430\u043a\u0438\u043c\"],\n[\"si\", null],\n[\",\", null],\n[\"\u2581Sam\", \"\u0421\u044d\u043c\"],\n[\"\u2581Mach\", \"\u041c\u0430\u0447\u0435\u043b\u043b\"],\n[\"ell\", null],\n[\",\", null],\n[\"\u2581g\", \"\u0437\u0430\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u044f\"],\n[\"agn\", null],\n[\"ant\", null],\n[\"\u2581deux\", \"\u0434\u0432\u0430\"],\n[\"\u2581livres\", \"\u0444\u0443\u043d\u0442\u0430\"],\n[\"\u2581par\", \"\u0437\u0430\"],\n[\"\u2581bo\", \"\u0431\u044b\u043a\u0430\"],\n[\"e\", null],\n[\"uf\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581une\", \"\u043f\u043e\u043b\u0444\u0443\u043d\u0442\u0430\"],\n[\"\u2581demi\", null],\n[\"-\", null],\n[\"liv\", null],\n[\"re\", null],\n[\"\u2581par\", \"\u0437\u0430\"],\n[\"\u2581m\", \"\u043e\u0432\u0446\u0443\"],\n[\"out\", null],\n[\"on\", null],\n[\",\", null],\n[\"\u2581dev\", \"\u0434\u043e\u043b\u0436\u0435\u043d\"],\n[\"ait\", null],\n[\"\u2581r\u00e9alis\", \"\u0431\u044b\u043b\"],\n[\"er\", null],\n[\"\u2581un\", null],\n[\"\u2581b\", \"\u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c\"],\n[\"\u00e9n\u00e9\", null],\n[\"f\", null],\n[\"ice\", null],\n[\"\u2581de\", null],\n[\"\u2581cin\", \"\u043f\u044f\u0442\u044c\u0434\u0435\u0441\u044f\u0442\"],\n[\"qu\", null],\n[\"ante\", null],\n[\"\u2581m\", \"\u0442\u044b\u0441\u044f\u0447\"],\n[\"ille\", null],\n[\"\u2581fran\", \"\u0444\u0440\u0430\u043d\u043a\u043e\u0432\"],\n[\"cs\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nGr\u00e8ve, aboye, Gr\u00e8ve, grouille !File, file, ma quenouille,File sa corde au bourreau Qui siffle dans le pr\u00e9au.Gr\u00e8ve, aboye, Gr\u00e8ve, grouille !\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Gr', '\u00e8ve', ',', '\u2581ab', 'o', 'ye', ',', '\u2581Gr', '\u00e8ve', ',', '\u2581gr', 'ou', 'ille', '\u2581!', 'File', ',', '\u2581file', ',', '\u2581ma', '\u2581qu', 'en', 'ou', 'ille', ',', 'File', '\u2581sa', '\u2581cor', 'de', '\u2581au', '\u2581b', 'our', 're', 'au', '\u2581Qu', 'i', '\u2581s', 'iff', 'le', '\u2581dans', '\u2581le', '\u2581pr\u00e9', 'au', '.', 'Gr', '\u00e8ve', ',', '\u2581ab', 'o', 'ye', ',', '\u2581Gr', '\u00e8ve', ',', '\u2581gr', 'ou', 'ille', '\u2581!']"}], "[\n[\"\u2581Gr\", \"Strike\"],\n[\"\u00e8ve\", null],\n[\",\", null],\n[\"\u2581ab\", \"bark\"],\n[\"o\", null],\n[\"ye\", null],\n[\",\", null],\n[\"\u2581Gr\", \"Strike\"],\n[\"\u00e8ve\", null],\n[\",\", null],\n[\"\u2581gr\", \"swarm\"],\n[\"ou\", null],\n[\"ille\", null],\n[\"\u2581!\", null],\n[\"File\", \"Spin\"],\n[\",\", null],\n[\"\u2581file\", \"spin\"],\n[\",\", null],\n[\"\u2581ma\", \"my\"],\n[\"\u2581qu\", \"distaff\"],\n[\"en\", null],\n[\"ou\", null],\n[\"ille\", null],\n[\",\", null],\n[\"File\", \"Spin\"],\n[\"\u2581sa\", \"his\"],\n[\"\u2581cor\", \"rope\"],\n[\"de\", null],\n[\"\u2581au\", \"for\"],\n[\"\u2581b\", \"executioner\"],\n[\"our\", null],\n[\"re\", null],\n[\"au\", null],\n[\"\u2581Qu\", \"Who\"],\n[\"i\", null],\n[\"\u2581s\", \"whistles\"],\n[\"iff\", null],\n[\"le\", null],\n[\"\u2581dans\", \"in\"],\n[\"\u2581le\", \"the\"],\n[\"\u2581pr\u00e9\", \"courtyard\"],\n[\"au\", null],\n[\".\", null],\n[\"Gr\", \"Strike\"],\n[\"\u00e8ve\", null],\n[\",\", null],\n[\"\u2581ab\", \"bark\"],\n[\"o\", null],\n[\"ye\", null],\n[\",\", null],\n[\"\u2581Gr\", \"Strike\"],\n[\"\u00e8ve\", null],\n[\",\", null],\n[\"\u2581gr\", \"swarm\"],\n[\"ou\", null],\n[\"ille\", null],\n[\"\u2581!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nGr\u00e8ve, aboye, Gr\u00e8ve, grouille !File, file, ma quenouille,File sa corde au bourreau Qui siffle dans le pr\u00e9au.Gr\u00e8ve, aboye, Gr\u00e8ve, grouille !\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Gr', '\u00e8ve', ',', '\u2581ab', 'o', 'ye', ',', '\u2581Gr', '\u00e8ve', ',', '\u2581gr', 'ou', 'ille', '\u2581!', 'File', ',', '\u2581file', ',', '\u2581ma', '\u2581qu', 'en', 'ou', 'ille', ',', 'File', '\u2581sa', '\u2581cor', 'de', '\u2581au', '\u2581b', 'our', 're', 'au', '\u2581Qu', 'i', '\u2581s', 'iff', 'le', '\u2581dans', '\u2581le', '\u2581pr\u00e9', 'au', '.', 'Gr', '\u00e8ve', ',', '\u2581ab', 'o', 'ye', ',', '\u2581Gr', '\u00e8ve', ',', '\u2581gr', 'ou', 'ille', '\u2581!']"}], "[\n[\"\u2581Gr\", \"\u0417\u0430\u0431\u0430\u0441\u0442\u043e\u0432\u043a\u0430\"],\n[\"\u00e8ve\", null],\n[\",\", null],\n[\"\u2581ab\", \"\u043b\u0430\u044f\u0442\u044c\"],\n[\"o\", null],\n[\"ye\", null],\n[\",\", null],\n[\"\u2581Gr\", \"\u0417\u0430\u0431\u0430\u0441\u0442\u043e\u0432\u043a\u0430\"],\n[\"\u00e8ve\", null],\n[\",\", null],\n[\"\u2581gr\", \"\u043a\u0438\u0448\u0435\u0442\u044c\"],\n[\"ou\", null],\n[\"ille\", null],\n[\"\u2581!\", null],\n[\"File\", \"\u041f\u0440\u044f\u0434\u0438\u0442\u0435\"],\n[\",\", null],\n[\"\u2581file\", \"\u043f\u0440\u044f\u0434\u0438\u0442\u0435\"],\n[\",\", null],\n[\"\u2581ma\", \"\u043c\u043e\u044f\"],\n[\"\u2581qu\", \"\u043f\u0440\u044f\u043b\u043a\u0430\"],\n[\"en\", null],\n[\"ou\", null],\n[\"ille\", null],\n[\",\", null],\n[\"File\", \"\u041f\u0440\u044f\u0434\u0438\u0442\u0435\"],\n[\"\u2581sa\", \"\u0435\u0433\u043e\"],\n[\"\u2581cor\", \"\u0432\u0435\u0440\u0435\u0432\u043a\u0443\"],\n[\"de\", null],\n[\"\u2581au\", \"\u0434\u043b\u044f\"],\n[\"\u2581b\", \"\u043f\u0430\u043b\u0430\u0447\u0430\"],\n[\"our\", null],\n[\"re\", null],\n[\"au\", null],\n[\"\u2581Qu\", \"\u041a\u043e\u0442\u043e\u0440\u044b\u0439\"],\n[\"i\", null],\n[\"\u2581s\", \"\u0441\u0432\u0438\u0441\u0442\u0438\u0442\"],\n[\"iff\", null],\n[\"le\", null],\n[\"\u2581dans\", \"\u0432\u043e\"],\n[\"\u2581le\", null],\n[\"\u2581pr\u00e9\", \"\u0434\u0432\u043e\u0440\u0435\"],\n[\"au\", null],\n[\".\", null],\n[\"Gr\", \"\u0417\u0430\u0431\u0430\u0441\u0442\u043e\u0432\u043a\u0430\"],\n[\"\u00e8ve\", null],\n[\",\", null],\n[\"\u2581ab\", \"\u043b\u0430\u044f\u0442\u044c\"],\n[\"o\", null],\n[\"ye\", null],\n[\",\", null],\n[\"\u2581Gr\", \"\u0417\u0430\u0431\u0430\u0441\u0442\u043e\u0432\u043a\u0430\"],\n[\"\u00e8ve\", null],\n[\",\", null],\n[\"\u2581gr\", \"\u043a\u0438\u0448\u0435\u0442\u044c\"],\n[\"ou\", null],\n[\"ille\", null],\n[\"\u2581!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nQuand elle eut un enfant, il le fallut mettre en nourrice. Rentr\u00e9 chez eux, le marmot fut g\u00e2t\u00e9 comme un prince.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Qu', 'and', '\u2581elle', '\u2581e', 'ut', '\u2581un', '\u2581enf', 'ant', ',', '\u2581il', '\u2581le', '\u2581fall', 'ut', '\u2581met', 'tre', '\u2581en', '\u2581n', 'our', 'rice', '.', '\u2581R', 'ent', 'r\u00e9', '\u2581chez', '\u2581eux', ',', '\u2581le', '\u2581mar', 'mot', '\u2581fut', '\u2581g', '\u00e2', 't\u00e9', '\u2581comme', '\u2581un', '\u2581prince', '.']"}], "[\n    [\"\u2581Qu\", \"When\"],\n    [\"and\", null],\n    [\"\u2581elle\", \"she\"],\n    [\"\u2581e\", null],\n    [\"ut\", null],\n    [\"\u2581un\", \"a\"],\n    [\"\u2581enf\", \"child\"],\n    [\"ant\", null],\n    [\",\", null],\n    [\"\u2581il\", \"it\"],\n    [\"\u2581le\", null],\n    [\"\u2581fall\", \"had\"],\n    [\"ut\", null],\n    [\"\u2581met\", \"put\"],\n    [\"tre\", null],\n    [\"\u2581en\", \"to\"],\n    [\"\u2581n\", \"nurse\"],\n    [\"our\", null],\n    [\"rice\", null],\n    [\".\", null],\n    [\"\u2581R\", \"Returned\"],\n    [\"ent\", null],\n    [\"r\u00e9\", null],\n    [\"\u2581chez\", \"home\"],\n    [\"\u2581eux\", null],\n    [\",\", null],\n    [\"\u2581le\", \"the\"],\n    [\"\u2581mar\", \"child\"],\n    [\"mot\", null],\n    [\"\u2581fut\", \"was\"],\n    [\"\u2581g\", \"spoiled\"],\n    [\"\u00e2\", null],\n    [\"t\u00e9\", null],\n    [\"\u2581comme\", \"like\"],\n    [\"\u2581un\", \"a\"],\n    [\"\u2581prince\", \"prince\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nQuand elle eut un enfant, il le fallut mettre en nourrice. Rentr\u00e9 chez eux, le marmot fut g\u00e2t\u00e9 comme un prince.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Qu', 'and', '\u2581elle', '\u2581e', 'ut', '\u2581un', '\u2581enf', 'ant', ',', '\u2581il', '\u2581le', '\u2581fall', 'ut', '\u2581met', 'tre', '\u2581en', '\u2581n', 'our', 'rice', '.', '\u2581R', 'ent', 'r\u00e9', '\u2581chez', '\u2581eux', ',', '\u2581le', '\u2581mar', 'mot', '\u2581fut', '\u2581g', '\u00e2', 't\u00e9', '\u2581comme', '\u2581un', '\u2581prince', '.']"}], "[\n  [\"\u2581Qu\", \"\u041a\u043e\u0433\u0434\u0430\"],\n  [\"and\", null],\n  [\"\u2581elle\", \"\u043e\u043d\u0430\"],\n  [\"\u2581e\", null],\n  [\"ut\", null],\n  [\"\u2581un\", null],\n  [\"\u2581enf\", \"\u0440\u0435\u0431\u0435\u043d\u043e\u043a\"],\n  [\"ant\", null],\n  [\",\", null],\n  [\"\u2581il\", null],\n  [\"\u2581le\", null],\n  [\"\u2581fall\", \"\u043f\u0440\u0438\u0448\u043b\u043e\u0441\u044c\"],\n  [\"ut\", null],\n  [\"\u2581met\", \"\u043e\u0442\u0434\u0430\u0442\u044c\"],\n  [\"tre\", null],\n  [\"\u2581en\", null],\n  [\"\u2581n\", \"\u043a\u043e\u0440\u043c\u0438\u043b\u0438\u0446\u0430\"],\n  [\"our\", null],\n  [\"rice\", null],\n  [\".\", null],\n  [\"\u2581R\", \"\u0412\u0435\u0440\u043d\u0443\u0432\u0448\u0438\u0441\u044c\"],\n  [\"ent\", null],\n  [\"r\u00e9\", null],\n  [\"\u2581chez\", \"\u0434\u043e\u043c\u043e\u0439\"],\n  [\"\u2581eux\", null],\n  [\",\", null],\n  [\"\u2581le\", null],\n  [\"\u2581mar\", \"\u043c\u0430\u043b\u044b\u0448\"],\n  [\"mot\", null],\n  [\"\u2581fut\", null],\n  [\"\u2581g\", \"\u0438\u0437\u0431\u0430\u043b\u043e\u0432\u0430\u043d\"],\n  [\"\u00e2\", null],\n  [\"t\u00e9\", null],\n  [\"\u2581comme\", \"\u043a\u0430\u043a\"],\n  [\"\u2581un\", null],\n  [\"\u2581prince\", \"\u043f\u0440\u0438\u043d\u0446\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n\u00ab Voil\u00e0 une histoire bien vraisemblable ! \u00bb dit le Pigeon d\u2019un air de profond m\u00e9pris. \u00ab J\u2019ai vu bien des petites filles dans mon temps, mais je n\u2019en ai jamais vu avec un cou comme cela.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00ab', '\u2581Vo', 'il', '\u00e0', '\u2581une', '\u2581h', 'istoire', '\u2581bien', '\u2581v', 'rais', 'embl', 'able', '\u2581!', '\u2581\u00bb', '\u2581dit', '\u2581le', '\u2581P', 'ige', 'on', '\u2581d', '\u2019', 'un', '\u2581air', '\u2581de', '\u2581prof', 'ond', '\u2581m\u00e9', 'pr', 'is', '.', '\u2581\u00ab', '\u2581J', '\u2019', 'ai', '\u2581vu', '\u2581bien', '\u2581des', '\u2581pet', 'ites', '\u2581fill', 'es', '\u2581dans', '\u2581mon', '\u2581temps', ',', '\u2581mais', '\u2581je', '\u2581n', '\u2019', 'en', '\u2581ai', '\u2581jamais', '\u2581vu', '\u2581avec', '\u2581un', '\u2581cou', '\u2581comme', '\u2581cela', '.']"}], "[\n[\"\u2581\u00ab\", null],\n[\"\u2581Vo\", \"Here\"],\n[\"il\", null],\n[\"\u00e0\", null],\n[\"\u2581une\", null],\n[\"\u2581h\", \"story\"],\n[\"istoire\", null],\n[\"\u2581bien\", null],\n[\"\u2581v\", \"plausible\"],\n[\"rais\", null],\n[\"embl\", null],\n[\"able\", null],\n[\"\u2581!\", null],\n[\"\u2581\u00bb\", null],\n[\"\u2581dit\", \"said\"],\n[\"\u2581le\", null],\n[\"\u2581P\", \"Pigeon\"],\n[\"ige\", null],\n[\"on\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"un\", null],\n[\"\u2581air\", \"air\"],\n[\"\u2581de\", null],\n[\"\u2581prof\", \"deep\"],\n[\"ond\", null],\n[\"\u2581m\u00e9\", \"contempt\"],\n[\"pr\", null],\n[\"is\", null],\n[\".\", null],\n[\"\u2581\u00ab\", null],\n[\"\u2581J\", \"I\"],\n[\"'\", null],\n[\"ai\", null],\n[\"\u2581vu\", \"seen\"],\n[\"\u2581bien\", null],\n[\"\u2581des\", null],\n[\"\u2581pet\", \"little\"],\n[\"ites\", null],\n[\"\u2581fill\", \"girls\"],\n[\"es\", null],\n[\"\u2581dans\", \"in\"],\n[\"\u2581mon\", \"my\"],\n[\"\u2581temps\", \"time\"],\n[\",\", null],\n[\"\u2581mais\", \"but\"],\n[\"\u2581je\", \"I\"],\n[\"\u2581n\", null],\n[\"'\", null],\n[\"en\", null],\n[\"\u2581ai\", null],\n[\"\u2581jamais\", \"never\"],\n[\"\u2581vu\", \"seen\"],\n[\"\u2581avec\", \"with\"],\n[\"\u2581un\", null],\n[\"\u2581cou\", \"neck\"],\n[\"\u2581comme\", \"like\"],\n[\"\u2581cela\", \"that\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n\u00ab Voil\u00e0 une histoire bien vraisemblable ! \u00bb dit le Pigeon d\u2019un air de profond m\u00e9pris. \u00ab J\u2019ai vu bien des petites filles dans mon temps, mais je n\u2019en ai jamais vu avec un cou comme cela.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00ab', '\u2581Vo', 'il', '\u00e0', '\u2581une', '\u2581h', 'istoire', '\u2581bien', '\u2581v', 'rais', 'embl', 'able', '\u2581!', '\u2581\u00bb', '\u2581dit', '\u2581le', '\u2581P', 'ige', 'on', '\u2581d', '\u2019', 'un', '\u2581air', '\u2581de', '\u2581prof', 'ond', '\u2581m\u00e9', 'pr', 'is', '.', '\u2581\u00ab', '\u2581J', '\u2019', 'ai', '\u2581vu', '\u2581bien', '\u2581des', '\u2581pet', 'ites', '\u2581fill', 'es', '\u2581dans', '\u2581mon', '\u2581temps', ',', '\u2581mais', '\u2581je', '\u2581n', '\u2019', 'en', '\u2581ai', '\u2581jamais', '\u2581vu', '\u2581avec', '\u2581un', '\u2581cou', '\u2581comme', '\u2581cela', '.']"}], "[\n[\"\u2581\u00ab\", null],\n[\"\u2581Vo\", \"\u0412\u043e\u0442\"],\n[\"il\", null],\n[\"\u00e0\", null],\n[\"\u2581une\", null],\n[\"\u2581h\", \"\u0438\u0441\u0442\u043e\u0440\u0438\u044f\"],\n[\"istoire\", null],\n[\"\u2581bien\", \"\u0432\u0435\u0441\u044c\u043c\u0430\"],\n[\"\u2581v\", \"\u043f\u0440\u0430\u0432\u0434\u043e\u043f\u043e\u0434\u043e\u0431\u043d\u0430\u044f\"],\n[\"rais\", null],\n[\"embl\", null],\n[\"able\", null],\n[\"\u2581!\", null],\n[\"\u2581\u00bb\", null],\n[\"\u2581dit\", \"\u0441\u043a\u0430\u0437\u0430\u043b\"],\n[\"\u2581le\", null],\n[\"\u2581P\", \"\u0413\u043e\u043b\u0443\u0431\u044c\"],\n[\"ige\", null],\n[\"on\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"un\", null],\n[\"\u2581air\", \"\u0441 \u0432\u0438\u0434\u043e\u043c\"],\n[\"\u2581de\", null],\n[\"\u2581prof\", \"\u0433\u043b\u0443\u0431\u043e\u043a\u043e\u0433\u043e\"],\n[\"ond\", null],\n[\"\u2581m\u00e9\", \"\u043f\u0440\u0435\u0437\u0440\u0435\u043d\u0438\u044f\"],\n[\"pr\", null],\n[\"is\", null],\n[\".\", null],\n[\"\u2581\u00ab\", null],\n[\"\u2581J\", \"\u042f\"],\n[\"'\", null],\n[\"ai\", null],\n[\"\u2581vu\", \"\u0432\u0438\u0434\u0435\u043b\"],\n[\"\u2581bien\", \"\u043d\u0435\u043c\u0430\u043b\u043e\"],\n[\"\u2581des\", null],\n[\"\u2581pet\", \"\u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0438\u0445\"],\n[\"ites\", null],\n[\"\u2581fill\", \"\u0434\u0435\u0432\u043e\u0447\u0435\u043a\"],\n[\"es\", null],\n[\"\u2581dans\", \"\u043d\u0430\"],\n[\"\u2581mon\", \"\u0441\u0432\u043e\u0451\u043c\"],\n[\"\u2581temps\", \"\u0432\u0435\u043a\u0443\"],\n[\",\", null],\n[\"\u2581mais\", \"\u043d\u043e\"],\n[\"\u2581je\", \"\u044f\"],\n[\"\u2581n\", \"\u043d\u0438\u043a\u043e\u0433\u0434\u0430\"],\n[\"'\", null],\n[\"en\", null],\n[\"\u2581ai\", null],\n[\"\u2581jamais\", null],\n[\"\u2581vu\", \"\u043d\u0435 \u0432\u0438\u0434\u0435\u043b\"],\n[\"\u2581avec\", \"\u0441\"],\n[\"\u2581un\", null],\n[\"\u2581cou\", \"\u0448\u0435\u0435\u0439\"],\n[\"\u2581comme\", \"\u043a\u0430\u043a\"],\n[\"\u2581cela\", \"\u044d\u0442\u0430\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nLe salon \u00e9tait une petite pi\u00e8ce simplement meubl\u00e9e, mais propre et confortable.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Le', '\u2581sal', 'on', '\u2581\u00e9tait', '\u2581une', '\u2581petite', '\u2581pi', '\u00e8ce', '\u2581sim', 'plement', '\u2581me', 'ub', 'l', '\u00e9e', ',', '\u2581mais', '\u2581propre', '\u2581et', '\u2581conf', 'ort', 'able', '.']"}], "[\n  [\"\u2581Le\", \"The\"],\n  [\"\u2581sal\", \"living room\"],\n  [\"on\", null],\n  [\"\u2581\u00e9tait\", null],\n  [\"\u2581une\", null],\n  [\"\u2581petite\", \"small\"],\n  [\"\u2581pi\", \"room\"],\n  [\"\u00e8ce\", null],\n  [\"\u2581sim\", \"simply\"],\n  [\"plement\", null],\n  [\"\u2581me\", \"furnished\"],\n  [\"ub\", null],\n  [\"l\", null],\n  [\"\u00e9e\", null],\n  [\",\", null],\n  [\"\u2581mais\", null],\n  [\"\u2581propre\", \"clean\"],\n  [\"\u2581et\", null],\n  [\"\u2581conf\", \"comfortable\"],\n  [\"ort\", null],\n  [\"able\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nLe salon \u00e9tait une petite pi\u00e8ce simplement meubl\u00e9e, mais propre et confortable.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Le', '\u2581sal', 'on', '\u2581\u00e9tait', '\u2581une', '\u2581petite', '\u2581pi', '\u00e8ce', '\u2581sim', 'plement', '\u2581me', 'ub', 'l', '\u00e9e', ',', '\u2581mais', '\u2581propre', '\u2581et', '\u2581conf', 'ort', 'able', '.']"}], "[\n  [\"\u2581Le\", \"null\"],\n  [\"\u2581sal\", \"\u0413\u043e\u0441\u0442\u0438\u043d\u0430\u044f\"],\n  [\"on\", \"null\"],\n  [\"\u2581\u00e9tait\", \"\u0431\u044b\u043b\u0430\"],\n  [\"\u2581une\", \"null\"],\n  [\"\u2581petite\", \"\u043c\u0430\u043b\u0435\u043d\u044c\u043a\u043e\u0439\"],\n  [\"\u2581pi\", \"\u043a\u043e\u043c\u043d\u0430\u0442\u043e\u0439\"],\n  [\"\u00e8ce\", \"null\"],\n  [\"\u2581sim\", \"\u043f\u0440\u043e\u0441\u0442\u043e\"],\n  [\"plement\", \"null\"],\n  [\"\u2581me\", \"\u043e\u0431\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u043e\u0439\"],\n  [\"ub\", \"null\"],\n  [\"l\", \"null\"],\n  [\"\u00e9e\", \"null\"],\n  [\",\", \"null\"],\n  [\"\u2581mais\", \"\u043d\u043e\"],\n  [\"\u2581propre\", \"\u0447\u0438\u0441\u0442\u043e\u0439\"],\n  [\"\u2581et\", \"\u0438\"],\n  [\"\u2581conf\", \"\u0443\u044e\u0442\u043d\u043e\u0439\"],\n  [\"ort\", \"null\"],\n  [\"able\", \"null\"],\n  [\".\", \"null\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nSi vous saviez que je n\u2019ai qu\u2019elle, que je suis vieille, que c\u2019est une b\u00e9n\u00e9diction que la sainte Vierge m\u2019envoie.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Si', '\u2581vous', '\u2581sav', 'iez', '\u2581que', '\u2581je', '\u2581n', '\u2019', 'ai', '\u2581qu', '\u2019', 'elle', ',', '\u2581que', '\u2581je', '\u2581suis', '\u2581vie', 'ille', ',', '\u2581que', '\u2581c', '\u2019', 'est', '\u2581une', '\u2581b', '\u00e9n', '\u00e9d', 'iction', '\u2581que', '\u2581la', '\u2581sain', 'te', '\u2581Vier', 'ge', '\u2581m', '\u2019', 'en', 'vo', 'ie', '.']"}], "[\n  [\"\u2581Si\", \"If\"],\n  [\"\u2581vous\", null],\n  [\"\u2581sav\", \"knew\"],\n  [\"iez\", null],\n  [\"\u2581que\", null],\n  [\"\u2581je\", null],\n  [\"\u2581n\", null],\n  [\"'\", null],\n  [\"ai\", null],\n  [\"\u2581qu\", null],\n  [\"'\", null],\n  [\"elle\", \"her\"],\n  [\",\", null],\n  [\"\u2581que\", null],\n  [\"\u2581je\", null],\n  [\"\u2581suis\", \"am\"],\n  [\"\u2581vie\", \"old\"],\n  [\"ille\", null],\n  [\",\", null],\n  [\"\u2581que\", null],\n  [\"\u2581c\", null],\n  [\"'\", null],\n  [\"est\", null],\n  [\"\u2581une\", null],\n  [\"\u2581b\", \"blessing\"],\n  [\"\u00e9n\", null],\n  [\"\u00e9d\", null],\n  [\"iction\", null],\n  [\"\u2581que\", null],\n  [\"\u2581la\", null],\n  [\"\u2581sain\", \"Holy\"],\n  [\"te\", null],\n  [\"\u2581Vier\", \"Virgin\"],\n  [\"ge\", null],\n  [\"\u2581m\", null],\n  [\"'\", null],\n  [\"en\", \"sends\"],\n  [\"vo\", null],\n  [\"ie\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nSi vous saviez que je n\u2019ai qu\u2019elle, que je suis vieille, que c\u2019est une b\u00e9n\u00e9diction que la sainte Vierge m\u2019envoie.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Si', '\u2581vous', '\u2581sav', 'iez', '\u2581que', '\u2581je', '\u2581n', '\u2019', 'ai', '\u2581qu', '\u2019', 'elle', ',', '\u2581que', '\u2581je', '\u2581suis', '\u2581vie', 'ille', ',', '\u2581que', '\u2581c', '\u2019', 'est', '\u2581une', '\u2581b', '\u00e9n', '\u00e9d', 'iction', '\u2581que', '\u2581la', '\u2581sain', 'te', '\u2581Vier', 'ge', '\u2581m', '\u2019', 'en', 'vo', 'ie', '.']"}], "[\n  [\"\u2581Si\", \"\u0415\u0441\u043b\u0438\"],\n  [\"\u2581vous\", null],\n  [\"\u2581sav\", \"\u0437\u043d\u0430\u043b\u0438\"],\n  [\"iez\", null],\n  [\"\u2581que\", null],\n  [\"\u2581je\", null],\n  [\"\u2581n\", null],\n  [\"'\", null],\n  [\"ai\", null],\n  [\"\u2581qu\", null],\n  [\"'\", null],\n  [\"elle\", \"\u043e\u043d\u0430\"],\n  [\",\", null],\n  [\"\u2581que\", null],\n  [\"\u2581je\", null],\n  [\"\u2581suis\", \"\u044f\u0432\u043b\u044f\u044e\u0441\u044c\"],\n  [\"\u2581vie\", \"\u0441\u0442\u0430\u0440\u043e\u0439\"],\n  [\"ille\", null],\n  [\",\", null],\n  [\"\u2581que\", null],\n  [\"\u2581c\", null],\n  [\"'\", null],\n  [\"est\", null],\n  [\"\u2581une\", null],\n  [\"\u2581b\", \"\u0431\u043b\u0430\u0433\u043e\u0441\u043b\u043e\u0432\u0435\u043d\u0438\u0435\"],\n  [\"\u00e9n\", null],\n  [\"\u00e9d\", null],\n  [\"iction\", null],\n  [\"\u2581que\", null],\n  [\"\u2581la\", null],\n  [\"\u2581sain\", \"\u0441\u0432\u044f\u0442\u0430\u044f\"],\n  [\"te\", null],\n  [\"\u2581Vier\", \"\u0414\u0435\u0432\u0430\"],\n  [\"ge\", null],\n  [\"\u2581m\", null],\n  [\"'\", null],\n  [\"en\", null],\n  [\"vo\", \"\u043f\u043e\u0441\u044b\u043b\u0430\u0435\u0442\"],\n  [\"ie\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nAu del\u00e0 verdoyaient les pr\u00e9s, au del\u00e0 s\u2019enfuyaient les routes, le long desquelles tra\u00eenaient encore quelques maisons de faubourg, d\u2019autant plus rares qu\u2019elles s\u2019\u00e9loignaient plus. Quelques-uns de ces faubourgs avaient de l\u2019importance.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Au', '\u2581del', '\u00e0', '\u2581ver', 'do', 'ya', 'ient', '\u2581les', '\u2581pr\u00e9s', ',', '\u2581au', '\u2581del', '\u00e0', '\u2581s', '\u2019', 'en', 'fu', 'ya', 'ient', '\u2581les', '\u2581routes', ',', '\u2581le', '\u2581long', '\u2581des', 'qu', 'elles', '\u2581tra', '\u00ee', 'na', 'ient', '\u2581encore', '\u2581quelques', '\u2581mais', 'ons', '\u2581de', '\u2581fa', 'ub', 'ourg', ',', '\u2581d', '\u2019', 'aut', 'ant', '\u2581plus', '\u2581r', 'ares', '\u2581qu', '\u2019', 'elles', '\u2581s', '\u2019', '\u00e9', 'lo', 'ign', 'aient', '\u2581plus', '.', '\u2581Qu', 'el', 'ques', '-', 'uns', '\u2581de', '\u2581ces', '\u2581fa', 'ub', 'our', 'gs', '\u2581avaient', '\u2581de', '\u2581l', '\u2019', 'import', 'ance', '.']"}], "[\n  [\"\u2581Au\", \"Beyond\"],\n  [\"\u2581del\", null],\n  [\"\u00e0\", null],\n  [\"\u2581ver\", null],\n  [\"do\", null],\n  [\"ya\", null],\n  [\"ient\", null],\n  [\"\u2581les\", null],\n  [\"\u2581pr\u00e9s\", \"meadows\"],\n  [\",\", null],\n  [\"\u2581au\", null],\n  [\"\u2581del\", null],\n  [\"\u00e0\", null],\n  [\"\u2581s\", null],\n  [\"'\", null],\n  [\"en\", null],\n  [\"fu\", null],\n  [\"ya\", null],\n  [\"ient\", null],\n  [\"\u2581les\", null],\n  [\"\u2581routes\", \"roads\"],\n  [\",\", null],\n  [\"\u2581le\", null],\n  [\"\u2581long\", \"along\"],\n  [\"\u2581des\", null],\n  [\"qu\", null],\n  [\"elles\", null],\n  [\"\u2581tra\", null],\n  [\"\u00ee\", null],\n  [\"na\", null],\n  [\"ient\", null],\n  [\"\u2581encore\", \"still\"],\n  [\"\u2581quelques\", \"some\"],\n  [\"\u2581mais\", null],\n  [\"ons\", null],\n  [\"\u2581de\", null],\n  [\"\u2581fa\", null],\n  [\"ub\", null],\n  [\"ourg\", \"suburb\"],\n  [\",\", null],\n  [\"\u2581d\", null],\n  [\"'\", null],\n  [\"aut\", null],\n  [\"ant\", null],\n  [\"\u2581plus\", null],\n  [\"\u2581r\", null],\n  [\"ares\", \"rare\"],\n  [\"\u2581qu\", null],\n  [\"'\", null],\n  [\"elles\", null],\n  [\"\u2581s\", null],\n  [\"'\", null],\n  [\"\u00e9\", null],\n  [\"lo\", null],\n  [\"ign\", null],\n  [\"aient\", null],\n  [\"\u2581plus\", \"farther\"],\n  [\".\", null],\n  [\"\u2581Qu\", null],\n  [\"el\", null],\n  [\"ques\", null],\n  [\"-\", null],\n  [\"uns\", \"Some\"],\n  [\"\u2581de\", null],\n  [\"\u2581ces\", null],\n  [\"\u2581fa\", null],\n  [\"ub\", null],\n  [\"our\", null],\n  [\"gs\", null],\n  [\"\u2581avaient\", \"had\"],\n  [\"\u2581de\", null],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"import\", null],\n  [\"ance\", \"importance\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nAu del\u00e0 verdoyaient les pr\u00e9s, au del\u00e0 s\u2019enfuyaient les routes, le long desquelles tra\u00eenaient encore quelques maisons de faubourg, d\u2019autant plus rares qu\u2019elles s\u2019\u00e9loignaient plus. Quelques-uns de ces faubourgs avaient de l\u2019importance.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Au', '\u2581del', '\u00e0', '\u2581ver', 'do', 'ya', 'ient', '\u2581les', '\u2581pr\u00e9s', ',', '\u2581au', '\u2581del', '\u00e0', '\u2581s', '\u2019', 'en', 'fu', 'ya', 'ient', '\u2581les', '\u2581routes', ',', '\u2581le', '\u2581long', '\u2581des', 'qu', 'elles', '\u2581tra', '\u00ee', 'na', 'ient', '\u2581encore', '\u2581quelques', '\u2581mais', 'ons', '\u2581de', '\u2581fa', 'ub', 'ourg', ',', '\u2581d', '\u2019', 'aut', 'ant', '\u2581plus', '\u2581r', 'ares', '\u2581qu', '\u2019', 'elles', '\u2581s', '\u2019', '\u00e9', 'lo', 'ign', 'aient', '\u2581plus', '.', '\u2581Qu', 'el', 'ques', '-', 'uns', '\u2581de', '\u2581ces', '\u2581fa', 'ub', 'our', 'gs', '\u2581avaient', '\u2581de', '\u2581l', '\u2019', 'import', 'ance', '.']"}], "[\n[\"\u2581Au\", \"\u0417\u0430\"],\n[\"\u2581del\", null],\n[\"\u00e0\", null],\n[\"\u2581ver\", null],\n[\"do\", null],\n[\"ya\", null],\n[\"ient\", null],\n[\"\u2581les\", null],\n[\"\u2581pr\u00e9s\", \"\u043b\u0443\u0433\u0430\"],\n[\",\", null],\n[\"\u2581au\", null],\n[\"\u2581del\", null],\n[\"\u00e0\", null],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"en\", null],\n[\"fu\", null],\n[\"ya\", null],\n[\"ient\", null],\n[\"\u2581les\", null],\n[\"\u2581routes\", \"\u0434\u043e\u0440\u043e\u0433\u0438\"],\n[\",\", null],\n[\"\u2581le\", null],\n[\"\u2581long\", \"\u0432\u0434\u043e\u043b\u044c\"],\n[\"\u2581des\", null],\n[\"qu\", null],\n[\"elles\", null],\n[\"\u2581tra\", null],\n[\"\u00ee\", null],\n[\"na\", null],\n[\"ient\", null],\n[\"\u2581encore\", \"\u0435\u0449\u0435\"],\n[\"\u2581quelques\", \"\u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e\"],\n[\"\u2581mais\", null],\n[\"ons\", null],\n[\"\u2581de\", null],\n[\"\u2581fa\", null],\n[\"ub\", null],\n[\"ourg\", null],\n[\",\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"aut\", null],\n[\"ant\", null],\n[\"\u2581plus\", null],\n[\"\u2581r\", null],\n[\"ares\", null],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"elles\", null],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"\u00e9\", null],\n[\"lo\", null],\n[\"ign\", null],\n[\"aient\", null],\n[\"\u2581plus\", null],\n[\".\", null],\n[\"\u2581Qu\", null],\n[\"el\", null],\n[\"ques\", null],\n[\"-\", null],\n[\"uns\", null],\n[\"\u2581de\", null],\n[\"\u2581ces\", null],\n[\"\u2581fa\", null],\n[\"ub\", null],\n[\"our\", null],\n[\"gs\", null],\n[\"\u2581avaient\", \"\u0438\u043c\u0435\u043b\u0438\"],\n[\"\u2581de\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"import\", null],\n[\"ance\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n\u00c0 la Wimerra, il a communiqu\u00e9 avec l\u2019un des siens, le forgeron de Black-Point, et a laiss\u00e9 des traces reconnaissables de notre passage.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00c0', '\u2581la', '\u2581W', 'imer', 'ra', ',', '\u2581il', '\u2581a', '\u2581commun', 'iqu', '\u00e9', '\u2581avec', '\u2581l', '\u2019', 'un', '\u2581des', '\u2581si', 'ens', ',', '\u2581le', '\u2581for', 'ger', 'on', '\u2581de', '\u2581Black', '-', 'Point', ',', '\u2581et', '\u2581a', '\u2581la', 'iss', '\u00e9', '\u2581des', '\u2581traces', '\u2581reconna', 'iss', 'ables', '\u2581de', '\u2581notre', '\u2581passage', '.']"}], "[\n[\"\u2581\u00c0\", \"At\"],\n[\"\u2581la\", null],\n[\"\u2581W\", \"Wimerra\"],\n[\"imer\", null],\n[\"ra\", null],\n[\",\", null],\n[\"\u2581il\", \"he\"],\n[\"\u2581a\", null],\n[\"\u2581commun\", \"communicated\"],\n[\"iqu\", null],\n[\"\u00e9\", null],\n[\"\u2581avec\", \"with\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"un\", \"one\"],\n[\"\u2581des\", null],\n[\"\u2581si\", \"his\"],\n[\"ens\", null],\n[\",\", null],\n[\"\u2581le\", null],\n[\"\u2581for\", \"blacksmith\"],\n[\"ger\", null],\n[\"on\", null],\n[\"\u2581de\", \"of\"],\n[\"\u2581Black\", \"Black\"],\n[\"-\", null],\n[\"Point\", \"Point\"],\n[\",\", null],\n[\"\u2581et\", \"and\"],\n[\"\u2581a\", null],\n[\"\u2581la\", \"left\"],\n[\"iss\", null],\n[\"\u00e9\", null],\n[\"\u2581des\", null],\n[\"\u2581traces\", \"traces\"],\n[\"\u2581reconna\", \"recognizable\"],\n[\"iss\", null],\n[\"ables\", null],\n[\"\u2581de\", \"of\"],\n[\"\u2581notre\", \"our\"],\n[\"\u2581passage\", \"passage\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n\u00c0 la Wimerra, il a communiqu\u00e9 avec l\u2019un des siens, le forgeron de Black-Point, et a laiss\u00e9 des traces reconnaissables de notre passage.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00c0', '\u2581la', '\u2581W', 'imer', 'ra', ',', '\u2581il', '\u2581a', '\u2581commun', 'iqu', '\u00e9', '\u2581avec', '\u2581l', '\u2019', 'un', '\u2581des', '\u2581si', 'ens', ',', '\u2581le', '\u2581for', 'ger', 'on', '\u2581de', '\u2581Black', '-', 'Point', ',', '\u2581et', '\u2581a', '\u2581la', 'iss', '\u00e9', '\u2581des', '\u2581traces', '\u2581reconna', 'iss', 'ables', '\u2581de', '\u2581notre', '\u2581passage', '.']"}], "[\n[\"\u2581\u00c0\", null],\n[\"\u2581la\", null],\n[\"\u2581W\", \"\u0412\"],\n[\"imer\", null],\n[\"ra\", null],\n[\",\", null],\n[\"\u2581il\", \"\u043e\u043d\"],\n[\"\u2581a\", null],\n[\"\u2581commun\", \"\u043e\u0431\u0449\u0430\u043b\u0441\u044f\"],\n[\"iqu\", null],\n[\"\u00e9\", null],\n[\"\u2581avec\", \"\u0441\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"un\", \"\u043e\u0434\u043d\u0438\u043c\"],\n[\"\u2581des\", \"\u0438\u0437\"],\n[\"\u2581si\", \"\u0441\u0432\u043e\u0438\u0445\"],\n[\"ens\", null],\n[\",\", null],\n[\"\u2581le\", null],\n[\"\u2581for\", \"\u043a\u0443\u0437\u043d\u0435\u0446\u043e\u043c\"],\n[\"ger\", null],\n[\"on\", null],\n[\"\u2581de\", \"\u0438\u0437\"],\n[\"\u2581Black\", \"\u0411\u043b\u044d\u043a\"],\n[\"-\", null],\n[\"Point\", \"\u041f\u043e\u0439\u043d\u0442\u0430\"],\n[\",\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581a\", null],\n[\"\u2581la\", \"\u043e\u0441\u0442\u0430\u0432\u0438\u043b\"],\n[\"iss\", null],\n[\"\u00e9\", null],\n[\"\u2581des\", null],\n[\"\u2581traces\", \"\u0441\u043b\u0435\u0434\u044b\"],\n[\"\u2581reconna\", \"\u0443\u0437\u043d\u0430\u0432\u0430\u0435\u043c\u044b\u0435\"],\n[\"iss\", null],\n[\"ables\", null],\n[\"\u2581de\", null],\n[\"\u2581notre\", \"\u043d\u0430\u0448\u0435\u0433\u043e\"],\n[\"\u2581passage\", \"\u043f\u0440\u043e\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nMonsieur l\u2019archev\u00eaque s\u2019est int\u00e9ress\u00e9 \u00e0 l\u2019enfant d\u2019\u00c9gypte, l\u2019a exorcis\u00e9, l\u2019a b\u00e9ni, lui a \u00f4t\u00e9 bien soigneusement le diable du corps, et l\u2019a envoy\u00e9 \u00e0 Paris pour \u00eatre expos\u00e9 sur le lit de bois, \u00e0 Notre-Dame, comme enfant trouv\u00e9.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Monsieur', '\u2581l', '\u2019', 'ar', 'chev', '\u00eaque', '\u2581s', '\u2019', 'est', '\u2581int\u00e9r', 'ess', '\u00e9', '\u2581\u00e0', '\u2581l', '\u2019', 'enf', 'ant', '\u2581d', '\u2019', '\u00c9', 'g', 'yp', 'te', ',', '\u2581l', '\u2019', 'a', '\u2581ex', 'or', 'cis', '\u00e9', ',', '\u2581l', '\u2019', 'a', '\u2581b', '\u00e9', 'ni', ',', '\u2581lui', '\u2581a', '\u2581', '\u00f4', 't\u00e9', '\u2581bien', '\u2581so', 'igne', 'usement', '\u2581le', '\u2581di', 'able', '\u2581du', '\u2581corps', ',', '\u2581et', '\u2581l', '\u2019', 'a', '\u2581en', 'voy', '\u00e9', '\u2581\u00e0', '\u2581Paris', '\u2581pour', '\u2581\u00eatre', '\u2581expos', '\u00e9', '\u2581sur', '\u2581le', '\u2581lit', '\u2581de', '\u2581bois', ',', '\u2581\u00e0', '\u2581Notre', '-', 'D', 'ame', ',', '\u2581comme', '\u2581enf', 'ant', '\u2581trouv', '\u00e9', '.']"}], "[\n[\"\u2581Monsieur\", \"Mister\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"ar\", null],\n[\"chev\", null],\n[\"\u00eaque\", \"archbishop\"],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"est\", null],\n[\"\u2581int\u00e9r\", null],\n[\"ess\", null],\n[\"\u00e9\", \"interested\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"enf\", null],\n[\"ant\", \"child\"],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"\u00c9\", null],\n[\"g\", null],\n[\"yp\", null],\n[\"te\", \"Egypt\"],\n[\",\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"a\", null],\n[\"\u2581ex\", null],\n[\"or\", null],\n[\"cis\", null],\n[\"\u00e9\", \"exorcised\"],\n[\",\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"a\", null],\n[\"\u2581b\", null],\n[\"\u00e9\", null],\n[\"ni\", \"blessed\"],\n[\",\", null],\n[\"\u2581lui\", \"him\"],\n[\"\u2581a\", null],\n[\"\u2581\", null],\n[\"\u00f4\", null],\n[\"t\u00e9\", \"removed\"],\n[\"\u2581bien\", \"very\"],\n[\"\u2581so\", null],\n[\"igne\", null],\n[\"usement\", \"carefully\"],\n[\"\u2581le\", null],\n[\"\u2581di\", null],\n[\"able\", \"devil\"],\n[\"\u2581du\", null],\n[\"\u2581corps\", \"body\"],\n[\",\", null],\n[\"\u2581et\", \"and\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"a\", null],\n[\"\u2581en\", null],\n[\"voy\", null],\n[\"\u00e9\", \"sent\"],\n[\"\u2581\u00e0\", \"to\"],\n[\"\u2581Paris\", \"Paris\"],\n[\"\u2581pour\", \"to\"],\n[\"\u2581\u00eatre\", \"be\"],\n[\"\u2581expos\", null],\n[\"\u00e9\", \"displayed\"],\n[\"\u2581sur\", \"on\"],\n[\"\u2581le\", null],\n[\"\u2581lit\", \"bed\"],\n[\"\u2581de\", \"of\"],\n[\"\u2581bois\", \"wood\"],\n[\",\", null],\n[\"\u2581\u00e0\", \"at\"],\n[\"\u2581Notre\", \"Notre\"],\n[\"-\", null],\n[\"D\", null],\n[\"ame\", \"Dame\"],\n[\",\", null],\n[\"\u2581comme\", \"as\"],\n[\"\u2581enf\", null],\n[\"ant\", \"child\"],\n[\"\u2581trouv\", null],\n[\"\u00e9\", \"found\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nMonsieur l\u2019archev\u00eaque s\u2019est int\u00e9ress\u00e9 \u00e0 l\u2019enfant d\u2019\u00c9gypte, l\u2019a exorcis\u00e9, l\u2019a b\u00e9ni, lui a \u00f4t\u00e9 bien soigneusement le diable du corps, et l\u2019a envoy\u00e9 \u00e0 Paris pour \u00eatre expos\u00e9 sur le lit de bois, \u00e0 Notre-Dame, comme enfant trouv\u00e9.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Monsieur', '\u2581l', '\u2019', 'ar', 'chev', '\u00eaque', '\u2581s', '\u2019', 'est', '\u2581int\u00e9r', 'ess', '\u00e9', '\u2581\u00e0', '\u2581l', '\u2019', 'enf', 'ant', '\u2581d', '\u2019', '\u00c9', 'g', 'yp', 'te', ',', '\u2581l', '\u2019', 'a', '\u2581ex', 'or', 'cis', '\u00e9', ',', '\u2581l', '\u2019', 'a', '\u2581b', '\u00e9', 'ni', ',', '\u2581lui', '\u2581a', '\u2581', '\u00f4', 't\u00e9', '\u2581bien', '\u2581so', 'igne', 'usement', '\u2581le', '\u2581di', 'able', '\u2581du', '\u2581corps', ',', '\u2581et', '\u2581l', '\u2019', 'a', '\u2581en', 'voy', '\u00e9', '\u2581\u00e0', '\u2581Paris', '\u2581pour', '\u2581\u00eatre', '\u2581expos', '\u00e9', '\u2581sur', '\u2581le', '\u2581lit', '\u2581de', '\u2581bois', ',', '\u2581\u00e0', '\u2581Notre', '-', 'D', 'ame', ',', '\u2581comme', '\u2581enf', 'ant', '\u2581trouv', '\u00e9', '.']"}], "[\n[\"\u2581Monsieur\", \"\u0413\u043e\u0441\u043f\u043e\u0434\u0438\u043d\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"ar\", null],\n[\"chev\", null],\n[\"\u00eaque\", null],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"est\", null],\n[\"\u2581int\u00e9r\", null],\n[\"ess\", null],\n[\"\u00e9\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"enf\", null],\n[\"ant\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"\u00c9\", null],\n[\"g\", null],\n[\"yp\", null],\n[\"te\", \"\u0415\u0433\u0438\u043f\u0442\u0430\"],\n[\",\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"a\", null],\n[\"\u2581ex\", null],\n[\"or\", null],\n[\"cis\", null],\n[\"\u00e9\", \"\u0438\u0437\u0433\u043d\u0430\u043b\"],\n[\",\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"a\", null],\n[\"\u2581b\", null],\n[\"\u00e9\", null],\n[\"ni\", \"\u0431\u043b\u0430\u0433\u043e\u0441\u043b\u043e\u0432\u0438\u043b\"],\n[\",\", null],\n[\"\u2581lui\", \"\u0435\u043c\u0443\"],\n[\"\u2581a\", null],\n[\"\u2581\", null],\n[\"\u00f4\", null],\n[\"t\u00e9\", \"\u0443\u0434\u0430\u043b\u0438\u043b\"],\n[\"\u2581bien\", \"\u0442\u0449\u0430\u0442\u0435\u043b\u044c\u043d\u043e\"],\n[\"\u2581so\", null],\n[\"igne\", null],\n[\"usement\", null],\n[\"\u2581le\", null],\n[\"\u2581di\", null],\n[\"able\", \"\u0434\u044c\u044f\u0432\u043e\u043b\u0430\"],\n[\"\u2581du\", \"\u0438\u0437\"],\n[\"\u2581corps\", \"\u0442\u0435\u043b\u0430\"],\n[\",\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"a\", null],\n[\"\u2581en\", null],\n[\"voy\", null],\n[\"\u00e9\", \"\u043e\u0442\u043f\u0440\u0430\u0432\u0438\u043b\"],\n[\"\u2581\u00e0\", \"\u0432\"],\n[\"\u2581Paris\", \"\u041f\u0430\u0440\u0438\u0436\"],\n[\"\u2581pour\", \"\u0447\u0442\u043e\u0431\u044b\"],\n[\"\u2581\u00eatre\", null],\n[\"\u2581expos\", null],\n[\"\u00e9\", \"\u0432\u044b\u0441\u0442\u0430\u0432\u0438\u0442\u044c\"],\n[\"\u2581sur\", \"\u043d\u0430\"],\n[\"\u2581le\", null],\n[\"\u2581lit\", \"\u043a\u0440\u043e\u0432\u0430\u0442\u0438\"],\n[\"\u2581de\", \"\u0438\u0437\"],\n[\"\u2581bois\", \"\u0434\u0435\u0440\u0435\u0432\u0430\"],\n[\",\", null],\n[\"\u2581\u00e0\", \"\u0432\"],\n[\"\u2581Notre\", null],\n[\"-\", null],\n[\"D\", null],\n[\"ame\", \"\u041d\u043e\u0442\u0440-\u0414\u0430\u043c\"],\n[\",\", null],\n[\"\u2581comme\", \"\u043a\u0430\u043a\"],\n[\"\u2581enf\", null],\n[\"ant\", \"\u043f\u043e\u0434\u043a\u0438\u0434\u044b\u0448\u0430\"],\n[\"\u2581trouv\", null],\n[\"\u00e9\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nDes cygnes \u00e0 t\u00eate noire s\u2019y \u00e9battaient capricieusement et disputaient l\u2019empire des eaux \u00e0 de nombreuses autruches qui gambadaient \u00e0 travers les ilanos.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Des', '\u2581c', 'yg', 'nes', '\u2581\u00e0', '\u2581t\u00eate', '\u2581no', 'ire', '\u2581s', '\u2019', 'y', '\u2581\u00e9', 'bat', 'taient', '\u2581cap', 'ric', 'ie', 'usement', '\u2581et', '\u2581disput', 'aient', '\u2581l', '\u2019', 'emp', 'ire', '\u2581des', '\u2581e', 'aux', '\u2581\u00e0', '\u2581de', '\u2581nombreuses', '\u2581aut', 'ru', 'ches', '\u2581qui', '\u2581g', 'amb', 'ada', 'ient', '\u2581\u00e0', '\u2581travers', '\u2581les', '\u2581il', 'anos', '.']"}], "[\n  [\"\u2581Des\", null],\n  [\"\u2581c\", \"swans\"],\n  [\"yg\", null],\n  [\"nes\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581t\u00eate\", \"head\"],\n  [\"\u2581no\", \"black\"],\n  [\"ire\", null],\n  [\"\u2581s\", null],\n  [\"'\", null],\n  [\"y\", null],\n  [\"\u2581\u00e9\", \"frolicked\"],\n  [\"bat\", null],\n  [\"taient\", null],\n  [\"\u2581cap\", \"capriciously\"],\n  [\"ric\", null],\n  [\"ie\", null],\n  [\"usement\", null],\n  [\"\u2581et\", null],\n  [\"\u2581disput\", \"disputed\"],\n  [\"aient\", null],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"emp\", \"empire\"],\n  [\"ire\", null],\n  [\"\u2581des\", null],\n  [\"\u2581e\", \"waters\"],\n  [\"aux\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581de\", null],\n  [\"\u2581nombreuses\", \"numerous\"],\n  [\"\u2581aut\", \"ostriches\"],\n  [\"ru\", null],\n  [\"ches\", null],\n  [\"\u2581qui\", null],\n  [\"\u2581g\", \"frolicked\"],\n  [\"amb\", null],\n  [\"ada\", null],\n  [\"ient\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581travers\", \"through\"],\n  [\"\u2581les\", null],\n  [\"\u2581il\", \"llanos\"],\n  [\"anos\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nDes cygnes \u00e0 t\u00eate noire s\u2019y \u00e9battaient capricieusement et disputaient l\u2019empire des eaux \u00e0 de nombreuses autruches qui gambadaient \u00e0 travers les ilanos.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Des', '\u2581c', 'yg', 'nes', '\u2581\u00e0', '\u2581t\u00eate', '\u2581no', 'ire', '\u2581s', '\u2019', 'y', '\u2581\u00e9', 'bat', 'taient', '\u2581cap', 'ric', 'ie', 'usement', '\u2581et', '\u2581disput', 'aient', '\u2581l', '\u2019', 'emp', 'ire', '\u2581des', '\u2581e', 'aux', '\u2581\u00e0', '\u2581de', '\u2581nombreuses', '\u2581aut', 'ru', 'ches', '\u2581qui', '\u2581g', 'amb', 'ada', 'ient', '\u2581\u00e0', '\u2581travers', '\u2581les', '\u2581il', 'anos', '.']"}], "[\n  [\"\u2581Des\", null],\n  [\"\u2581c\", \"\u0427\u0435\u0440\u043d\u043e\u0448\u0435\u0438\u0435\"],\n  [\"yg\", null],\n  [\"nes\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581t\u00eate\", null],\n  [\"\u2581no\", null],\n  [\"ire\", null],\n  [\"\u2581s\", null],\n  [\"'\", null],\n  [\"y\", null],\n  [\"\u2581\u00e9\", \"\u0440\u0435\u0437\u0432\u0438\u043b\u0438\u0441\u044c\"],\n  [\"bat\", null],\n  [\"taient\", null],\n  [\"\u2581cap\", null],\n  [\"ric\", null],\n  [\"ie\", null],\n  [\"usement\", null],\n  [\"\u2581et\", null],\n  [\"\u2581disput\", \"\u043e\u0441\u043f\u0430\u0440\u0438\u0432\u0430\u043b\u0438\"],\n  [\"aient\", null],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"emp\", \"\u0433\u043e\u0441\u043f\u043e\u0434\u0441\u0442\u0432\u043e\"],\n  [\"ire\", null],\n  [\"\u2581des\", null],\n  [\"\u2581e\", \"\u0432\u043e\u0434\"],\n  [\"aux\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581de\", null],\n  [\"\u2581nombreuses\", \"\u043c\u043d\u043e\u0433\u043e\u0447\u0438\u0441\u043b\u0435\u043d\u043d\u044b\u0445\"],\n  [\"\u2581aut\", \"\u0441\u0442\u0440\u0430\u0443\u0441\u043e\u0432\"],\n  [\"ru\", null],\n  [\"ches\", null],\n  [\"\u2581qui\", null],\n  [\"\u2581g\", \"\u0440\u0435\u0437\u0432\u0438\u043b\u0438\u0441\u044c\"],\n  [\"amb\", null],\n  [\"ada\", null],\n  [\"ient\", null],\n  [\"\u2581\u00e0\", null],\n  [\"\u2581travers\", \"\u043f\u043e\"],\n  [\"\u2581les\", null],\n  [\"\u2581il\", \"\u043b\u044c\u044f\u043d\u043e\u0441\u0430\u043c\"],\n  [\"anos\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nAlors, emport\u00e9e dans ses souvenirs comme dans un torrent qui bouillonne, elle arriva bient\u00f4t \u00e0 se rappeler la journ\u00e9e de la veille.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Al', 'ors', ',', '\u2581em', 'port', '\u00e9e', '\u2581dans', '\u2581ses', '\u2581sou', 'ven', 'irs', '\u2581comme', '\u2581dans', '\u2581un', '\u2581tor', 'rent', '\u2581qui', '\u2581bou', 'ill', 'onne', ',', '\u2581elle', '\u2581arriv', 'a', '\u2581b', 'ient', '\u00f4t', '\u2581\u00e0', '\u2581se', '\u2581rapp', 'eler', '\u2581la', '\u2581journ', '\u00e9e', '\u2581de', '\u2581la', '\u2581ve', 'ille', '.']"}], "[\n[\"\u2581Al\", \"Then\"],\n[\"ors\", null],\n[\",\", null],\n[\"\u2581em\", \"carried\"],\n[\"port\", null],\n[\"\u00e9e\", null],\n[\"\u2581dans\", \"in\"],\n[\"\u2581ses\", \"her\"],\n[\"\u2581sou\", \"memories\"],\n[\"ven\", null],\n[\"irs\", null],\n[\"\u2581comme\", \"as\"],\n[\"\u2581dans\", \"in\"],\n[\"\u2581un\", \"a\"],\n[\"\u2581tor\", \"raging\"],\n[\"rent\", null],\n[\"\u2581qui\", \"that\"],\n[\"\u2581bou\", \"bubbles\"],\n[\"ill\", null],\n[\"onne\", null],\n[\",\", null],\n[\"\u2581elle\", \"she\"],\n[\"\u2581arriv\", \"came\"],\n[\"a\", null],\n[\"\u2581b\", \"soon\"],\n[\"ient\", null],\n[\"\u00f4t\", null],\n[\"\u2581\u00e0\", \"to\"],\n[\"\u2581se\", null],\n[\"\u2581rapp\", \"remember\"],\n[\"eler\", null],\n[\"\u2581la\", \"the\"],\n[\"\u2581journ\", \"day\"],\n[\"\u00e9e\", null],\n[\"\u2581de\", \"of\"],\n[\"\u2581la\", \"the\"],\n[\"\u2581ve\", \"previous\"],\n[\"ille\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nAlors, emport\u00e9e dans ses souvenirs comme dans un torrent qui bouillonne, elle arriva bient\u00f4t \u00e0 se rappeler la journ\u00e9e de la veille.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Al', 'ors', ',', '\u2581em', 'port', '\u00e9e', '\u2581dans', '\u2581ses', '\u2581sou', 'ven', 'irs', '\u2581comme', '\u2581dans', '\u2581un', '\u2581tor', 'rent', '\u2581qui', '\u2581bou', 'ill', 'onne', ',', '\u2581elle', '\u2581arriv', 'a', '\u2581b', 'ient', '\u00f4t', '\u2581\u00e0', '\u2581se', '\u2581rapp', 'eler', '\u2581la', '\u2581journ', '\u00e9e', '\u2581de', '\u2581la', '\u2581ve', 'ille', '.']"}], "[\n[\"\u2581Al\", \"\u0418\u0442\u0430\u043a\"],\n[\"ors\", null],\n[\",\", null],\n[\"\u2581em\", \"\u0443\u043d\u0435\u0441\u0435\u043d\u043d\u0430\u044f\"],\n[\"port\", null],\n[\"\u00e9e\", null],\n[\"\u2581dans\", null],\n[\"\u2581ses\", null],\n[\"\u2581sou\", \"\u0432\u043e\u0441\u043f\u043e\u043c\u0438\u043d\u0430\u043d\u0438\u044f\u043c\u0438\"],\n[\"ven\", null],\n[\"irs\", null],\n[\"\u2581comme\", null],\n[\"\u2581dans\", null],\n[\"\u2581un\", null],\n[\"\u2581tor\", \"\u043f\u043e\u0442\u043e\u043a\"],\n[\"rent\", null],\n[\"\u2581qui\", null],\n[\"\u2581bou\", \"\u0431\u0443\u0440\u043b\u044f\u0449\u0438\u0439\"],\n[\"ill\", null],\n[\"onne\", null],\n[\",\", null],\n[\"\u2581elle\", \"\u043e\u043d\u0430\"],\n[\"\u2581arriv\", \"\u0432\u0441\u043a\u043e\u0440\u0435\"],\n[\"a\", null],\n[\"\u2581b\", null],\n[\"ient\", null],\n[\"\u00f4t\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581se\", null],\n[\"\u2581rapp\", \"\u0432\u0441\u043f\u043e\u043c\u043d\u0438\u043b\u0430\"],\n[\"eler\", null],\n[\"\u2581la\", null],\n[\"\u2581journ\", \"\u0434\u0435\u043d\u044c\"],\n[\"\u00e9e\", null],\n[\"\u2581de\", null],\n[\"\u2581la\", null],\n[\"\u2581ve\", \"\u043d\u0430\u043a\u0430\u043d\u0443\u043d\u0435\"],\n[\"ille\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nL\u00e0-bas, c\u2019est l\u2019abbaye Saint-Martin, chanteuse aigre et f\u00eal\u00e9e ; ici, la voix sinistre et bourrue de la Bastille ; \u00e0 l\u2019autre bout, la grosse Tour du Louvre, avec sa basse-taille.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581L', '\u00e0', '-', 'bas', ',', '\u2581c', '\u2019', 'est', '\u2581l', '\u2019', 'abb', 'aye', '\u2581Saint', '-', 'Martin', ',', '\u2581chante', 'use', '\u2581a', 'ig', 're', '\u2581et', '\u2581f', '\u00ea', 'l', '\u00e9e', '\u2581;', '\u2581', 'ici', ',', '\u2581la', '\u2581voix', '\u2581sin', 'istre', '\u2581et', '\u2581b', 'our', 'r', 'ue', '\u2581de', '\u2581la', '\u2581Bast', 'ille', '\u2581;', '\u2581\u00e0', '\u2581l', '\u2019', 'autre', '\u2581bout', ',', '\u2581la', '\u2581gros', 'se', '\u2581Tour', '\u2581du', '\u2581Lou', 'vre', ',', '\u2581avec', '\u2581sa', '\u2581bas', 'se', '-', 't', 'aille', '.']"}], "[\n[\"\u2581L\", \"Over\"],\n[\"\u00e0\", null],\n[\"-\", null],\n[\"bas\", null],\n[\",\", null],\n[\"\u2581c\", null],\n[\"'\", null],\n[\"est\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"abb\", \"abbey\"],\n[\"aye\", null],\n[\"\u2581Saint\", \"Saint\"],\n[\"-\", null],\n[\"Martin\", \"Martin\"],\n[\",\", null],\n[\"\u2581chante\", \"singer\"],\n[\"use\", null],\n[\"\u2581a\", null],\n[\"ig\", \"sour\"],\n[\"re\", null],\n[\"\u2581et\", \"and\"],\n[\"\u2581f\", \"cracked\"],\n[\"\u00ea\", null],\n[\"l\", null],\n[\"\u00e9e\", null],\n[\"\u2581;\", null],\n[\"\u2581\", null],\n[\"ici\", \"here\"],\n[\",\", null],\n[\"\u2581la\", \"the\"],\n[\"\u2581voix\", \"voice\"],\n[\"\u2581sin\", \"sinister\"],\n[\"istre\", null],\n[\"\u2581et\", \"and\"],\n[\"\u2581b\", \"gruff\"],\n[\"our\", null],\n[\"r\", null],\n[\"ue\", null],\n[\"\u2581de\", \"of\"],\n[\"\u2581la\", \"the\"],\n[\"\u2581Bast\", \"Bastille\"],\n[\"ille\", null],\n[\"\u2581;\", null],\n[\"\u2581\u00e0\", \"at\"],\n[\"\u2581l\", \"the\"],\n[\"'\", null],\n[\"autre\", \"other\"],\n[\"\u2581bout\", \"end\"],\n[\",\", null],\n[\"\u2581la\", \"the\"],\n[\"\u2581gros\", \"big\"],\n[\"se\", null],\n[\"\u2581Tour\", \"Tower\"],\n[\"\u2581du\", \"of\"],\n[\"\u2581Lou\", \"Louvre\"],\n[\"vre\", null],\n[\",\", null],\n[\"\u2581avec\", \"with\"],\n[\"\u2581sa\", \"its\"],\n[\"\u2581bas\", \"bass\"],\n[\"se\", null],\n[\"-\", null],\n[\"t\", \"tone\"],\n[\"aille\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nL\u00e0-bas, c\u2019est l\u2019abbaye Saint-Martin, chanteuse aigre et f\u00eal\u00e9e ; ici, la voix sinistre et bourrue de la Bastille ; \u00e0 l\u2019autre bout, la grosse Tour du Louvre, avec sa basse-taille.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581L', '\u00e0', '-', 'bas', ',', '\u2581c', '\u2019', 'est', '\u2581l', '\u2019', 'abb', 'aye', '\u2581Saint', '-', 'Martin', ',', '\u2581chante', 'use', '\u2581a', 'ig', 're', '\u2581et', '\u2581f', '\u00ea', 'l', '\u00e9e', '\u2581;', '\u2581', 'ici', ',', '\u2581la', '\u2581voix', '\u2581sin', 'istre', '\u2581et', '\u2581b', 'our', 'r', 'ue', '\u2581de', '\u2581la', '\u2581Bast', 'ille', '\u2581;', '\u2581\u00e0', '\u2581l', '\u2019', 'autre', '\u2581bout', ',', '\u2581la', '\u2581gros', 'se', '\u2581Tour', '\u2581du', '\u2581Lou', 'vre', ',', '\u2581avec', '\u2581sa', '\u2581bas', 'se', '-', 't', 'aille', '.']"}], "[\n[\"\u2581L\", \"\u0422\u0430\u043c\"],\n[\"\u00e0\", null],\n[\"-\", null],\n[\"bas\", null],\n[\",\", null],\n[\"\u2581c\", null],\n[\"'\", null],\n[\"est\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"abb\", \"\u0430\u0431\u0431\u0430\u0442\u0441\u0442\u0432\u043e\"],\n[\"aye\", null],\n[\"\u2581Saint\", \"\u0421\u0435\u043d\"],\n[\"-\", null],\n[\"Martin\", \"\u041c\u0430\u0440\u0442\u0435\u043d\"],\n[\",\", null],\n[\"\u2581chante\", \"\u043f\u0435\u0432\u0438\u0446\u0430\"],\n[\"use\", null],\n[\"\u2581a\", null],\n[\"ig\", \"\u0440\u0435\u0437\u043a\u0430\u044f\"],\n[\"re\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581f\", \"\u043d\u0430\u0434\u0442\u0440\u0435\u0441\u043d\u0443\u0442\u0430\u044f\"],\n[\"\u00ea\", null],\n[\"l\", null],\n[\"\u00e9e\", null],\n[\"\u2581;\", null],\n[\"\u2581\", null],\n[\"ici\", \"\u0437\u0434\u0435\u0441\u044c\"],\n[\",\", null],\n[\"\u2581la\", null],\n[\"\u2581voix\", \"\u0433\u043e\u043b\u043e\u0441\"],\n[\"\u2581sin\", \"\u0437\u043b\u043e\u0432\u0435\u0449\u0438\u0439\"],\n[\"istre\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581b\", \"\u0433\u0440\u0443\u0431\u044b\u0439\"],\n[\"our\", null],\n[\"r\", null],\n[\"ue\", null],\n[\"\u2581de\", null],\n[\"\u2581la\", null],\n[\"\u2581Bast\", \"\u0411\u0430\u0441\u0442\u0438\u043b\u0438\u0438\"],\n[\"ille\", null],\n[\"\u2581;\", null],\n[\"\u2581\u00e0\", \"\u043d\u0430\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"autre\", \"\u0434\u0440\u0443\u0433\u043e\u043c\"],\n[\"\u2581bout\", \"\u043a\u043e\u043d\u0446\u0435\"],\n[\",\", null],\n[\"\u2581la\", null],\n[\"\u2581gros\", \"\u0431\u043e\u043b\u044c\u0448\u0430\u044f\"],\n[\"se\", null],\n[\"\u2581Tour\", \"\u0411\u0430\u0448\u043d\u044f\"],\n[\"\u2581du\", null],\n[\"\u2581Lou\", \"\u041b\u0443\u0432\u0440\u0430\"],\n[\"vre\", null],\n[\",\", null],\n[\"\u2581avec\", \"\u0441\u043e\"],\n[\"\u2581sa\", \"\u0441\u0432\u043e\u0438\u043c\"],\n[\"\u2581bas\", \"\u043d\u0438\u0437\u043a\u0438\u043c\"],\n[\"se\", null],\n[\"-\", null],\n[\"t\", \"\u0431\u0430\u0441\u043e\u043c\"],\n[\"aille\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nJe n'avais pour tout ornement qu'une petite perle, dernier souvenir de Mlle Temple. Je la mis et nous descend\u00eemes.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Je', '\u2581n', \"'\", 'ava', 'is', '\u2581pour', '\u2581tout', '\u2581orn', 'ement', '\u2581qu', \"'\", 'une', '\u2581petite', '\u2581per', 'le', ',', '\u2581dernier', '\u2581sou', 'venir', '\u2581de', '\u2581M', 'lle', '\u2581Temple', '.', '\u2581Je', '\u2581la', '\u2581mis', '\u2581et', '\u2581nous', '\u2581descend', '\u00ee', 'mes', '.']"}], "[\n  [\"\u2581Je\", \"I\"],\n  [\"\u2581n\", null],\n  [\"'\", null],\n  [\"ava\", null],\n  [\"is\", null],\n  [\"\u2581pour\", \"for\"],\n  [\"\u2581tout\", \"all\"],\n  [\"\u2581orn\", null],\n  [\"ement\", \"adornment\"],\n  [\"\u2581qu\", null],\n  [\"'\", null],\n  [\"une\", \"a\"],\n  [\"\u2581petite\", \"small\"],\n  [\"\u2581per\", null],\n  [\"le\", \"pearl\"],\n  [\",\", null],\n  [\"\u2581dernier\", \"last\"],\n  [\"\u2581sou\", null],\n  [\"venir\", \"memory\"],\n  [\"\u2581de\", \"of\"],\n  [\"\u2581M\", null],\n  [\"lle\", \"Miss\"],\n  [\"\u2581Temple\", \"Temple\"],\n  [\".\", null],\n  [\"\u2581Je\", \"I\"],\n  [\"\u2581la\", \"it\"],\n  [\"\u2581mis\", \"put on\"],\n  [\"\u2581et\", \"and\"],\n  [\"\u2581nous\", \"we\"],\n  [\"\u2581descend\", null],\n  [\"\u00ee\", null],\n  [\"mes\", \"went down\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nJe n'avais pour tout ornement qu'une petite perle, dernier souvenir de Mlle Temple. Je la mis et nous descend\u00eemes.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Je', '\u2581n', \"'\", 'ava', 'is', '\u2581pour', '\u2581tout', '\u2581orn', 'ement', '\u2581qu', \"'\", 'une', '\u2581petite', '\u2581per', 'le', ',', '\u2581dernier', '\u2581sou', 'venir', '\u2581de', '\u2581M', 'lle', '\u2581Temple', '.', '\u2581Je', '\u2581la', '\u2581mis', '\u2581et', '\u2581nous', '\u2581descend', '\u00ee', 'mes', '.']"}], "[\n[\"\u2581Je\", \"\u042f\"],\n[\"\u2581n\", null],\n[\"'\", null],\n[\"ava\", null],\n[\"is\", null],\n[\"\u2581pour\", \"\u0434\u043b\u044f\"],\n[\"\u2581tout\", \"\u0432\u0441\u0435\u0433\u043e\"],\n[\"\u2581orn\", null],\n[\"ement\", \"\u0443\u043a\u0440\u0430\u0448\u0435\u043d\u0438\u044f\"],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"une\", \"\u043e\u0434\u043d\u0430\"],\n[\"\u2581petite\", \"\u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0430\u044f\"],\n[\"\u2581per\", null],\n[\"le\", \"\u0436\u0435\u043c\u0447\u0443\u0436\u0438\u043d\u0430\"],\n[\",\", null],\n[\"\u2581dernier\", \"\u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435\"],\n[\"\u2581sou\", null],\n[\"venir\", \"\u0432\u043e\u0441\u043f\u043e\u043c\u0438\u043d\u0430\u043d\u0438\u0435\"],\n[\"\u2581de\", \"\u043e\"],\n[\"\u2581M\", null],\n[\"lle\", \"\u043c\u0438\u0441\u0441\"],\n[\"\u2581Temple\", \"\u0422\u0435\u043c\u043f\u043b\"],\n[\".\", null],\n[\"\u2581Je\", \"\u042f\"],\n[\"\u2581la\", \"\u0435\u0435\"],\n[\"\u2581mis\", \"\u043d\u0430\u0434\u0435\u043b\u0430\"],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581nous\", \"\u043c\u044b\"],\n[\"\u2581descend\", null],\n[\"\u00ee\", null],\n[\"mes\", \"\u0441\u043f\u0443\u0441\u0442\u0438\u043b\u0438\u0441\u044c\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n-- Des documents! des documents! s\u2019\u00e9cria lady Helena. -- Seulement, r\u00e9pondit Glenarvan, ils paraissent \u00eatre rong\u00e9s par l\u2019humidit\u00e9, et il est impossible de les retirer, car ils adh\u00e8rent aux parois de la bouteille.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581--', '\u2581Des', '\u2581documents', '!', '\u2581des', '\u2581documents', '!', '\u2581s', '\u2019', '\u00e9', 'cri', 'a', '\u2581lady', '\u2581Hel', 'ena', '.', '\u2581--', '\u2581Se', 'u', 'lement', ',', '\u2581r\u00e9pond', 'it', '\u2581Glen', 'ar', 'van', ',', '\u2581ils', '\u2581para', 'issent', '\u2581\u00eatre', '\u2581r', 'ong', '\u00e9s', '\u2581par', '\u2581l', '\u2019', 'hum', 'id', 'it\u00e9', ',', '\u2581et', '\u2581il', '\u2581est', '\u2581impossible', '\u2581de', '\u2581les', '\u2581retir', 'er', ',', '\u2581car', '\u2581ils', '\u2581ad', 'h', '\u00e8rent', '\u2581aux', '\u2581par', 'ois', '\u2581de', '\u2581la', '\u2581b', 'oute', 'ille', '.']"}], "[\n[\"\u2581--\", null],\n[\"\u2581Des\", \"Documents\"],\n[\"\u2581documents\", null],\n[\"!\", null],\n[\"\u2581des\", \"documents\"],\n[\"\u2581documents\", null],\n[\"!\", null],\n[\"\u2581s\", \"exclaimed\"],\n[\"'\", null],\n[\"\u00e9\", null],\n[\"cri\", null],\n[\"a\", null],\n[\"\u2581lady\", \"Lady\"],\n[\"\u2581Hel\", \"Helena\"],\n[\"ena\", null],\n[\".\", null],\n[\"\u2581--\", null],\n[\"\u2581Se\", \"Only\"],\n[\"u\", null],\n[\"lement\", null],\n[\",\", null],\n[\"\u2581r\u00e9pond\", \"replied\"],\n[\"it\", null],\n[\"\u2581Glen\", \"Glenarvan\"],\n[\"ar\", null],\n[\"van\", null],\n[\",\", null],\n[\"\u2581ils\", \"they\"],\n[\"\u2581para\", \"seem\"],\n[\"issent\", null],\n[\"\u2581\u00eatre\", \"to be\"],\n[\"\u2581r\", \"eaten\"],\n[\"ong\", null],\n[\"\u00e9s\", null],\n[\"\u2581par\", \"by\"],\n[\"\u2581l\", \"the\"],\n[\"'\", null],\n[\"hum\", \"moisture\"],\n[\"id\", null],\n[\"it\u00e9\", null],\n[\",\", null],\n[\"\u2581et\", \"and\"],\n[\"\u2581il\", \"it\"],\n[\"\u2581est\", \"is\"],\n[\"\u2581impossible\", \"impossible\"],\n[\"\u2581de\", \"to\"],\n[\"\u2581les\", \"them\"],\n[\"\u2581retir\", \"remove\"],\n[\"er\", null],\n[\",\", null],\n[\"\u2581car\", \"because\"],\n[\"\u2581ils\", \"they\"],\n[\"\u2581ad\", \"adhere\"],\n[\"h\", null],\n[\"\u00e8rent\", null],\n[\"\u2581aux\", \"to the\"],\n[\"\u2581par\", \"walls\"],\n[\"ois\", null],\n[\"\u2581de\", \"of\"],\n[\"\u2581la\", \"the\"],\n[\"\u2581b\", \"bottle\"],\n[\"oute\", null],\n[\"ille\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n-- Des documents! des documents! s\u2019\u00e9cria lady Helena. -- Seulement, r\u00e9pondit Glenarvan, ils paraissent \u00eatre rong\u00e9s par l\u2019humidit\u00e9, et il est impossible de les retirer, car ils adh\u00e8rent aux parois de la bouteille.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581--', '\u2581Des', '\u2581documents', '!', '\u2581des', '\u2581documents', '!', '\u2581s', '\u2019', '\u00e9', 'cri', 'a', '\u2581lady', '\u2581Hel', 'ena', '.', '\u2581--', '\u2581Se', 'u', 'lement', ',', '\u2581r\u00e9pond', 'it', '\u2581Glen', 'ar', 'van', ',', '\u2581ils', '\u2581para', 'issent', '\u2581\u00eatre', '\u2581r', 'ong', '\u00e9s', '\u2581par', '\u2581l', '\u2019', 'hum', 'id', 'it\u00e9', ',', '\u2581et', '\u2581il', '\u2581est', '\u2581impossible', '\u2581de', '\u2581les', '\u2581retir', 'er', ',', '\u2581car', '\u2581ils', '\u2581ad', 'h', '\u00e8rent', '\u2581aux', '\u2581par', 'ois', '\u2581de', '\u2581la', '\u2581b', 'oute', 'ille', '.']"}], "[\n[\"\u2581--\", null],\n[\"\u2581Des\", \"\u0414\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u044b\"],\n[\"\u2581documents\", null],\n[\"!\", null],\n[\"\u2581des\", null],\n[\"\u2581documents\", \"\u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u044b\"],\n[\"!\", null],\n[\"\u2581s\", null],\n[\"'\", null],\n[\"\u00e9\", null],\n[\"cri\", null],\n[\"a\", null],\n[\"\u2581lady\", \"\u043b\u0435\u0434\u0438\"],\n[\"\u2581Hel\", \"\u0415\u043b\u0435\u043d\u0430\"],\n[\"ena\", null],\n[\".\", null],\n[\"\u2581--\", null],\n[\"\u2581Se\", \"\u0422\u043e\u043b\u044c\u043a\u043e\"],\n[\"u\", null],\n[\"lement\", null],\n[\",\", null],\n[\"\u2581r\u00e9pond\", \"\u043e\u0442\u0432\u0435\u0442\u0438\u043b\"],\n[\"it\", null],\n[\"\u2581Glen\", \"\u0413\u043b\u0435\u043d\u0430\u0440\u0432\u0430\u043d\"],\n[\"ar\", null],\n[\"van\", null],\n[\",\", null],\n[\"\u2581ils\", \"\u043e\u043d\u0438\"],\n[\"\u2581para\", \"\u043a\u0430\u0436\u0443\u0442\u0441\u044f\"],\n[\"issent\", null],\n[\"\u2581\u00eatre\", null],\n[\"\u2581r\", \"\u0438\u0437\u044a\u0435\u0434\u0435\u043d\u043d\u044b\u043c\u0438\"],\n[\"ong\", null],\n[\"\u00e9s\", null],\n[\"\u2581par\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"hum\", \"\u0432\u043b\u0430\u0433\u043e\u0439\"],\n[\"id\", null],\n[\"it\u00e9\", null],\n[\",\", null],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581il\", \"\u043d\u0435\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\"],\n[\"\u2581est\", null],\n[\"\u2581impossible\", null],\n[\"\u2581de\", null],\n[\"\u2581les\", \"\u0438\u0445\"],\n[\"\u2581retir\", \"\u0438\u0437\u0432\u043b\u0435\u0447\u044c\"],\n[\"er\", null],\n[\",\", null],\n[\"\u2581car\", \"\u043f\u043e\u0442\u043e\u043c\u0443\"],\n[\"\u2581ils\", \"\u0447\u0442\u043e\"],\n[\"\u2581ad\", \"\u043f\u0440\u0438\u043b\u0438\u043f\u043b\u0438\"],\n[\"h\", null],\n[\"\u00e8rent\", null],\n[\"\u2581aux\", \"\u043a\"],\n[\"\u2581par\", \"\u0441\u0442\u0435\u043d\u043a\u0430\u043c\"],\n[\"ois\", null],\n[\"\u2581de\", null],\n[\"\u2581la\", null],\n[\"\u2581b\", \"\u0431\u0443\u0442\u044b\u043b\u043a\u0438\"],\n[\"oute\", null],\n[\"ille\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nJ'\u00e9tais dans une petite chambre. Pr\u00e8s du feu se trouvait une table ronde; sur un fauteuil \u00e0 dos \u00e9lev\u00e9 et de forme antique \u00e9tait assise la plus propre et la plus mignonne petite dame qu'on puisse imaginer.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581J', \"'\", '\u00e9ta', 'is', '\u2581dans', '\u2581une', '\u2581petite', '\u2581cham', 'bre', '.', '\u2581Pr', '\u00e8s', '\u2581du', '\u2581feu', '\u2581se', '\u2581trouv', 'ait', '\u2581une', '\u2581table', '\u2581r', 'onde', ';', '\u2581sur', '\u2581un', '\u2581fa', 'ute', 'uil', '\u2581\u00e0', '\u2581dos', '\u2581\u00e9', 'lev', '\u00e9', '\u2581et', '\u2581de', '\u2581forme', '\u2581ant', 'ique', '\u2581\u00e9tait', '\u2581ass', 'ise', '\u2581la', '\u2581plus', '\u2581propre', '\u2581et', '\u2581la', '\u2581plus', '\u2581m', 'ign', 'onne', '\u2581petite', '\u2581d', 'ame', '\u2581qu', \"'\", 'on', '\u2581pu', 'isse', '\u2581imag', 'iner', '.']"}], "[\n  [\"\u2581J\", \"I\"],\n  [\"'\", null],\n  [\"\u00e9ta\", null],\n  [\"is\", null],\n  [\"\u2581dans\", \"in\"],\n  [\"\u2581une\", \"a\"],\n  [\"\u2581petite\", \"small\"],\n  [\"\u2581cham\", null],\n  [\"bre\", \"room\"],\n  [\".\", null],\n  [\"\u2581Pr\", null],\n  [\"\u00e8s\", \"Near\"],\n  [\"\u2581du\", \"the\"],\n  [\"\u2581feu\", \"fire\"],\n  [\"\u2581se\", null],\n  [\"\u2581trouv\", null],\n  [\"ait\", \"was\"],\n  [\"\u2581une\", \"a\"],\n  [\"\u2581table\", \"table\"],\n  [\"\u2581r\", null],\n  [\"onde\", \"round\"],\n  [\";\", null],\n  [\"\u2581sur\", \"on\"],\n  [\"\u2581un\", \"an\"],\n  [\"\u2581fa\", null],\n  [\"ute\", null],\n  [\"uil\", \"armchair\"],\n  [\"\u2581\u00e0\", \"with\"],\n  [\"\u2581dos\", \"back\"],\n  [\"\u2581\u00e9\", null],\n  [\"lev\", null],\n  [\"\u00e9\", \"high\"],\n  [\"\u2581et\", \"and\"],\n  [\"\u2581de\", \"of\"],\n  [\"\u2581forme\", \"shape\"],\n  [\"\u2581ant\", null],\n  [\"ique\", \"antique\"],\n  [\"\u2581\u00e9tait\", \"was\"],\n  [\"\u2581ass\", null],\n  [\"ise\", \"seated\"],\n  [\"\u2581la\", \"the\"],\n  [\"\u2581plus\", \"most\"],\n  [\"\u2581propre\", \"neat\"],\n  [\"\u2581et\", \"and\"],\n  [\"\u2581la\", \"the\"],\n  [\"\u2581plus\", \"most\"],\n  [\"\u2581m\", null],\n  [\"ign\", null],\n  [\"onne\", \"charming\"],\n  [\"\u2581petite\", \"little\"],\n  [\"\u2581d\", null],\n  [\"ame\", \"lady\"],\n  [\"\u2581qu\", \"that\"],\n  [\"'\", null],\n  [\"on\", \"one\"],\n  [\"\u2581pu\", null],\n  [\"isse\", \"could\"],\n  [\"\u2581imag\", null],\n  [\"iner\", \"imagine\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nJ'\u00e9tais dans une petite chambre. Pr\u00e8s du feu se trouvait une table ronde; sur un fauteuil \u00e0 dos \u00e9lev\u00e9 et de forme antique \u00e9tait assise la plus propre et la plus mignonne petite dame qu'on puisse imaginer.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581J', \"'\", '\u00e9ta', 'is', '\u2581dans', '\u2581une', '\u2581petite', '\u2581cham', 'bre', '.', '\u2581Pr', '\u00e8s', '\u2581du', '\u2581feu', '\u2581se', '\u2581trouv', 'ait', '\u2581une', '\u2581table', '\u2581r', 'onde', ';', '\u2581sur', '\u2581un', '\u2581fa', 'ute', 'uil', '\u2581\u00e0', '\u2581dos', '\u2581\u00e9', 'lev', '\u00e9', '\u2581et', '\u2581de', '\u2581forme', '\u2581ant', 'ique', '\u2581\u00e9tait', '\u2581ass', 'ise', '\u2581la', '\u2581plus', '\u2581propre', '\u2581et', '\u2581la', '\u2581plus', '\u2581m', 'ign', 'onne', '\u2581petite', '\u2581d', 'ame', '\u2581qu', \"'\", 'on', '\u2581pu', 'isse', '\u2581imag', 'iner', '.']"}], "[\n[\"\u2581J\", \"\u042f\"],\n[\"'\", null],\n[\"\u00e9ta\", null],\n[\"is\", null],\n[\"\u2581dans\", null],\n[\"\u2581une\", null],\n[\"\u2581petite\", \"\u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0430\u044f\"],\n[\"\u2581cham\", null],\n[\"bre\", null],\n[\".\", null],\n[\"\u2581Pr\", null],\n[\"\u00e8s\", null],\n[\"\u2581du\", null],\n[\"\u2581feu\", \"\u043e\u0433\u043d\u044f\"],\n[\"\u2581se\", null],\n[\"\u2581trouv\", null],\n[\"ait\", null],\n[\"\u2581une\", null],\n[\"\u2581table\", \"\u0441\u0442\u043e\u043b\"],\n[\"\u2581r\", null],\n[\"onde\", null],\n[\";\", null],\n[\"\u2581sur\", null],\n[\"\u2581un\", null],\n[\"\u2581fa\", null],\n[\"ute\", null],\n[\"uil\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581dos\", \"\u0441\u043f\u0438\u043d\u043a\u043e\u0439\"],\n[\"\u2581\u00e9\", null],\n[\"lev\", null],\n[\"\u00e9\", null],\n[\"\u2581et\", null],\n[\"\u2581de\", null],\n[\"\u2581forme\", \"\u0444\u043e\u0440\u043c\u044b\"],\n[\"\u2581ant\", null],\n[\"ique\", null],\n[\"\u2581\u00e9tait\", null],\n[\"\u2581ass\", null],\n[\"ise\", null],\n[\"\u2581la\", null],\n[\"\u2581plus\", null],\n[\"\u2581propre\", \"\u043e\u043f\u0440\u044f\u0442\u043d\u0430\u044f\"],\n[\"\u2581et\", null],\n[\"\u2581la\", null],\n[\"\u2581plus\", null],\n[\"\u2581m\", null],\n[\"ign\", null],\n[\"onne\", null],\n[\"\u2581petite\", \"\u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0430\u044f\"],\n[\"\u2581d\", null],\n[\"ame\", null],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"on\", null],\n[\"\u2581pu\", null],\n[\"isse\", null],\n[\"\u2581imag\", null],\n[\"iner\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nJohn n'avait beaucoup d'affection ni pour sa m\u00e8re ni pour ses soeurs.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581John', '\u2581n', \"'\", 'avait', '\u2581beaucoup', '\u2581d', \"'\", 'aff', 'ection', '\u2581ni', '\u2581pour', '\u2581sa', '\u2581m\u00e8re', '\u2581ni', '\u2581pour', '\u2581ses', '\u2581so', 'eurs', '.']"}], "[\n  [\"\u2581John\", \"John\"],\n  [\"\u2581n\", null],\n  [\"'\", null],\n  [\"avait\", \"had\"],\n  [\"\u2581beaucoup\", \"much\"],\n  [\"\u2581d\", null],\n  [\"'\", null],\n  [\"aff\", \"affection\"],\n  [\"ection\", null],\n  [\"\u2581ni\", null],\n  [\"\u2581pour\", null],\n  [\"\u2581sa\", null],\n  [\"\u2581m\u00e8re\", \"mother\"],\n  [\"\u2581ni\", null],\n  [\"\u2581pour\", null],\n  [\"\u2581ses\", null],\n  [\"\u2581so\", \"sisters\"],\n  [\"eurs\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nJohn n'avait beaucoup d'affection ni pour sa m\u00e8re ni pour ses soeurs.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581John', '\u2581n', \"'\", 'avait', '\u2581beaucoup', '\u2581d', \"'\", 'aff', 'ection', '\u2581ni', '\u2581pour', '\u2581sa', '\u2581m\u00e8re', '\u2581ni', '\u2581pour', '\u2581ses', '\u2581so', 'eurs', '.']"}], "[\n  [\"\u2581John\", \"\u0414\u0436\u043e\u043d\"],\n  [\"\u2581n\", null],\n  [\"'\", null],\n  [\"avait\", \"\u0438\u0441\u043f\u044b\u0442\u044b\u0432\u0430\u043b\"],\n  [\"\u2581beaucoup\", null],\n  [\"\u2581d\", null],\n  [\"'\", null],\n  [\"aff\", null],\n  [\"ection\", null],\n  [\"\u2581ni\", null],\n  [\"\u2581pour\", null],\n  [\"\u2581sa\", null],\n  [\"\u2581m\u00e8re\", \"\u043c\u0430\u0442\u044c\"],\n  [\"\u2581ni\", null],\n  [\"\u2581pour\", null],\n  [\"\u2581ses\", null],\n  [\"\u2581so\", null],\n  [\"eurs\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n\u00ab Mais non, je rampe encore ; je m\u2019\u00e9corche la face et les genoux aux cailloux de la voie souterraine.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00ab', '\u2581Mais', '\u2581non', ',', '\u2581je', '\u2581ram', 'pe', '\u2581encore', '\u2581;', '\u2581je', '\u2581m', '\u2019', '\u00e9', 'cor', 'che', '\u2581la', '\u2581face', '\u2581et', '\u2581les', '\u2581gen', 'oux', '\u2581aux', '\u2581ca', 'ill', 'oux', '\u2581de', '\u2581la', '\u2581vo', 'ie', '\u2581sou', 'ter', 'raine', '.']"}], "[\n  [\"\u2581\u00ab\", null],\n  [\"\u2581Mais\", \"But\"],\n  [\"\u2581non\", \"no\"],\n  [\",\", null],\n  [\"\u2581je\", null],\n  [\"\u2581ram\", \"crawl\"],\n  [\"pe\", null],\n  [\"\u2581encore\", \"still\"],\n  [\"\u2581;\", null],\n  [\"\u2581je\", null],\n  [\"\u2581m\", null],\n  [\"'\", null],\n  [\"\u00e9\", null],\n  [\"cor\", \"scratch\"],\n  [\"che\", null],\n  [\"\u2581la\", null],\n  [\"\u2581face\", \"face\"],\n  [\"\u2581et\", \"and\"],\n  [\"\u2581les\", null],\n  [\"\u2581gen\", \"knees\"],\n  [\"oux\", null],\n  [\"\u2581aux\", \"on\"],\n  [\"\u2581ca\", \"stones\"],\n  [\"ill\", null],\n  [\"oux\", null],\n  [\"\u2581de\", \"of\"],\n  [\"\u2581la\", null],\n  [\"\u2581vo\", \"underground\"],\n  [\"ie\", null],\n  [\"\u2581sou\", null],\n  [\"ter\", null],\n  [\"raine\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n\u00ab Mais non, je rampe encore ; je m\u2019\u00e9corche la face et les genoux aux cailloux de la voie souterraine.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00ab', '\u2581Mais', '\u2581non', ',', '\u2581je', '\u2581ram', 'pe', '\u2581encore', '\u2581;', '\u2581je', '\u2581m', '\u2019', '\u00e9', 'cor', 'che', '\u2581la', '\u2581face', '\u2581et', '\u2581les', '\u2581gen', 'oux', '\u2581aux', '\u2581ca', 'ill', 'oux', '\u2581de', '\u2581la', '\u2581vo', 'ie', '\u2581sou', 'ter', 'raine', '.']"}], "[\n[\"\u2581\u00ab\", null],\n[\"\u2581Mais\", \"\u041d\u043e\"],\n[\"\u2581non\", null],\n[\",\", null],\n[\"\u2581je\", null],\n[\"\u2581ram\", \"\u043f\u043e\u043b\u0437\u0443\"],\n[\"pe\", null],\n[\"\u2581encore\", \"\u0435\u0449\u0435\"],\n[\"\u2581;\", null],\n[\"\u2581je\", null],\n[\"\u2581m\", \"\u0446\u0430\u0440\u0430\u043f\u0430\u044e\"],\n[\"'\", null],\n[\"\u00e9\", null],\n[\"cor\", null],\n[\"che\", null],\n[\"\u2581la\", null],\n[\"\u2581face\", \"\u043b\u0438\u0446\u043e\"],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581les\", null],\n[\"\u2581gen\", \"\u043a\u043e\u043b\u0435\u043d\u0438\"],\n[\"oux\", null],\n[\"\u2581aux\", \"\u043e\"],\n[\"\u2581ca\", \"\u043a\u0430\u043c\u043d\u0438\"],\n[\"ill\", null],\n[\"oux\", null],\n[\"\u2581de\", null],\n[\"\u2581la\", null],\n[\"\u2581vo\", \"\u043f\u043e\u0434\u0437\u0435\u043c\u043d\u043e\u0433\u043e\"],\n[\"ie\", null],\n[\"\u2581sou\", null],\n[\"ter\", null],\n[\"raine\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nTout salut semblait impossible, quand la voix du major se fit entendre.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Tout', '\u2581sal', 'ut', '\u2581semb', 'lait', '\u2581impossible', ',', '\u2581quand', '\u2581la', '\u2581voix', '\u2581du', '\u2581major', '\u2581se', '\u2581fit', '\u2581ent', 'endre', '.']"}], "[\n  [\"\u2581Tout\", \"All\"],\n  [\"\u2581sal\", \"salvation\"],\n  [\"ut\", null],\n  [\"\u2581semb\", \"seemed\"],\n  [\"lait\", null],\n  [\"\u2581impossible\", \"impossible\"],\n  [\",\", null],\n  [\"\u2581quand\", \"when\"],\n  [\"\u2581la\", null],\n  [\"\u2581voix\", \"voice\"],\n  [\"\u2581du\", null],\n  [\"\u2581major\", \"major\"],\n  [\"\u2581se\", null],\n  [\"\u2581fit\", \"was\"],\n  [\"\u2581ent\", \"heard\"],\n  [\"endre\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nTout salut semblait impossible, quand la voix du major se fit entendre.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Tout', '\u2581sal', 'ut', '\u2581semb', 'lait', '\u2581impossible', ',', '\u2581quand', '\u2581la', '\u2581voix', '\u2581du', '\u2581major', '\u2581se', '\u2581fit', '\u2581ent', 'endre', '.']"}], "[\n  [\"\u2581Tout\", \"\u0412\u0441\u044f\u043a\u043e\u0435\"],\n  [\"\u2581sal\", \"\u0441\u043f\u0430\u0441\u0435\u043d\u0438\u0435\"],\n  [\"ut\", null],\n  [\"\u2581semb\", \"\u043a\u0430\u0437\u0430\u043b\u043e\u0441\u044c\"],\n  [\"lait\", null],\n  [\"\u2581impossible\", \"\u043d\u0435\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u043c\"],\n  [\",\", null],\n  [\"\u2581quand\", \"\u043a\u043e\u0433\u0434\u0430\"],\n  [\"\u2581la\", null],\n  [\"\u2581voix\", \"\u0433\u043e\u043b\u043e\u0441\"],\n  [\"\u2581du\", null],\n  [\"\u2581major\", \"\u043c\u0430\u0439\u043e\u0440\u0430\"],\n  [\"\u2581se\", null],\n  [\"\u2581fit\", \"\u0440\u0430\u0437\u0434\u0430\u043b\u0441\u044f\"],\n  [\"\u2581ent\", null],\n  [\"endre\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nMais Paganel lui apprit que, sous cette latitude, les sauvages fr\u00e9quentaient principalement les plaines du Murray, situ\u00e9es \u00e0 cent milles dans l\u2019est.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Mais', '\u2581P', 'ag', 'anel', '\u2581lui', '\u2581app', 'rit', '\u2581que', ',', '\u2581sous', '\u2581cette', '\u2581latitude', ',', '\u2581les', '\u2581sau', 'v', 'ages', '\u2581fr', '\u00e9qu', 'enta', 'ient', '\u2581princip', 'alement', '\u2581les', '\u2581pla', 'ines', '\u2581du', '\u2581Murray', ',', '\u2581situ', '\u00e9es', '\u2581\u00e0', '\u2581cent', '\u2581mill', 'es', '\u2581dans', '\u2581l', '\u2019', 'est', '.']"}], "[\n[\"\u2581Mais\", \"But\"],\n[\"\u2581P\", \"Paganel\"],\n[\"ag\", null],\n[\"anel\", null],\n[\"\u2581lui\", null],\n[\"\u2581app\", \"informed\"],\n[\"rit\", null],\n[\"\u2581que\", null],\n[\",\", null],\n[\"\u2581sous\", \"under\"],\n[\"\u2581cette\", \"this\"],\n[\"\u2581latitude\", \"latitude\"],\n[\",\", null],\n[\"\u2581les\", null],\n[\"\u2581sau\", \"savages\"],\n[\"v\", null],\n[\"ages\", null],\n[\"\u2581fr\", \"frequented\"],\n[\"\u00e9qu\", null],\n[\"enta\", null],\n[\"ient\", null],\n[\"\u2581princip\", \"mainly\"],\n[\"alement\", null],\n[\"\u2581les\", null],\n[\"\u2581pla\", \"plains\"],\n[\"ines\", null],\n[\"\u2581du\", \"of\"],\n[\"\u2581Murray\", \"Murray\"],\n[\",\", null],\n[\"\u2581situ\", \"located\"],\n[\"\u00e9es\", null],\n[\"\u2581\u00e0\", \"at\"],\n[\"\u2581cent\", \"hundred\"],\n[\"\u2581mill\", \"miles\"],\n[\"es\", null],\n[\"\u2581dans\", \"in\"],\n[\"\u2581l\", \"the\"],\n[\"'\", null],\n[\"est\", \"east\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nMais Paganel lui apprit que, sous cette latitude, les sauvages fr\u00e9quentaient principalement les plaines du Murray, situ\u00e9es \u00e0 cent milles dans l\u2019est.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Mais', '\u2581P', 'ag', 'anel', '\u2581lui', '\u2581app', 'rit', '\u2581que', ',', '\u2581sous', '\u2581cette', '\u2581latitude', ',', '\u2581les', '\u2581sau', 'v', 'ages', '\u2581fr', '\u00e9qu', 'enta', 'ient', '\u2581princip', 'alement', '\u2581les', '\u2581pla', 'ines', '\u2581du', '\u2581Murray', ',', '\u2581situ', '\u00e9es', '\u2581\u00e0', '\u2581cent', '\u2581mill', 'es', '\u2581dans', '\u2581l', '\u2019', 'est', '.']"}], "[\n  [\"\u2581Mais\", \"\u041d\u043e\"],\n  [\"\u2581P\", \"\u041f\u0430\u0433\u0430\u043d\u0435\u043b\u044c\"],\n  [\"ag\", null],\n  [\"anel\", null],\n  [\"\u2581lui\", null],\n  [\"\u2581app\", \"\u0441\u043e\u043e\u0431\u0449\u0438\u043b\"],\n  [\"rit\", null],\n  [\"\u2581que\", null],\n  [\",\", null],\n  [\"\u2581sous\", \"\u043f\u043e\u0434\"],\n  [\"\u2581cette\", \"\u044d\u0442\u043e\u0439\"],\n  [\"\u2581latitude\", \"\u0448\u0438\u0440\u043e\u0442\u043e\u0439\"],\n  [\",\", null],\n  [\"\u2581les\", null],\n  [\"\u2581sau\", \"\u0434\u0438\u043a\u0430\u0440\u0438\"],\n  [\"v\", null],\n  [\"ages\", null],\n  [\"\u2581fr\", \"\u043f\u043e\u0441\u0435\u0449\u0430\u043b\u0438\"],\n  [\"\u00e9qu\", null],\n  [\"enta\", null],\n  [\"ient\", null],\n  [\"\u2581princip\", \"\u0433\u043b\u0430\u0432\u043d\u044b\u043c\"],\n  [\"alement\", null],\n  [\"\u2581les\", null],\n  [\"\u2581pla\", \"\u0440\u0430\u0432\u043d\u0438\u043d\u044b\"],\n  [\"ines\", null],\n  [\"\u2581du\", null],\n  [\"\u2581Murray\", \"\u041c\u0443\u0440\u0440\u0435\u044f\"],\n  [\",\", null],\n  [\"\u2581situ\", \"\u0440\u0430\u0441\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u043d\u044b\u0435\"],\n  [\"\u00e9es\", null],\n  [\"\u2581\u00e0\", \"\u0432\"],\n  [\"\u2581cent\", \"\u0441\u0442\u0430\"],\n  [\"\u2581mill\", \"\u043c\u0438\u043b\u044f\u0445\"],\n  [\"es\", null],\n  [\"\u2581dans\", \"\u043d\u0430\"],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"est\", \"\u0432\u043e\u0441\u0442\u043e\u043a\u0435\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n\u2013 Est-ce que vous ne trouvez pas, r\u00e9pondit l\u2019archidiacre d\u2019un air de profonde r\u00e9flexion, que l\u2019habit de ces cavaliers que nous venons de voir est plus beau que le v\u00f4tre et que le mien ? \u00bb Gringoire hocha la t\u00eate.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u2013', '\u2581Est', '-', 'ce', '\u2581que', '\u2581vous', '\u2581ne', '\u2581trouve', 'z', '\u2581pas', ',', '\u2581r\u00e9pond', 'it', '\u2581l', '\u2019', 'arch', 'idia', 'cre', '\u2581d', '\u2019', 'un', '\u2581air', '\u2581de', '\u2581prof', 'onde', '\u2581r\u00e9', 'flex', 'ion', ',', '\u2581que', '\u2581l', '\u2019', 'hab', 'it', '\u2581de', '\u2581ces', '\u2581cav', 'ali', 'ers', '\u2581que', '\u2581nous', '\u2581ven', 'ons', '\u2581de', '\u2581voir', '\u2581est', '\u2581plus', '\u2581be', 'au', '\u2581que', '\u2581le', '\u2581v', '\u00f4', 'tre', '\u2581et', '\u2581que', '\u2581le', '\u2581m', 'ien', '\u2581?', '\u2581\u00bb', '\u2581Gr', 'ing', 'oire', '\u2581hoch', 'a', '\u2581la', '\u2581t\u00eate', '.']"}], "[\n[\"\u2581\u2013\", null],\n[\"\u2581Est\", \"Is\"],\n[\"-\", null],\n[\"ce\", null],\n[\"\u2581que\", null],\n[\"\u2581vous\", null],\n[\"\u2581ne\", null],\n[\"\u2581trouve\", \"find\"],\n[\"z\", null],\n[\"\u2581pas\", null],\n[\",\", null],\n[\"\u2581r\u00e9pond\", \"replied\"],\n[\"it\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"arch\", \"archdeacon\"],\n[\"idia\", null],\n[\"cre\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"un\", null],\n[\"\u2581air\", \"air\"],\n[\"\u2581de\", null],\n[\"\u2581prof\", \"deep\"],\n[\"onde\", null],\n[\"\u2581r\u00e9\", \"reflection\"],\n[\"flex\", null],\n[\"ion\", null],\n[\",\", null],\n[\"\u2581que\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"hab\", \"attire\"],\n[\"it\", null],\n[\"\u2581de\", null],\n[\"\u2581ces\", null],\n[\"\u2581cav\", \"horsemen\"],\n[\"ali\", null],\n[\"ers\", null],\n[\"\u2581que\", null],\n[\"\u2581nous\", null],\n[\"\u2581ven\", \"just\"],\n[\"ons\", null],\n[\"\u2581de\", null],\n[\"\u2581voir\", \"seen\"],\n[\"\u2581est\", null],\n[\"\u2581plus\", null],\n[\"\u2581be\", \"beautiful\"],\n[\"au\", null],\n[\"\u2581que\", null],\n[\"\u2581le\", null],\n[\"\u2581v\", \"yours\"],\n[\"\u00f4\", null],\n[\"tre\", null],\n[\"\u2581et\", null],\n[\"\u2581que\", null],\n[\"\u2581le\", null],\n[\"\u2581m\", \"mine\"],\n[\"ien\", null],\n[\"\u2581?\", null],\n[\"\u2581\u00bb\", null],\n[\"\u2581Gr\", \"Gringoire\"],\n[\"ing\", null],\n[\"oire\", null],\n[\"\u2581hoch\", \"nodded\"],\n[\"a\", null],\n[\"\u2581la\", null],\n[\"\u2581t\u00eate\", \"head\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n\u2013 Est-ce que vous ne trouvez pas, r\u00e9pondit l\u2019archidiacre d\u2019un air de profonde r\u00e9flexion, que l\u2019habit de ces cavaliers que nous venons de voir est plus beau que le v\u00f4tre et que le mien ? \u00bb Gringoire hocha la t\u00eate.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u2013', '\u2581Est', '-', 'ce', '\u2581que', '\u2581vous', '\u2581ne', '\u2581trouve', 'z', '\u2581pas', ',', '\u2581r\u00e9pond', 'it', '\u2581l', '\u2019', 'arch', 'idia', 'cre', '\u2581d', '\u2019', 'un', '\u2581air', '\u2581de', '\u2581prof', 'onde', '\u2581r\u00e9', 'flex', 'ion', ',', '\u2581que', '\u2581l', '\u2019', 'hab', 'it', '\u2581de', '\u2581ces', '\u2581cav', 'ali', 'ers', '\u2581que', '\u2581nous', '\u2581ven', 'ons', '\u2581de', '\u2581voir', '\u2581est', '\u2581plus', '\u2581be', 'au', '\u2581que', '\u2581le', '\u2581v', '\u00f4', 'tre', '\u2581et', '\u2581que', '\u2581le', '\u2581m', 'ien', '\u2581?', '\u2581\u00bb', '\u2581Gr', 'ing', 'oire', '\u2581hoch', 'a', '\u2581la', '\u2581t\u00eate', '.']"}], "[\n  [\"\u2581\u2013\", null],\n  [\"\u2581Est\", \"\u0420\u0430\u0437\u0432\u0435\"],\n  [\"-\", null],\n  [\"ce\", null],\n  [\"\u2581que\", null],\n  [\"\u2581vous\", \"\u0432\u044b\"],\n  [\"\u2581ne\", null],\n  [\"\u2581trouve\", \"\u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0435\"],\n  [\"z\", null],\n  [\"\u2581pas\", null],\n  [\",\", null],\n  [\"\u2581r\u00e9pond\", \"\u043e\u0442\u0432\u0435\u0442\u0438\u043b\"],\n  [\"it\", null],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"arch\", \"\u0430\u0440\u0445\u0438\u0434\u044c\u044f\u043a\u043e\u043d\"],\n  [\"idia\", null],\n  [\"cre\", null],\n  [\"\u2581d\", null],\n  [\"'\", null],\n  [\"un\", null],\n  [\"\u2581air\", \"\u0441 \u0432\u0438\u0434\u043e\u043c\"],\n  [\"\u2581de\", null],\n  [\"\u2581prof\", \"\u0433\u043b\u0443\u0431\u043e\u043a\u043e\u0439\"],\n  [\"onde\", null],\n  [\"\u2581r\u00e9\", \"\u0437\u0430\u0434\u0443\u043c\u0447\u0438\u0432\u043e\u0441\u0442\u0438\"],\n  [\"flex\", null],\n  [\"ion\", null],\n  [\",\", null],\n  [\"\u2581que\", \"\u0447\u0442\u043e\"],\n  [\"\u2581l\", null],\n  [\"'\", null],\n  [\"hab\", \"\u043e\u0434\u0435\u0436\u0434\u0430\"],\n  [\"it\", null],\n  [\"\u2581de\", null],\n  [\"\u2581ces\", \"\u044d\u0442\u0438\u0445\"],\n  [\"\u2581cav\", \"\u0432\u0441\u0430\u0434\u043d\u0438\u043a\u043e\u0432\"],\n  [\"ali\", null],\n  [\"ers\", null],\n  [\"\u2581que\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u0445\"],\n  [\"\u2581nous\", \"\u043c\u044b\"],\n  [\"\u2581ven\", \"\u0442\u043e\u043b\u044c\u043a\u043e\"],\n  [\"ons\", null],\n  [\"\u2581de\", null],\n  [\"\u2581voir\", \"\u0432\u0438\u0434\u0435\u043b\u0438\"],\n  [\"\u2581est\", null],\n  [\"\u2581plus\", \"\u043a\u0440\u0430\u0441\u0438\u0432\u0435\u0435\"],\n  [\"\u2581be\", null],\n  [\"au\", null],\n  [\"\u2581que\", \"\u0447\u0435\u043c\"],\n  [\"\u2581le\", null],\n  [\"\u2581v\", \"\u0432\u0430\u0448\u0430\"],\n  [\"\u00f4\", null],\n  [\"tre\", null],\n  [\"\u2581et\", \"\u0438\"],\n  [\"\u2581que\", null],\n  [\"\u2581le\", null],\n  [\"\u2581m\", \"\u043c\u043e\u044f\"],\n  [\"ien\", null],\n  [\"\u2581?\", null],\n  [\"\u2581\u00bb\", null],\n  [\"\u2581Gr\", \"\u0413\u0440\u0435\u043d\u0433\u0443\u0430\u0440\"],\n  [\"ing\", null],\n  [\"oire\", null],\n  [\"\u2581hoch\", \"\u043f\u043e\u043a\u0430\u0447\u0430\u043b\"],\n  [\"a\", null],\n  [\"\u2581la\", null],\n  [\"\u2581t\u00eate\", \"\u0433\u043e\u043b\u043e\u0432\u043e\u0439\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nEn moins de cinq minutes, ils atteignirent le sommet du mont; l\u00e0, ils se retourn\u00e8rent afin de juger la situation et de prendre une direction qui p\u00fbt d\u00e9pister les maoris.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581En', '\u2581moins', '\u2581de', '\u2581cinq', '\u2581minutes', ',', '\u2581ils', '\u2581atte', 'ign', 'ir', 'ent', '\u2581le', '\u2581som', 'met', '\u2581du', '\u2581mont', ';', '\u2581l\u00e0', ',', '\u2581ils', '\u2581se', '\u2581ret', 'ourn', '\u00e8rent', '\u2581afin', '\u2581de', '\u2581jug', 'er', '\u2581la', '\u2581situation', '\u2581et', '\u2581de', '\u2581prendre', '\u2581une', '\u2581direction', '\u2581qui', '\u2581p', '\u00fbt', '\u2581d\u00e9p', 'ister', '\u2581les', '\u2581ma', 'oris', '.']"}], "[\n  [\"\u2581En\", \"In\"],\n  [\"\u2581moins\", \"less\"],\n  [\"\u2581de\", null],\n  [\"\u2581cinq\", \"five\"],\n  [\"\u2581minutes\", \"minutes\"],\n  [\",\", null],\n  [\"\u2581ils\", \"they\"],\n  [\"\u2581atte\", \"reached\"],\n  [\"ign\", null],\n  [\"ir\", null],\n  [\"ent\", null],\n  [\"\u2581le\", null],\n  [\"\u2581som\", \"summit\"],\n  [\"met\", null],\n  [\"\u2581du\", null],\n  [\"\u2581mont\", \"mountain\"],\n  [\";\", null],\n  [\"\u2581l\u00e0\", \"there\"],\n  [\",\", null],\n  [\"\u2581ils\", \"they\"],\n  [\"\u2581se\", null],\n  [\"\u2581ret\", \"turned\"],\n  [\"ourn\", null],\n  [\"\u00e8rent\", null],\n  [\"\u2581afin\", \"in order\"],\n  [\"\u2581de\", null],\n  [\"\u2581jug\", \"assess\"],\n  [\"er\", null],\n  [\"\u2581la\", null],\n  [\"\u2581situation\", \"situation\"],\n  [\"\u2581et\", \"and\"],\n  [\"\u2581de\", null],\n  [\"\u2581prendre\", \"choose\"],\n  [\"\u2581une\", null],\n  [\"\u2581direction\", \"direction\"],\n  [\"\u2581qui\", \"that\"],\n  [\"\u2581p\", \"could\"],\n  [\"\u00fbt\", null],\n  [\"\u2581d\u00e9p\", \"throw off\"],\n  [\"ister\", null],\n  [\"\u2581les\", null],\n  [\"\u2581ma\", \"Maoris\"],\n  [\"oris\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nEn moins de cinq minutes, ils atteignirent le sommet du mont; l\u00e0, ils se retourn\u00e8rent afin de juger la situation et de prendre une direction qui p\u00fbt d\u00e9pister les maoris.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581En', '\u2581moins', '\u2581de', '\u2581cinq', '\u2581minutes', ',', '\u2581ils', '\u2581atte', 'ign', 'ir', 'ent', '\u2581le', '\u2581som', 'met', '\u2581du', '\u2581mont', ';', '\u2581l\u00e0', ',', '\u2581ils', '\u2581se', '\u2581ret', 'ourn', '\u00e8rent', '\u2581afin', '\u2581de', '\u2581jug', 'er', '\u2581la', '\u2581situation', '\u2581et', '\u2581de', '\u2581prendre', '\u2581une', '\u2581direction', '\u2581qui', '\u2581p', '\u00fbt', '\u2581d\u00e9p', 'ister', '\u2581les', '\u2581ma', 'oris', '.']"}], "[\n  [\"\u2581En\", \"\u041c\u0435\u043d\u044c\u0448\u0435\"],\n  [\"\u2581moins\", \"\u0447\u0435\u043c\"],\n  [\"\u2581de\", null],\n  [\"\u2581cinq\", \"\u043f\u044f\u0442\u044c\"],\n  [\"\u2581minutes\", \"\u043c\u0438\u043d\u0443\u0442\"],\n  [\",\", null],\n  [\"\u2581ils\", \"\u043e\u043d\u0438\"],\n  [\"\u2581atte\", \"\u0434\u043e\u0441\u0442\u0438\u0433\u043b\u0438\"],\n  [\"ign\", null],\n  [\"ir\", null],\n  [\"ent\", null],\n  [\"\u2581le\", null],\n  [\"\u2581som\", \"\u0432\u0435\u0440\u0448\u0438\u043d\u044b\"],\n  [\"met\", null],\n  [\"\u2581du\", null],\n  [\"\u2581mont\", \"\u0433\u043e\u0440\u044b\"],\n  [\";\", null],\n  [\"\u2581l\u00e0\", \"\u0442\u0430\u043c\"],\n  [\",\", null],\n  [\"\u2581ils\", \"\u043e\u043d\u0438\"],\n  [\"\u2581se\", null],\n  [\"\u2581ret\", \"\u043f\u043e\u0432\u0435\u0440\u043d\u0443\u043b\u0438\u0441\u044c\"],\n  [\"ourn\", null],\n  [\"\u00e8rent\", null],\n  [\"\u2581afin\", \"\u0447\u0442\u043e\u0431\u044b\"],\n  [\"\u2581de\", null],\n  [\"\u2581jug\", \"\u043e\u0446\u0435\u043d\u0438\u0442\u044c\"],\n  [\"er\", null],\n  [\"\u2581la\", null],\n  [\"\u2581situation\", \"\u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044e\"],\n  [\"\u2581et\", \"\u0438\"],\n  [\"\u2581de\", null],\n  [\"\u2581prendre\", \"\u0432\u044b\u0431\u0440\u0430\u0442\u044c\"],\n  [\"\u2581une\", null],\n  [\"\u2581direction\", \"\u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435\"],\n  [\"\u2581qui\", \"\u043a\u043e\u0442\u043e\u0440\u043e\u0435\"],\n  [\"\u2581p\", \"\u043c\u043e\u0433\u043b\u043e\"],\n  [\"\u00fbt\", null],\n  [\"\u2581d\u00e9p\", \"\u0441\u0431\u0438\u0442\u044c\"],\n  [\"ister\", null],\n  [\"\u2581les\", null],\n  [\"\u2581ma\", \"\u043c\u0430\u043e\u0440\u0438\"],\n  [\"oris\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nLes assi\u00e9g\u00e9s se trouvaient alors dans une situation relativement moins dangereuse; tant que dureraient les munitions, tant que la barri\u00e8re de feu se dresserait \u00e0 l\u2019entr\u00e9e de la _ramada_, l\u2019envahissement n\u2019\u00e9tait pas \u00e0 craindre.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Les', '\u2581as', 'si', '\u00e9g', '\u00e9s', '\u2581se', '\u2581trou', 'va', 'ient', '\u2581alors', '\u2581dans', '\u2581une', '\u2581situation', '\u2581relativ', 'ement', '\u2581moins', '\u2581d', 'ang', 'ere', 'use', ';', '\u2581tant', '\u2581que', '\u2581dur', 'era', 'ient', '\u2581les', '\u2581m', 'un', 'itions', ',', '\u2581tant', '\u2581que', '\u2581la', '\u2581bar', 'ri\u00e8re', '\u2581de', '\u2581feu', '\u2581se', '\u2581dress', 'er', 'ait', '\u2581\u00e0', '\u2581l', '\u2019', 'entr', '\u00e9e', '\u2581de', '\u2581la', '\u2581_', 'ram', 'ada', '_,', '\u2581l', '\u2019', 'env', 'ah', 'issement', '\u2581n', '\u2019', '\u00e9tait', '\u2581pas', '\u2581\u00e0', '\u2581cra', 'indre', '.']"}], "[\n[\"\u2581Les\", \"The\"],\n[\"\u2581as\", \"besieged\"],\n[\"si\", null],\n[\"\u00e9g\", null],\n[\"\u00e9s\", null],\n[\"\u2581se\", null],\n[\"\u2581trou\", \"found\"],\n[\"va\", null],\n[\"ient\", null],\n[\"\u2581alors\", \"themselves\"],\n[\"\u2581dans\", \"in\"],\n[\"\u2581une\", \"a\"],\n[\"\u2581situation\", \"situation\"],\n[\"\u2581relativ\", \"relatively\"],\n[\"ement\", null],\n[\"\u2581moins\", \"less\"],\n[\"\u2581d\", null],\n[\"ang\", \"dangerous\"],\n[\"ere\", null],\n[\"use\", null],\n[\";\", null],\n[\"\u2581tant\", \"as long\"],\n[\"\u2581que\", \"as\"],\n[\"\u2581dur\", \"last\"],\n[\"era\", null],\n[\"ient\", null],\n[\"\u2581les\", \"the\"],\n[\"\u2581m\", \"ammunition\"],\n[\"un\", null],\n[\"itions\", null],\n[\",\", null],\n[\"\u2581tant\", \"as long\"],\n[\"\u2581que\", \"as\"],\n[\"\u2581la\", \"the\"],\n[\"\u2581bar\", \"barrier\"],\n[\"ri\u00e8re\", null],\n[\"\u2581de\", \"of\"],\n[\"\u2581feu\", \"fire\"],\n[\"\u2581se\", null],\n[\"\u2581dress\", \"stood\"],\n[\"er\", null],\n[\"ait\", null],\n[\"\u2581\u00e0\", \"at\"],\n[\"\u2581l\", \"the\"],\n[\"'\", null],\n[\"entr\", \"entrance\"],\n[\"\u00e9e\", null],\n[\"\u2581de\", \"of\"],\n[\"\u2581la\", \"the\"],\n[\"\u2581_\", null],\n[\"ram\", \"ramada\"],\n[\"ada\", null],\n[\"_,\", null],\n[\"\u2581l\", \"the\"],\n[\"'\", null],\n[\"env\", \"invasion\"],\n[\"ah\", null],\n[\"issement\", null],\n[\"\u2581n\", \"was\"],\n[\"'\", null],\n[\"\u00e9tait\", null],\n[\"\u2581pas\", \"not\"],\n[\"\u2581\u00e0\", \"to\"],\n[\"\u2581cra\", \"fear\"],\n[\"indre\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nLes assi\u00e9g\u00e9s se trouvaient alors dans une situation relativement moins dangereuse; tant que dureraient les munitions, tant que la barri\u00e8re de feu se dresserait \u00e0 l\u2019entr\u00e9e de la _ramada_, l\u2019envahissement n\u2019\u00e9tait pas \u00e0 craindre.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Les', '\u2581as', 'si', '\u00e9g', '\u00e9s', '\u2581se', '\u2581trou', 'va', 'ient', '\u2581alors', '\u2581dans', '\u2581une', '\u2581situation', '\u2581relativ', 'ement', '\u2581moins', '\u2581d', 'ang', 'ere', 'use', ';', '\u2581tant', '\u2581que', '\u2581dur', 'era', 'ient', '\u2581les', '\u2581m', 'un', 'itions', ',', '\u2581tant', '\u2581que', '\u2581la', '\u2581bar', 'ri\u00e8re', '\u2581de', '\u2581feu', '\u2581se', '\u2581dress', 'er', 'ait', '\u2581\u00e0', '\u2581l', '\u2019', 'entr', '\u00e9e', '\u2581de', '\u2581la', '\u2581_', 'ram', 'ada', '_,', '\u2581l', '\u2019', 'env', 'ah', 'issement', '\u2581n', '\u2019', '\u00e9tait', '\u2581pas', '\u2581\u00e0', '\u2581cra', 'indre', '.']"}], "[\n[\"\u2581Les\", \"\u041e\u0441\u0430\u0436\u0434\u0435\u043d\u043d\u044b\u0435\"],\n[\"\u2581as\", null],\n[\"si\", null],\n[\"\u00e9g\", null],\n[\"\u00e9s\", null],\n[\"\u2581se\", null],\n[\"\u2581trou\", null],\n[\"va\", null],\n[\"ient\", null],\n[\"\u2581alors\", \"\u0442\u043e\u0433\u0434\u0430\"],\n[\"\u2581dans\", \"\u0432\"],\n[\"\u2581une\", null],\n[\"\u2581situation\", \"\u0441\u0438\u0442\u0443\u0430\u0446\u0438\u0438\"],\n[\"\u2581relativ\", null],\n[\"ement\", null],\n[\"\u2581moins\", \"\u043c\u0435\u043d\u0435\u0435\"],\n[\"\u2581d\", null],\n[\"ang\", null],\n[\"ere\", null],\n[\"use\", null],\n[\";\", null],\n[\"\u2581tant\", \"\u043f\u043e\u043a\u0430\"],\n[\"\u2581que\", null],\n[\"\u2581dur\", null],\n[\"era\", null],\n[\"ient\", null],\n[\"\u2581les\", null],\n[\"\u2581m\", null],\n[\"un\", null],\n[\"itions\", null],\n[\",\", null],\n[\"\u2581tant\", \"\u043f\u043e\u043a\u0430\"],\n[\"\u2581que\", null],\n[\"\u2581la\", null],\n[\"\u2581bar\", null],\n[\"ri\u00e8re\", null],\n[\"\u2581de\", null],\n[\"\u2581feu\", \"\u043e\u0433\u043d\u044f\"],\n[\"\u2581se\", null],\n[\"\u2581dress\", null],\n[\"er\", null],\n[\"ait\", null],\n[\"\u2581\u00e0\", \"\u0443\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"entr\", null],\n[\"\u00e9e\", null],\n[\"\u2581de\", null],\n[\"\u2581la\", null],\n[\"\u2581_\", null],\n[\"ram\", null],\n[\"ada\", null],\n[\"_,\", null],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"env\", null],\n[\"ah\", null],\n[\"issement\", null],\n[\"\u2581n\", null],\n[\"'\", null],\n[\"\u00e9tait\", null],\n[\"\u2581pas\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581cra\", null],\n[\"indre\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nIl y a des jours o\u00f9 je voudrais en \u00eatre d\u00e9barrass\u00e9; car je n'attache plus aucune valeur au tronc qui l'a produite, parce que j'ai vu qu'une poussi\u00e8re d'or pouvait seule lui servir d'engrais. Non, je n'aime pas cette enfant, surtout quand elle est aussi pr\u00e9tentieuse que maintenant.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Il', '\u2581y', '\u2581a', '\u2581des', '\u2581jours', '\u2581o\u00f9', '\u2581je', '\u2581v', 'oud', 'rais', '\u2581en', '\u2581\u00eatre', '\u2581d\u00e9', 'bar', 'rass', '\u00e9', ';', '\u2581car', '\u2581je', '\u2581n', \"'\", 'att', 'ache', '\u2581plus', '\u2581auc', 'une', '\u2581vale', 'ur', '\u2581au', '\u2581tr', 'onc', '\u2581qui', '\u2581l', \"'\", 'a', '\u2581produ', 'ite', ',', '\u2581par', 'ce', '\u2581que', '\u2581j', \"'\", 'ai', '\u2581vu', '\u2581qu', \"'\", 'une', '\u2581p', 'ous', 'si', '\u00e8re', '\u2581d', \"'\", 'or', '\u2581pouv', 'ait', '\u2581seule', '\u2581lui', '\u2581servir', '\u2581d', \"'\", 'eng', 'rais', '.', '\u2581Non', ',', '\u2581je', '\u2581n', \"'\", 'a', 'ime', '\u2581pas', '\u2581cette', '\u2581enf', 'ant', ',', '\u2581surtout', '\u2581quand', '\u2581elle', '\u2581est', '\u2581aussi', '\u2581pr', '\u00e9t', 'ent', 'ie', 'use', '\u2581que', '\u2581mainten', 'ant', '.']"}], "[[\"\u2581Il\", \"There\"],[\"\u2581y\", null],[\"\u2581a\", null],[\"\u2581des\", null],[\"\u2581jours\", \"days\"],[\"\u2581o\u00f9\", \"when\"],[\"\u2581je\", null],[\"\u2581v\", \"would\"],[\"oud\", null],[\"rais\", null],[\"\u2581en\", null],[\"\u2581\u00eatre\", \"be\"],[\"\u2581d\u00e9\", \"rid\"],[\"bar\", null],[\"rass\", null],[\"\u00e9\", null],[\"\u2581;\", null],[\"\u2581car\", \"because\"],[\"\u2581je\", null],[\"\u2581n\", null],[\"'\", null],[\"att\", \"attach\"],[\"ache\", null],[\"\u2581plus\", null],[\"\u2581auc\", \"any\"],[\"une\", null],[\"\u2581vale\", \"value\"],[\"ur\", null],[\"\u2581au\", null],[\"\u2581tr\", \"trunk\"],[\"onc\", null],[\"\u2581qui\", \"that\"],[\"\u2581l\", null],[\"'\", null],[\"a\", null],[\"\u2581produ\", \"produced\"],[\"ite\", null],[\",\", null],[\"\u2581par\", \"because\"],[\"ce\", null],[\"\u2581que\", null],[\"\u2581j\", null],[\"'\", null],[\"ai\", null],[\"\u2581vu\", \"seen\"],[\"\u2581qu\", null],[\"'\", null],[\"une\", null],[\"\u2581p\", \"dust\"],[\"ous\", null],[\"si\", null],[\"\u00e8re\", null],[\"\u2581d\", null],[\"'\", null],[\"or\", \"gold\"],[\"\u2581pouv\", \"could\"],[\"ait\", null],[\"\u2581seule\", \"only\"],[\"\u2581lui\", null],[\"\u2581servir\", \"serve\"],[\"\u2581d\", null],[\"'\", null],[\"eng\", \"fertilizer\"],[\"rais\", null],[\".\", null],[\"\u2581Non\", \"No\"],[\",\", null],[\"\u2581je\", null],[\"\u2581n\", null],[\"'\", null],[\"a\", \"love\"],[\"ime\", null],[\"\u2581pas\", null],[\"\u2581cette\", \"this\"],[\"\u2581enf\", \"child\"],[\"ant\", null],[\",\", null],[\"\u2581surtout\", \"especially\"],[\"\u2581quand\", \"when\"],[\"\u2581elle\", \"she\"],[\"\u2581est\", \"is\"],[\"\u2581aussi\", \"as\"],[\"\u2581pr\", \"pretentious\"],[\"\u00e9t\", null],[\"ent\", null],[\"ie\", null],[\"use\", null],[\"\u2581que\", \"as\"],[\"\u2581mainten\", \"now\"],[\"ant\", null],[\".\", null]]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nIl y a des jours o\u00f9 je voudrais en \u00eatre d\u00e9barrass\u00e9; car je n'attache plus aucune valeur au tronc qui l'a produite, parce que j'ai vu qu'une poussi\u00e8re d'or pouvait seule lui servir d'engrais. Non, je n'aime pas cette enfant, surtout quand elle est aussi pr\u00e9tentieuse que maintenant.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Il', '\u2581y', '\u2581a', '\u2581des', '\u2581jours', '\u2581o\u00f9', '\u2581je', '\u2581v', 'oud', 'rais', '\u2581en', '\u2581\u00eatre', '\u2581d\u00e9', 'bar', 'rass', '\u00e9', ';', '\u2581car', '\u2581je', '\u2581n', \"'\", 'att', 'ache', '\u2581plus', '\u2581auc', 'une', '\u2581vale', 'ur', '\u2581au', '\u2581tr', 'onc', '\u2581qui', '\u2581l', \"'\", 'a', '\u2581produ', 'ite', ',', '\u2581par', 'ce', '\u2581que', '\u2581j', \"'\", 'ai', '\u2581vu', '\u2581qu', \"'\", 'une', '\u2581p', 'ous', 'si', '\u00e8re', '\u2581d', \"'\", 'or', '\u2581pouv', 'ait', '\u2581seule', '\u2581lui', '\u2581servir', '\u2581d', \"'\", 'eng', 'rais', '.', '\u2581Non', ',', '\u2581je', '\u2581n', \"'\", 'a', 'ime', '\u2581pas', '\u2581cette', '\u2581enf', 'ant', ',', '\u2581surtout', '\u2581quand', '\u2581elle', '\u2581est', '\u2581aussi', '\u2581pr', '\u00e9t', 'ent', 'ie', 'use', '\u2581que', '\u2581mainten', 'ant', '.']"}], "[\n[\"\u2581Il\", \"\u0411\u044b\u0432\u0430\u044e\u0442\"],\n[\"\u2581y\", null],\n[\"\u2581a\", null],\n[\"\u2581des\", null],\n[\"\u2581jours\", \"\u0434\u043d\u0438\"],\n[\"\u2581o\u00f9\", \"\u043a\u043e\u0433\u0434\u0430\"],\n[\"\u2581je\", \"\u044f\"],\n[\"\u2581v\", \"\u0445\u043e\u0442\u0435\u043b\"],\n[\"oud\", null],\n[\"rais\", null],\n[\"\u2581en\", null],\n[\"\u2581\u00eatre\", \"\u0431\u044b\u0442\u044c\"],\n[\"\u2581d\u00e9\", \"\u0438\u0437\u0431\u0430\u0432\u043b\u0435\u043d\"],\n[\"bar\", null],\n[\"rass\", null],\n[\"\u00e9\", null],\n[\";\", null],\n[\"\u2581car\", \"\u043f\u043e\u0442\u043e\u043c\u0443\"],\n[\"\u2581je\", \"\u044f\"],\n[\"\u2581n\", null],\n[\"'\", null],\n[\"att\", \"\u043d\u0435\"],\n[\"ache\", null],\n[\"\u2581plus\", \"\u0431\u043e\u043b\u044c\u0448\u0435\"],\n[\"\u2581auc\", \"\u043d\u0438\u043a\u0430\u043a\u043e\u0439\"],\n[\"une\", null],\n[\"\u2581vale\", \"\u0446\u0435\u043d\u043d\u043e\u0441\u0442\u0438\"],\n[\"ur\", null],\n[\"\u2581au\", null],\n[\"\u2581tr\", \"\u0441\u0442\u0432\u043e\u043b\u0443\"],\n[\"onc\", null],\n[\"\u2581qui\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u0439\"],\n[\"\u2581l\", null],\n[\"'\", null],\n[\"a\", null],\n[\"\u2581produ\", \"\u043f\u0440\u043e\u0438\u0437\u0432\u0435\u043b\"],\n[\"ite\", null],\n[\",\", null],\n[\"\u2581par\", \"\u043f\u043e\u0442\u043e\u043c\u0443\"],\n[\"ce\", null],\n[\"\u2581que\", \"\u0447\u0442\u043e\"],\n[\"\u2581j\", \"\u044f\"],\n[\"'\", null],\n[\"ai\", null],\n[\"\u2581vu\", \"\u0443\u0432\u0438\u0434\u0435\u043b\"],\n[\"\u2581qu\", \"\u0447\u0442\u043e\"],\n[\"'\", null],\n[\"une\", null],\n[\"\u2581p\", \"\u0437\u043e\u043b\u043e\u0442\u0430\u044f\"],\n[\"ous\", null],\n[\"si\", null],\n[\"\u00e8re\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"or\", null],\n[\"\u2581pouv\", \"\u043c\u043e\u0433\u043b\u0430\"],\n[\"ait\", null],\n[\"\u2581seule\", \"\u0442\u043e\u043b\u044c\u043a\u043e\"],\n[\"\u2581lui\", \"\u0435\u043c\u0443\"],\n[\"\u2581servir\", \"\u0441\u043b\u0443\u0436\u0438\u0442\u044c\"],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"eng\", \"\u0443\u0434\u043e\u0431\u0440\u0435\u043d\u0438\u0435\u043c\"],\n[\"rais\", null],\n[\".\", null],\n[\"\u2581Non\", \"\u041d\u0435\u0442\"],\n[\",\", null],\n[\"\u2581je\", \"\u044f\"],\n[\"\u2581n\", null],\n[\"'\", null],\n[\"a\", \"\u043d\u0435\"],\n[\"ime\", null],\n[\"\u2581pas\", null],\n[\"\u2581cette\", \"\u044d\u0442\u043e\u0433\u043e\"],\n[\"\u2581enf\", \"\u0440\u0435\u0431\u0435\u043d\u043a\u0430\"],\n[\"ant\", null],\n[\",\", null],\n[\"\u2581surtout\", \"\u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\"],\n[\"\u2581quand\", \"\u043a\u043e\u0433\u0434\u0430\"],\n[\"\u2581elle\", \"\u043e\u043d\u0430\"],\n[\"\u2581est\", null],\n[\"\u2581aussi\", \"\u0442\u0430\u043a\u0430\u044f\"],\n[\"\u2581pr\", \"\u043f\u0440\u0435\u0442\u0435\u043d\u0446\u0438\u043e\u0437\u043d\u0430\u044f\"],\n[\"\u00e9t\", null],\n[\"ent\", null],\n[\"ie\", null],\n[\"use\", null],\n[\"\u2581que\", \"\u043a\u0430\u043a\"],\n[\"\u2581mainten\", \"\u0441\u0435\u0439\u0447\u0430\u0441\"],\n[\"ant\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\nOr, comme John Mangles se chargeait d\u2019en faire un marin, Glenarvan un homme de coeur, le major un gar\u00e7on de sang-froid, lady Helena un \u00eatre bon et g\u00e9n\u00e9reux, Mary Grant un \u00e9l\u00e8ve reconnaissant envers de pareils ma\u00eetres, Robert devait \u00e9videmment devenir un jour un gentleman accompli.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Or', ',', '\u2581comme', '\u2581John', '\u2581M', 'angles', '\u2581se', '\u2581charge', 'ait', '\u2581d', '\u2019', 'en', '\u2581faire', '\u2581un', '\u2581mar', 'in', ',', '\u2581Glen', 'ar', 'van', '\u2581un', '\u2581homme', '\u2581de', '\u2581co', 'eur', ',', '\u2581le', '\u2581major', '\u2581un', '\u2581gar', '\u00e7on', '\u2581de', '\u2581sang', '-', 'f', 'roid', ',', '\u2581lady', '\u2581Hel', 'ena', '\u2581un', '\u2581\u00eatre', '\u2581bon', '\u2581et', '\u2581g\u00e9n', '\u00e9re', 'ux', ',', '\u2581Mary', '\u2581Grant', '\u2581un', '\u2581\u00e9l', '\u00e8ve', '\u2581reconna', 'issant', '\u2581en', 'vers', '\u2581de', '\u2581pare', 'ils', '\u2581ma', '\u00ee', 'tres', ',', '\u2581Robert', '\u2581dev', 'ait', '\u2581\u00e9v', 'ide', 'm', 'ment', '\u2581de', 'venir', '\u2581un', '\u2581jour', '\u2581un', '\u2581gentleman', '\u2581accomp', 'li', '.']"}], "[\n[\"\u2581Or\", \"Now\"],\n[\",\", null],\n[\"\u2581comme\", null],\n[\"\u2581John\", \"John\"],\n[\"\u2581M\", null],\n[\"angles\", \"Mangles\"],\n[\"\u2581se\", null],\n[\"\u2581charge\", null],\n[\"ait\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"en\", null],\n[\"\u2581faire\", \"make\"],\n[\"\u2581un\", null],\n[\"\u2581mar\", null],\n[\"in\", \"sailor\"],\n[\",\", null],\n[\"\u2581Glen\", null],\n[\"ar\", null],\n[\"van\", \"Glenarvan\"],\n[\"\u2581un\", null],\n[\"\u2581homme\", \"man\"],\n[\"\u2581de\", null],\n[\"\u2581co\", null],\n[\"eur\", \"heart\"],\n[\",\", null],\n[\"\u2581le\", null],\n[\"\u2581major\", \"major\"],\n[\"\u2581un\", null],\n[\"\u2581gar\", null],\n[\"\u00e7on\", \"boy\"],\n[\"\u2581de\", null],\n[\"\u2581sang\", \"cold\"],\n[\"-\", null],\n[\"f\", null],\n[\"roid\", null],\n[\",\", null],\n[\"\u2581lady\", \"Lady\"],\n[\"\u2581Hel\", null],\n[\"ena\", \"Helena\"],\n[\"\u2581un\", null],\n[\"\u2581\u00eatre\", \"being\"],\n[\"\u2581bon\", \"good\"],\n[\"\u2581et\", \"and\"],\n[\"\u2581g\u00e9n\", null],\n[\"\u00e9re\", null],\n[\"ux\", \"generous\"],\n[\",\", null],\n[\"\u2581Mary\", \"Mary\"],\n[\"\u2581Grant\", \"Grant\"],\n[\"\u2581un\", null],\n[\"\u2581\u00e9l\", null],\n[\"\u00e8ve\", \"student\"],\n[\"\u2581reconna\", null],\n[\"issant\", \"grateful\"],\n[\"\u2581en\", null],\n[\"vers\", null],\n[\"\u2581de\", null],\n[\"\u2581pare\", null],\n[\"ils\", \"such\"],\n[\"\u2581ma\", null],\n[\"\u00ee\", null],\n[\"tres\", \"teachers\"],\n[\",\", null],\n[\"\u2581Robert\", \"Robert\"],\n[\"\u2581dev\", null],\n[\"ait\", null],\n[\"\u2581\u00e9v\", null],\n[\"ide\", null],\n[\"m\", null],\n[\"ment\", \"obviously\"],\n[\"\u2581de\", null],\n[\"venir\", \"become\"],\n[\"\u2581un\", null],\n[\"\u2581jour\", \"day\"],\n[\"\u2581un\", null],\n[\"\u2581gentleman\", \"gentleman\"],\n[\"\u2581accomp\", null],\n[\"li\", \"accomplished\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\nOr, comme John Mangles se chargeait d\u2019en faire un marin, Glenarvan un homme de coeur, le major un gar\u00e7on de sang-froid, lady Helena un \u00eatre bon et g\u00e9n\u00e9reux, Mary Grant un \u00e9l\u00e8ve reconnaissant envers de pareils ma\u00eetres, Robert devait \u00e9videmment devenir un jour un gentleman accompli.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Or', ',', '\u2581comme', '\u2581John', '\u2581M', 'angles', '\u2581se', '\u2581charge', 'ait', '\u2581d', '\u2019', 'en', '\u2581faire', '\u2581un', '\u2581mar', 'in', ',', '\u2581Glen', 'ar', 'van', '\u2581un', '\u2581homme', '\u2581de', '\u2581co', 'eur', ',', '\u2581le', '\u2581major', '\u2581un', '\u2581gar', '\u00e7on', '\u2581de', '\u2581sang', '-', 'f', 'roid', ',', '\u2581lady', '\u2581Hel', 'ena', '\u2581un', '\u2581\u00eatre', '\u2581bon', '\u2581et', '\u2581g\u00e9n', '\u00e9re', 'ux', ',', '\u2581Mary', '\u2581Grant', '\u2581un', '\u2581\u00e9l', '\u00e8ve', '\u2581reconna', 'issant', '\u2581en', 'vers', '\u2581de', '\u2581pare', 'ils', '\u2581ma', '\u00ee', 'tres', ',', '\u2581Robert', '\u2581dev', 'ait', '\u2581\u00e9v', 'ide', 'm', 'ment', '\u2581de', 'venir', '\u2581un', '\u2581jour', '\u2581un', '\u2581gentleman', '\u2581accomp', 'li', '.']"}], "[\n[\"\u2581Or\", \"\u0418\u0442\u0430\u043a\"],\n[\",\", null],\n[\"\u2581comme\", null],\n[\"\u2581John\", \"\u0414\u0436\u043e\u043d\"],\n[\"\u2581M\", null],\n[\"angles\", \"\u041c\u044d\u043d\u0433\u043b\u0441\"],\n[\"\u2581se\", null],\n[\"\u2581charge\", null],\n[\"ait\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"en\", null],\n[\"\u2581faire\", \"\u0441\u0434\u0435\u043b\u0430\u0442\u044c\"],\n[\"\u2581un\", null],\n[\"\u2581mar\", null],\n[\"in\", \"\u043c\u043e\u0440\u044f\u043a\u043e\u043c\"],\n[\",\", null],\n[\"\u2581Glen\", null],\n[\"ar\", null],\n[\"van\", \"\u0413\u043b\u0435\u043d\u0430\u0440\u0432\u0430\u043d\"],\n[\"\u2581un\", null],\n[\"\u2581homme\", \"\u0447\u0435\u043b\u043e\u0432\u0435\u043a\u043e\u043c\"],\n[\"\u2581de\", null],\n[\"\u2581co\", null],\n[\"eur\", \"\u0441\u0435\u0440\u0434\u0446\u0430\"],\n[\",\", null],\n[\"\u2581le\", null],\n[\"\u2581major\", \"\u043c\u0430\u0439\u043e\u0440\"],\n[\"\u2581un\", null],\n[\"\u2581gar\", null],\n[\"\u00e7on\", \"\u043f\u0430\u0440\u043d\u0435\u043c\"],\n[\"\u2581de\", null],\n[\"\u2581sang\", null],\n[\"-\", null],\n[\"f\", null],\n[\"roid\", \"\u0445\u043b\u0430\u0434\u043d\u043e\u043a\u0440\u043e\u0432\u043d\u044b\u043c\"],\n[\",\", null],\n[\"\u2581lady\", \"\u043b\u0435\u0434\u0438\"],\n[\"\u2581Hel\", null],\n[\"ena\", \"\u042d\u043b\u0435\u043d\u0430\"],\n[\"\u2581un\", null],\n[\"\u2581\u00eatre\", \"\u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043e\u043c\"],\n[\"\u2581bon\", \"\u0434\u043e\u0431\u0440\u044b\u043c\"],\n[\"\u2581et\", \"\u0438\"],\n[\"\u2581g\u00e9n\", null],\n[\"\u00e9re\", null],\n[\"ux\", \"\u0449\u0435\u0434\u0440\u044b\u043c\"],\n[\",\", null],\n[\"\u2581Mary\", \"\u041c\u044d\u0440\u0438\"],\n[\"\u2581Grant\", \"\u0413\u0440\u0430\u043d\u0442\"],\n[\"\u2581un\", null],\n[\"\u2581\u00e9l\", null],\n[\"\u00e8ve\", \"\u0443\u0447\u0435\u043d\u0438\u043a\u043e\u043c\"],\n[\"\u2581reconna\", null],\n[\"issant\", \"\u0431\u043b\u0430\u0433\u043e\u0434\u0430\u0440\u043d\u044b\u043c\"],\n[\"\u2581en\", null],\n[\"vers\", \"\u043a\"],\n[\"\u2581de\", null],\n[\"\u2581pare\", null],\n[\"ils\", \"\u0442\u0430\u043a\u0438\u043c\"],\n[\"\u2581ma\", null],\n[\"\u00ee\", null],\n[\"tres\", \"\u0443\u0447\u0438\u0442\u0435\u043b\u044f\u043c\"],\n[\",\", null],\n[\"\u2581Robert\", \"\u0420\u043e\u0431\u0435\u0440\u0442\"],\n[\"\u2581dev\", null],\n[\"ait\", null],\n[\"\u2581\u00e9v\", null],\n[\"ide\", null],\n[\"m\", null],\n[\"ment\", \"\u043e\u0447\u0435\u0432\u0438\u0434\u043d\u043e\"],\n[\"\u2581de\", null],\n[\"venir\", \"\u0441\u0442\u0430\u0442\u044c\"],\n[\"\u2581un\", null],\n[\"\u2581jour\", \"\u043e\u0434\u043d\u0430\u0436\u0434\u044b\"],\n[\"\u2581un\", null],\n[\"\u2581gentleman\", \"\u0434\u0436\u0435\u043d\u0442\u043b\u044c\u043c\u0435\u043d\u043e\u043c\"],\n[\"\u2581accomp\", null],\n[\"li\", \"\u0441\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u043d\u044b\u043c\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to English.\n\n---=== TEXT ===---\n\n-- Au manoir de Ferndean, une propri\u00e9t\u00e9 qu'il poss\u00e8de \u00e0 trente milles d'ici \u00e0 peu pr\u00e8s; c'est un endroit tout \u00e0 fait d\u00e9sert.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581--', '\u2581Au', '\u2581man', 'oir', '\u2581de', '\u2581Fer', 'nde', 'an', ',', '\u2581une', '\u2581propri', '\u00e9t\u00e9', '\u2581qu', \"'\", 'il', '\u2581poss', '\u00e8de', '\u2581\u00e0', '\u2581tr', 'ente', '\u2581mill', 'es', '\u2581d', \"'\", 'ici', '\u2581\u00e0', '\u2581peu', '\u2581pr\u00e8s', ';', '\u2581c', \"'\", 'est', '\u2581un', '\u2581end', 'roit', '\u2581tout', '\u2581\u00e0', '\u2581fait', '\u2581d\u00e9', 'sert', '.']"}], "[\n[\"\u2581--\", null],\n[\"\u2581Au\", \"At\"],\n[\"\u2581man\", \"manor\"],\n[\"oir\", null],\n[\"\u2581de\", null],\n[\"\u2581Fer\", \"Ferndean\"],\n[\"nde\", null],\n[\"an\", null],\n[\",\", null],\n[\"\u2581une\", null],\n[\"\u2581propri\", \"property\"],\n[\"\u00e9t\u00e9\", null],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"il\", null],\n[\"\u2581poss\", \"owns\"],\n[\"\u00e8de\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581tr\", \"thirty\"],\n[\"ente\", null],\n[\"\u2581mill\", \"miles\"],\n[\"es\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"ici\", \"here\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581peu\", null],\n[\"\u2581pr\u00e8s\", \"about\"],\n[\";\", null],\n[\"\u2581c\", null],\n[\"'\", null],\n[\"est\", null],\n[\"\u2581un\", null],\n[\"\u2581end\", \"place\"],\n[\"roit\", null],\n[\"\u2581tout\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581fait\", \"completely\"],\n[\"\u2581d\u00e9\", \"deserted\"],\n[\"sert\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from French to Russian.\n\n---=== TEXT ===---\n\n-- Au manoir de Ferndean, une propri\u00e9t\u00e9 qu'il poss\u00e8de \u00e0 trente milles d'ici \u00e0 peu pr\u00e8s; c'est un endroit tout \u00e0 fait d\u00e9sert.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581--', '\u2581Au', '\u2581man', 'oir', '\u2581de', '\u2581Fer', 'nde', 'an', ',', '\u2581une', '\u2581propri', '\u00e9t\u00e9', '\u2581qu', \"'\", 'il', '\u2581poss', '\u00e8de', '\u2581\u00e0', '\u2581tr', 'ente', '\u2581mill', 'es', '\u2581d', \"'\", 'ici', '\u2581\u00e0', '\u2581peu', '\u2581pr\u00e8s', ';', '\u2581c', \"'\", 'est', '\u2581un', '\u2581end', 'roit', '\u2581tout', '\u2581\u00e0', '\u2581fait', '\u2581d\u00e9', 'sert', '.']"}], "[\n[\"\u2581--\", null],\n[\"\u2581Au\", \"\u0412\"],\n[\"\u2581man\", \"\u043f\u043e\u043c\u0435\u0441\u0442\u044c\u0435\"],\n[\"oir\", null],\n[\"\u2581de\", null],\n[\"\u2581Fer\", \"\u0424\u0435\u0440\u043d\u0434\u0435\u043d\"],\n[\"nde\", null],\n[\"an\", null],\n[\",\", null],\n[\"\u2581une\", null],\n[\"\u2581propri\", \"\u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0441\u0442\u044c\"],\n[\"\u00e9t\u00e9\", null],\n[\"\u2581qu\", null],\n[\"'\", null],\n[\"il\", null],\n[\"\u2581poss\", \"\u0432\u043b\u0430\u0434\u0435\u0435\u0442\"],\n[\"\u00e8de\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581tr\", \"\u0442\u0440\u0438\u0434\u0446\u0430\u0442\u044c\"],\n[\"ente\", null],\n[\"\u2581mill\", \"\u043c\u0438\u043b\u044c\"],\n[\"es\", null],\n[\"\u2581d\", null],\n[\"'\", null],\n[\"ici\", \"\u043e\u0442\u0441\u044e\u0434\u0430\"],\n[\"\u2581\u00e0\", null],\n[\"\u2581peu\", null],\n[\"\u2581pr\u00e8s\", \"\u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e\"],\n[\";\", null],\n[\"\u2581c\", null],\n[\"'\", null],\n[\"est\", null],\n[\"\u2581un\", null],\n[\"\u2581end\", \"\u043c\u0435\u0441\u0442\u043e\"],\n[\"roit\", null],\n[\"\u2581tout\", null],\n[\"\u2581\u00e0\", null],\n[\"\u2581fait\", \"\u0441\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u043d\u043e\"],\n[\"\u2581d\u00e9\", \"\u043f\u0443\u0441\u0442\u044b\u043d\u043d\u043e\u0435\"],\n[\"sert\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0422\u043e\u043b\u044c\u043a\u043e \u0432 \u0441\u0430\u043c\u043e\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435 \u0432\u0440\u0435\u043c\u044f, \u043f\u043e \u043f\u043e\u0432\u043e\u0434\u0443 \u0441\u0432\u043e\u0438\u0445 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0439 \u043a \u0410\u043d\u043d\u0435, \u0412\u0440\u043e\u043d\u0441\u043a\u0438\u0439 \u043d\u0430\u0447\u0438\u043d\u0430\u043b \u0447\u0443\u0432\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c, \u0447\u0442\u043e \u0441\u0432\u043e\u0434 \u0435\u0433\u043e \u043f\u0440\u0430\u0432\u0438\u043b \u043d\u0435 \u0432\u043f\u043e\u043b\u043d\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u043b \u0432\u0441\u0435 \u0443\u0441\u043b\u043e\u0432\u0438\u044f, \u0438 \u0432 \u0431\u0443\u0434\u0443\u0449\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u043b\u0438\u0441\u044c \u0442\u0440\u0443\u0434\u043d\u043e\u0441\u0442\u0438 \u0438 \u0441\u043e\u043c\u043d\u0435\u043d\u0438\u044f, \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0412\u0440\u043e\u043d\u0441\u043a\u0438\u0439 \u0443\u0436\u0435 \u043d\u0435 \u043d\u0430\u0445\u043e\u0434\u0438\u043b \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u044f\u0449\u0435\u0439 \u043d\u0438\u0442\u0438.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0422\u043e', '\u043b\u044c\u043a\u043e', '\u2581\u0432', '\u2581\u0441\u0430\u043c\u043e', '\u0435', '\u2581\u043f\u043e\u0441\u043b\u0435\u0434', '\u043d\u0435\u0435', '\u2581\u0432\u0440\u0435\u043c\u044f', ',', '\u2581\u043f\u043e', '\u2581\u043f', '\u043e\u0432\u043e', '\u0434\u0443', '\u2581\u0441\u0432\u043e\u0438\u0445', '\u2581\u043e\u0442\u043d\u043e', '\u0448\u0435', '\u043d\u0438\u0439', '\u2581\u043a', '\u2581\u0410\u043d', '\u043d\u0435', ',', '\u2581\u0412', '\u0440\u043e\u043d', '\u0441\u043a\u0438\u0439', '\u2581\u043d\u0430', '\u0447\u0438\u043d\u0430', '\u043b', '\u2581\u0447\u0443', '\u0432', '\u0441\u0442\u0432\u043e\u0432\u0430', '\u0442\u044c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0441\u0432\u043e', '\u0434', '\u2581\u0435\u0433\u043e', '\u2581\u043f\u0440\u0430\u0432\u0438', '\u043b', '\u2581\u043d\u0435', '\u2581\u0432', '\u043f\u043e\u043b', '\u043d\u0435', '\u2581\u043e\u043f\u0440\u0435\u0434\u0435', '\u043b\u044f', '\u043b', '\u2581\u0432\u0441\u0435', '\u2581\u0443\u0441', '\u043b\u043e\u0432\u0438', '\u044f', ',', '\u2581\u0438', '\u2581\u0432', '\u2581\u0431\u0443\u0434\u0443', '\u0449\u0435', '\u043c', '\u2581\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432', '\u043b\u044f', '\u043b\u0438\u0441\u044c', '\u2581\u0442', '\u0440\u0443\u0434', '\u043d\u043e\u0441\u0442\u0438', '\u2581\u0438', '\u2581\u0441\u043e', '\u043c', '\u043d\u0435\u043d\u0438\u044f', ',', '\u2581\u0432', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0445', '\u2581\u0412', '\u0440\u043e\u043d', '\u0441\u043a\u0438\u0439', '\u2581\u0443\u0436\u0435', '\u2581\u043d\u0435', '\u2581\u043d\u0430\u0445\u043e\u0434\u0438', '\u043b', '\u2581\u0440\u0443\u043a\u043e\u0432\u043e', '\u0434\u044f', '\u0449\u0435', '\u0439', '\u2581\u043d\u0438', '\u0442\u0438', '.']"}], "[\n  [\"\u2581\u0422\u043e\", \"Only\"],\n  [\"\u043b\u044c\u043a\u043e\", null],\n  [\"\u2581\u0432\", null],\n  [\"\u2581\u0441\u0430\u043c\u043e\", \"recent\"],\n  [\"\u0435\", null],\n  [\"\u2581\u043f\u043e\u0441\u043b\u0435\u0434\", null],\n  [\"\u043d\u0435\u0435\", null],\n  [\"\u2581\u0432\u0440\u0435\u043c\u044f\", \"time\"],\n  [\",\", null],\n  [\"\u2581\u043f\u043e\", null],\n  [\"\u2581\u043f\", null],\n  [\"\u043e\u0432\u043e\", null],\n  [\"\u0434\u0443\", null],\n  [\"\u2581\u0441\u0432\u043e\u0438\u0445\", \"his\"],\n  [\"\u2581\u043e\u0442\u043d\u043e\", \"relationship\"],\n  [\"\u0448\u0435\", null],\n  [\"\u043d\u0438\u0439\", null],\n  [\"\u2581\u043a\", null],\n  [\"\u2581\u0410\u043d\", \"Anna\"],\n  [\"\u043d\u0435\", null],\n  [\",\", null],\n  [\"\u2581\u0412\", \"Vronsky\"],\n  [\"\u0440\u043e\u043d\", null],\n  [\"\u0441\u043a\u0438\u0439\", null],\n  [\"\u2581\u043d\u0430\", \"began\"],\n  [\"\u0447\u0438\u043d\u0430\", null],\n  [\"\u043b\", null],\n  [\"\u2581\u0447\u0443\", \"feel\"],\n  [\"\u0432\", null],\n  [\"\u0441\u0442\u0432\u043e\u0432\u0430\", null],\n  [\"\u0442\u044c\", null],\n  [\",\", null],\n  [\"\u2581\u0447\u0442\u043e\", \"that\"],\n  [\"\u2581\u0441\u0432\u043e\", \"code\"],\n  [\"\u0434\", null],\n  [\"\u2581\u0435\u0433\u043e\", \"his\"],\n  [\"\u2581\u043f\u0440\u0430\u0432\u0438\", \"rules\"],\n  [\"\u043b\", null],\n  [\"\u2581\u043d\u0435\", \"not\"],\n  [\"\u2581\u0432\", null],\n  [\"\u043f\u043e\u043b\", null],\n  [\"\u043d\u0435\", null],\n  [\"\u2581\u043e\u043f\u0440\u0435\u0434\u0435\", \"define\"],\n  [\"\u043b\u044f\", null],\n  [\"\u043b\", null],\n  [\"\u2581\u0432\u0441\u0435\", \"all\"],\n  [\"\u2581\u0443\u0441\", \"conditions\"],\n  [\"\u043b\u043e\u0432\u0438\", null],\n  [\"\u044f\", null],\n  [\",\", null],\n  [\"\u2581\u0438\", \"and\"],\n  [\"\u2581\u0432\", \"in\"],\n  [\"\u2581\u0431\u0443\u0434\u0443\", \"future\"],\n  [\"\u0449\u0435\", null],\n  [\"\u043c\", null],\n  [\"\u2581\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\", \"appeared\"],\n  [\"\u043b\u044f\", null],\n  [\"\u043b\u0438\u0441\u044c\", null],\n  [\"\u2581\u0442\", \"difficulties\"],\n  [\"\u0440\u0443\u0434\", null],\n  [\"\u043d\u043e\u0441\u0442\u0438\", null],\n  [\"\u2581\u0438\", \"and\"],\n  [\"\u2581\u0441\u043e\", \"doubts\"],\n  [\"\u043c\", null],\n  [\"\u043d\u0435\u043d\u0438\u044f\", null],\n  [\",\", null],\n  [\"\u2581\u0432\", \"in\"],\n  [\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0445\", \"which\"],\n  [\"\u2581\u0412\", \"Vronsky\"],\n  [\"\u0440\u043e\u043d\", null],\n  [\"\u0441\u043a\u0438\u0439\", null],\n  [\"\u2581\u0443\u0436\u0435\", \"already\"],\n  [\"\u2581\u043d\u0435\", \"not\"],\n  [\"\u2581\u043d\u0430\u0445\u043e\u0434\u0438\", \"find\"],\n  [\"\u043b\", null],\n  [\"\u2581\u0440\u0443\u043a\u043e\u0432\u043e\", \"guiding\"],\n  [\"\u0434\u044f\", null],\n  [\"\u0449\u0435\", null],\n  [\"\u0439\", null],\n  [\"\u2581\u043d\u0438\", \"thread\"],\n  [\"\u0442\u0438\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u0422\u043e\u043b\u044c\u043a\u043e \u0432 \u0441\u0430\u043c\u043e\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435 \u0432\u0440\u0435\u043c\u044f, \u043f\u043e \u043f\u043e\u0432\u043e\u0434\u0443 \u0441\u0432\u043e\u0438\u0445 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0439 \u043a \u0410\u043d\u043d\u0435, \u0412\u0440\u043e\u043d\u0441\u043a\u0438\u0439 \u043d\u0430\u0447\u0438\u043d\u0430\u043b \u0447\u0443\u0432\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c, \u0447\u0442\u043e \u0441\u0432\u043e\u0434 \u0435\u0433\u043e \u043f\u0440\u0430\u0432\u0438\u043b \u043d\u0435 \u0432\u043f\u043e\u043b\u043d\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u043b \u0432\u0441\u0435 \u0443\u0441\u043b\u043e\u0432\u0438\u044f, \u0438 \u0432 \u0431\u0443\u0434\u0443\u0449\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u043b\u0438\u0441\u044c \u0442\u0440\u0443\u0434\u043d\u043e\u0441\u0442\u0438 \u0438 \u0441\u043e\u043c\u043d\u0435\u043d\u0438\u044f, \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0412\u0440\u043e\u043d\u0441\u043a\u0438\u0439 \u0443\u0436\u0435 \u043d\u0435 \u043d\u0430\u0445\u043e\u0434\u0438\u043b \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u044f\u0449\u0435\u0439 \u043d\u0438\u0442\u0438.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0422\u043e', '\u043b\u044c\u043a\u043e', '\u2581\u0432', '\u2581\u0441\u0430\u043c\u043e', '\u0435', '\u2581\u043f\u043e\u0441\u043b\u0435\u0434', '\u043d\u0435\u0435', '\u2581\u0432\u0440\u0435\u043c\u044f', ',', '\u2581\u043f\u043e', '\u2581\u043f', '\u043e\u0432\u043e', '\u0434\u0443', '\u2581\u0441\u0432\u043e\u0438\u0445', '\u2581\u043e\u0442\u043d\u043e', '\u0448\u0435', '\u043d\u0438\u0439', '\u2581\u043a', '\u2581\u0410\u043d', '\u043d\u0435', ',', '\u2581\u0412', '\u0440\u043e\u043d', '\u0441\u043a\u0438\u0439', '\u2581\u043d\u0430', '\u0447\u0438\u043d\u0430', '\u043b', '\u2581\u0447\u0443', '\u0432', '\u0441\u0442\u0432\u043e\u0432\u0430', '\u0442\u044c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0441\u0432\u043e', '\u0434', '\u2581\u0435\u0433\u043e', '\u2581\u043f\u0440\u0430\u0432\u0438', '\u043b', '\u2581\u043d\u0435', '\u2581\u0432', '\u043f\u043e\u043b', '\u043d\u0435', '\u2581\u043e\u043f\u0440\u0435\u0434\u0435', '\u043b\u044f', '\u043b', '\u2581\u0432\u0441\u0435', '\u2581\u0443\u0441', '\u043b\u043e\u0432\u0438', '\u044f', ',', '\u2581\u0438', '\u2581\u0432', '\u2581\u0431\u0443\u0434\u0443', '\u0449\u0435', '\u043c', '\u2581\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432', '\u043b\u044f', '\u043b\u0438\u0441\u044c', '\u2581\u0442', '\u0440\u0443\u0434', '\u043d\u043e\u0441\u0442\u0438', '\u2581\u0438', '\u2581\u0441\u043e', '\u043c', '\u043d\u0435\u043d\u0438\u044f', ',', '\u2581\u0432', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0445', '\u2581\u0412', '\u0440\u043e\u043d', '\u0441\u043a\u0438\u0439', '\u2581\u0443\u0436\u0435', '\u2581\u043d\u0435', '\u2581\u043d\u0430\u0445\u043e\u0434\u0438', '\u043b', '\u2581\u0440\u0443\u043a\u043e\u0432\u043e', '\u0434\u044f', '\u0449\u0435', '\u0439', '\u2581\u043d\u0438', '\u0442\u0438', '.']"}], "[\n[\"\u2581\u0422\u043e\", \"Seulement\"],\n[\"\u043b\u044c\u043a\u043e\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0441\u0430\u043c\u043e\", \"r\u00e9cent\"],\n[\"\u0435\", null],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\u0434\", \"dernier\"],\n[\"\u043d\u0435\u0435\", null],\n[\"\u2581\u0432\u0440\u0435\u043c\u044f\", \"temps\"],\n[\",\", null],\n[\"\u2581\u043f\u043e\", null],\n[\"\u2581\u043f\", \"propos\"],\n[\"\u043e\u0432\u043e\", null],\n[\"\u0434\u0443\", null],\n[\"\u2581\u0441\u0432\u043e\u0438\u0445\", \"ses\"],\n[\"\u2581\u043e\u0442\u043d\u043e\", \"relations\"],\n[\"\u0448\u0435\", null],\n[\"\u043d\u0438\u0439\", null],\n[\"\u2581\u043a\", \"avec\"],\n[\"\u2581\u0410\u043d\", \"Anna\"],\n[\"\u043d\u0435\", null],\n[\",\", null],\n[\"\u2581\u0412\", \"Vronsky\"],\n[\"\u0440\u043e\u043d\", null],\n[\"\u0441\u043a\u0438\u0439\", null],\n[\"\u2581\u043d\u0430\", \"commen\u00e7ait\"],\n[\"\u0447\u0438\u043d\u0430\", null],\n[\"\u043b\", null],\n[\"\u2581\u0447\u0443\", \"sentir\"],\n[\"\u0432\", null],\n[\"\u0441\u0442\u0432\u043e\u0432\u0430\", null],\n[\"\u0442\u044c\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", \"que\"],\n[\"\u2581\u0441\u0432\u043e\", \"ensemble\"],\n[\"\u0434\", null],\n[\"\u2581\u0435\u0433\u043e\", \"ses\"],\n[\"\u2581\u043f\u0440\u0430\u0432\u0438\", \"r\u00e8gles\"],\n[\"\u043b\", null],\n[\"\u2581\u043d\u0435\", \"ne\"],\n[\"\u2581\u0432\", null],\n[\"\u043f\u043e\u043b\", \"d\u00e9finissait\"],\n[\"\u043d\u0435\", null],\n[\"\u2581\u043e\u043f\u0440\u0435\u0434\u0435\", null],\n[\"\u043b\u044f\", null],\n[\"\u043b\", null],\n[\"\u2581\u0432\u0441\u0435\", \"toutes\"],\n[\"\u2581\u0443\u0441\", \"conditions\"],\n[\"\u043b\u043e\u0432\u0438\", null],\n[\"\u044f\", null],\n[\",\", null],\n[\"\u2581\u0438\", \"et\"],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0431\u0443\u0434\u0443\", \"futur\"],\n[\"\u0449\u0435\", null],\n[\"\u043c\", null],\n[\"\u2581\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\", \"apparaissaient\"],\n[\"\u043b\u044f\", null],\n[\"\u043b\u0438\u0441\u044c\", null],\n[\"\u2581\u0442\", \"difficult\u00e9s\"],\n[\"\u0440\u0443\u0434\", null],\n[\"\u043d\u043e\u0441\u0442\u0438\", null],\n[\"\u2581\u0438\", \"et\"],\n[\"\u2581\u0441\u043e\", \"doutes\"],\n[\"\u043c\", null],\n[\"\u043d\u0435\u043d\u0438\u044f\", null],\n[\",\", null],\n[\"\u2581\u0432\", \"dans\"],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0445\", \"lesquels\"],\n[\"\u2581\u0412\", \"Vronsky\"],\n[\"\u0440\u043e\u043d\", null],\n[\"\u0441\u043a\u0438\u0439\", null],\n[\"\u2581\u0443\u0436\u0435\", \"ne\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u043d\u0430\u0445\u043e\u0434\u0438\", \"trouvait\"],\n[\"\u043b\", null],\n[\"\u2581\u0440\u0443\u043a\u043e\u0432\u043e\", \"fil\"],\n[\"\u0434\u044f\", null],\n[\"\u0449\u0435\", null],\n[\"\u0439\", null],\n[\"\u2581\u043d\u0438\", \"conducteur\"],\n[\"\u0442\u0438\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\nPardon, -- \u043f\u0440\u0438\u0431\u0430\u0432\u0438\u043b\u0430 \u043e\u043d\u0430, \u0433\u043b\u044f\u0434\u044f \u043d\u0430 \u043e\u043f\u044f\u0442\u044c \u0432\u043e\u0448\u0435\u0434\u0448\u0435\u0433\u043e \u0441 \u0434\u0440\u0443\u0433\u043e\u0439 \u0437\u0430\u043f\u0438\u0441\u043a\u043e\u0439 \u043b\u0430\u043a\u0435\u044f.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581P', 'ardon', ',', '\u2581--', '\u2581\u043f\u0440\u0438', '\u0431\u0430', '\u0432\u0438', '\u043b\u0430', '\u2581\u043e\u043d\u0430', ',', '\u2581\u0433', '\u043b\u044f', '\u0434\u044f', '\u2581\u043d\u0430', '\u2581\u043e\u043f', '\u044f', '\u0442\u044c', '\u2581\u0432\u043e', '\u0448\u0435', '\u0434', '\u0448\u0435\u0433\u043e', '\u2581\u0441', '\u2581\u0434\u0440\u0443\u0433\u043e', '\u0439', '\u2581\u0437\u0430', '\u043f\u0438', '\u0441\u043a\u043e\u0439', '\u2581\u043b\u0430', '\u043a\u0435', '\u044f', '.']"}], "[\n  [\"\u2581P\", \"Pardon\"],\n  [\"ardon\", null],\n  [\",\", null],\n  [\"\u2581--\", null],\n  [\"\u2581\u043f\u0440\u0438\", \"added\"],\n  [\"\u0431\u0430\", null],\n  [\"\u0432\u0438\", null],\n  [\"\u043b\u0430\", null],\n  [\"\u2581\u043e\u043d\u0430\", \"she\"],\n  [\",\", null],\n  [\"\u2581\u0433\", \"looking\"],\n  [\"\u043b\u044f\", null],\n  [\"\u0434\u044f\", null],\n  [\"\u2581\u043d\u0430\", \"at\"],\n  [\"\u2581\u043e\u043f\", \"again\"],\n  [\"\u044f\", null],\n  [\"\u0442\u044c\", null],\n  [\"\u2581\u0432\u043e\", \"entered\"],\n  [\"\u0448\u0435\", null],\n  [\"\u0434\", null],\n  [\"\u0448\u0435\u0433\u043e\", null],\n  [\"\u2581\u0441\", \"with\"],\n  [\"\u2581\u0434\u0440\u0443\u0433\u043e\", \"another\"],\n  [\"\u0439\", null],\n  [\"\u2581\u0437\u0430\", \"note\"],\n  [\"\u043f\u0438\", null],\n  [\"\u0441\u043a\u043e\u0439\", null],\n  [\"\u2581\u043b\u0430\", \"servant\"],\n  [\"\u043a\u0435\", null],\n  [\"\u044f\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\nPardon, -- \u043f\u0440\u0438\u0431\u0430\u0432\u0438\u043b\u0430 \u043e\u043d\u0430, \u0433\u043b\u044f\u0434\u044f \u043d\u0430 \u043e\u043f\u044f\u0442\u044c \u0432\u043e\u0448\u0435\u0434\u0448\u0435\u0433\u043e \u0441 \u0434\u0440\u0443\u0433\u043e\u0439 \u0437\u0430\u043f\u0438\u0441\u043a\u043e\u0439 \u043b\u0430\u043a\u0435\u044f.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581P', 'ardon', ',', '\u2581--', '\u2581\u043f\u0440\u0438', '\u0431\u0430', '\u0432\u0438', '\u043b\u0430', '\u2581\u043e\u043d\u0430', ',', '\u2581\u0433', '\u043b\u044f', '\u0434\u044f', '\u2581\u043d\u0430', '\u2581\u043e\u043f', '\u044f', '\u0442\u044c', '\u2581\u0432\u043e', '\u0448\u0435', '\u0434', '\u0448\u0435\u0433\u043e', '\u2581\u0441', '\u2581\u0434\u0440\u0443\u0433\u043e', '\u0439', '\u2581\u0437\u0430', '\u043f\u0438', '\u0441\u043a\u043e\u0439', '\u2581\u043b\u0430', '\u043a\u0435', '\u044f', '.']"}], "[\n[\"\u2581P\", \"Pardon\"],\n[\"ardon\", null],\n[\",\", null],\n[\"\u2581--\", null],\n[\"\u2581\u043f\u0440\u0438\", \"ajouta\"],\n[\"\u0431\u0430\", null],\n[\"\u0432\u0438\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\u043e\u043d\u0430\", \"elle\"],\n[\",\", null],\n[\"\u2581\u0433\", \"regardant\"],\n[\"\u043b\u044f\", null],\n[\"\u0434\u044f\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u043e\u043f\", \"encore\"],\n[\"\u044f\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0432\u043e\", \"entr\u00e9\"],\n[\"\u0448\u0435\", null],\n[\"\u0434\", null],\n[\"\u0448\u0435\u0433\u043e\", null],\n[\"\u2581\u0441\", \"avec\"],\n[\"\u2581\u0434\u0440\u0443\u0433\u043e\", \"autre\"],\n[\"\u0439\", null],\n[\"\u2581\u0437\u0430\", \"note\"],\n[\"\u043f\u0438\", null],\n[\"\u0441\u043a\u043e\u0439\", null],\n[\"\u2581\u043b\u0430\", \"valet\"],\n[\"\u043a\u0435\", null],\n[\"\u044f\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0422\u0443\u0442 \u0431\u044b\u043b\u0430 \u0438 \u043f\u0440\u043e\u0441\u044c\u0431\u0430 \u043e \u043f\u0440\u043e\u0449\u0435\u043d\u0438\u0438, \u0438 \u0434\u043e\u0432\u0435\u0440\u0438\u0435 \u043a \u043d\u0435\u043c\u0443, \u0438 \u043b\u0430\u0441\u043a\u0430, \u043d\u0435\u0436\u043d\u0430\u044f, \u0440\u043e\u0431\u043a\u0430\u044f \u043b\u0430\u0441\u043a\u0430, \u0438 \u043e\u0431\u0435\u0449\u0430\u043d\u0438\u0435, \u0438 \u043d\u0430\u0434\u0435\u0436\u0434\u0430, \u0438 \u043b\u044e\u0431\u043e\u0432\u044c \u043a \u043d\u0435\u043c\u0443, \u0432 \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043e\u043d \u043d\u0435 \u043c\u043e\u0433 \u043d\u0435 \u0432\u0435\u0440\u0438\u0442\u044c \u0438 \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0434\u0443\u0448\u0438\u043b\u0430 \u0435\u0433\u043e \u0441\u0447\u0430\u0441\u0442\u044c\u0435\u043c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0422', '\u0443\u0442', '\u2581\u0431\u044b\u043b\u0430', '\u2581\u0438', '\u2581\u043f\u0440\u043e', '\u0441\u044c', '\u0431\u0430', '\u2581\u043e', '\u2581\u043f\u0440\u043e', '\u0449\u0435', '\u043d\u0438\u0438', ',', '\u2581\u0438', '\u2581\u0434', '\u043e\u0432\u0435', '\u0440\u0438', '\u0435', '\u2581\u043a', '\u2581\u043d\u0435', '\u043c\u0443', ',', '\u2581\u0438', '\u2581\u043b\u0430', '\u0441\u043a\u0430', ',', '\u2581\u043d\u0435', '\u0436', '\u043d\u0430\u044f', ',', '\u2581\u0440\u043e', '\u0431', '\u043a\u0430\u044f', '\u2581\u043b\u0430', '\u0441\u043a\u0430', ',', '\u2581\u0438', '\u2581\u043e\u0431', '\u0435', '\u0449\u0430', '\u043d\u0438\u0435', ',', '\u2581\u0438', '\u2581\u043d\u0430', '\u0434\u0435', '\u0436\u0434\u0430', ',', '\u2581\u0438', '\u2581\u043b\u044e', '\u0431\u043e\u0432', '\u044c', '\u2581\u043a', '\u2581\u043d\u0435', '\u043c\u0443', ',', '\u2581\u0432', '\u2581\u043a\u043e\u0442\u043e', '\u0440\u0443', '\u044e', '\u2581\u043e\u043d', '\u2581\u043d\u0435', '\u2581\u043c\u043e', '\u0433', '\u2581\u043d\u0435', '\u2581\u0432\u0435', '\u0440\u0438', '\u0442\u044c', '\u2581\u0438', '\u2581\u043a\u043e\u0442\u043e\u0440\u0430\u044f', '\u2581\u0434\u0443', '\u0448\u0438', '\u043b\u0430', '\u2581\u0435\u0433\u043e', '\u2581\u0441', '\u0447\u0430', '\u0441\u0442\u044c', '\u0435\u043c', '.']"}], "[\n[\"\u2581\u0422\", \"Here\"],\n[\"\u0443\u0442\", null],\n[\"\u2581\u0431\u044b\u043b\u0430\", \"was\"],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043f\u0440\u043e\", \"forgiveness\"],\n[\"\u0441\u044c\", null],\n[\"\u0431\u0430\", null],\n[\"\u2581\u043e\", null],\n[\"\u2581\u043f\u0440\u043e\", null],\n[\"\u0449\u0435\", null],\n[\"\u043d\u0438\u0438\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0434\", \"trust\"],\n[\"\u043e\u0432\u0435\", null],\n[\"\u0440\u0438\", null],\n[\"\u0435\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u043d\u0435\", \"him\"],\n[\"\u043c\u0443\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043b\u0430\", \"tenderness\"],\n[\"\u0441\u043a\u0430\", null],\n[\",\", null],\n[\"\u2581\u043d\u0435\", \"gentle\"],\n[\"\u0436\", null],\n[\"\u043d\u0430\u044f\", null],\n[\",\", null],\n[\"\u2581\u0440\u043e\", \"timid\"],\n[\"\u0431\", null],\n[\"\u043a\u0430\u044f\", null],\n[\"\u2581\u043b\u0430\", \"tenderness\"],\n[\"\u0441\u043a\u0430\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043e\u0431\", \"promise\"],\n[\"\u0435\", null],\n[\"\u0449\u0430\", null],\n[\"\u043d\u0438\u0435\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043d\u0430\", \"hope\"],\n[\"\u0434\u0435\", null],\n[\"\u0436\u0434\u0430\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043b\u044e\", \"love\"],\n[\"\u0431\u043e\u0432\", null],\n[\"\u044c\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u043d\u0435\", \"him\"],\n[\"\u043c\u0443\", null],\n[\",\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\", \"which\"],\n[\"\u0440\u0443\", null],\n[\"\u044e\", null],\n[\"\u2581\u043e\u043d\", \"he\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u043c\u043e\", \"could\"],\n[\"\u0433\", null],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0432\u0435\", \"believe\"],\n[\"\u0440\u0438\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u0430\u044f\", \"which\"],\n[\"\u2581\u0434\u0443\", \"overwhelmed\"],\n[\"\u0448\u0438\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\u0435\u0433\u043e\", \"him\"],\n[\"\u2581\u0441\", \"with\"],\n[\"\u0447\u0430\", \"happiness\"],\n[\"\u0441\u0442\u044c\", null],\n[\"\u0435\u043c\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u0422\u0443\u0442 \u0431\u044b\u043b\u0430 \u0438 \u043f\u0440\u043e\u0441\u044c\u0431\u0430 \u043e \u043f\u0440\u043e\u0449\u0435\u043d\u0438\u0438, \u0438 \u0434\u043e\u0432\u0435\u0440\u0438\u0435 \u043a \u043d\u0435\u043c\u0443, \u0438 \u043b\u0430\u0441\u043a\u0430, \u043d\u0435\u0436\u043d\u0430\u044f, \u0440\u043e\u0431\u043a\u0430\u044f \u043b\u0430\u0441\u043a\u0430, \u0438 \u043e\u0431\u0435\u0449\u0430\u043d\u0438\u0435, \u0438 \u043d\u0430\u0434\u0435\u0436\u0434\u0430, \u0438 \u043b\u044e\u0431\u043e\u0432\u044c \u043a \u043d\u0435\u043c\u0443, \u0432 \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043e\u043d \u043d\u0435 \u043c\u043e\u0433 \u043d\u0435 \u0432\u0435\u0440\u0438\u0442\u044c \u0438 \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0434\u0443\u0448\u0438\u043b\u0430 \u0435\u0433\u043e \u0441\u0447\u0430\u0441\u0442\u044c\u0435\u043c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0422', '\u0443\u0442', '\u2581\u0431\u044b\u043b\u0430', '\u2581\u0438', '\u2581\u043f\u0440\u043e', '\u0441\u044c', '\u0431\u0430', '\u2581\u043e', '\u2581\u043f\u0440\u043e', '\u0449\u0435', '\u043d\u0438\u0438', ',', '\u2581\u0438', '\u2581\u0434', '\u043e\u0432\u0435', '\u0440\u0438', '\u0435', '\u2581\u043a', '\u2581\u043d\u0435', '\u043c\u0443', ',', '\u2581\u0438', '\u2581\u043b\u0430', '\u0441\u043a\u0430', ',', '\u2581\u043d\u0435', '\u0436', '\u043d\u0430\u044f', ',', '\u2581\u0440\u043e', '\u0431', '\u043a\u0430\u044f', '\u2581\u043b\u0430', '\u0441\u043a\u0430', ',', '\u2581\u0438', '\u2581\u043e\u0431', '\u0435', '\u0449\u0430', '\u043d\u0438\u0435', ',', '\u2581\u0438', '\u2581\u043d\u0430', '\u0434\u0435', '\u0436\u0434\u0430', ',', '\u2581\u0438', '\u2581\u043b\u044e', '\u0431\u043e\u0432', '\u044c', '\u2581\u043a', '\u2581\u043d\u0435', '\u043c\u0443', ',', '\u2581\u0432', '\u2581\u043a\u043e\u0442\u043e', '\u0440\u0443', '\u044e', '\u2581\u043e\u043d', '\u2581\u043d\u0435', '\u2581\u043c\u043e', '\u0433', '\u2581\u043d\u0435', '\u2581\u0432\u0435', '\u0440\u0438', '\u0442\u044c', '\u2581\u0438', '\u2581\u043a\u043e\u0442\u043e\u0440\u0430\u044f', '\u2581\u0434\u0443', '\u0448\u0438', '\u043b\u0430', '\u2581\u0435\u0433\u043e', '\u2581\u0441', '\u0447\u0430', '\u0441\u0442\u044c', '\u0435\u043c', '.']"}], "[\n[\"\u2581\u0422\", \"Ici\"],\n[\"\u0443\u0442\", null],\n[\"\u2581\u0431\u044b\u043b\u0430\", \"\u00e9tait\"],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043f\u0440\u043e\", \"demande\"],\n[\"\u0441\u044c\", null],\n[\"\u0431\u0430\", null],\n[\"\u2581\u043e\", null],\n[\"\u2581\u043f\u0440\u043e\", \"pardon\"],\n[\"\u0449\u0435\", null],\n[\"\u043d\u0438\u0438\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0434\", \"confiance\"],\n[\"\u043e\u0432\u0435\", null],\n[\"\u0440\u0438\", null],\n[\"\u0435\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u043d\u0435\", \"lui\"],\n[\"\u043c\u0443\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043b\u0430\", \"caresse\"],\n[\"\u0441\u043a\u0430\", null],\n[\",\", null],\n[\"\u2581\u043d\u0435\", \"tendre\"],\n[\"\u0436\", null],\n[\"\u043d\u0430\u044f\", null],\n[\",\", null],\n[\"\u2581\u0440\u043e\", \"timide\"],\n[\"\u0431\", null],\n[\"\u043a\u0430\u044f\", null],\n[\"\u2581\u043b\u0430\", \"caresse\"],\n[\"\u0441\u043a\u0430\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043e\u0431\", \"promesse\"],\n[\"\u0435\", null],\n[\"\u0449\u0430\", null],\n[\"\u043d\u0438\u0435\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043d\u0430\", \"espoir\"],\n[\"\u0434\u0435\", null],\n[\"\u0436\u0434\u0430\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043b\u044e\", \"amour\"],\n[\"\u0431\u043e\u0432\", null],\n[\"\u044c\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u043d\u0435\", \"lui\"],\n[\"\u043c\u0443\", null],\n[\",\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\", \"laquelle\"],\n[\"\u0440\u0443\", null],\n[\"\u044e\", null],\n[\"\u2581\u043e\u043d\", \"il\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u043c\u043e\", \"pouvait\"],\n[\"\u0433\", null],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0432\u0435\", \"croire\"],\n[\"\u0440\u0438\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u0430\u044f\", \"qui\"],\n[\"\u2581\u0434\u0443\", \"\u00e9touffait\"],\n[\"\u0448\u0438\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\u0435\u0433\u043e\", \"le\"],\n[\"\u2581\u0441\", \"bonheur\"],\n[\"\u0447\u0430\", null],\n[\"\u0441\u0442\u044c\", null],\n[\"\u0435\u043c\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0421\u0443\u0445\u043e\u0441\u0442\u044c \u044d\u0442\u0430 \u043e\u0433\u043e\u0440\u0447\u0438\u043b\u0430 \u041a\u0438\u0442\u0438, \u0438 \u043e\u043d\u0430 \u043d\u0435 \u043c\u043e\u0433\u043b\u0430 \u0443\u0434\u0435\u0440\u0436\u0430\u0442\u044c\u0441\u044f \u043e\u0442 \u0436\u0435\u043b\u0430\u043d\u0438\u044f \u0437\u0430\u0433\u043b\u0430\u0434\u0438\u0442\u044c \u0445\u043e\u043b\u043e\u0434\u043d\u043e\u0441\u0442\u044c \u043c\u0430\u0442\u0435\u0440\u0438.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0421\u0443', '\u0445\u043e', '\u0441\u0442\u044c', '\u2581\u044d\u0442\u0430', '\u2581\u043e', '\u0433\u043e\u0440', '\u0447\u0438', '\u043b\u0430', '\u2581\u041a\u0438', '\u0442\u0438', ',', '\u2581\u0438', '\u2581\u043e\u043d\u0430', '\u2581\u043d\u0435', '\u2581\u043c\u043e', '\u0433\u043b\u0430', '\u2581\u0443', '\u0434\u0435\u0440\u0436\u0430', '\u0442\u044c\u0441\u044f', '\u2581\u043e\u0442', '\u2581\u0436\u0435', '\u043b\u0430', '\u043d\u0438\u044f', '\u2581\u0437\u0430', '\u0433\u043b\u0430', '\u0434\u0438', '\u0442\u044c', '\u2581\u0445\u043e', '\u043b\u043e', '\u0434', '\u043d\u043e\u0441\u0442\u044c', '\u2581\u043c\u0430\u0442\u0435\u0440\u0438', '.']"}], "[\n  [\"\u2581\u0421\u0443\", \"Dryness\"],\n  [\"\u0445\u043e\", null],\n  [\"\u0441\u0442\u044c\", null],\n  [\"\u2581\u044d\u0442\u0430\", \"this\"],\n  [\"\u2581\u043e\", \"upset\"],\n  [\"\u0433\u043e\u0440\", null],\n  [\"\u0447\u0438\", null],\n  [\"\u043b\u0430\", null],\n  [\"\u2581\u041a\u0438\", \"Kitty\"],\n  [\"\u0442\u0438\", null],\n  [\",\", null],\n  [\"\u2581\u0438\", \"and\"],\n  [\"\u2581\u043e\u043d\u0430\", \"she\"],\n  [\"\u2581\u043d\u0435\", \"could\"],\n  [\"\u2581\u043c\u043e\", null],\n  [\"\u0433\u043b\u0430\", null],\n  [\"\u2581\u0443\", \"not\"],\n  [\"\u0434\u0435\u0440\u0436\u0430\", \"resist\"],\n  [\"\u0442\u044c\u0441\u044f\", null],\n  [\"\u2581\u043e\u0442\", \"the\"],\n  [\"\u2581\u0436\u0435\", \"desire\"],\n  [\"\u043b\u0430\", null],\n  [\"\u043d\u0438\u044f\", null],\n  [\"\u2581\u0437\u0430\", \"to\"],\n  [\"\u0433\u043b\u0430\", \"smooth\"],\n  [\"\u0434\u0438\", null],\n  [\"\u0442\u044c\", null],\n  [\"\u2581\u0445\u043e\", \"coldness\"],\n  [\"\u043b\u043e\", null],\n  [\"\u0434\", null],\n  [\"\u043d\u043e\u0441\u0442\u044c\", null],\n  [\"\u2581\u043c\u0430\u0442\u0435\u0440\u0438\", \"mother's\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u0421\u0443\u0445\u043e\u0441\u0442\u044c \u044d\u0442\u0430 \u043e\u0433\u043e\u0440\u0447\u0438\u043b\u0430 \u041a\u0438\u0442\u0438, \u0438 \u043e\u043d\u0430 \u043d\u0435 \u043c\u043e\u0433\u043b\u0430 \u0443\u0434\u0435\u0440\u0436\u0430\u0442\u044c\u0441\u044f \u043e\u0442 \u0436\u0435\u043b\u0430\u043d\u0438\u044f \u0437\u0430\u0433\u043b\u0430\u0434\u0438\u0442\u044c \u0445\u043e\u043b\u043e\u0434\u043d\u043e\u0441\u0442\u044c \u043c\u0430\u0442\u0435\u0440\u0438.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0421\u0443', '\u0445\u043e', '\u0441\u0442\u044c', '\u2581\u044d\u0442\u0430', '\u2581\u043e', '\u0433\u043e\u0440', '\u0447\u0438', '\u043b\u0430', '\u2581\u041a\u0438', '\u0442\u0438', ',', '\u2581\u0438', '\u2581\u043e\u043d\u0430', '\u2581\u043d\u0435', '\u2581\u043c\u043e', '\u0433\u043b\u0430', '\u2581\u0443', '\u0434\u0435\u0440\u0436\u0430', '\u0442\u044c\u0441\u044f', '\u2581\u043e\u0442', '\u2581\u0436\u0435', '\u043b\u0430', '\u043d\u0438\u044f', '\u2581\u0437\u0430', '\u0433\u043b\u0430', '\u0434\u0438', '\u0442\u044c', '\u2581\u0445\u043e', '\u043b\u043e', '\u0434', '\u043d\u043e\u0441\u0442\u044c', '\u2581\u043c\u0430\u0442\u0435\u0440\u0438', '.']"}], "[\n  [\"\u2581\u0421\u0443\", \"Cette\"],\n  [\"\u0445\u043e\", null],\n  [\"\u0441\u0442\u044c\", null],\n  [\"\u2581\u044d\u0442\u0430\", null],\n  [\"\u2581\u043e\", null],\n  [\"\u0433\u043e\u0440\", null],\n  [\"\u0447\u0438\", null],\n  [\"\u043b\u0430\", null],\n  [\"\u2581\u041a\u0438\", \"Kitty\"],\n  [\"\u0442\u0438\", null],\n  [\",\", null],\n  [\"\u2581\u0438\", \"et\"],\n  [\"\u2581\u043e\u043d\u0430\", \"elle\"],\n  [\"\u2581\u043d\u0435\", \"ne\"],\n  [\"\u2581\u043c\u043e\", \"pouvait\"],\n  [\"\u0433\u043b\u0430\", null],\n  [\"\u2581\u0443\", null],\n  [\"\u0434\u0435\u0440\u0436\u0430\", \"s'emp\u00eacher\"],\n  [\"\u0442\u044c\u0441\u044f\", null],\n  [\"\u2581\u043e\u0442\", \"de\"],\n  [\"\u2581\u0436\u0435\", \"vouloir\"],\n  [\"\u043b\u0430\", null],\n  [\"\u043d\u0438\u044f\", null],\n  [\"\u2581\u0437\u0430\", \"att\u00e9nuer\"],\n  [\"\u0433\u043b\u0430\", null],\n  [\"\u0434\u0438\", null],\n  [\"\u0442\u044c\", null],\n  [\"\u2581\u0445\u043e\", \"froideur\"],\n  [\"\u043b\u043e\", null],\n  [\"\u0434\", null],\n  [\"\u043d\u043e\u0441\u0442\u044c\", null],\n  [\"\u2581\u043c\u0430\u0442\u0435\u0440\u0438\", \"m\u00e8re\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0410\u043d\u043d\u0430 \u0437\u0430\u043c\u0435\u0442\u0438\u043b\u0430 \u044d\u0442\u043e\u0442 \u0432\u0437\u0434\u043e\u0445, \u0432\u044b\u043a\u0430\u0437\u044b\u0432\u0430\u0432\u0448\u0438\u0439 \u043d\u0435\u0441\u043e\u0433\u043b\u0430\u0441\u0438\u0435, \u0438 \u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0430\u043b\u0430.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0410\u043d', '\u043d\u0430', '\u2581\u0437\u0430\u043c\u0435', '\u0442\u0438', '\u043b\u0430', '\u2581\u044d\u0442\u043e\u0442', '\u2581\u0432\u0437', '\u0434\u043e', '\u0445', ',', '\u2581\u0432\u044b', '\u043a\u0430', '\u0437\u044b\u0432\u0430', '\u0432\u0448\u0438\u0439', '\u2581\u043d\u0435', '\u0441\u043e', '\u0433\u043b\u0430', '\u0441\u0438', '\u0435', ',', '\u2581\u0438', '\u2581\u043f\u0440\u043e\u0434\u043e\u043b', '\u0436\u0430', '\u043b\u0430', '.']"}], "[\n  [\"\u2581\u0410\u043d\", \"Anna\"],\n  [\"\u043d\u0430\", null],\n  [\"\u2581\u0437\u0430\u043c\u0435\", \"noticed\"],\n  [\"\u0442\u0438\", null],\n  [\"\u043b\u0430\", null],\n  [\"\u2581\u044d\u0442\u043e\u0442\", \"this\"],\n  [\"\u2581\u0432\u0437\", \"sigh\"],\n  [\"\u0434\u043e\", null],\n  [\"\u0445\", null],\n  [\",\", null],\n  [\"\u2581\u0432\u044b\", \"showing\"],\n  [\"\u043a\u0430\", null],\n  [\"\u0437\u044b\u0432\u0430\", null],\n  [\"\u0432\u0448\u0438\u0439\", null],\n  [\"\u2581\u043d\u0435\", \"disagreement\"],\n  [\"\u0441\u043e\", null],\n  [\"\u0433\u043b\u0430\", null],\n  [\"\u0441\u0438\", null],\n  [\"\u0435\", null],\n  [\",\", null],\n  [\"\u2581\u0438\", \"and\"],\n  [\"\u2581\u043f\u0440\u043e\u0434\u043e\u043b\", \"continued\"],\n  [\"\u0436\u0430\", null],\n  [\"\u043b\u0430\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u0410\u043d\u043d\u0430 \u0437\u0430\u043c\u0435\u0442\u0438\u043b\u0430 \u044d\u0442\u043e\u0442 \u0432\u0437\u0434\u043e\u0445, \u0432\u044b\u043a\u0430\u0437\u044b\u0432\u0430\u0432\u0448\u0438\u0439 \u043d\u0435\u0441\u043e\u0433\u043b\u0430\u0441\u0438\u0435, \u0438 \u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0430\u043b\u0430.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0410\u043d', '\u043d\u0430', '\u2581\u0437\u0430\u043c\u0435', '\u0442\u0438', '\u043b\u0430', '\u2581\u044d\u0442\u043e\u0442', '\u2581\u0432\u0437', '\u0434\u043e', '\u0445', ',', '\u2581\u0432\u044b', '\u043a\u0430', '\u0437\u044b\u0432\u0430', '\u0432\u0448\u0438\u0439', '\u2581\u043d\u0435', '\u0441\u043e', '\u0433\u043b\u0430', '\u0441\u0438', '\u0435', ',', '\u2581\u0438', '\u2581\u043f\u0440\u043e\u0434\u043e\u043b', '\u0436\u0430', '\u043b\u0430', '.']"}], "[\n  [\"\u2581\u0410\u043d\", \"Anna\"],\n  [\"\u043d\u0430\", null],\n  [\"\u2581\u0437\u0430\u043c\u0435\", \"remarquer\"],\n  [\"\u0442\u0438\", null],\n  [\"\u043b\u0430\", null],\n  [\"\u2581\u044d\u0442\u043e\u0442\", \"ce\"],\n  [\"\u2581\u0432\u0437\", \"soupir\"],\n  [\"\u0434\u043e\", null],\n  [\"\u0445\", null],\n  [\",\", null],\n  [\"\u2581\u0432\u044b\", \"montrer\"],\n  [\"\u043a\u0430\", null],\n  [\"\u0437\u044b\u0432\u0430\", null],\n  [\"\u0432\u0448\u0438\u0439\", null],\n  [\"\u2581\u043d\u0435\", \"d\u00e9saccord\"],\n  [\"\u0441\u043e\", null],\n  [\"\u0433\u043b\u0430\", null],\n  [\"\u0441\u0438\", null],\n  [\"\u0435\", null],\n  [\",\", null],\n  [\"\u2581\u0438\", \"et\"],\n  [\"\u2581\u043f\u0440\u043e\u0434\u043e\u043b\", \"continuer\"],\n  [\"\u0436\u0430\", null],\n  [\"\u043b\u0430\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n-- \u0414\u0430, \u044f \u043f\u043e\u0442\u0435\u0440\u044f\u043b \u0434\u0430\u0436\u0435 \u043b\u044e\u0431\u043e\u0432\u044c \u043a \u0441\u044b\u043d\u0443, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0441 \u043d\u0438\u043c \u0441\u0432\u044f\u0437\u0430\u043d\u043e \u043c\u043e\u0435 \u043e\u0442\u0432\u0440\u0430\u0449\u0435\u043d\u0438\u0435 \u043a \u0432\u0430\u043c.\u041d\u043e \u044f \u0432\u0441\u0435-\u0442\u0430\u043a\u0438 \u0432\u043e\u0437\u044c\u043c\u0443 \u0435\u0433\u043e.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581--', '\u2581\u0414\u0430', ',', '\u2581\u044f', '\u2581\u043f\u043e', '\u0442\u0435', '\u0440\u044f', '\u043b', '\u2581\u0434\u0430', '\u0436\u0435', '\u2581\u043b\u044e', '\u0431\u043e\u0432', '\u044c', '\u2581\u043a', '\u2581\u0441\u044b', '\u043d\u0443', ',', '\u2581\u043f\u043e', '\u0442\u043e\u043c\u0443', '\u2581\u0447\u0442\u043e', '\u2581\u0441', '\u2581', '\u043d\u0438\u043c', '\u2581\u0441\u0432\u044f', '\u0437\u0430', '\u043d\u043e', '\u2581\u043c\u043e', '\u0435', '\u2581\u043e\u0442', '\u0432\u0440\u0430', '\u0449\u0435\u043d\u0438\u0435', '\u2581\u043a', '\u2581\u0432\u0430', '\u043c', '.', '\u041d', '\u043e', '\u2581\u044f', '\u2581\u0432\u0441\u0435', '-', '\u0442\u0430', '\u043a\u0438', '\u2581\u0432\u043e\u0437', '\u044c', '\u043c\u0443', '\u2581\u0435\u0433\u043e', '.']"}], "[\n[\"\u2581--\", null],\n[\"\u2581\u0414\u0430\", \"Yes\"],\n[\",\", null],\n[\"\u2581\u044f\", \"I\"],\n[\"\u2581\u043f\u043e\", \"lost\"],\n[\"\u0442\u0435\", null],\n[\"\u0440\u044f\", null],\n[\"\u043b\", null],\n[\"\u2581\u0434\u0430\", \"even\"],\n[\"\u0436\u0435\", null],\n[\"\u2581\u043b\u044e\", \"love\"],\n[\"\u0431\u043e\u0432\", null],\n[\"\u044c\", null],\n[\"\u2581\u043a\", \"for\"],\n[\"\u2581\u0441\u044b\", \"son\"],\n[\"\u043d\u0443\", null],\n[\",\", null],\n[\"\u2581\u043f\u043e\", \"because\"],\n[\"\u0442\u043e\u043c\u0443\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0441\", \"with\"],\n[\"\u2581\", null],\n[\"\u043d\u0438\u043c\", \"him\"],\n[\"\u2581\u0441\u0432\u044f\", \"associated\"],\n[\"\u0437\u0430\", null],\n[\"\u043d\u043e\", null],\n[\"\u2581\u043c\u043e\", \"my\"],\n[\"\u0435\", null],\n[\"\u2581\u043e\u0442\", \"disgust\"],\n[\"\u0432\u0440\u0430\", null],\n[\"\u0449\u0435\u043d\u0438\u0435\", null],\n[\"\u2581\u043a\", \"towards\"],\n[\"\u2581\u0432\u0430\", \"you\"],\n[\"\u043c\", null],\n[\".\", null],\n[\"\u041d\", \"But\"],\n[\"\u043e\", null],\n[\"\u2581\u044f\", \"I\"],\n[\"\u2581\u0432\u0441\u0435\", \"still\"],\n[\"-\", null],\n[\"\u0442\u0430\", null],\n[\"\u043a\u0438\", null],\n[\"\u2581\u0432\u043e\u0437\", \"will take\"],\n[\"\u044c\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u0435\u0433\u043e\", \"him\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n-- \u0414\u0430, \u044f \u043f\u043e\u0442\u0435\u0440\u044f\u043b \u0434\u0430\u0436\u0435 \u043b\u044e\u0431\u043e\u0432\u044c \u043a \u0441\u044b\u043d\u0443, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0441 \u043d\u0438\u043c \u0441\u0432\u044f\u0437\u0430\u043d\u043e \u043c\u043e\u0435 \u043e\u0442\u0432\u0440\u0430\u0449\u0435\u043d\u0438\u0435 \u043a \u0432\u0430\u043c.\u041d\u043e \u044f \u0432\u0441\u0435-\u0442\u0430\u043a\u0438 \u0432\u043e\u0437\u044c\u043c\u0443 \u0435\u0433\u043e.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581--', '\u2581\u0414\u0430', ',', '\u2581\u044f', '\u2581\u043f\u043e', '\u0442\u0435', '\u0440\u044f', '\u043b', '\u2581\u0434\u0430', '\u0436\u0435', '\u2581\u043b\u044e', '\u0431\u043e\u0432', '\u044c', '\u2581\u043a', '\u2581\u0441\u044b', '\u043d\u0443', ',', '\u2581\u043f\u043e', '\u0442\u043e\u043c\u0443', '\u2581\u0447\u0442\u043e', '\u2581\u0441', '\u2581', '\u043d\u0438\u043c', '\u2581\u0441\u0432\u044f', '\u0437\u0430', '\u043d\u043e', '\u2581\u043c\u043e', '\u0435', '\u2581\u043e\u0442', '\u0432\u0440\u0430', '\u0449\u0435\u043d\u0438\u0435', '\u2581\u043a', '\u2581\u0432\u0430', '\u043c', '.', '\u041d', '\u043e', '\u2581\u044f', '\u2581\u0432\u0441\u0435', '-', '\u0442\u0430', '\u043a\u0438', '\u2581\u0432\u043e\u0437', '\u044c', '\u043c\u0443', '\u2581\u0435\u0433\u043e', '.']"}], "[\n[\"\u2581--\", null],\n[\"\u2581\u0414\u0430\", \"Oui\"],\n[\",\", null],\n[\"\u2581\u044f\", \"je\"],\n[\"\u2581\u043f\u043e\", \"perdu\"],\n[\"\u0442\u0435\", null],\n[\"\u0440\u044f\", null],\n[\"\u043b\", null],\n[\"\u2581\u0434\u0430\", \"m\u00eame\"],\n[\"\u0436\u0435\", null],\n[\"\u2581\u043b\u044e\", \"amour\"],\n[\"\u0431\u043e\u0432\", null],\n[\"\u044c\", null],\n[\"\u2581\u043a\", \"pour\"],\n[\"\u2581\u0441\u044b\", \"fils\"],\n[\"\u043d\u0443\", null],\n[\",\", null],\n[\"\u2581\u043f\u043e\", \"parce\"],\n[\"\u0442\u043e\u043c\u0443\", null],\n[\"\u2581\u0447\u0442\u043e\", \"que\"],\n[\"\u2581\u0441\", \"avec\"],\n[\"\u2581\", null],\n[\"\u043d\u0438\u043c\", \"lui\"],\n[\"\u2581\u0441\u0432\u044f\", \"li\u00e9\"],\n[\"\u0437\u0430\", null],\n[\"\u043d\u043e\", null],\n[\"\u2581\u043c\u043e\", \"mon\"],\n[\"\u0435\", null],\n[\"\u2581\u043e\u0442\", \"d\u00e9go\u00fbt\"],\n[\"\u0432\u0440\u0430\", null],\n[\"\u0449\u0435\u043d\u0438\u0435\", null],\n[\"\u2581\u043a\", \"pour\"],\n[\"\u2581\u0432\u0430\", \"vous\"],\n[\"\u043c\", null],\n[\".\", null],\n[\"\u041d\", \"Mais\"],\n[\"\u043e\", null],\n[\"\u2581\u044f\", \"je\"],\n[\"\u2581\u0432\u0441\u0435\", \"quand\"],\n[\"-\", null],\n[\"\u0442\u0430\", \"m\u00eame\"],\n[\"\u043a\u0438\", null],\n[\"\u2581\u0432\u043e\u0437\", \"prendrai\"],\n[\"\u044c\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u0435\u0433\u043e\", \"le\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041f\u0440\u0430\u0432\u0434\u0430, \u0447\u0442\u043e \u043d\u0430 \u0441\u043a\u043e\u0442\u043d\u043e\u043c \u0434\u0432\u043e\u0440\u0435 \u0434\u0435\u043b\u043e \u0448\u043b\u043e \u0434\u043e \u0441\u0438\u0445 \u043f\u043e\u0440 \u043d\u0435 \u043b\u0443\u0447\u0448\u0435, \u0447\u0435\u043c \u043f\u0440\u0435\u0436\u0434\u0435, \u0438 \u0418\u0432\u0430\u043d \u0441\u0438\u043b\u044c\u043d\u043e \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u043e\u0432\u0430\u043b \u0442\u0435\u043f\u043b\u043e\u043c\u0443 \u043f\u043e\u043c\u0435\u0449\u0435\u043d\u0438\u044e \u043a\u043e\u0440\u043e\u0432 \u0438 \u0441\u043b\u0438\u0432\u043e\u0447\u043d\u043e\u043c\u0443 \u043c\u0430\u0441\u043b\u0443, \u0443\u0442\u0432\u0435\u0440\u0436\u0434\u0430\u044f, \u0447\u0442\u043e \u043a\u043e\u0440\u043e\u0432\u0435 \u043d\u0430 \u0445\u043e\u043b\u043e\u0434\u0443 \u043f\u043e\u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u043c\u0435\u043d\u044c\u0448\u0435 \u043a\u043e\u0440\u043c\u0443 \u0438 \u0447\u0442\u043e \u0441\u043c\u0435\u0442\u0430\u043d\u043d\u043e\u0435 \u043c\u0430\u0441\u043b\u043e \u0441\u043f\u043e\u0440\u0435\u0435, \u0438 \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043b \u0436\u0430\u043b\u043e\u0432\u0430\u043d\u044c\u044f, \u043a\u0430\u043a \u0438 \u0432 \u0441\u0442\u0430\u0440\u0438\u043d\u0443, \u0438 \u043d\u0438\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043d\u0435 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043e\u0432\u0430\u043b\u0441\u044f \u0442\u0435\u043c, \u0447\u0442\u043e \u0434\u0435\u043d\u044c\u0433\u0438, \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c\u044b\u0435 \u0438\u043c, \u0431\u044b\u043b\u0438 \u043d\u0435 \u0436\u0430\u043b\u043e\u0432\u0430\u043d\u044c\u0435, \u0430 \u0432\u044b\u0434\u0430\u0447\u0430 \u0432\u043f\u0435\u0440\u0435\u0434 \u0434\u043e\u043b\u0438 \u0431\u0430\u0440\u044b\u0448\u0430.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041f', '\u0440\u0430\u0432', '\u0434\u0430', ',', '\u2581\u0447\u0442\u043e', '\u2581\u043d\u0430', '\u2581\u0441\u043a\u043e', '\u0442', '\u043d\u043e\u043c', '\u2581\u0434\u0432\u043e', '\u0440\u0435', '\u2581\u0434\u0435', '\u043b\u043e', '\u2581\u0448', '\u043b\u043e', '\u2581\u0434\u043e', '\u2581\u0441\u0438', '\u0445', '\u2581\u043f\u043e\u0440', '\u2581\u043d\u0435', '\u2581\u043b\u0443\u0447', '\u0448\u0435', ',', '\u2581\u0447\u0435\u043c', '\u2581\u043f\u0440\u0435', '\u0436\u0434\u0435', ',', '\u2581\u0438', '\u2581\u0418\u0432\u0430\u043d', '\u2581\u0441\u0438', '\u043b\u044c\u043d\u043e', '\u2581\u043f\u0440\u043e\u0442\u0438', '\u0432\u043e', '\u0434\u0435\u0439', '\u0441\u0442\u0432\u043e\u0432\u0430\u043b', '\u2581\u0442\u0435', '\u043f\u043b\u043e', '\u043c\u0443', '\u2581\u043f\u043e\u043c\u0435', '\u0449\u0435', '\u043d\u0438\u044e', '\u2581\u043a\u043e', '\u0440\u043e\u0432', '\u2581\u0438', '\u2581\u0441', '\u043b\u0438', '\u0432\u043e', '\u0447\u043d\u043e', '\u043c\u0443', '\u2581\u043c\u0430', '\u0441\u043b\u0443', ',', '\u2581\u0443', '\u0442\u0432\u0435\u0440', '\u0436\u0434\u0430', '\u044f', ',', '\u2581\u0447\u0442\u043e', '\u2581\u043a\u043e', '\u0440\u043e\u0432', '\u0435', '\u2581\u043d\u0430', '\u2581\u0445\u043e', '\u043b\u043e', '\u0434\u0443', '\u2581\u043f\u043e', '\u0442\u0440\u0435', '\u0431\u0443', '\u0435\u0442\u0441\u044f', '\u2581\u043c', '\u0435\u043d\u044c', '\u0448\u0435', '\u2581\u043a\u043e\u0440', '\u043c\u0443', '\u2581\u0438', '\u2581\u0447\u0442\u043e', '\u2581\u0441', '\u043c\u0435', '\u0442\u0430\u043d', '\u043d\u043e\u0435', '\u2581\u043c\u0430', '\u0441\u043b\u043e', '\u2581\u0441\u043f\u043e', '\u0440\u0435', '\u0435', ',', '\u2581\u0438', '\u2581\u0442\u0440\u0435', '\u0431', '\u043e\u0432\u0430', '\u043b', '\u2581', '\u0436\u0430', '\u043b\u043e\u0432', '\u0430\u043d', '\u044c\u044f', ',', '\u2581\u043a\u0430\u043a', '\u2581\u0438', '\u2581\u0432', '\u2581\u0441\u0442\u0430', '\u0440\u0438', '\u043d\u0443', ',', '\u2581\u0438', '\u2581\u043d\u0438', '\u0441\u043a\u043e', '\u043b\u044c\u043a\u043e', '\u2581\u043d\u0435', '\u2581\u0438\u043d\u0442\u0435', '\u0440\u0435\u0441', '\u043e\u0432\u0430', '\u043b\u0441\u044f', '\u2581\u0442\u0435\u043c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0434\u0435\u043d\u044c', '\u0433\u0438', ',', '\u2581\u043f\u043e\u043b\u0443', '\u0447\u0430', '\u0435\u043c', '\u044b\u0435', '\u2581\u0438\u043c', ',', '\u2581\u0431\u044b\u043b\u0438', '\u2581\u043d\u0435', '\u2581', '\u0436\u0430', '\u043b\u043e\u0432', '\u0430\u043d', '\u044c\u0435', ',', '\u2581\u0430', '\u2581\u0432\u044b', '\u0434\u0430', '\u0447\u0430', '\u2581\u0432', '\u043f\u0435\u0440\u0435', '\u0434', '\u2581\u0434\u043e', '\u043b\u0438', '\u2581\u0431', '\u0430\u0440', '\u044b', '\u0448\u0430', '.']"}], "[\n[\"\u2581\u041f\", \"Truth\"],\n[\"\u0440\u0430\u0432\", null],\n[\"\u0434\u0430\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u0441\u043a\u043e\", null],\n[\"\u0442\", null],\n[\"\u043d\u043e\u043c\", null],\n[\"\u2581\u0434\u0432\u043e\", \"farm\"],\n[\"\u0440\u0435\", null],\n[\"\u2581\u0434\u0435\", null],\n[\"\u043b\u043e\", null],\n[\"\u2581\u0448\", \"went\"],\n[\"\u043b\u043e\", null],\n[\"\u2581\u0434\u043e\", null],\n[\"\u2581\u0441\u0438\", null],\n[\"\u0445\", null],\n[\"\u2581\u043f\u043e\u0440\", null],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u043b\u0443\u0447\", \"better\"],\n[\"\u0448\u0435\", null],\n[\",\", null],\n[\"\u2581\u0447\u0435\u043c\", null],\n[\"\u2581\u043f\u0440\u0435\", \"before\"],\n[\"\u0436\u0434\u0435\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0418\u0432\u0430\u043d\", \"Ivan\"],\n[\"\u2581\u0441\u0438\", null],\n[\"\u043b\u044c\u043d\u043e\", null],\n[\"\u2581\u043f\u0440\u043e\u0442\u0438\", \"opposed\"],\n[\"\u0432\u043e\", null],\n[\"\u0434\u0435\u0439\", null],\n[\"\u0441\u0442\u0432\u043e\u0432\u0430\u043b\", null],\n[\"\u2581\u0442\u0435\", null],\n[\"\u043f\u043b\u043e\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u043f\u043e\u043c\u0435\", \"housing\"],\n[\"\u0449\u0435\", null],\n[\"\u043d\u0438\u044e\", null],\n[\"\u2581\u043a\u043e\", \"cows\"],\n[\"\u0440\u043e\u0432\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0441\", null],\n[\"\u043b\u0438\", null],\n[\"\u0432\u043e\", null],\n[\"\u0447\u043d\u043e\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u043c\u0430\", \"butter\"],\n[\"\u0441\u043b\u0443\", null],\n[\",\", null],\n[\"\u2581\u0443\", null],\n[\"\u0442\u0432\u0435\u0440\", \"claiming\"],\n[\"\u0436\u0434\u0430\", null],\n[\"\u044f\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u043a\u043e\", \"cow\"],\n[\"\u0440\u043e\u0432\", null],\n[\"\u0435\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u0445\u043e\", \"cold\"],\n[\"\u043b\u043e\", null],\n[\"\u0434\u0443\", null],\n[\"\u2581\u043f\u043e\", \"need\"],\n[\"\u0442\u0440\u0435\", null],\n[\"\u0431\u0443\", null],\n[\"\u0435\u0442\u0441\u044f\", null],\n[\"\u2581\u043c\", \"less\"],\n[\"\u0435\u043d\u044c\", null],\n[\"\u0448\u0435\", null],\n[\"\u2581\u043a\u043e\u0440\", \"feed\"],\n[\"\u043c\u0443\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0441\", null],\n[\"\u043c\u0435\", null],\n[\"\u0442\u0430\u043d\", null],\n[\"\u043d\u043e\u0435\", null],\n[\"\u2581\u043c\u0430\", \"butter\"],\n[\"\u0441\u043b\u043e\", null],\n[\"\u2581\u0441\u043f\u043e\", \"quicker\"],\n[\"\u0440\u0435\", null],\n[\"\u0435\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0442\u0440\u0435\", \"demanded\"],\n[\"\u0431\", null],\n[\"\u043e\u0432\u0430\", null],\n[\"\u043b\", null],\n[\"\u2581\", null],\n[\"\u0436\u0430\", \"salary\"],\n[\"\u043b\u043e\u0432\", null],\n[\"\u0430\u043d\", null],\n[\"\u044c\u044f\", null],\n[\",\", null],\n[\"\u2581\u043a\u0430\u043a\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0441\u0442\u0430\", \"old\"],\n[\"\u0440\u0438\", null],\n[\"\u043d\u0443\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043d\u0438\", null],\n[\"\u0441\u043a\u043e\", null],\n[\"\u043b\u044c\u043a\u043e\", null],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0438\u043d\u0442\u0435\", \"interested\"],\n[\"\u0440\u0435\u0441\", null],\n[\"\u043e\u0432\u0430\", null],\n[\"\u043b\u0441\u044f\", null],\n[\"\u2581\u0442\u0435\u043c\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0434\u0435\u043d\u044c\", \"money\"],\n[\"\u0433\u0438\", null],\n[\",\", null],\n[\"\u2581\u043f\u043e\u043b\u0443\", \"received\"],\n[\"\u0447\u0430\", null],\n[\"\u0435\u043c\", null],\n[\"\u044b\u0435\", null],\n[\"\u2581\u0438\u043c\", null],\n[\",\", null],\n[\"\u2581\u0431\u044b\u043b\u0438\", null],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\", null],\n[\"\u0436\u0430\", \"salary\"],\n[\"\u043b\u043e\u0432\", null],\n[\"\u0430\u043d\", null],\n[\"\u044c\u0435\", null],\n[\",\", null],\n[\"\u2581\u0430\", null],\n[\"\u2581\u0432\u044b\", \"advance\"],\n[\"\u0434\u0430\", null],\n[\"\u0447\u0430\", null],\n[\"\u2581\u0432\", null],\n[\"\u043f\u0435\u0440\u0435\", null],\n[\"\u0434\", null],\n[\"\u2581\u0434\u043e\", \"share\"],\n[\"\u043b\u0438\", null],\n[\"\u2581\u0431\", \"profit\"],\n[\"\u0430\u0440\", null],\n[\"\u044b\", null],\n[\"\u0448\u0430\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u041f\u0440\u0430\u0432\u0434\u0430, \u0447\u0442\u043e \u043d\u0430 \u0441\u043a\u043e\u0442\u043d\u043e\u043c \u0434\u0432\u043e\u0440\u0435 \u0434\u0435\u043b\u043e \u0448\u043b\u043e \u0434\u043e \u0441\u0438\u0445 \u043f\u043e\u0440 \u043d\u0435 \u043b\u0443\u0447\u0448\u0435, \u0447\u0435\u043c \u043f\u0440\u0435\u0436\u0434\u0435, \u0438 \u0418\u0432\u0430\u043d \u0441\u0438\u043b\u044c\u043d\u043e \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u043e\u0432\u0430\u043b \u0442\u0435\u043f\u043b\u043e\u043c\u0443 \u043f\u043e\u043c\u0435\u0449\u0435\u043d\u0438\u044e \u043a\u043e\u0440\u043e\u0432 \u0438 \u0441\u043b\u0438\u0432\u043e\u0447\u043d\u043e\u043c\u0443 \u043c\u0430\u0441\u043b\u0443, \u0443\u0442\u0432\u0435\u0440\u0436\u0434\u0430\u044f, \u0447\u0442\u043e \u043a\u043e\u0440\u043e\u0432\u0435 \u043d\u0430 \u0445\u043e\u043b\u043e\u0434\u0443 \u043f\u043e\u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u043c\u0435\u043d\u044c\u0448\u0435 \u043a\u043e\u0440\u043c\u0443 \u0438 \u0447\u0442\u043e \u0441\u043c\u0435\u0442\u0430\u043d\u043d\u043e\u0435 \u043c\u0430\u0441\u043b\u043e \u0441\u043f\u043e\u0440\u0435\u0435, \u0438 \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043b \u0436\u0430\u043b\u043e\u0432\u0430\u043d\u044c\u044f, \u043a\u0430\u043a \u0438 \u0432 \u0441\u0442\u0430\u0440\u0438\u043d\u0443, \u0438 \u043d\u0438\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043d\u0435 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043e\u0432\u0430\u043b\u0441\u044f \u0442\u0435\u043c, \u0447\u0442\u043e \u0434\u0435\u043d\u044c\u0433\u0438, \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c\u044b\u0435 \u0438\u043c, \u0431\u044b\u043b\u0438 \u043d\u0435 \u0436\u0430\u043b\u043e\u0432\u0430\u043d\u044c\u0435, \u0430 \u0432\u044b\u0434\u0430\u0447\u0430 \u0432\u043f\u0435\u0440\u0435\u0434 \u0434\u043e\u043b\u0438 \u0431\u0430\u0440\u044b\u0448\u0430.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041f', '\u0440\u0430\u0432', '\u0434\u0430', ',', '\u2581\u0447\u0442\u043e', '\u2581\u043d\u0430', '\u2581\u0441\u043a\u043e', '\u0442', '\u043d\u043e\u043c', '\u2581\u0434\u0432\u043e', '\u0440\u0435', '\u2581\u0434\u0435', '\u043b\u043e', '\u2581\u0448', '\u043b\u043e', '\u2581\u0434\u043e', '\u2581\u0441\u0438', '\u0445', '\u2581\u043f\u043e\u0440', '\u2581\u043d\u0435', '\u2581\u043b\u0443\u0447', '\u0448\u0435', ',', '\u2581\u0447\u0435\u043c', '\u2581\u043f\u0440\u0435', '\u0436\u0434\u0435', ',', '\u2581\u0438', '\u2581\u0418\u0432\u0430\u043d', '\u2581\u0441\u0438', '\u043b\u044c\u043d\u043e', '\u2581\u043f\u0440\u043e\u0442\u0438', '\u0432\u043e', '\u0434\u0435\u0439', '\u0441\u0442\u0432\u043e\u0432\u0430\u043b', '\u2581\u0442\u0435', '\u043f\u043b\u043e', '\u043c\u0443', '\u2581\u043f\u043e\u043c\u0435', '\u0449\u0435', '\u043d\u0438\u044e', '\u2581\u043a\u043e', '\u0440\u043e\u0432', '\u2581\u0438', '\u2581\u0441', '\u043b\u0438', '\u0432\u043e', '\u0447\u043d\u043e', '\u043c\u0443', '\u2581\u043c\u0430', '\u0441\u043b\u0443', ',', '\u2581\u0443', '\u0442\u0432\u0435\u0440', '\u0436\u0434\u0430', '\u044f', ',', '\u2581\u0447\u0442\u043e', '\u2581\u043a\u043e', '\u0440\u043e\u0432', '\u0435', '\u2581\u043d\u0430', '\u2581\u0445\u043e', '\u043b\u043e', '\u0434\u0443', '\u2581\u043f\u043e', '\u0442\u0440\u0435', '\u0431\u0443', '\u0435\u0442\u0441\u044f', '\u2581\u043c', '\u0435\u043d\u044c', '\u0448\u0435', '\u2581\u043a\u043e\u0440', '\u043c\u0443', '\u2581\u0438', '\u2581\u0447\u0442\u043e', '\u2581\u0441', '\u043c\u0435', '\u0442\u0430\u043d', '\u043d\u043e\u0435', '\u2581\u043c\u0430', '\u0441\u043b\u043e', '\u2581\u0441\u043f\u043e', '\u0440\u0435', '\u0435', ',', '\u2581\u0438', '\u2581\u0442\u0440\u0435', '\u0431', '\u043e\u0432\u0430', '\u043b', '\u2581', '\u0436\u0430', '\u043b\u043e\u0432', '\u0430\u043d', '\u044c\u044f', ',', '\u2581\u043a\u0430\u043a', '\u2581\u0438', '\u2581\u0432', '\u2581\u0441\u0442\u0430', '\u0440\u0438', '\u043d\u0443', ',', '\u2581\u0438', '\u2581\u043d\u0438', '\u0441\u043a\u043e', '\u043b\u044c\u043a\u043e', '\u2581\u043d\u0435', '\u2581\u0438\u043d\u0442\u0435', '\u0440\u0435\u0441', '\u043e\u0432\u0430', '\u043b\u0441\u044f', '\u2581\u0442\u0435\u043c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0434\u0435\u043d\u044c', '\u0433\u0438', ',', '\u2581\u043f\u043e\u043b\u0443', '\u0447\u0430', '\u0435\u043c', '\u044b\u0435', '\u2581\u0438\u043c', ',', '\u2581\u0431\u044b\u043b\u0438', '\u2581\u043d\u0435', '\u2581', '\u0436\u0430', '\u043b\u043e\u0432', '\u0430\u043d', '\u044c\u0435', ',', '\u2581\u0430', '\u2581\u0432\u044b', '\u0434\u0430', '\u0447\u0430', '\u2581\u0432', '\u043f\u0435\u0440\u0435', '\u0434', '\u2581\u0434\u043e', '\u043b\u0438', '\u2581\u0431', '\u0430\u0440', '\u044b', '\u0448\u0430', '.']"}], "[[\"\u2581\u041f\", \"Vrai\"],\n[\"\u0440\u0430\u0432\", null],\n[\"\u0434\u0430\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u0441\u043a\u043e\", \"ferme\"],\n[\"\u0442\", null],\n[\"\u043d\u043e\u043c\", null],\n[\"\u2581\u0434\u0432\u043e\", null],\n[\"\u0440\u0435\", null],\n[\"\u2581\u0434\u0435\", \"affaires\"],\n[\"\u043b\u043e\", null],\n[\"\u2581\u0448\", null],\n[\"\u043b\u043e\", null],\n[\"\u2581\u0434\u043e\", null],\n[\"\u2581\u0441\u0438\", null],\n[\"\u0445\", null],\n[\"\u2581\u043f\u043e\u0440\", null],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u043b\u0443\u0447\", \"mieux\"],\n[\"\u0448\u0435\", null],\n[\",\", null],\n[\"\u2581\u0447\u0435\u043c\", null],\n[\"\u2581\u043f\u0440\u0435\", \"avant\"],\n[\"\u0436\u0434\u0435\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0418\u0432\u0430\u043d\", \"Ivan\"],\n[\"\u2581\u0441\u0438\", \"fortement\"],\n[\"\u043b\u044c\u043d\u043e\", null],\n[\"\u2581\u043f\u0440\u043e\u0442\u0438\", \"s'opposait\"],\n[\"\u0432\u043e\", null],\n[\"\u0434\u0435\u0439\", null],\n[\"\u0441\u0442\u0432\u043e\u0432\u0430\u043b\", null],\n[\"\u2581\u0442\u0435\", \"chauff\u00e9\"],\n[\"\u043f\u043b\u043e\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u043f\u043e\u043c\u0435\", \"local\"],\n[\"\u0449\u0435\", null],\n[\"\u043d\u0438\u044e\", null],\n[\"\u2581\u043a\u043e\", \"vaches\"],\n[\"\u0440\u043e\u0432\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0441\", \"beurre\"],\n[\"\u043b\u0438\", null],\n[\"\u0432\u043e\", null],\n[\"\u0447\u043d\u043e\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u043c\u0430\", null],\n[\"\u0441\u043b\u0443\", null],\n[\",\", null],\n[\"\u2581\u0443\", \"affirmant\"],\n[\"\u0442\u0432\u0435\u0440\", null],\n[\"\u0436\u0434\u0430\", null],\n[\"\u044f\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u043a\u043e\", \"vache\"],\n[\"\u0440\u043e\u0432\", null],\n[\"\u0435\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u0445\u043e\", \"froid\"],\n[\"\u043b\u043e\", null],\n[\"\u0434\u0443\", null],\n[\"\u2581\u043f\u043e\", \"n\u00e9cessiterait\"],\n[\"\u0442\u0440\u0435\", null],\n[\"\u0431\u0443\", null],\n[\"\u0435\u0442\u0441\u044f\", null],\n[\"\u2581\u043c\", \"moins\"],\n[\"\u0435\u043d\u044c\", null],\n[\"\u0448\u0435\", null],\n[\"\u2581\u043a\u043e\u0440\", \"nourriture\"],\n[\"\u043c\u0443\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0441\", \"cr\u00e8me\"],\n[\"\u043c\u0435\", null],\n[\"\u0442\u0430\u043d\", null],\n[\"\u043d\u043e\u0435\", null],\n[\"\u2581\u043c\u0430\", \"beurre\"],\n[\"\u0441\u043b\u043e\", null],\n[\"\u2581\u0441\u043f\u043e\", \"plus\"],\n[\"\u0440\u0435\", null],\n[\"\u0435\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0442\u0440\u0435\", \"exigeait\"],\n[\"\u0431\", null],\n[\"\u043e\u0432\u0430\", null],\n[\"\u043b\", null],\n[\"\u2581\", null],\n[\"\u0436\u0430\", \"salaire\"],\n[\"\u043b\u043e\u0432\", null],\n[\"\u0430\u043d\", null],\n[\"\u044c\u044f\", null],\n[\",\", null],\n[\"\u2581\u043a\u0430\u043a\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0441\u0442\u0430\", \"autrefois\"],\n[\"\u0440\u0438\", null],\n[\"\u043d\u0443\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u043d\u0438\", \"aucunement\"],\n[\"\u0441\u043a\u043e\", null],\n[\"\u043b\u044c\u043a\u043e\", null],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0438\u043d\u0442\u0435\", \"int\u00e9ressait\"],\n[\"\u0440\u0435\u0441\", null],\n[\"\u043e\u0432\u0430\", null],\n[\"\u043b\u0441\u044f\", null],\n[\"\u2581\u0442\u0435\u043c\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0434\u0435\u043d\u044c\", \"argent\"],\n[\"\u0433\u0438\", null],\n[\",\", null],\n[\"\u2581\u043f\u043e\u043b\u0443\", \"recevait\"],\n[\"\u0447\u0430\", null],\n[\"\u0435\u043c\", null],\n[\"\u044b\u0435\", null],\n[\"\u2581\u0438\u043c\", null],\n[\",\", null],\n[\"\u2581\u0431\u044b\u043b\u0438\", null],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\", null],\n[\"\u0436\u0430\", \"salaire\"],\n[\"\u043b\u043e\u0432\", null],\n[\"\u0430\u043d\", null],\n[\"\u044c\u0435\", null],\n[\",\", null],\n[\"\u2581\u0430\", null],\n[\"\u2581\u0432\u044b\", \"avance\"],\n[\"\u0434\u0430\", null],\n[\"\u0447\u0430\", null],\n[\"\u2581\u0432\", null],\n[\"\u043f\u0435\u0440\u0435\", null],\n[\"\u0434\", null],\n[\"\u2581\u0434\u043e\", \"part\"],\n[\"\u043b\u0438\", null],\n[\"\u2581\u0431\", \"profit\"],\n[\"\u0430\u0440\", null],\n[\"\u044b\", null],\n[\"\u0448\u0430\", null],\n[\".\", null]]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041a\u043d\u044f\u0433\u0438\u043d\u044f, \u0443\u0441\u043b\u044b\u0445\u0430\u0432 \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u0412\u0430\u0440\u0435\u043d\u044c\u043a\u0430 \u0445\u043e\u0440\u043e\u0448\u043e \u043f\u043e\u0435\u0442, \u0432\u043e\u043f\u0440\u043e\u0441\u0438\u043b\u0430 \u0435\u0435 \u043f\u0440\u0438\u0439\u0442\u0438 \u043a \u043d\u0438\u043c \u043f\u0435\u0442\u044c \u0432\u0435\u0447\u0435\u0440\u043e\u043c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041a', '\u043d\u044f', '\u0433\u0438', '\u043d\u044f', ',', '\u2581\u0443\u0441', '\u043b\u044b', '\u0445\u0430', '\u0432', '\u2581\u043e', '\u2581\u0442\u043e\u043c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0412\u0430', '\u0440\u0435', '\u043d\u044c', '\u043a\u0430', '\u2581\u0445\u043e', '\u0440\u043e', '\u0448\u043e', '\u2581\u043f\u043e', '\u0435\u0442', ',', '\u2581\u0432\u043e\u043f\u0440\u043e', '\u0441\u0438', '\u043b\u0430', '\u2581\u0435', '\u0435', '\u2581\u043f\u0440\u0438', '\u0439', '\u0442\u0438', '\u2581\u043a', '\u2581', '\u043d\u0438\u043c', '\u2581\u043f', '\u0435\u0442', '\u044c', '\u2581\u0432\u0435', '\u0447\u0435', '\u0440\u043e\u043c', '.']"}], "[\n[\"\u2581\u041a\", \"Princess\"],\n[\"\u043d\u044f\", null],\n[\"\u0433\u0438\", null],\n[\"\u043d\u044f\", null],\n[\",\", null],\n[\"\u2581\u0443\u0441\", \"hearing\"],\n[\"\u043b\u044b\", null],\n[\"\u0445\u0430\", null],\n[\"\u0432\", null],\n[\"\u2581\u043e\", null],\n[\"\u2581\u0442\u043e\u043c\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0412\u0430\", \"Varenka\"],\n[\"\u0440\u0435\", null],\n[\"\u043d\u044c\", null],\n[\"\u043a\u0430\", null],\n[\"\u2581\u0445\u043e\", \"well\"],\n[\"\u0440\u043e\", null],\n[\"\u0448\u043e\", null],\n[\"\u2581\u043f\u043e\", \"sings\"],\n[\"\u0435\u0442\", null],\n[\",\", null],\n[\"\u2581\u0432\u043e\u043f\u0440\u043e\", \"asked\"],\n[\"\u0441\u0438\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\u0435\", \"her\"],\n[\"\u0435\", null],\n[\"\u2581\u043f\u0440\u0438\", \"come\"],\n[\"\u0439\", null],\n[\"\u0442\u0438\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\", null],\n[\"\u043d\u0438\u043c\", \"them\"],\n[\"\u2581\u043f\", \"sing\"],\n[\"\u0435\u0442\", null],\n[\"\u044c\", null],\n[\"\u2581\u0432\u0435\", \"evening\"],\n[\"\u0447\u0435\", null],\n[\"\u0440\u043e\u043c\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u041a\u043d\u044f\u0433\u0438\u043d\u044f, \u0443\u0441\u043b\u044b\u0445\u0430\u0432 \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u0412\u0430\u0440\u0435\u043d\u044c\u043a\u0430 \u0445\u043e\u0440\u043e\u0448\u043e \u043f\u043e\u0435\u0442, \u0432\u043e\u043f\u0440\u043e\u0441\u0438\u043b\u0430 \u0435\u0435 \u043f\u0440\u0438\u0439\u0442\u0438 \u043a \u043d\u0438\u043c \u043f\u0435\u0442\u044c \u0432\u0435\u0447\u0435\u0440\u043e\u043c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041a', '\u043d\u044f', '\u0433\u0438', '\u043d\u044f', ',', '\u2581\u0443\u0441', '\u043b\u044b', '\u0445\u0430', '\u0432', '\u2581\u043e', '\u2581\u0442\u043e\u043c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0412\u0430', '\u0440\u0435', '\u043d\u044c', '\u043a\u0430', '\u2581\u0445\u043e', '\u0440\u043e', '\u0448\u043e', '\u2581\u043f\u043e', '\u0435\u0442', ',', '\u2581\u0432\u043e\u043f\u0440\u043e', '\u0441\u0438', '\u043b\u0430', '\u2581\u0435', '\u0435', '\u2581\u043f\u0440\u0438', '\u0439', '\u0442\u0438', '\u2581\u043a', '\u2581', '\u043d\u0438\u043c', '\u2581\u043f', '\u0435\u0442', '\u044c', '\u2581\u0432\u0435', '\u0447\u0435', '\u0440\u043e\u043c', '.']"}], "[\n[\"\u2581\u041a\", \"Princesse\"],\n[\"\u043d\u044f\", null],\n[\"\u0433\u0438\", null],\n[\"\u043d\u044f\", null],\n[\",\", null],\n[\"\u2581\u0443\u0441\", \"apprenant\"],\n[\"\u043b\u044b\", null],\n[\"\u0445\u0430\", null],\n[\"\u0432\", null],\n[\"\u2581\u043e\", null],\n[\"\u2581\u0442\u043e\u043c\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0412\u0430\", \"Varenka\"],\n[\"\u0440\u0435\", null],\n[\"\u043d\u044c\", null],\n[\"\u043a\u0430\", null],\n[\"\u2581\u0445\u043e\", \"bien\"],\n[\"\u0440\u043e\", null],\n[\"\u0448\u043e\", null],\n[\"\u2581\u043f\u043e\", \"chante\"],\n[\"\u0435\u0442\", null],\n[\",\", null],\n[\"\u2581\u0432\u043e\u043f\u0440\u043e\", \"demanda\"],\n[\"\u0441\u0438\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\u0435\", null],\n[\"\u0435\", null],\n[\"\u2581\u043f\u0440\u0438\", \"venir\"],\n[\"\u0439\", null],\n[\"\u0442\u0438\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\", null],\n[\"\u043d\u0438\u043c\", \"eux\"],\n[\"\u2581\u043f\", \"chanter\"],\n[\"\u0435\u0442\", null],\n[\"\u044c\", null],\n[\"\u2581\u0432\u0435\", \"soir\"],\n[\"\u0447\u0435\", null],\n[\"\u0440\u043e\u043c\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041f\u043e \u043c\u043e\u0435\u043c\u0443 \u043c\u043d\u0435\u043d\u0438\u044e, \u0434\u0435\u0439\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c \u043d\u0430 \u0434\u0440\u0443\u0433\u043e\u0439 \u043d\u0430\u0440\u043e\u0434 \u043c\u043e\u0436\u0435\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u043e\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0438\u043c\u0435\u0435\u0442 \u0432\u044b\u0441\u0448\u0435\u0435 \u0440\u0430\u0437\u0432\u0438\u0442\u0438\u0435, \u043a\u043e\u0442\u043e\u0440\u044b\u0439...\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041f\u043e', '\u2581\u043c\u043e', '\u0435', '\u043c\u0443', '\u2581\u043c', '\u043d\u0435', '\u043d\u0438\u044e', ',', '\u2581\u0434\u0435\u0439', '\u0441\u0442\u0432\u043e\u0432\u0430', '\u0442\u044c', '\u2581\u043d\u0430', '\u2581\u0434\u0440\u0443\u0433\u043e', '\u0439', '\u2581\u043d\u0430\u0440\u043e\u0434', '\u2581\u043c\u043e\u0436\u0435\u0442', '\u2581\u0442\u043e\u043b\u044c\u043a\u043e', '\u2581\u0442\u043e', '\u0442', ',', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439', '\u2581\u0438\u043c\u0435\u0435\u0442', '\u2581\u0432\u044b', '\u0441', '\u0448\u0435\u0435', '\u2581\u0440\u0430\u0437\u0432\u0438', '\u0442\u0438\u0435', ',', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439', '...']"}], "[\n[\"\u2581\u041f\u043e\", \"In\"],\n[\"\u2581\u043c\u043e\", null],\n[\"\u0435\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u043c\", null],\n[\"\u043d\u0435\", null],\n[\"\u043d\u0438\u044e\", \"opinion\"],\n[\",\", null],\n[\"\u2581\u0434\u0435\u0439\", null],\n[\"\u0441\u0442\u0432\u043e\u0432\u0430\", null],\n[\"\u0442\u044c\", \"act\"],\n[\"\u2581\u043d\u0430\", \"on\"],\n[\"\u2581\u0434\u0440\u0443\u0433\u043e\", null],\n[\"\u0439\", \"another\"],\n[\"\u2581\u043d\u0430\u0440\u043e\u0434\", \"people\"],\n[\"\u2581\u043c\u043e\u0436\u0435\u0442\", \"can\"],\n[\"\u2581\u0442\u043e\u043b\u044c\u043a\u043e\", \"only\"],\n[\"\u2581\u0442\u043e\", null],\n[\"\u0442\", \"that\"],\n[\",\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439\", \"which\"],\n[\"\u2581\u0438\u043c\u0435\u0435\u0442\", \"has\"],\n[\"\u2581\u0432\u044b\", null],\n[\"\u0441\", null],\n[\"\u0448\u0435\u0435\", \"higher\"],\n[\"\u2581\u0440\u0430\u0437\u0432\u0438\", null],\n[\"\u0442\u0438\u0435\", \"development\"],\n[\",\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439\", \"which\"],\n[\"...\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u041f\u043e \u043c\u043e\u0435\u043c\u0443 \u043c\u043d\u0435\u043d\u0438\u044e, \u0434\u0435\u0439\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c \u043d\u0430 \u0434\u0440\u0443\u0433\u043e\u0439 \u043d\u0430\u0440\u043e\u0434 \u043c\u043e\u0436\u0435\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u043e\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0438\u043c\u0435\u0435\u0442 \u0432\u044b\u0441\u0448\u0435\u0435 \u0440\u0430\u0437\u0432\u0438\u0442\u0438\u0435, \u043a\u043e\u0442\u043e\u0440\u044b\u0439...\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041f\u043e', '\u2581\u043c\u043e', '\u0435', '\u043c\u0443', '\u2581\u043c', '\u043d\u0435', '\u043d\u0438\u044e', ',', '\u2581\u0434\u0435\u0439', '\u0441\u0442\u0432\u043e\u0432\u0430', '\u0442\u044c', '\u2581\u043d\u0430', '\u2581\u0434\u0440\u0443\u0433\u043e', '\u0439', '\u2581\u043d\u0430\u0440\u043e\u0434', '\u2581\u043c\u043e\u0436\u0435\u0442', '\u2581\u0442\u043e\u043b\u044c\u043a\u043e', '\u2581\u0442\u043e', '\u0442', ',', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439', '\u2581\u0438\u043c\u0435\u0435\u0442', '\u2581\u0432\u044b', '\u0441', '\u0448\u0435\u0435', '\u2581\u0440\u0430\u0437\u0432\u0438', '\u0442\u0438\u0435', ',', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439', '...']"}], "[\n    [\"\u2581\u041f\u043e\", \"Selon\"],\n    [\"\u2581\u043c\u043e\", null],\n    [\"\u0435\", null],\n    [\"\u043c\u0443\", null],\n    [\"\u2581\u043c\", null],\n    [\"\u043d\u0435\", null],\n    [\"\u043d\u0438\u044e\", null],\n    [\",\", null],\n    [\"\u2581\u0434\u0435\u0439\", \"agir\"],\n    [\"\u0441\u0442\u0432\u043e\u0432\u0430\", null],\n    [\"\u0442\u044c\", null],\n    [\"\u2581\u043d\u0430\", \"sur\"],\n    [\"\u2581\u0434\u0440\u0443\u0433\u043e\", \"autre\"],\n    [\"\u0439\", null],\n    [\"\u2581\u043d\u0430\u0440\u043e\u0434\", \"peuple\"],\n    [\"\u2581\u043c\u043e\u0436\u0435\u0442\", \"peut\"],\n    [\"\u2581\u0442\u043e\u043b\u044c\u043a\u043e\", \"seulement\"],\n    [\"\u2581\u0442\u043e\", \"celui\"],\n    [\"\u0442\", null],\n    [\",\", null],\n    [\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439\", \"qui\"],\n    [\"\u2581\u0438\u043c\u0435\u0435\u0442\", \"a\"],\n    [\"\u2581\u0432\u044b\", \"sup\u00e9rieur\"],\n    [\"\u0441\", null],\n    [\"\u0448\u0435\u0435\", null],\n    [\"\u2581\u0440\u0430\u0437\u0432\u0438\", \"d\u00e9veloppement\"],\n    [\"\u0442\u0438\u0435\", null],\n    [\",\", null],\n    [\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\u0439\", \"qui\"],\n    [\"...\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0411\u044b\u043b \u043e\u0434\u0438\u043d \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442\u0441\u043a\u0438\u0439 \u0442\u043e\u0432\u0430\u0440\u0438\u0449, \u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043e\u043d \u0441\u0431\u043b\u0438\u0437\u0438\u043b\u0441\u044f \u043f\u043e\u0441\u043b\u0435 \u0438 \u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043e\u043d \u043c\u043e\u0433 \u0431\u044b \u043f\u043e\u0433\u043e\u0432\u043e\u0440\u0438\u0442\u044c \u043e \u043b\u0438\u0447\u043d\u043e\u043c \u0433\u043e\u0440\u0435; \u043d\u043e \u0442\u043e\u0432\u0430\u0440\u0438\u0449 \u044d\u0442\u043e\u0442 \u0431\u044b\u043b \u043f\u043e\u043f\u0435\u0447\u0438\u0442\u0435\u043b\u0435\u043c \u0432 \u0434\u0430\u043b\u044c\u043d\u0435\u043c \u0443\u0447\u0435\u0431\u043d\u043e\u043c \u043e\u043a\u0440\u0443\u0433\u0435.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0411\u044b', '\u043b', '\u2581\u043e\u0434\u0438\u043d', '\u2581\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435', '\u0442', '\u0441\u043a\u0438\u0439', '\u2581\u0442\u043e\u0432\u0430', '\u0440\u0438', '\u0449', ',', '\u2581\u0441', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b', '\u043c', '\u2581\u043e\u043d', '\u2581\u0441', '\u0431\u043b\u0438', '\u0437\u0438', '\u043b\u0441\u044f', '\u2581\u043f\u043e\u0441\u043b\u0435', '\u2581\u0438', '\u2581\u0441', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b', '\u043c', '\u2581\u043e\u043d', '\u2581\u043c\u043e', '\u0433', '\u2581\u0431\u044b', '\u2581\u043f\u043e', '\u0433\u043e\u0432\u043e', '\u0440\u0438', '\u0442\u044c', '\u2581\u043e', '\u2581\u043b\u0438', '\u0447', '\u043d\u043e\u043c', '\u2581\u0433\u043e', '\u0440\u0435', ';', '\u2581\u043d\u043e', '\u2581\u0442\u043e\u0432\u0430', '\u0440\u0438', '\u0449', '\u2581\u044d\u0442\u043e\u0442', '\u2581\u0431\u044b\u043b', '\u2581\u043f\u043e', '\u043f\u0435', '\u0447\u0438', '\u0442\u0435\u043b\u0435\u043c', '\u2581\u0432', '\u2581\u0434\u0430', '\u043b\u044c', '\u043d\u0435\u043c', '\u2581\u0443\u0447\u0435', '\u0431', '\u043d\u043e\u043c', '\u2581\u043e\u043a\u0440\u0443', '\u0433\u0435', '.']"}], "[\n[\"\u2581\u0411\u044b\", \"There\"],\n[\"\u043b\", null],\n[\"\u2581\u043e\u0434\u0438\u043d\", \"one\"],\n[\"\u2581\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\", \"university\"],\n[\"\u0442\", null],\n[\"\u0441\u043a\u0438\u0439\", null],\n[\"\u2581\u0442\u043e\u0432\u0430\", \"comrade\"],\n[\"\u0440\u0438\", null],\n[\"\u0449\", null],\n[\",\", null],\n[\"\u2581\u0441\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\", \"whom\"],\n[\"\u043c\", null],\n[\"\u2581\u043e\u043d\", \"he\"],\n[\"\u2581\u0441\", null],\n[\"\u0431\u043b\u0438\", \"became\"],\n[\"\u0437\u0438\", null],\n[\"\u043b\u0441\u044f\", null],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\", \"later\"],\n[\"\u2581\u0438\", \"and\"],\n[\"\u2581\u0441\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\", \"whom\"],\n[\"\u043c\", null],\n[\"\u2581\u043e\u043d\", \"he\"],\n[\"\u2581\u043c\u043e\", \"could\"],\n[\"\u0433\", null],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u043f\u043e\", \"talk\"],\n[\"\u0433\u043e\u0432\u043e\", null],\n[\"\u0440\u0438\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043e\", \"about\"],\n[\"\u2581\u043b\u0438\", \"personal\"],\n[\"\u0447\", null],\n[\"\u043d\u043e\u043c\", null],\n[\"\u2581\u0433\u043e\", \"grief\"],\n[\"\u0440\u0435\", null],\n[\";\", null],\n[\"\u2581\u043d\u043e\", \"but\"],\n[\"\u2581\u0442\u043e\u0432\u0430\", \"comrade\"],\n[\"\u0440\u0438\", null],\n[\"\u0449\", null],\n[\"\u2581\u044d\u0442\u043e\u0442\", \"this\"],\n[\"\u2581\u0431\u044b\u043b\", \"was\"],\n[\"\u2581\u043f\u043e\", \"trustee\"],\n[\"\u043f\u0435\", null],\n[\"\u0447\u0438\", null],\n[\"\u0442\u0435\u043b\u0435\u043c\", null],\n[\"\u2581\u0432\", \"in\"],\n[\"\u2581\u0434\u0430\", \"distant\"],\n[\"\u043b\u044c\", null],\n[\"\u043d\u0435\u043c\", null],\n[\"\u2581\u0443\u0447\u0435\", \"educational\"],\n[\"\u0431\", null],\n[\"\u043d\u043e\u043c\", null],\n[\"\u2581\u043e\u043a\u0440\u0443\", \"district\"],\n[\"\u0433\u0435\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u0411\u044b\u043b \u043e\u0434\u0438\u043d \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442\u0441\u043a\u0438\u0439 \u0442\u043e\u0432\u0430\u0440\u0438\u0449, \u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043e\u043d \u0441\u0431\u043b\u0438\u0437\u0438\u043b\u0441\u044f \u043f\u043e\u0441\u043b\u0435 \u0438 \u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043e\u043d \u043c\u043e\u0433 \u0431\u044b \u043f\u043e\u0433\u043e\u0432\u043e\u0440\u0438\u0442\u044c \u043e \u043b\u0438\u0447\u043d\u043e\u043c \u0433\u043e\u0440\u0435; \u043d\u043e \u0442\u043e\u0432\u0430\u0440\u0438\u0449 \u044d\u0442\u043e\u0442 \u0431\u044b\u043b \u043f\u043e\u043f\u0435\u0447\u0438\u0442\u0435\u043b\u0435\u043c \u0432 \u0434\u0430\u043b\u044c\u043d\u0435\u043c \u0443\u0447\u0435\u0431\u043d\u043e\u043c \u043e\u043a\u0440\u0443\u0433\u0435.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0411\u044b', '\u043b', '\u2581\u043e\u0434\u0438\u043d', '\u2581\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435', '\u0442', '\u0441\u043a\u0438\u0439', '\u2581\u0442\u043e\u0432\u0430', '\u0440\u0438', '\u0449', ',', '\u2581\u0441', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b', '\u043c', '\u2581\u043e\u043d', '\u2581\u0441', '\u0431\u043b\u0438', '\u0437\u0438', '\u043b\u0441\u044f', '\u2581\u043f\u043e\u0441\u043b\u0435', '\u2581\u0438', '\u2581\u0441', '\u2581\u043a\u043e\u0442\u043e\u0440\u044b', '\u043c', '\u2581\u043e\u043d', '\u2581\u043c\u043e', '\u0433', '\u2581\u0431\u044b', '\u2581\u043f\u043e', '\u0433\u043e\u0432\u043e', '\u0440\u0438', '\u0442\u044c', '\u2581\u043e', '\u2581\u043b\u0438', '\u0447', '\u043d\u043e\u043c', '\u2581\u0433\u043e', '\u0440\u0435', ';', '\u2581\u043d\u043e', '\u2581\u0442\u043e\u0432\u0430', '\u0440\u0438', '\u0449', '\u2581\u044d\u0442\u043e\u0442', '\u2581\u0431\u044b\u043b', '\u2581\u043f\u043e', '\u043f\u0435', '\u0447\u0438', '\u0442\u0435\u043b\u0435\u043c', '\u2581\u0432', '\u2581\u0434\u0430', '\u043b\u044c', '\u043d\u0435\u043c', '\u2581\u0443\u0447\u0435', '\u0431', '\u043d\u043e\u043c', '\u2581\u043e\u043a\u0440\u0443', '\u0433\u0435', '.']"}], "[\n[\"\u2581\u0411\u044b\", \"Il y avait\"],\n[\"\u043b\", null],\n[\"\u2581\u043e\u0434\u0438\u043d\", \"un\"],\n[\"\u2581\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\", \"universitaire\"],\n[\"\u0442\", null],\n[\"\u0441\u043a\u0438\u0439\", null],\n[\"\u2581\u0442\u043e\u0432\u0430\", \"camarade\"],\n[\"\u0440\u0438\", null],\n[\"\u0449\", null],\n[\",\", null],\n[\"\u2581\u0441\", \"avec\"],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\", \"lequel\"],\n[\"\u043c\", null],\n[\"\u2581\u043e\u043d\", \"il\"],\n[\"\u2581\u0441\", null],\n[\"\u0431\u043b\u0438\", \"s'\u00e9tait rapproch\u00e9\"],\n[\"\u0437\u0438\", null],\n[\"\u043b\u0441\u044f\", null],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\", \"plus tard\"],\n[\"\u2581\u0438\", \"et\"],\n[\"\u2581\u0441\", \"avec\"],\n[\"\u2581\u043a\u043e\u0442\u043e\u0440\u044b\", \"lequel\"],\n[\"\u043c\", null],\n[\"\u2581\u043e\u043d\", \"il\"],\n[\"\u2581\u043c\u043e\", \"pouvait\"],\n[\"\u0433\", null],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u043f\u043e\", \"parler\"],\n[\"\u0433\u043e\u0432\u043e\", null],\n[\"\u0440\u0438\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043e\", \"de\"],\n[\"\u2581\u043b\u0438\", \"personnel\"],\n[\"\u0447\", null],\n[\"\u043d\u043e\u043c\", null],\n[\"\u2581\u0433\u043e\", \"chagrin\"],\n[\"\u0440\u0435\", null],\n[\";\", null],\n[\"\u2581\u043d\u043e\", \"mais\"],\n[\"\u2581\u0442\u043e\u0432\u0430\", \"camarade\"],\n[\"\u0440\u0438\", null],\n[\"\u0449\", null],\n[\"\u2581\u044d\u0442\u043e\u0442\", \"ce\"],\n[\"\u2581\u0431\u044b\u043b\", \"\u00e9tait\"],\n[\"\u2581\u043f\u043e\", \"curateur\"],\n[\"\u043f\u0435\", null],\n[\"\u0447\u0438\", null],\n[\"\u0442\u0435\u043b\u0435\u043c\", null],\n[\"\u2581\u0432\", \"dans\"],\n[\"\u2581\u0434\u0430\", \"lointain\"],\n[\"\u043b\u044c\", null],\n[\"\u043d\u0435\u043c\", null],\n[\"\u2581\u0443\u0447\u0435\", \"scolaire\"],\n[\"\u0431\", null],\n[\"\u043d\u043e\u043c\", null],\n[\"\u2581\u043e\u043a\u0440\u0443\", \"district\"],\n[\"\u0433\u0435\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041d\u0430 \u0442\u0440\u0435\u0442\u0438\u0439 \u0434\u0435\u043d\u044c \u041d\u0438\u043a\u043e\u043b\u0430\u0439 \u0432\u044b\u0437\u0432\u0430\u043b \u0431\u0440\u0430\u0442\u0430 \u0432\u044b\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u043e\u043f\u044f\u0442\u044c \u0435\u043c\u0443 \u0441\u0432\u043e\u0439 \u043f\u043b\u0430\u043d \u0438 \u0441\u0442\u0430\u043b \u043d\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0441\u0443\u0436\u0434\u0430\u0442\u044c \u0435\u0433\u043e, \u043d\u043e \u0441\u0442\u0430\u043b \u0443\u043c\u044b\u0448\u043b\u0435\u043d\u043d\u043e \u0441\u043c\u0435\u0448\u0438\u0432\u0430\u0442\u044c \u0435\u0433\u043e \u0441 \u043a\u043e\u043c\u043c\u0443\u043d\u0438\u0437\u043c\u043e\u043c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041d\u0430', '\u2581\u0442\u0440\u0435', '\u0442\u0438\u0439', '\u2581\u0434\u0435\u043d\u044c', '\u2581\u041d\u0438\u043a\u043e\u043b\u0430\u0439', '\u2581\u0432\u044b', '\u0437\u0432\u0430', '\u043b', '\u2581\u0431\u0440\u0430', '\u0442\u0430', '\u2581\u0432\u044b', '\u0441\u043a\u0430', '\u0437\u0430', '\u0442\u044c', '\u2581\u043e\u043f', '\u044f', '\u0442\u044c', '\u2581\u0435\u043c\u0443', '\u2581\u0441\u0432\u043e\u0439', '\u2581\u043f', '\u043b\u0430\u043d', '\u2581\u0438', '\u2581\u0441\u0442\u0430\u043b', '\u2581\u043d\u0435', '\u2581\u0442\u043e\u043b\u044c\u043a\u043e', '\u2581\u043e\u0441\u0443', '\u0436\u0434\u0430', '\u0442\u044c', '\u2581\u0435\u0433\u043e', ',', '\u2581\u043d\u043e', '\u2581\u0441\u0442\u0430\u043b', '\u2581\u0443', '\u043c\u044b', '\u0448\u043b\u0435\u043d', '\u043d\u043e', '\u2581\u0441', '\u043c\u0435', '\u0448\u0438', '\u0432\u0430\u0442\u044c', '\u2581\u0435\u0433\u043e', '\u2581\u0441', '\u2581\u043a\u043e\u043c\u043c\u0443', '\u043d\u0438', '\u0437', '\u043c\u043e\u043c', '.']"}], "[\n[\"\u2581\u041d\u0430\", \"On\"],\n[\"\u2581\u0442\u0440\u0435\", \"third\"],\n[\"\u0442\u0438\u0439\", null],\n[\"\u2581\u0434\u0435\u043d\u044c\", \"day\"],\n[\"\u2581\u041d\u0438\u043a\u043e\u043b\u0430\u0439\", \"Nikolai\"],\n[\"\u2581\u0432\u044b\", null],\n[\"\u0437\u0432\u0430\", null],\n[\"\u043b\", null],\n[\"\u2581\u0431\u0440\u0430\", \"brother\"],\n[\"\u0442\u0430\", null],\n[\"\u2581\u0432\u044b\", null],\n[\"\u0441\u043a\u0430\", null],\n[\"\u0437\u0430\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043e\u043f\", \"again\"],\n[\"\u044f\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0435\u043c\u0443\", \"him\"],\n[\"\u2581\u0441\u0432\u043e\u0439\", \"his\"],\n[\"\u2581\u043f\", \"plan\"],\n[\"\u043b\u0430\u043d\", null],\n[\"\u2581\u0438\", \"and\"],\n[\"\u2581\u0441\u0442\u0430\u043b\", \"began\"],\n[\"\u2581\u043d\u0435\", \"not\"],\n[\"\u2581\u0442\u043e\u043b\u044c\u043a\u043e\", \"only\"],\n[\"\u2581\u043e\u0441\u0443\", \"condemn\"],\n[\"\u0436\u0434\u0430\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0435\u0433\u043e\", \"it\"],\n[\",\", null],\n[\"\u2581\u043d\u043e\", \"but\"],\n[\"\u2581\u0441\u0442\u0430\u043b\", \"began\"],\n[\"\u2581\u0443\", null],\n[\"\u043c\u044b\", null],\n[\"\u0448\u043b\u0435\u043d\", null],\n[\"\u043d\u043e\", null],\n[\"\u2581\u0441\", null],\n[\"\u043c\u0435\", null],\n[\"\u0448\u0438\", null],\n[\"\u0432\u0430\u0442\u044c\", null],\n[\"\u2581\u0435\u0433\u043e\", \"it\"],\n[\"\u2581\u0441\", \"with\"],\n[\"\u2581\u043a\u043e\u043c\u043c\u0443\", \"communism\"],\n[\"\u043d\u0438\", null],\n[\"\u0437\", null],\n[\"\u043c\u043e\u043c\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u041d\u0430 \u0442\u0440\u0435\u0442\u0438\u0439 \u0434\u0435\u043d\u044c \u041d\u0438\u043a\u043e\u043b\u0430\u0439 \u0432\u044b\u0437\u0432\u0430\u043b \u0431\u0440\u0430\u0442\u0430 \u0432\u044b\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u043e\u043f\u044f\u0442\u044c \u0435\u043c\u0443 \u0441\u0432\u043e\u0439 \u043f\u043b\u0430\u043d \u0438 \u0441\u0442\u0430\u043b \u043d\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0441\u0443\u0436\u0434\u0430\u0442\u044c \u0435\u0433\u043e, \u043d\u043e \u0441\u0442\u0430\u043b \u0443\u043c\u044b\u0448\u043b\u0435\u043d\u043d\u043e \u0441\u043c\u0435\u0448\u0438\u0432\u0430\u0442\u044c \u0435\u0433\u043e \u0441 \u043a\u043e\u043c\u043c\u0443\u043d\u0438\u0437\u043c\u043e\u043c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041d\u0430', '\u2581\u0442\u0440\u0435', '\u0442\u0438\u0439', '\u2581\u0434\u0435\u043d\u044c', '\u2581\u041d\u0438\u043a\u043e\u043b\u0430\u0439', '\u2581\u0432\u044b', '\u0437\u0432\u0430', '\u043b', '\u2581\u0431\u0440\u0430', '\u0442\u0430', '\u2581\u0432\u044b', '\u0441\u043a\u0430', '\u0437\u0430', '\u0442\u044c', '\u2581\u043e\u043f', '\u044f', '\u0442\u044c', '\u2581\u0435\u043c\u0443', '\u2581\u0441\u0432\u043e\u0439', '\u2581\u043f', '\u043b\u0430\u043d', '\u2581\u0438', '\u2581\u0441\u0442\u0430\u043b', '\u2581\u043d\u0435', '\u2581\u0442\u043e\u043b\u044c\u043a\u043e', '\u2581\u043e\u0441\u0443', '\u0436\u0434\u0430', '\u0442\u044c', '\u2581\u0435\u0433\u043e', ',', '\u2581\u043d\u043e', '\u2581\u0441\u0442\u0430\u043b', '\u2581\u0443', '\u043c\u044b', '\u0448\u043b\u0435\u043d', '\u043d\u043e', '\u2581\u0441', '\u043c\u0435', '\u0448\u0438', '\u0432\u0430\u0442\u044c', '\u2581\u0435\u0433\u043e', '\u2581\u0441', '\u2581\u043a\u043e\u043c\u043c\u0443', '\u043d\u0438', '\u0437', '\u043c\u043e\u043c', '.']"}], "[\n[\"\u2581\u041d\u0430\", \"Le\"],\n[\"\u2581\u0442\u0440\u0435\", \"troisi\u00e8me\"],\n[\"\u0442\u0438\u0439\", null],\n[\"\u2581\u0434\u0435\u043d\u044c\", \"jour\"],\n[\"\u2581\u041d\u0438\u043a\u043e\u043b\u0430\u0439\", \"Nicolas\"],\n[\"\u2581\u0432\u044b\", null],\n[\"\u0437\u0432\u0430\", null],\n[\"\u043b\", null],\n[\"\u2581\u0431\u0440\u0430\", \"fr\u00e8re\"],\n[\"\u0442\u0430\", null],\n[\"\u2581\u0432\u044b\", null],\n[\"\u0441\u043a\u0430\", null],\n[\"\u0437\u0430\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043e\u043f\", \"encore\"],\n[\"\u044f\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0435\u043c\u0443\", \"lui\"],\n[\"\u2581\u0441\u0432\u043e\u0439\", \"son\"],\n[\"\u2581\u043f\", \"plan\"],\n[\"\u043b\u0430\u043d\", null],\n[\"\u2581\u0438\", \"et\"],\n[\"\u2581\u0441\u0442\u0430\u043b\", \"commen\u00e7a\"],\n[\"\u2581\u043d\u0435\", \"non\"],\n[\"\u2581\u0442\u043e\u043b\u044c\u043a\u043e\", \"seulement\"],\n[\"\u2581\u043e\u0441\u0443\", \"condamner\"],\n[\"\u0436\u0434\u0430\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0435\u0433\u043e\", \"le\"],\n[\",\", null],\n[\"\u2581\u043d\u043e\", \"mais\"],\n[\"\u2581\u0441\u0442\u0430\u043b\", \"commen\u00e7a\"],\n[\"\u2581\u0443\", null],\n[\"\u043c\u044b\", null],\n[\"\u0448\u043b\u0435\u043d\", null],\n[\"\u043d\u043e\", null],\n[\"\u2581\u0441\", null],\n[\"\u043c\u0435\", null],\n[\"\u0448\u0438\", null],\n[\"\u0432\u0430\u0442\u044c\", null],\n[\"\u2581\u0435\u0433\u043e\", \"le\"],\n[\"\u2581\u0441\", \"avec\"],\n[\"\u2581\u043a\u043e\u043c\u043c\u0443\", \"communisme\"],\n[\"\u043d\u0438\", null],\n[\"\u0437\", null],\n[\"\u043c\u043e\u043c\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041e\u043d \u0441\u0435\u043b, \u043e\u0431\u0432\u0435\u0440\u043d\u0443\u043b \u0441\u0435\u0431\u0435 \u043d\u043e\u0433\u0438 \u0442\u0438\u0433\u0440\u043e\u0432\u044b\u043c \u043f\u043b\u0435\u0434\u043e\u043c \u0438 \u0437\u0430\u043a\u0443\u0440\u0438\u043b \u0441\u0438\u0433\u0430\u0440\u0443. -- \u041a\u0430\u043a \u044d\u0442\u043e \u0442\u044b \u043d\u0435 \u043a\u0443\u0440\u0438\u0448\u044c!\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041e\u043d', '\u2581\u0441\u0435', '\u043b', ',', '\u2581\u043e\u0431', '\u0432\u0435\u0440', '\u043d\u0443', '\u043b', '\u2581\u0441\u0435\u0431\u0435', '\u2581\u043d\u043e', '\u0433\u0438', '\u2581\u0442\u0438', '\u0433', '\u0440\u043e\u0432', '\u044b\u043c', '\u2581\u043f', '\u043b\u0435', '\u0434\u043e\u043c', '\u2581\u0438', '\u2581\u0437\u0430', '\u043a\u0443', '\u0440\u0438', '\u043b', '\u2581\u0441\u0438', '\u0433\u0430', '\u0440\u0443', '.', '\u2581--', '\u2581\u041a\u0430', '\u043a', '\u2581\u044d\u0442\u043e', '\u2581\u0442\u044b', '\u2581\u043d\u0435', '\u2581\u043a\u0443', '\u0440\u0438', '\u0448\u044c', '!']"}], "[\n  [\"\u2581\u041e\u043d\", \"He\"],\n  [\"\u2581\u0441\u0435\", \"sat\"],\n  [\"\u043b\", null],\n  [\",\", null],\n  [\"\u2581\u043e\u0431\", \"wrapped\"],\n  [\"\u0432\u0435\u0440\", null],\n  [\"\u043d\u0443\", null],\n  [\"\u043b\", null],\n  [\"\u2581\u0441\u0435\u0431\u0435\", \"himself\"],\n  [\"\u2581\u043d\u043e\", \"legs\"],\n  [\"\u0433\u0438\", null],\n  [\"\u2581\u0442\u0438\", \"tiger\"],\n  [\"\u0433\", null],\n  [\"\u0440\u043e\u0432\", null],\n  [\"\u044b\u043c\", null],\n  [\"\u2581\u043f\", \"blanket\"],\n  [\"\u043b\u0435\", null],\n  [\"\u0434\u043e\u043c\", null],\n  [\"\u2581\u0438\", null],\n  [\"\u2581\u0437\u0430\", \"lit\"],\n  [\"\u043a\u0443\", null],\n  [\"\u0440\u0438\", null],\n  [\"\u043b\", null],\n  [\"\u2581\u0441\u0438\", \"cigar\"],\n  [\"\u0433\u0430\", null],\n  [\"\u0440\u0443\", null],\n  [\".\", null],\n  [\"\u2581--\", null],\n  [\"\u2581\u041a\u0430\", \"How\"],\n  [\"\u043a\", null],\n  [\"\u2581\u044d\u0442\u043e\", null],\n  [\"\u2581\u0442\u044b\", \"you\"],\n  [\"\u2581\u043d\u0435\", \"not\"],\n  [\"\u2581\u043a\u0443\", \"smoke\"],\n  [\"\u0440\u0438\", null],\n  [\"\u0448\u044c\", null],\n  [\"!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u041e\u043d \u0441\u0435\u043b, \u043e\u0431\u0432\u0435\u0440\u043d\u0443\u043b \u0441\u0435\u0431\u0435 \u043d\u043e\u0433\u0438 \u0442\u0438\u0433\u0440\u043e\u0432\u044b\u043c \u043f\u043b\u0435\u0434\u043e\u043c \u0438 \u0437\u0430\u043a\u0443\u0440\u0438\u043b \u0441\u0438\u0433\u0430\u0440\u0443. -- \u041a\u0430\u043a \u044d\u0442\u043e \u0442\u044b \u043d\u0435 \u043a\u0443\u0440\u0438\u0448\u044c!\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041e\u043d', '\u2581\u0441\u0435', '\u043b', ',', '\u2581\u043e\u0431', '\u0432\u0435\u0440', '\u043d\u0443', '\u043b', '\u2581\u0441\u0435\u0431\u0435', '\u2581\u043d\u043e', '\u0433\u0438', '\u2581\u0442\u0438', '\u0433', '\u0440\u043e\u0432', '\u044b\u043c', '\u2581\u043f', '\u043b\u0435', '\u0434\u043e\u043c', '\u2581\u0438', '\u2581\u0437\u0430', '\u043a\u0443', '\u0440\u0438', '\u043b', '\u2581\u0441\u0438', '\u0433\u0430', '\u0440\u0443', '.', '\u2581--', '\u2581\u041a\u0430', '\u043a', '\u2581\u044d\u0442\u043e', '\u2581\u0442\u044b', '\u2581\u043d\u0435', '\u2581\u043a\u0443', '\u0440\u0438', '\u0448\u044c', '!']"}], "[\n[\"\u2581\u041e\u043d\", \"Il\"],\n[\"\u2581\u0441\u0435\", \"s'est assis\"],\n[\"\u043b\", null],\n[\",\", null],\n[\"\u2581\u043e\u0431\", \"envelopp\u00e9\"],\n[\"\u0432\u0435\u0440\", null],\n[\"\u043d\u0443\", null],\n[\"\u043b\", null],\n[\"\u2581\u0441\u0435\u0431\u0435\", \"ses\"],\n[\"\u2581\u043d\u043e\", \"jambes\"],\n[\"\u0433\u0438\", null],\n[\"\u2581\u0442\u0438\", \"couverture\"],\n[\"\u0433\", null],\n[\"\u0440\u043e\u0432\", null],\n[\"\u044b\u043c\", null],\n[\"\u2581\u043f\", \"plaid\"],\n[\"\u043b\u0435\", null],\n[\"\u0434\u043e\u043c\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0437\u0430\", \"allum\u00e9\"],\n[\"\u043a\u0443\", null],\n[\"\u0440\u0438\", null],\n[\"\u043b\", null],\n[\"\u2581\u0441\u0438\", \"cigare\"],\n[\"\u0433\u0430\", null],\n[\"\u0440\u0443\", null],\n[\".\", null],\n[\"\u2581--\", null],\n[\"\u2581\u041a\u0430\", \"Comment\"],\n[\"\u043a\", null],\n[\"\u2581\u044d\u0442\u043e\", null],\n[\"\u2581\u0442\u044b\", \"tu\"],\n[\"\u2581\u043d\u0435\", \"ne\"],\n[\"\u2581\u043a\u0443\", \"fumes\"],\n[\"\u0440\u0438\", null],\n[\"\u0448\u044c\", null],\n[\"!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041a\u043d\u044f\u0433\u0438\u043d\u044f \u041c\u044f\u0433\u043a\u0430\u044f \u0441\u0438\u0434\u0435\u043b\u0430 \u043f\u043e\u0441\u0435\u0440\u0435\u0434\u0438\u043d\u0435 \u043c\u0435\u0436\u0434\u0443 \u043e\u0431\u043e\u0438\u043c\u0438 \u043a\u0440\u0443\u0436\u043a\u0430\u043c\u0438 \u0438, \u043f\u0440\u0438\u0441\u043b\u0443\u0448\u0438\u0432\u0430\u044f\u0441\u044c, \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u043b\u0430 \u0443\u0447\u0430\u0441\u0442\u0438\u0435 \u0442\u043e \u0432 \u0442\u043e\u043c, \u0442\u043e \u0432 \u0434\u0440\u0443\u0433\u043e\u043c. -- \u041c\u043d\u0435 \u043d\u044b\u043d\u0447\u0435 \u0442\u0440\u0438 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430 \u0441\u043a\u0430\u0437\u0430\u043b\u0438 \u044d\u0442\u0443 \u0441\u0430\u043c\u0443\u044e \u0444\u0440\u0430\u0437\u0443 \u043f\u0440\u043e \u041a\u0430\u0443\u043b\u044c\u0431\u0430\u0445\u0430, \u0442\u043e\u0447\u043d\u043e \u0441\u0433\u043e\u0432\u043e\u0440\u0438\u043b\u0438\u0441\u044c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041a', '\u043d\u044f', '\u0433\u0438', '\u043d\u044f', '\u2581\u041c', '\u044f', '\u0433', '\u043a\u0430\u044f', '\u2581\u0441\u0438', '\u0434\u0435\u043b\u0430', '\u2581\u043f\u043e\u0441\u0435', '\u0440\u0435\u0434\u0438', '\u043d\u0435', '\u2581\u043c\u0435\u0436\u0434\u0443', '\u2581\u043e\u0431\u043e', '\u0438', '\u043c\u0438', '\u2581\u043a', '\u0440\u0443', '\u0436', '\u043a\u0430\u043c\u0438', '\u2581\u0438', ',', '\u2581\u043f\u0440\u0438', '\u0441\u043b\u0443', '\u0448\u0438', '\u0432\u0430\u044f', '\u0441\u044c', ',', '\u2581\u043f\u0440\u0438', '\u043d\u0438\u043c\u0430', '\u043b\u0430', '\u2581\u0443\u0447\u0430\u0441\u0442\u0438\u0435', '\u2581\u0442\u043e', '\u2581\u0432', '\u2581\u0442\u043e\u043c', ',', '\u2581\u0442\u043e', '\u2581\u0432', '\u2581\u0434\u0440\u0443', '\u0433\u043e\u043c', '.', '\u2581--', '\u2581\u041c', '\u043d\u0435', '\u2581', '\u043d\u044b', '\u043d', '\u0447\u0435', '\u2581\u0442\u0440\u0438', '\u2581\u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430', '\u2581\u0441', '\u043a\u0430\u0437\u0430', '\u043b\u0438', '\u2581\u044d', '\u0442\u0443', '\u2581\u0441\u0430', '\u043c\u0443', '\u044e', '\u2581\u0444', '\u0440\u0430', '\u0437\u0443', '\u2581\u043f\u0440\u043e', '\u2581\u041a\u0430', '\u0443', '\u043b\u044c', '\u0431\u0430', '\u0445\u0430', ',', '\u2581\u0442\u043e', '\u0447\u043d\u043e', '\u2581\u0441', '\u0433\u043e\u0432\u043e', '\u0440\u0438', '\u043b\u0438\u0441\u044c', '.']"}], "[\n[\"\u2581\u041a\", \"Princess\"],\n[\"\u043d\u044f\", null],\n[\"\u0433\u0438\", null],\n[\"\u043d\u044f\", null],\n[\"\u2581\u041c\", \"Myagkaya\"],\n[\"\u044f\", null],\n[\"\u0433\", null],\n[\"\u043a\u0430\u044f\", null],\n[\"\u2581\u0441\u0438\", \"sat\"],\n[\"\u0434\u0435\u043b\u0430\", null],\n[\"\u2581\u043f\u043e\u0441\u0435\", \"in\"],\n[\"\u0440\u0435\u0434\u0438\", null],\n[\"\u043d\u0435\", null],\n[\"\u2581\u043c\u0435\u0436\u0434\u0443\", \"between\"],\n[\"\u2581\u043e\u0431\u043e\", \"both\"],\n[\"\u0438\", null],\n[\"\u043c\u0438\", null],\n[\"\u2581\u043a\", \"circles\"],\n[\"\u0440\u0443\", null],\n[\"\u0436\", null],\n[\"\u043a\u0430\u043c\u0438\", null],\n[\"\u2581\u0438\", \"and\"],\n[\",\", null],\n[\"\u2581\u043f\u0440\u0438\", \"listening\"],\n[\"\u0441\u043b\u0443\", null],\n[\"\u0448\u0438\", null],\n[\"\u0432\u0430\u044f\", null],\n[\"\u0441\u044c\", null],\n[\",\", null],\n[\"\u2581\u043f\u0440\u0438\", \"participated\"],\n[\"\u043d\u0438\u043c\u0430\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\u0443\u0447\u0430\u0441\u0442\u0438\u0435\", null],\n[\"\u2581\u0442\u043e\", \"now\"],\n[\"\u2581\u0432\", \"in\"],\n[\"\u2581\u0442\u043e\u043c\", \"this\"],\n[\",\", null],\n[\"\u2581\u0442\u043e\", \"now\"],\n[\"\u2581\u0432\", \"in\"],\n[\"\u2581\u0434\u0440\u0443\", \"other\"],\n[\"\u0433\u043e\u043c\", null],\n[\".\", null],\n[\"\u2581--\", null],\n[\"\u2581\u041c\", \"Today\"],\n[\"\u043d\u0435\", null],\n[\"\u2581\", null],\n[\"\u043d\u044b\", null],\n[\"\u043d\", null],\n[\"\u0447\u0435\", null],\n[\"\u2581\u0442\u0440\u0438\", \"three\"],\n[\"\u2581\u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430\", \"people\"],\n[\"\u2581\u0441\", \"told\"],\n[\"\u043a\u0430\u0437\u0430\", null],\n[\"\u043b\u0438\", null],\n[\"\u2581\u044d\", \"this\"],\n[\"\u0442\u0443\", null],\n[\"\u2581\u0441\u0430\", \"very\"],\n[\"\u043c\u0443\", null],\n[\"\u044e\", null],\n[\"\u2581\u0444\", \"phrase\"],\n[\"\u0440\u0430\", null],\n[\"\u0437\u0443\", null],\n[\"\u2581\u043f\u0440\u043e\", \"about\"],\n[\"\u2581\u041a\u0430\", \"Kaulbach\"],\n[\"\u0443\", null],\n[\"\u043b\u044c\", null],\n[\"\u0431\u0430\", null],\n[\"\u0445\u0430\", null],\n[\",\", null],\n[\"\u2581\u0442\u043e\", \"as\"],\n[\"\u0447\u043d\u043e\", null],\n[\"\u2581\u0441\", \"if\"],\n[\"\u0433\u043e\u0432\u043e\", \"agreed\"],\n[\"\u0440\u0438\", null],\n[\"\u043b\u0438\u0441\u044c\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u041a\u043d\u044f\u0433\u0438\u043d\u044f \u041c\u044f\u0433\u043a\u0430\u044f \u0441\u0438\u0434\u0435\u043b\u0430 \u043f\u043e\u0441\u0435\u0440\u0435\u0434\u0438\u043d\u0435 \u043c\u0435\u0436\u0434\u0443 \u043e\u0431\u043e\u0438\u043c\u0438 \u043a\u0440\u0443\u0436\u043a\u0430\u043c\u0438 \u0438, \u043f\u0440\u0438\u0441\u043b\u0443\u0448\u0438\u0432\u0430\u044f\u0441\u044c, \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u043b\u0430 \u0443\u0447\u0430\u0441\u0442\u0438\u0435 \u0442\u043e \u0432 \u0442\u043e\u043c, \u0442\u043e \u0432 \u0434\u0440\u0443\u0433\u043e\u043c. -- \u041c\u043d\u0435 \u043d\u044b\u043d\u0447\u0435 \u0442\u0440\u0438 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430 \u0441\u043a\u0430\u0437\u0430\u043b\u0438 \u044d\u0442\u0443 \u0441\u0430\u043c\u0443\u044e \u0444\u0440\u0430\u0437\u0443 \u043f\u0440\u043e \u041a\u0430\u0443\u043b\u044c\u0431\u0430\u0445\u0430, \u0442\u043e\u0447\u043d\u043e \u0441\u0433\u043e\u0432\u043e\u0440\u0438\u043b\u0438\u0441\u044c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041a', '\u043d\u044f', '\u0433\u0438', '\u043d\u044f', '\u2581\u041c', '\u044f', '\u0433', '\u043a\u0430\u044f', '\u2581\u0441\u0438', '\u0434\u0435\u043b\u0430', '\u2581\u043f\u043e\u0441\u0435', '\u0440\u0435\u0434\u0438', '\u043d\u0435', '\u2581\u043c\u0435\u0436\u0434\u0443', '\u2581\u043e\u0431\u043e', '\u0438', '\u043c\u0438', '\u2581\u043a', '\u0440\u0443', '\u0436', '\u043a\u0430\u043c\u0438', '\u2581\u0438', ',', '\u2581\u043f\u0440\u0438', '\u0441\u043b\u0443', '\u0448\u0438', '\u0432\u0430\u044f', '\u0441\u044c', ',', '\u2581\u043f\u0440\u0438', '\u043d\u0438\u043c\u0430', '\u043b\u0430', '\u2581\u0443\u0447\u0430\u0441\u0442\u0438\u0435', '\u2581\u0442\u043e', '\u2581\u0432', '\u2581\u0442\u043e\u043c', ',', '\u2581\u0442\u043e', '\u2581\u0432', '\u2581\u0434\u0440\u0443', '\u0433\u043e\u043c', '.', '\u2581--', '\u2581\u041c', '\u043d\u0435', '\u2581', '\u043d\u044b', '\u043d', '\u0447\u0435', '\u2581\u0442\u0440\u0438', '\u2581\u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430', '\u2581\u0441', '\u043a\u0430\u0437\u0430', '\u043b\u0438', '\u2581\u044d', '\u0442\u0443', '\u2581\u0441\u0430', '\u043c\u0443', '\u044e', '\u2581\u0444', '\u0440\u0430', '\u0437\u0443', '\u2581\u043f\u0440\u043e', '\u2581\u041a\u0430', '\u0443', '\u043b\u044c', '\u0431\u0430', '\u0445\u0430', ',', '\u2581\u0442\u043e', '\u0447\u043d\u043e', '\u2581\u0441', '\u0433\u043e\u0432\u043e', '\u0440\u0438', '\u043b\u0438\u0441\u044c', '.']"}], "[\n[\"\u2581\u041a\", \"La princesse\"],\n[\"\u043d\u044f\", null],\n[\"\u0433\u0438\", null],\n[\"\u043d\u044f\", null],\n[\"\u2581\u041c\", \"Miagka\u00efa\"],\n[\"\u044f\", null],\n[\"\u0433\", null],\n[\"\u043a\u0430\u044f\", null],\n[\"\u2581\u0441\u0438\", \"\u00e9tait assise\"],\n[\"\u0434\u0435\u043b\u0430\", null],\n[\"\u2581\u043f\u043e\u0441\u0435\", \"au milieu\"],\n[\"\u0440\u0435\u0434\u0438\", null],\n[\"\u043d\u0435\", null],\n[\"\u2581\u043c\u0435\u0436\u0434\u0443\", \"entre\"],\n[\"\u2581\u043e\u0431\u043e\", \"les deux\"],\n[\"\u0438\", null],\n[\"\u043c\u0438\", null],\n[\"\u2581\u043a\", \"cercles\"],\n[\"\u0440\u0443\", null],\n[\"\u0436\", null],\n[\"\u043a\u0430\u043c\u0438\", null],\n[\"\u2581\u0438\", \"et\"],\n[\",\", null],\n[\"\u2581\u043f\u0440\u0438\", \"\u00e9coutant\"],\n[\"\u0441\u043b\u0443\", null],\n[\"\u0448\u0438\", null],\n[\"\u0432\u0430\u044f\", null],\n[\"\u0441\u044c\", null],\n[\",\", null],\n[\"\u2581\u043f\u0440\u0438\", \"participait\"],\n[\"\u043d\u0438\u043c\u0430\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\u0443\u0447\u0430\u0441\u0442\u0438\u0435\", null],\n[\"\u2581\u0442\u043e\", \"tant\u00f4t\"],\n[\"\u2581\u0432\", \"\u00e0\"],\n[\"\u2581\u0442\u043e\u043c\", \"l'un\"],\n[\",\", null],\n[\"\u2581\u0442\u043e\", \"tant\u00f4t\"],\n[\"\u2581\u0432\", \"\u00e0\"],\n[\"\u2581\u0434\u0440\u0443\", \"l'autre\"],\n[\"\u0433\u043e\u043c\", null],\n[\".\", null],\n[\"\u2581--\", null],\n[\"\u2581\u041c\", \"Aujourd'hui\"],\n[\"\u043d\u0435\", null],\n[\"\u2581\", null],\n[\"\u043d\u044b\", null],\n[\"\u043d\", null],\n[\"\u0447\u0435\", null],\n[\"\u2581\u0442\u0440\u0438\", \"trois\"],\n[\"\u2581\u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430\", \"personnes\"],\n[\"\u2581\u0441\", \"m'ont dit\"],\n[\"\u043a\u0430\u0437\u0430\", null],\n[\"\u043b\u0438\", null],\n[\"\u2581\u044d\", \"cette\"],\n[\"\u0442\u0443\", null],\n[\"\u2581\u0441\u0430\", \"m\u00eame\"],\n[\"\u043c\u0443\", null],\n[\"\u044e\", null],\n[\"\u2581\u0444\", \"phrase\"],\n[\"\u0440\u0430\", null],\n[\"\u0437\u0443\", null],\n[\"\u2581\u043f\u0440\u043e\", \"sur\"],\n[\"\u2581\u041a\u0430\", \"Kaulbach\"],\n[\"\u0443\", null],\n[\"\u043b\u044c\", null],\n[\"\u0431\u0430\", null],\n[\"\u0445\u0430\", null],\n[\",\", null],\n[\"\u2581\u0442\u043e\", \"comme si\"],\n[\"\u0447\u043d\u043e\", null],\n[\"\u2581\u0441\", \"elles s'\u00e9taient\"],\n[\"\u0433\u043e\u0432\u043e\", \"concert\u00e9es\"],\n[\"\u0440\u0438\", null],\n[\"\u043b\u0438\u0441\u044c\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0422\u043e\u0447\u043d\u043e \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043a\u0430\u043a\u043e\u0435-\u043d\u0438\u0431\u0443\u0434\u044c \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0438 \u0431\u043e\u0440\u044c\u0431\u0430 \u0432 \u0431\u0435\u0441\u043a\u043e\u043d\u0435\u0447\u043d\u043e\u043c!\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0422\u043e', '\u0447\u043d\u043e', '\u2581\u043c\u043e\u0436\u0435\u0442', '\u2581\u0431\u044b\u0442\u044c', '\u2581\u043a\u0430', '\u043a\u043e', '\u0435', '-', '\u043d\u0438', '\u0431\u0443', '\u0434\u044c', '\u2581\u043d\u0430\u043f\u0440\u0430\u0432', '\u043b\u0435\u043d\u0438\u0435', '\u2581\u0438', '\u2581\u0431\u043e\u0440', '\u044c', '\u0431\u0430', '\u2581\u0432', '\u2581\u0431\u0435', '\u0441\u043a\u043e', '\u043d\u0435', '\u0447', '\u043d\u043e\u043c', '!']"}], "[\n    [\"\u2581\u0422\u043e\", \"Certainly\"],\n    [\"\u0447\u043d\u043e\", null],\n    [\"\u2581\u043c\u043e\u0436\u0435\u0442\", \"can\"],\n    [\"\u2581\u0431\u044b\u0442\u044c\", \"be\"],\n    [\"\u2581\u043a\u0430\", \"some\"],\n    [\"\u043a\u043e\", null],\n    [\"\u0435\", null],\n    [\"-\", null],\n    [\"\u043d\u0438\", null],\n    [\"\u0431\u0443\", null],\n    [\"\u0434\u044c\", null],\n    [\"\u2581\u043d\u0430\u043f\u0440\u0430\u0432\", \"direction\"],\n    [\"\u043b\u0435\u043d\u0438\u0435\", null],\n    [\"\u2581\u0438\", null],\n    [\"\u2581\u0431\u043e\u0440\", \"struggle\"],\n    [\"\u044c\", null],\n    [\"\u0431\u0430\", null],\n    [\"\u2581\u0432\", null],\n    [\"\u2581\u0431\u0435\", \"infinite\"],\n    [\"\u0441\u043a\u043e\", null],\n    [\"\u043d\u0435\", null],\n    [\"\u0447\", null],\n    [\"\u043d\u043e\u043c\", null],\n    [\"!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to French.\n\n---=== TEXT ===---\n\n\u0422\u043e\u0447\u043d\u043e \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043a\u0430\u043a\u043e\u0435-\u043d\u0438\u0431\u0443\u0434\u044c \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0438 \u0431\u043e\u0440\u044c\u0431\u0430 \u0432 \u0431\u0435\u0441\u043a\u043e\u043d\u0435\u0447\u043d\u043e\u043c!\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0422\u043e', '\u0447\u043d\u043e', '\u2581\u043c\u043e\u0436\u0435\u0442', '\u2581\u0431\u044b\u0442\u044c', '\u2581\u043a\u0430', '\u043a\u043e', '\u0435', '-', '\u043d\u0438', '\u0431\u0443', '\u0434\u044c', '\u2581\u043d\u0430\u043f\u0440\u0430\u0432', '\u043b\u0435\u043d\u0438\u0435', '\u2581\u0438', '\u2581\u0431\u043e\u0440', '\u044c', '\u0431\u0430', '\u2581\u0432', '\u2581\u0431\u0435', '\u0441\u043a\u043e', '\u043d\u0435', '\u0447', '\u043d\u043e\u043c', '!']"}], "[\n  [\"\u2581\u0422\u043e\", \"Certainement\"],\n  [\"\u0447\u043d\u043e\", null],\n  [\"\u2581\u043c\u043e\u0436\u0435\u0442\", \"peut\"],\n  [\"\u2581\u0431\u044b\u0442\u044c\", \"\u00eatre\"],\n  [\"\u2581\u043a\u0430\", \"quelque\"],\n  [\"\u043a\u043e\", null],\n  [\"\u0435\", null],\n  [\"-\", null],\n  [\"\u043d\u0438\", null],\n  [\"\u0431\u0443\", null],\n  [\"\u0434\u044c\", null],\n  [\"\u2581\u043d\u0430\u043f\u0440\u0430\u0432\", \"direction\"],\n  [\"\u043b\u0435\u043d\u0438\u0435\", null],\n  [\"\u2581\u0438\", null],\n  [\"\u2581\u0431\u043e\u0440\", \"lutte\"],\n  [\"\u044c\", null],\n  [\"\u0431\u0430\", null],\n  [\"\u2581\u0432\", null],\n  [\"\u2581\u0431\u0435\", \"infini\"],\n  [\"\u0441\u043a\u043e\", null],\n  [\"\u043d\u0435\", null],\n  [\"\u0447\", null],\n  [\"\u043d\u043e\u043c\", null],\n  [\"!\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nManchester United have signed a \"genius\" in Nemanja Matic, believes Jose Mourinho, after the midfielder made his debut against Sampdoria on Wednesday evening.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Manchester', '\u2581United', '\u2581have', '\u2581signed', '\u2581a', '\u2581\"', 'gen', 'ius', '\"', '\u2581in', '\u2581N', 'eman', 'ja', '\u2581M', 'atic', ',', '\u2581believes', '\u2581Jose', '\u2581M', 'our', 'inho', ',', '\u2581after', '\u2581the', '\u2581mid', 'f', 'iel', 'der', '\u2581made', '\u2581his', '\u2581debut', '\u2581against', '\u2581S', 'amp', 'd', 'oria', '\u2581on', '\u2581Wednesday', '\u2581evening', '.']"}], "[\n[\"\u2581Manchester\", \"\u66fc\u5f7b\u65af\u7279\"],\n[\"\u2581United\", \"\u8054\"],\n[\"\u2581have\", null],\n[\"\u2581signed\", \"\u7b7e\u4e0b\u4e86\"],\n[\"\u2581a\", null],\n[\"\u2581\\\"\", null],\n[\"gen\", \"\u5929\u624d\"],\n[\"ius\", null],\n[\"\\\"\", null],\n[\"\u2581in\", null],\n[\"\u2581N\", \"\u5185\u9a6c\u5c3c\u4e9a\"],\n[\"eman\", null],\n[\"ja\", null],\n[\"\u2581M\", \"\u9a6c\u8482\u5947\"],\n[\"atic\", null],\n[\",\", null],\n[\"\u2581believes\", \"\u8ba4\u4e3a\"],\n[\"\u2581Jose\", \"\u82e5\u6cfd\"],\n[\"\u2581M\", \"\u7a46\u91cc\u5c3c\u5965\"],\n[\"our\", null],\n[\"inho\", null],\n[\",\", null],\n[\"\u2581after\", null],\n[\"\u2581the\", null],\n[\"\u2581mid\", \"\u4e2d\u573a\"],\n[\"f\", null],\n[\"iel\", null],\n[\"der\", null],\n[\"\u2581made\", \"\u5b8c\u6210\u4e86\"],\n[\"\u2581his\", null],\n[\"\u2581debut\", \"\u9996\u79c0\"],\n[\"\u2581against\", \"\u5bf9\u9635\"],\n[\"\u2581S\", \"\u6851\u666e\u591a\u5229\u4e9a\"],\n[\"amp\", null],\n[\"d\", null],\n[\"oria\", null],\n[\"\u2581on\", null],\n[\"\u2581Wednesday\", \"\u5468\u4e09\"],\n[\"\u2581evening\", \"\u665a\u4e0a\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nManchester United have signed a \"genius\" in Nemanja Matic, believes Jose Mourinho, after the midfielder made his debut against Sampdoria on Wednesday evening.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Manchester', '\u2581United', '\u2581have', '\u2581signed', '\u2581a', '\u2581\"', 'gen', 'ius', '\"', '\u2581in', '\u2581N', 'eman', 'ja', '\u2581M', 'atic', ',', '\u2581believes', '\u2581Jose', '\u2581M', 'our', 'inho', ',', '\u2581after', '\u2581the', '\u2581mid', 'f', 'iel', 'der', '\u2581made', '\u2581his', '\u2581debut', '\u2581against', '\u2581S', 'amp', 'd', 'oria', '\u2581on', '\u2581Wednesday', '\u2581evening', '.']"}], "[\n[\"\u2581Manchester\", \"\u041c\u0430\u043d\u0447\u0435\u0441\u0442\u0435\u0440\"],\n[\"\u2581United\", \"\u042e\u043d\u0430\u0439\u0442\u0435\u0434\"],\n[\"\u2581have\", null],\n[\"\u2581signed\", \"\u049b\u043e\u043b \u049b\u043e\u0439\u0434\u044b\"],\n[\"\u2581a\", null],\n[\"\u2581\\\"\", null],\n[\"gen\", \"\u0434\u0430\u043d\u044b\u0448\u043f\u0430\u043d\"],\n[\"ius\", null],\n[\"\\\"\", null],\n[\"\u2581in\", null],\n[\"\u2581N\", \"\u041d\u0435\u043c\u0430\u043d\u044c\u044f\"],\n[\"eman\", null],\n[\"ja\", null],\n[\"\u2581M\", \"\u041c\u0430\u0442\u0438\u0447\"],\n[\"atic\", null],\n[\",\", null],\n[\"\u2581believes\", \"\u0441\u0435\u043d\u0435\u0434\u0456\"],\n[\"\u2581Jose\", \"\u0416\u043e\u0437\u0435\"],\n[\"\u2581M\", \"\u041c\u043e\u0443\u0440\u0438\u043d\u044c\u044e\"],\n[\"our\", null],\n[\"inho\", null],\n[\",\", null],\n[\"\u2581after\", null],\n[\"\u2581the\", null],\n[\"\u2581mid\", \"\u0436\u0430\u0440\u0442\u044b\u043b\u0430\u0439\"],\n[\"f\", null],\n[\"iel\", null],\n[\"der\", null],\n[\"\u2581made\", \"\u0436\u0430\u0441\u0430\u0434\u044b\"],\n[\"\u2581his\", null],\n[\"\u2581debut\", \"\u0434\u0435\u0431\u044e\u0442\"],\n[\"\u2581against\", \"\u049b\u0430\u0440\u0441\u044b\"],\n[\"\u2581S\", \"\u0421\u0430\u043c\u043f\u0434\u043e\u0440\u0438\u044f\"],\n[\"amp\", null],\n[\"d\", null],\n[\"oria\", null],\n[\"\u2581on\", null],\n[\"\u2581Wednesday\", \"\u0441\u04d9\u0440\u0441\u0435\u043d\u0431\u0456\"],\n[\"\u2581evening\", \"\u043a\u0435\u0448\u0456\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nManchester United have signed a \"genius\" in Nemanja Matic, believes Jose Mourinho, after the midfielder made his debut against Sampdoria on Wednesday evening.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Manchester', '\u2581United', '\u2581have', '\u2581signed', '\u2581a', '\u2581\"', 'gen', 'ius', '\"', '\u2581in', '\u2581N', 'eman', 'ja', '\u2581M', 'atic', ',', '\u2581believes', '\u2581Jose', '\u2581M', 'our', 'inho', ',', '\u2581after', '\u2581the', '\u2581mid', 'f', 'iel', 'der', '\u2581made', '\u2581his', '\u2581debut', '\u2581against', '\u2581S', 'amp', 'd', 'oria', '\u2581on', '\u2581Wednesday', '\u2581evening', '.']"}], "[\n[\"\u2581Manchester\", \"\u041c\u0430\u043d\u0447\u0435\u0441\u0442\u0435\u0440\"],\n[\"\u2581United\", \"\u042e\u043d\u0430\u0439\u0442\u0435\u0434\"],\n[\"\u2581have\", null],\n[\"\u2581signed\", \"\u043f\u043e\u0434\u043f\u0438\u0441\u0430\u043b\u0438\"],\n[\"\u2581a\", null],\n[\"\u2581\"\", null],\n[\"gen\", \"\u0433\u0435\u043d\u0438\u0439\"],\n[\"ius\", null],\n[\"\"\", null],\n[\"\u2581in\", null],\n[\"\u2581N\", \"\u041d\u0435\u043c\u0430\u043d\u044c\u044f\"],\n[\"eman\", null],\n[\"ja\", null],\n[\"\u2581M\", \"\u041c\u0430\u0442\u0438\u0447\"],\n[\"atic\", null],\n[\",\", null],\n[\"\u2581believes\", \"\u0441\u0447\u0438\u0442\u0430\u0435\u0442\"],\n[\"\u2581Jose\", \"\u0416\u043e\u0437\u0435\"],\n[\"\u2581M\", \"\u041c\u043e\u0443\u0440\u0438\u043d\u044c\u044e\"],\n[\"our\", null],\n[\"inho\", null],\n[\",\", null],\n[\"\u2581after\", \"\u043f\u043e\u0441\u043b\u0435 \u0442\u043e\u0433\u043e, \u043a\u0430\u043a\"],\n[\"\u2581the\", null],\n[\"\u2581mid\", \"\u043f\u043e\u043b\u0443\u0437\u0430\u0449\u0438\u0442\u043d\u0438\u043a\"],\n[\"f\", null],\n[\"iel\", null],\n[\"der\", null],\n[\"\u2581made\", \"\u043f\u0440\u043e\u0432\u0435\u043b\"],\n[\"\u2581his\", null],\n[\"\u2581debut\", \"\u0434\u0435\u0431\u044e\u0442\"],\n[\"\u2581against\", \"\u043f\u0440\u043e\u0442\u0438\u0432\"],\n[\"\u2581S\", \"\u0421\u0430\u043c\u043f\u0434\u043e\u0440\u0438\u044f\"],\n[\"amp\", null],\n[\"d\", null],\n[\"oria\", null],\n[\"\u2581on\", null],\n[\"\u2581Wednesday\", \"\u0441\u0440\u0435\u0434\u0443\"],\n[\"\u2581evening\", \"\u0432\u0435\u0447\u0435\u0440\u043e\u043c\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\n\"The idea is to showcase this week things that are made in America,\" Spicer said.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\"', 'The', '\u2581idea', '\u2581is', '\u2581to', '\u2581show', 'case', '\u2581this', '\u2581week', '\u2581things', '\u2581that', '\u2581are', '\u2581made', '\u2581in', '\u2581America', ',\"', '\u2581Sp', 'icer', '\u2581said', '.']"}], "[\n  [\"\u2581\\\"\", null],\n  [\"The\", null],\n  [\"\u2581idea\", \"\u60f3\u6cd5\"],\n  [\"\u2581is\", null],\n  [\"\u2581to\", null],\n  [\"\u2581show\", \"\u5c55\u793a\"],\n  [\"case\", null],\n  [\"\u2581this\", null],\n  [\"\u2581week\", \"\u672c\u5468\"],\n  [\"\u2581things\", \"\u4e1c\u897f\"],\n  [\"\u2581that\", null],\n  [\"\u2581are\", null],\n  [\"\u2581made\", \"\u5236\u9020\"],\n  [\"\u2581in\", null],\n  [\"\u2581America\", \"\u7f8e\u56fd\"],\n  [\",\\\"\", null],\n  [\"\u2581Sp\", \"\u65af\u6d3e\u585e\"],\n  [\"icer\", null],\n  [\"\u2581said\", \"\u8bf4\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\n\"The idea is to showcase this week things that are made in America,\" Spicer said.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\"', 'The', '\u2581idea', '\u2581is', '\u2581to', '\u2581show', 'case', '\u2581this', '\u2581week', '\u2581things', '\u2581that', '\u2581are', '\u2581made', '\u2581in', '\u2581America', ',\"', '\u2581Sp', 'icer', '\u2581said', '.']"}], "[\n  [\"\u2581\\\"\", null],\n  [\"The\", null],\n  [\"\u2581idea\", \"\u0438\u0434\u0435\u044f\"],\n  [\"\u2581is\", null],\n  [\"\u2581to\", null],\n  [\"\u2581show\", \"\u043a\u04e9\u0440\u0441\u0435\u0442\u0443\"],\n  [\"case\", null],\n  [\"\u2581this\", null],\n  [\"\u2581week\", \"\u0430\u043f\u0442\u0430\"],\n  [\"\u2581things\", \"\u0437\u0430\u0442\u0442\u0430\u0440\u0434\u044b\"],\n  [\"\u2581that\", null],\n  [\"\u2581are\", null],\n  [\"\u2581made\", \"\u0436\u0430\u0441\u0430\u043b\u0493\u0430\u043d\"],\n  [\"\u2581in\", null],\n  [\"\u2581America\", \"\u0410\u043c\u0435\u0440\u0438\u043a\u0430\u0434\u0430\"],\n  [\",\\\"\", null],\n  [\"\u2581Sp\", \"\u0421\u043f\u0430\u0439\u0441\u0435\u0440\"],\n  [\"icer\", null],\n  [\"\u2581said\", \"\u0434\u0435\u0434\u0456\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\n\"The idea is to showcase this week things that are made in America,\" Spicer said.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\"', 'The', '\u2581idea', '\u2581is', '\u2581to', '\u2581show', 'case', '\u2581this', '\u2581week', '\u2581things', '\u2581that', '\u2581are', '\u2581made', '\u2581in', '\u2581America', ',\"', '\u2581Sp', 'icer', '\u2581said', '.']"}], "[\n[\"\u2581\\\"\", null],\n[\"The\", null],\n[\"\u2581idea\", \"\u0438\u0434\u0435\u044f\"],\n[\"\u2581is\", null],\n[\"\u2581to\", null],\n[\"\u2581show\", \"\u043f\u0440\u043e\u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c\"],\n[\"case\", null],\n[\"\u2581this\", null],\n[\"\u2581week\", \"\u043d\u0435\u0434\u0435\u043b\u044e\"],\n[\"\u2581things\", \"\u0432\u0435\u0449\u0438\"],\n[\"\u2581that\", null],\n[\"\u2581are\", null],\n[\"\u2581made\", \"\u0441\u0434\u0435\u043b\u0430\u043d\u044b\"],\n[\"\u2581in\", null],\n[\"\u2581America\", \"\u0410\u043c\u0435\u0440\u0438\u043a\u0435\"],\n[\",\\\"\", null],\n[\"\u2581Sp\", \"\u0421\u043f\u0430\u0439\u0441\u0435\u0440\"],\n[\"icer\", null],\n[\"\u2581said\", \"\u0441\u043a\u0430\u0437\u0430\u043b\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nThe method of medical age assessment, which consists of taking X-rays of wisdom teeth and MRI scans of knee joints, then having dentists and radiologists analyse them, has also been criticised in some quarters.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581method', '\u2581of', '\u2581medical', '\u2581age', '\u2581assessment', ',', '\u2581which', '\u2581consists', '\u2581of', '\u2581taking', '\u2581X', '-', 'r', 'ays', '\u2581of', '\u2581wisdom', '\u2581teeth', '\u2581and', '\u2581M', 'RI', '\u2581sc', 'ans', '\u2581of', '\u2581knee', '\u2581joint', 's', ',', '\u2581then', '\u2581having', '\u2581dent', 'ists', '\u2581and', '\u2581radi', 'ologists', '\u2581analy', 'se', '\u2581them', ',', '\u2581has', '\u2581also', '\u2581been', '\u2581critic', 'ised', '\u2581in', '\u2581some', '\u2581quarters', '.']"}], "[\n[\"\u2581The\", \"\u8fd9\u79cd\"],\n[\"\u2581method\", \"\u65b9\u6cd5\"],\n[\"\u2581of\", null],\n[\"\u2581medical\", \"\u533b\u5b66\"],\n[\"\u2581age\", \"\u5e74\u9f84\"],\n[\"\u2581assessment\", \"\u8bc4\u4f30\"],\n[\",\", null],\n[\"\u2581which\", null],\n[\"\u2581consists\", \"\u5305\u62ec\"],\n[\"\u2581of\", null],\n[\"\u2581taking\", \"\u62cd\u6444\"],\n[\"\u2581X\", \"X\u5149\"],\n[\"-\", null],\n[\"r\", null],\n[\"ays\", null],\n[\"\u2581of\", null],\n[\"\u2581wisdom\", \"\u667a\u9f7f\"],\n[\"\u2581teeth\", null],\n[\"\u2581and\", \"\u548c\"],\n[\"\u2581M\", \"\u6838\u78c1\u5171\u632f\"],\n[\"RI\", null],\n[\"\u2581sc\", null],\n[\"ans\", null],\n[\"\u2581of\", null],\n[\"\u2581knee\", \"\u819d\u5173\u8282\"],\n[\"\u2581joint\", null],\n[\"s\", null],\n[\",\", null],\n[\"\u2581then\", \"\u7136\u540e\"],\n[\"\u2581having\", \"\u8ba9\"],\n[\"\u2581dent\", \"\u7259\u79d1\u533b\u751f\"],\n[\"ists\", null],\n[\"\u2581and\", \"\u548c\"],\n[\"\u2581radi\", \"\u653e\u5c04\u79d1\u533b\u751f\"],\n[\"ologists\", null],\n[\"\u2581analy\", \"\u5206\u6790\"],\n[\"se\", null],\n[\"\u2581them\", \"\u5b83\u4eec\"],\n[\",\", null],\n[\"\u2581has\", null],\n[\"\u2581also\", \"\u4e5f\"],\n[\"\u2581been\", null],\n[\"\u2581critic\", \"\u53d7\u5230\u6279\u8bc4\"],\n[\"ised\", null],\n[\"\u2581in\", \"\u5728\"],\n[\"\u2581some\", \"\u4e00\u4e9b\"],\n[\"\u2581quarters\", \"\u65b9\u9762\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nThe method of medical age assessment, which consists of taking X-rays of wisdom teeth and MRI scans of knee joints, then having dentists and radiologists analyse them, has also been criticised in some quarters.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581method', '\u2581of', '\u2581medical', '\u2581age', '\u2581assessment', ',', '\u2581which', '\u2581consists', '\u2581of', '\u2581taking', '\u2581X', '-', 'r', 'ays', '\u2581of', '\u2581wisdom', '\u2581teeth', '\u2581and', '\u2581M', 'RI', '\u2581sc', 'ans', '\u2581of', '\u2581knee', '\u2581joint', 's', ',', '\u2581then', '\u2581having', '\u2581dent', 'ists', '\u2581and', '\u2581radi', 'ologists', '\u2581analy', 'se', '\u2581them', ',', '\u2581has', '\u2581also', '\u2581been', '\u2581critic', 'ised', '\u2581in', '\u2581some', '\u2581quarters', '.']"}], "[\n[\"\u2581The\", \"\u041c\u0435\u0434\u0438\u0446\u0438\u043d\u0430\u043b\u044b\u049b\"],\n[\"\u2581method\", \"\u04d9\u0434\u0456\u0441\"],\n[\"\u2581of\", null],\n[\"\u2581medical\", \"\u043c\u0435\u0434\u0438\u0446\u0438\u043d\u0430\u043b\u044b\u049b\"],\n[\"\u2581age\", \"\u0436\u0430\u0441\"],\n[\"\u2581assessment\", \"\u0431\u0430\u0493\u0430\u043b\u0430\u0443\"],\n[\",\", null],\n[\"\u2581which\", null],\n[\"\u2581consists\", \"\u0442\u04b1\u0440\u0430\u0434\u044b\"],\n[\"\u2581of\", null],\n[\"\u2581taking\", \"\u0442\u04af\u0441\u0456\u0440\u0443\"],\n[\"\u2581X\", \"\u0440\u0435\u043d\u0442\u0433\u0435\u043d\"],\n[\"-\", null],\n[\"r\", null],\n[\"ays\", null],\n[\"\u2581of\", null],\n[\"\u2581wisdom\", \"\u0430\u049b\u044b\u043b\"],\n[\"\u2581teeth\", \"\u0442\u0456\u0441\u0442\u0435\u0440\u0456\u043d\u0456\u04a3\"],\n[\"\u2581and\", \"\u0436\u04d9\u043d\u0435\"],\n[\"\u2581M\", \"\u041c\u0420\u0422\"],\n[\"RI\", null],\n[\"\u2581sc\", null],\n[\"ans\", null],\n[\"\u2581of\", null],\n[\"\u2581knee\", \"\u0442\u0456\u0437\u0435\"],\n[\"\u2581joint\", \"\u0431\u0443\u044b\u043d\u0434\u0430\u0440\u044b\u043d\u044b\u04a3\"],\n[\"s\", null],\n[\",\", null],\n[\"\u2581then\", \"\u0441\u043e\u0434\u0430\u043d \u043a\u0435\u0439\u0456\u043d\"],\n[\"\u2581having\", null],\n[\"\u2581dent\", \"\u0442\u0456\u0441\"],\n[\"ists\", null],\n[\"\u2581and\", \"\u0436\u04d9\u043d\u0435\"],\n[\"\u2581radi\", \"\u0440\u0430\u0434\u0438\u043e\"],\n[\"ologists\", null],\n[\"\u2581analy\", \"\u0442\u0430\u043b\u0434\u0430\"],\n[\"se\", null],\n[\"\u2581them\", \"\u043e\u043b\u0430\u0440\u0434\u044b\"],\n[\",\", null],\n[\"\u2581has\", null],\n[\"\u2581also\", \"\u0441\u043e\u043d\u044b\u043c\u0435\u043d \u049b\u0430\u0442\u0430\u0440\"],\n[\"\u2581been\", null],\n[\"\u2581critic\", \"\u0441\u044b\u043d\u0493\u0430\"],\n[\"ised\", null],\n[\"\u2581in\", null],\n[\"\u2581some\", \"\u043a\u0435\u0439\u0431\u0456\u0440\"],\n[\"\u2581quarters\", \"\u043e\u0440\u0442\u0430\u0434\u0430\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThe method of medical age assessment, which consists of taking X-rays of wisdom teeth and MRI scans of knee joints, then having dentists and radiologists analyse them, has also been criticised in some quarters.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581method', '\u2581of', '\u2581medical', '\u2581age', '\u2581assessment', ',', '\u2581which', '\u2581consists', '\u2581of', '\u2581taking', '\u2581X', '-', 'r', 'ays', '\u2581of', '\u2581wisdom', '\u2581teeth', '\u2581and', '\u2581M', 'RI', '\u2581sc', 'ans', '\u2581of', '\u2581knee', '\u2581joint', 's', ',', '\u2581then', '\u2581having', '\u2581dent', 'ists', '\u2581and', '\u2581radi', 'ologists', '\u2581analy', 'se', '\u2581them', ',', '\u2581has', '\u2581also', '\u2581been', '\u2581critic', 'ised', '\u2581in', '\u2581some', '\u2581quarters', '.']"}], "[\n  [\"\u2581The\", \"\u041c\u0435\u0442\u043e\u0434\"],\n  [\"\u2581method\", null],\n  [\"\u2581of\", null],\n  [\"\u2581medical\", \"\u043c\u0435\u0434\u0438\u0446\u0438\u043d\u0441\u043a\u043e\u0439\"],\n  [\"\u2581age\", \"\u0432\u043e\u0437\u0440\u0430\u0441\u0442\u043d\u043e\u0439\"],\n  [\"\u2581assessment\", \"\u043e\u0446\u0435\u043d\u043a\u0438\"],\n  [\",\", null],\n  [\"\u2581which\", null],\n  [\"\u2581consists\", \"\u0441\u043e\u0441\u0442\u043e\u0438\u0442\"],\n  [\"\u2581of\", null],\n  [\"\u2581taking\", \"\u0438\u0437\"],\n  [\"\u2581X\", \"\u0440\u0435\u043d\u0442\u0433\u0435\u043d\u043e\u0432\u0441\u043a\u0438\u0445\"],\n  [\"-\", null],\n  [\"r\", null],\n  [\"ays\", null],\n  [\"\u2581of\", null],\n  [\"\u2581wisdom\", \"\u0437\u0443\u0431\u043e\u0432\"],\n  [\"\u2581teeth\", null],\n  [\"\u2581and\", \"\u0438\"],\n  [\"\u2581M\", \"\u041c\u0420\u0422\"],\n  [\"RI\", null],\n  [\"\u2581sc\", null],\n  [\"ans\", null],\n  [\"\u2581of\", null],\n  [\"\u2581knee\", \"\u043a\u043e\u043b\u0435\u043d\u043d\u044b\u0445\"],\n  [\"\u2581joint\", \"\u0441\u0443\u0441\u0442\u0430\u0432\u043e\u0432\"],\n  [\"s\", null],\n  [\",\", null],\n  [\"\u2581then\", \"\u0437\u0430\u0442\u0435\u043c\"],\n  [\"\u2581having\", null],\n  [\"\u2581dent\", \"\u0441\u0442\u043e\u043c\u0430\u0442\u043e\u043b\u043e\u0433\u0438\"],\n  [\"ists\", null],\n  [\"\u2581and\", \"\u0438\"],\n  [\"\u2581radi\", \"\u0440\u0430\u0434\u0438\u043e\u043b\u043e\u0433\u0438\"],\n  [\"ologists\", null],\n  [\"\u2581analy\", \"\u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u044e\u0442\"],\n  [\"se\", null],\n  [\"\u2581them\", \"\u0438\u0445\"],\n  [\",\", null],\n  [\"\u2581has\", null],\n  [\"\u2581also\", \"\u0442\u0430\u043a\u0436\u0435\"],\n  [\"\u2581been\", null],\n  [\"\u2581critic\", \"\u043f\u043e\u0434\u0432\u0435\u0440\u0433\u0441\u044f\"],\n  [\"ised\", null],\n  [\"\u2581in\", null],\n  [\"\u2581some\", \"\u043a\u0440\u0438\u0442\u0438\u043a\u0435\"],\n  [\"\u2581quarters\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nAfter Entering a new era, to carry out the new guidelines and start a new journey, we must give our firm support\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581After', '\u2581Enter', 'ing', '\u2581a', '\u2581new', '\u2581era', ',', '\u2581to', '\u2581carry', '\u2581out', '\u2581the', '\u2581new', '\u2581guidelines', '\u2581and', '\u2581start', '\u2581a', '\u2581new', '\u2581journey', ',', '\u2581we', '\u2581must', '\u2581give', '\u2581our', '\u2581firm', '\u2581support']"}], "[\n  [\"\u2581After\", \"\u8fdb\u5165\"],\n  [\"\u2581Enter\", \"\u8fdb\u5165\"],\n  [\"ing\", null],\n  [\"\u2581a\", null],\n  [\"\u2581new\", \"\u65b0\"],\n  [\"\u2581era\", \"\u65f6\u4ee3\"],\n  [\",\", null],\n  [\"\u2581to\", null],\n  [\"\u2581carry\", \"\u8d2f\u5f7b\"],\n  [\"\u2581out\", null],\n  [\"\u2581the\", null],\n  [\"\u2581new\", \"\u65b0\"],\n  [\"\u2581guidelines\", \"\u65b9\u9488\"],\n  [\"\u2581and\", null],\n  [\"\u2581start\", \"\u5f00\u542f\"],\n  [\"\u2581a\", null],\n  [\"\u2581new\", \"\u65b0\"],\n  [\"\u2581journey\", \"\u5f81\u7a0b\"],\n  [\",\", null],\n  [\"\u2581we\", \"\u6211\u4eec\"],\n  [\"\u2581must\", \"\u5fc5\u987b\"],\n  [\"\u2581give\", \"\u7ed9\u4e88\"],\n  [\"\u2581our\", \"\u6211\u4eec\u7684\"],\n  [\"\u2581firm\", \"\u575a\u5b9a\"],\n  [\"\u2581support\", \"\u652f\u6301\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nAfter Entering a new era, to carry out the new guidelines and start a new journey, we must give our firm support\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581After', '\u2581Enter', 'ing', '\u2581a', '\u2581new', '\u2581era', ',', '\u2581to', '\u2581carry', '\u2581out', '\u2581the', '\u2581new', '\u2581guidelines', '\u2581and', '\u2581start', '\u2581a', '\u2581new', '\u2581journey', ',', '\u2581we', '\u2581must', '\u2581give', '\u2581our', '\u2581firm', '\u2581support']"}], "[\n  [\"\u2581After\", \"\u041a\u0435\u0439\u0456\u043d\"],\n  [\"\u2581Enter\", \"\u041a\u0456\u0440\u0443\"],\n  [\"ing\", null],\n  [\"\u2581a\", null],\n  [\"\u2581new\", \"\u0436\u0430\u04a3\u0430\"],\n  [\"\u2581era\", \"\u0434\u04d9\u0443\u0456\u0440\"],\n  [\",\", null],\n  [\"\u2581to\", null],\n  [\"\u2581carry\", \"\u0436\u04af\u0437\u0435\u0433\u0435 \u0430\u0441\u044b\u0440\u0443\"],\n  [\"\u2581out\", null],\n  [\"\u2581the\", null],\n  [\"\u2581new\", \"\u0436\u0430\u04a3\u0430\"],\n  [\"\u2581guidelines\", \"\u043d\u04b1\u0441\u049b\u0430\u0443\u043b\u0430\u0440\"],\n  [\"\u2581and\", null],\n  [\"\u2581start\", \"\u0431\u0430\u0441\u0442\u0430\u0443\"],\n  [\"\u2581a\", null],\n  [\"\u2581new\", \"\u0436\u0430\u04a3\u0430\"],\n  [\"\u2581journey\", \"\u0441\u0430\u043f\u0430\u0440\"],\n  [\",\", null],\n  [\"\u2581we\", \"\u0431\u0456\u0437\"],\n  [\"\u2581must\", null],\n  [\"\u2581give\", \"\u0431\u0435\u0440\u0443\"],\n  [\"\u2581our\", \"\u0431\u0456\u0437\u0434\u0456\u04a3\"],\n  [\"\u2581firm\", \"\u0431\u0435\u0440\u0456\u043a\"],\n  [\"\u2581support\", \"\u049b\u043e\u043b\u0434\u0430\u0443\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nAfter Entering a new era, to carry out the new guidelines and start a new journey, we must give our firm support\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581After', '\u2581Enter', 'ing', '\u2581a', '\u2581new', '\u2581era', ',', '\u2581to', '\u2581carry', '\u2581out', '\u2581the', '\u2581new', '\u2581guidelines', '\u2581and', '\u2581start', '\u2581a', '\u2581new', '\u2581journey', ',', '\u2581we', '\u2581must', '\u2581give', '\u2581our', '\u2581firm', '\u2581support']"}], "[\n    [\"\u2581After\", \"\u041f\u043e\u0441\u043b\u0435\"],\n    [\"\u2581Enter\", \"\u0412\u0441\u0442\u0443\u043f\u043b\u0435\u043d\u0438\u0435\"],\n    [\"ing\", null],\n    [\"\u2581a\", null],\n    [\"\u2581new\", \"\u043d\u043e\u0432\u0443\u044e\"],\n    [\"\u2581era\", \"\u044d\u043f\u043e\u0445\u0443\"],\n    [\",\", null],\n    [\"\u2581to\", null],\n    [\"\u2581carry\", \"\u043e\u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0438\u0442\u044c\"],\n    [\"\u2581out\", null],\n    [\"\u2581the\", null],\n    [\"\u2581new\", \"\u043d\u043e\u0432\u044b\u0435\"],\n    [\"\u2581guidelines\", \"\u0440\u0443\u043a\u043e\u0432\u043e\u0434\u044f\u0449\u0438\u0435 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u044b\"],\n    [\"\u2581and\", null],\n    [\"\u2581start\", \"\u043d\u0430\u0447\u0430\u0442\u044c\"],\n    [\"\u2581a\", null],\n    [\"\u2581new\", \"\u043d\u043e\u0432\u043e\u0435\"],\n    [\"\u2581journey\", \"\u043f\u0443\u0442\u0435\u0448\u0435\u0441\u0442\u0432\u0438\u0435\"],\n    [\",\", null],\n    [\"\u2581we\", \"\u043c\u044b\"],\n    [\"\u2581must\", \"\u0434\u043e\u043b\u0436\u043d\u044b\"],\n    [\"\u2581give\", \"\u043e\u043a\u0430\u0437\u0430\u0442\u044c\"],\n    [\"\u2581our\", \"\u043d\u0430\u0448\u0443\"],\n    [\"\u2581firm\", \"\u0442\u0432\u0435\u0440\u0434\u0443\u044e\"],\n    [\"\u2581support\", \"\u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0443\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nMeanwhile, Apple emphasized in its statement that, \u201cApple has a strong data privacy and security mechanism and will not create a backdoor in any of our systems.\u201d\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Meanwhile', ',', '\u2581Apple', '\u2581emphas', 'ized', '\u2581in', '\u2581its', '\u2581statement', '\u2581that', ',', '\u2581\u201c', 'App', 'le', '\u2581has', '\u2581a', '\u2581strong', '\u2581data', '\u2581privacy', '\u2581and', '\u2581security', '\u2581mechanism', '\u2581and', '\u2581will', '\u2581not', '\u2581create', '\u2581a', '\u2581back', 'door', '\u2581in', '\u2581any', '\u2581of', '\u2581our', '\u2581systems', '.\u201d']"}], "[\n[\"\u2581Meanwhile\", \"\u540c\u65f6\"],\n[\",\", null],\n[\"\u2581Apple\", \"\u82f9\u679c\"],\n[\"\u2581emphas\", \"\u5f3a\u8c03\"],\n[\"ized\", null],\n[\"\u2581in\", null],\n[\"\u2581its\", null],\n[\"\u2581statement\", \"\u58f0\u660e\"],\n[\"\u2581that\", null],\n[\",\", null],\n[\"\u2581\"\", null],\n[\"App\", \"\u82f9\u679c\"],\n[\"le\", null],\n[\"\u2581has\", null],\n[\"\u2581a\", null],\n[\"\u2581strong\", \"\u5f3a\u5927\u7684\"],\n[\"\u2581data\", \"\u6570\u636e\"],\n[\"\u2581privacy\", \"\u9690\u79c1\"],\n[\"\u2581and\", null],\n[\"\u2581security\", \"\u5b89\u5168\"],\n[\"\u2581mechanism\", \"\u673a\u5236\"],\n[\"\u2581and\", null],\n[\"\u2581will\", null],\n[\"\u2581not\", null],\n[\"\u2581create\", \"\u521b\u5efa\"],\n[\"\u2581a\", null],\n[\"\u2581back\", \"\u540e\u95e8\"],\n[\"door\", null],\n[\"\u2581in\", null],\n[\"\u2581any\", null],\n[\"\u2581of\", null],\n[\"\u2581our\", null],\n[\"\u2581systems\", \"\u7cfb\u7edf\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nMeanwhile, Apple emphasized in its statement that, \u201cApple has a strong data privacy and security mechanism and will not create a backdoor in any of our systems.\u201d\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Meanwhile', ',', '\u2581Apple', '\u2581emphas', 'ized', '\u2581in', '\u2581its', '\u2581statement', '\u2581that', ',', '\u2581\u201c', 'App', 'le', '\u2581has', '\u2581a', '\u2581strong', '\u2581data', '\u2581privacy', '\u2581and', '\u2581security', '\u2581mechanism', '\u2581and', '\u2581will', '\u2581not', '\u2581create', '\u2581a', '\u2581back', 'door', '\u2581in', '\u2581any', '\u2581of', '\u2581our', '\u2581systems', '.\u201d']"}], "[\n[\"\u2581Meanwhile\", \"\u041e\u0441\u044b \u043e\u0440\u0430\u0439\u0434\u0430\"],\n[\",\", null],\n[\"\u2581Apple\", \"Apple\"],\n[\"\u2581emphas\", \"\u0431\u0430\u0441\u0430 \u0430\u0439\u0442\u0442\u044b\"],\n[\"ized\", null],\n[\"\u2581in\", null],\n[\"\u2581its\", null],\n[\"\u2581statement\", \"\u043c\u04d9\u043b\u0456\u043c\u0434\u0435\u043c\u0435\u0441\u0456\u043d\u0434\u0435\"],\n[\"\u2581that\", null],\n[\",\", null],\n[\"\u2581\\\"\", null],\n[\"App\", \"Apple\"],\n[\"le\", null],\n[\"\u2581has\", null],\n[\"\u2581a\", null],\n[\"\u2581strong\", \"\u043a\u04af\u0448\u0442\u0456\"],\n[\"\u2581data\", \"\u0434\u0435\u0440\u0435\u043a\u0442\u0435\u0440\"],\n[\"\u2581privacy\", \"\u049b\u04b1\u043f\u0438\u044f\u043b\u044b\u043b\u044b\u0493\u044b\"],\n[\"\u2581and\", \"\u0436\u04d9\u043d\u0435\"],\n[\"\u2581security\", \"\u049b\u0430\u0443\u0456\u043f\u0441\u0456\u0437\u0434\u0456\u043a\"],\n[\"\u2581mechanism\", \"\u043c\u0435\u0445\u0430\u043d\u0438\u0437\u043c\u0456\"],\n[\"\u2581and\", \"\u0436\u04d9\u043d\u0435\"],\n[\"\u2581will\", null],\n[\"\u2581not\", null],\n[\"\u2581create\", \"\u0436\u0430\u0441\u0430\u043c\u0430\u0439\u0434\u044b\"],\n[\"\u2581a\", null],\n[\"\u2581back\", \"\u0430\u0440\u0442\u049b\u044b\"],\n[\"door\", null],\n[\"\u2581in\", null],\n[\"\u2581any\", \"\u0435\u0448\u049b\u0430\u043d\u0434\u0430\u0439\"],\n[\"\u2581of\", null],\n[\"\u2581our\", \"\u0431\u0456\u0437\u0434\u0456\u04a3\"],\n[\"\u2581systems\", \"\u0436\u04af\u0439\u0435\u043b\u0435\u0440\u0456\u043c\u0456\u0437\u0434\u0435\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nMeanwhile, Apple emphasized in its statement that, \u201cApple has a strong data privacy and security mechanism and will not create a backdoor in any of our systems.\u201d\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Meanwhile', ',', '\u2581Apple', '\u2581emphas', 'ized', '\u2581in', '\u2581its', '\u2581statement', '\u2581that', ',', '\u2581\u201c', 'App', 'le', '\u2581has', '\u2581a', '\u2581strong', '\u2581data', '\u2581privacy', '\u2581and', '\u2581security', '\u2581mechanism', '\u2581and', '\u2581will', '\u2581not', '\u2581create', '\u2581a', '\u2581back', 'door', '\u2581in', '\u2581any', '\u2581of', '\u2581our', '\u2581systems', '.\u201d']"}], "[\n[\"\u2581Meanwhile\", \"\u041c\u0435\u0436\u0434\u0443 \u0442\u0435\u043c\"],\n[\",\", null],\n[\"\u2581Apple\", \"Apple\"],\n[\"\u2581emphas\", \"\u043f\u043e\u0434\u0447\u0435\u0440\u043a\u043d\u0443\u043b\u0430\"],\n[\"ized\", null],\n[\"\u2581in\", null],\n[\"\u2581its\", null],\n[\"\u2581statement\", \"\u0437\u0430\u044f\u0432\u043b\u0435\u043d\u0438\u0438\"],\n[\"\u2581that\", null],\n[\",\", null],\n[\"\u2581\"\", null],\n[\"App\", \"Apple\"],\n[\"le\", null],\n[\"\u2581has\", null],\n[\"\u2581a\", null],\n[\"\u2581strong\", \"\u0441\u0438\u043b\u044c\u043d\u044b\u0439\"],\n[\"\u2581data\", \"\u0434\u0430\u043d\u043d\u044b\u0445\"],\n[\"\u2581privacy\", \"\u043a\u043e\u043d\u0444\u0438\u0434\u0435\u043d\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438\"],\n[\"\u2581and\", null],\n[\"\u2581security\", \"\u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u0438\"],\n[\"\u2581mechanism\", \"\u043c\u0435\u0445\u0430\u043d\u0438\u0437\u043c\"],\n[\"\u2581and\", null],\n[\"\u2581will\", null],\n[\"\u2581not\", null],\n[\"\u2581create\", \"\u0441\u043e\u0437\u0434\u0430\u0432\u0430\u0442\u044c\"],\n[\"\u2581a\", null],\n[\"\u2581back\", \"\u0447\u0435\u0440\u043d\u044b\u0439\"],\n[\"door\", null],\n[\"\u2581in\", null],\n[\"\u2581any\", null],\n[\"\u2581of\", null],\n[\"\u2581our\", null],\n[\"\u2581systems\", \"\u0441\u0438\u0441\u0442\u0435\u043c\u0430\u0445\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nLocal Women's Federation invited teachers from party schools to explain to local villagers the contents they were particularly concerned about in the 19th CPC National Congress report in a plain manner.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Local', '\u2581Women', \"'\", 's', '\u2581Federation', '\u2581invited', '\u2581teachers', '\u2581from', '\u2581party', '\u2581schools', '\u2581to', '\u2581explain', '\u2581to', '\u2581local', '\u2581vill', 'agers', '\u2581the', '\u2581contents', '\u2581they', '\u2581were', '\u2581particularly', '\u2581concerned', '\u2581about', '\u2581in', '\u2581the', '\u2581', '1', '9', 'th', '\u2581C', 'PC', '\u2581National', '\u2581Congress', '\u2581report', '\u2581in', '\u2581a', '\u2581plain', '\u2581manner', '.']"}], "[\n[\"\u2581Local\", \"\u5f53\u5730\"],\n[\"\u2581Women\", \"\u5987\u5973\"],\n[\"'\", null],\n[\"s\", null],\n[\"\u2581Federation\", \"\u8054\u5408\u4f1a\"],\n[\"\u2581invited\", \"\u9080\u8bf7\"],\n[\"\u2581teachers\", \"\u6559\u5e08\"],\n[\"\u2581from\", null],\n[\"\u2581party\", \"\u515a\"],\n[\"\u2581schools\", \"\u5b66\u6821\"],\n[\"\u2581to\", null],\n[\"\u2581explain\", \"\u89e3\u91ca\"],\n[\"\u2581to\", null],\n[\"\u2581local\", \"\u5f53\u5730\"],\n[\"\u2581vill\", null],\n[\"agers\", \"\u6751\u6c11\"],\n[\"\u2581the\", null],\n[\"\u2581contents\", \"\u5185\u5bb9\"],\n[\"\u2581they\", \"\u4ed6\u4eec\"],\n[\"\u2581were\", null],\n[\"\u2581particularly\", \"\u7279\u522b\"],\n[\"\u2581concerned\", \"\u5173\u5fc3\"],\n[\"\u2581about\", null],\n[\"\u2581in\", null],\n[\"\u2581the\", null],\n[\"\u2581\", null],\n[\"1\", null],\n[\"9\", null],\n[\"th\", \"\u7b2c\u5341\u4e5d\u6b21\"],\n[\"\u2581C\", null],\n[\"PC\", \"\u4e2d\u5171\"],\n[\"\u2581National\", \"\u5168\u56fd\"],\n[\"\u2581Congress\", \"\u4ee3\u8868\u5927\u4f1a\"],\n[\"\u2581report\", \"\u62a5\u544a\"],\n[\"\u2581in\", null],\n[\"\u2581a\", null],\n[\"\u2581plain\", \"\u7b80\u5355\"],\n[\"\u2581manner\", \"\u65b9\u5f0f\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nLocal Women's Federation invited teachers from party schools to explain to local villagers the contents they were particularly concerned about in the 19th CPC National Congress report in a plain manner.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Local', '\u2581Women', \"'\", 's', '\u2581Federation', '\u2581invited', '\u2581teachers', '\u2581from', '\u2581party', '\u2581schools', '\u2581to', '\u2581explain', '\u2581to', '\u2581local', '\u2581vill', 'agers', '\u2581the', '\u2581contents', '\u2581they', '\u2581were', '\u2581particularly', '\u2581concerned', '\u2581about', '\u2581in', '\u2581the', '\u2581', '1', '9', 'th', '\u2581C', 'PC', '\u2581National', '\u2581Congress', '\u2581report', '\u2581in', '\u2581a', '\u2581plain', '\u2581manner', '.']"}], "[\n  [\"\u2581Local\", \"\u0416\u0435\u0440\u0433\u0456\u043b\u0456\u043a\u0442\u0456\"],\n  [\"\u2581Women\", \"\u04d8\u0439\u0435\u043b\u0434\u0435\u0440\"],\n  [\"'\", null],\n  [\"s\", null],\n  [\"\u2581Federation\", \"\u0424\u0435\u0434\u0435\u0440\u0430\u0446\u0438\u044f\u0441\u044b\"],\n  [\"\u2581invited\", \"\u0448\u0430\u049b\u044b\u0440\u0434\u044b\"],\n  [\"\u2581teachers\", \"\u043c\u04b1\u0493\u0430\u043b\u0456\u043c\u0434\u0435\u0440\u0434\u0456\"],\n  [\"\u2581from\", null],\n  [\"\u2581party\", \"\u043f\u0430\u0440\u0442\u0438\u044f\"],\n  [\"\u2581schools\", \"\u043c\u0435\u043a\u0442\u0435\u043f\u0442\u0435\u0440\u0456\u043d\u0435\u043d\"],\n  [\"\u2581to\", null],\n  [\"\u2581explain\", \"\u0442\u04af\u0441\u0456\u043d\u0434\u0456\u0440\u0443\"],\n  [\"\u2581to\", null],\n  [\"\u2581local\", \"\u0436\u0435\u0440\u0433\u0456\u043b\u0456\u043a\u0442\u0456\"],\n  [\"\u2581vill\", \"\u0430\u0443\u044b\u043b\"],\n  [\"agers\", null],\n  [\"\u2581the\", null],\n  [\"\u2581contents\", \"\u043c\u0430\u0437\u043c\u04b1\u043d\u044b\u043d\"],\n  [\"\u2581they\", \"\u043e\u043b\u0430\u0440\"],\n  [\"\u2581were\", null],\n  [\"\u2581particularly\", \"\u0435\u0440\u0435\u043a\u0448\u0435\"],\n  [\"\u2581concerned\", \"\u0430\u043b\u0430\u04a3\u0434\u0430\u0493\u0430\u043d\"],\n  [\"\u2581about\", null],\n  [\"\u2581in\", null],\n  [\"\u2581the\", null],\n  [\"\u2581\", null],\n  [\"1\", null],\n  [\"9\", null],\n  [\"th\", null],\n  [\"\u2581C\", \"\u049a\u041a\u041f\"],\n  [\"PC\", null],\n  [\"\u2581National\", \"\u04b0\u043b\u0442\u0442\u044b\u049b\"],\n  [\"\u2581Congress\", \"\u041a\u043e\u043d\u0433\u0440\u0435\u0441\u0441\"],\n  [\"\u2581report\", \"\u0431\u0430\u044f\u043d\u0434\u0430\u043c\u0430\u0441\u044b\u043d\u0434\u0430\"],\n  [\"\u2581in\", null],\n  [\"\u2581a\", null],\n  [\"\u2581plain\", \"\u049b\u0430\u0440\u0430\u043f\u0430\u0439\u044b\u043c\"],\n  [\"\u2581manner\", \"\u0442\u04d9\u0441\u0456\u043b\u043c\u0435\u043d\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nLocal Women's Federation invited teachers from party schools to explain to local villagers the contents they were particularly concerned about in the 19th CPC National Congress report in a plain manner.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Local', '\u2581Women', \"'\", 's', '\u2581Federation', '\u2581invited', '\u2581teachers', '\u2581from', '\u2581party', '\u2581schools', '\u2581to', '\u2581explain', '\u2581to', '\u2581local', '\u2581vill', 'agers', '\u2581the', '\u2581contents', '\u2581they', '\u2581were', '\u2581particularly', '\u2581concerned', '\u2581about', '\u2581in', '\u2581the', '\u2581', '1', '9', 'th', '\u2581C', 'PC', '\u2581National', '\u2581Congress', '\u2581report', '\u2581in', '\u2581a', '\u2581plain', '\u2581manner', '.']"}], "[\n[\"\u2581Local\", \"\u041c\u0435\u0441\u0442\u043d\u0430\u044f\"],\n[\"\u2581Women\", \"\u0416\u0435\u043d\u0441\u043a\u0430\u044f\"],\n[\"'\", null],\n[\"s\", null],\n[\"\u2581Federation\", \"\u0424\u0435\u0434\u0435\u0440\u0430\u0446\u0438\u044f\"],\n[\"\u2581invited\", \"\u043f\u0440\u0438\u0433\u043b\u0430\u0441\u0438\u043b\u0430\"],\n[\"\u2581teachers\", \"\u043f\u0440\u0435\u043f\u043e\u0434\u0430\u0432\u0430\u0442\u0435\u043b\u0435\u0439\"],\n[\"\u2581from\", null],\n[\"\u2581party\", \"\u043f\u0430\u0440\u0442\u0438\u0439\u043d\u044b\u0445\"],\n[\"\u2581schools\", \"\u0448\u043a\u043e\u043b\"],\n[\"\u2581to\", null],\n[\"\u2581explain\", \"\u043e\u0431\u044a\u044f\u0441\u043d\u0438\u0442\u044c\"],\n[\"\u2581to\", null],\n[\"\u2581local\", \"\u043c\u0435\u0441\u0442\u043d\u044b\u043c\"],\n[\"\u2581vill\", null],\n[\"agers\", \"\u0436\u0438\u0442\u0435\u043b\u044f\u043c\"],\n[\"\u2581the\", null],\n[\"\u2581contents\", \"\u0441\u043e\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u0435\"],\n[\"\u2581they\", null],\n[\"\u2581were\", null],\n[\"\u2581particularly\", \"\u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\"],\n[\"\u2581concerned\", \"\u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043e\u0432\u0430\u0432\u0448\u0435\u0435\"],\n[\"\u2581about\", null],\n[\"\u2581in\", null],\n[\"\u2581the\", null],\n[\"\u2581\", null],\n[\"1\", null],\n[\"9\", null],\n[\"th\", \"19-\u0433\u043e\"],\n[\"\u2581C\", null],\n[\"PC\", \"\u041a\u041f\u041a\"],\n[\"\u2581National\", \"\u0412\u0441\u0435\u043a\u0438\u0442\u0430\u0439\u0441\u043a\u043e\u0433\u043e\"],\n[\"\u2581Congress\", \"\u0441\u044a\u0435\u0437\u0434\u0430\"],\n[\"\u2581report\", \"\u0434\u043e\u043a\u043b\u0430\u0434\u0430\"],\n[\"\u2581in\", null],\n[\"\u2581a\", null],\n[\"\u2581plain\", \"\u043f\u0440\u043e\u0441\u0442\u044b\u043c\"],\n[\"\u2581manner\", \"\u0441\u043f\u043e\u0441\u043e\u0431\u043e\u043c\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nDeborah Gold, chief executive at NAT (National Aids Trust), said: 'This is a pivotal moment in the fight against HIV.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Deb', 'or', 'ah', '\u2581Gold', ',', '\u2581chief', '\u2581executive', '\u2581at', '\u2581NAT', '\u2581(', 'National', '\u2581A', 'ids', '\u2581Trust', '),', '\u2581said', ':', \"\u2581'\", 'This', '\u2581is', '\u2581a', '\u2581p', 'iv', 'otal', '\u2581moment', '\u2581in', '\u2581the', '\u2581fight', '\u2581against', '\u2581HIV', '.']"}], "[\n    [\"\u2581Deb\", \"\u9edb\u535a\u62c9\"],\n    [\"or\", null],\n    [\"ah\", null],\n    [\"\u2581Gold\", \"\u6208\u5c14\u5fb7\"],\n    [\",\", null],\n    [\"\u2581chief\", \"\u9996\u5e2d\"],\n    [\"\u2581executive\", \"\u6267\u884c\u5b98\"],\n    [\"\u2581at\", null],\n    [\"\u2581NAT\", \"NAT\"],\n    [\"\u2581(\", null],\n    [\"National\", \"\u56fd\u5bb6\"],\n    [\"\u2581A\", \"\u827e\u6ecb\u75c5\"],\n    [\"ids\", null],\n    [\"\u2581Trust\", \"\u4fe1\u6258\"],\n    [\")\", null],\n    [\"\u2581said\", \"\u8bf4\"],\n    [\":\", null],\n    [\"\u2581'\", null],\n    [\"This\", \"\u8fd9\"],\n    [\"\u2581is\", null],\n    [\"\u2581a\", null],\n    [\"\u2581p\", \"\u5173\u952e\"],\n    [\"iv\", null],\n    [\"otal\", null],\n    [\"\u2581moment\", \"\u65f6\u523b\"],\n    [\"\u2581in\", null],\n    [\"\u2581the\", null],\n    [\"\u2581fight\", \"\u6297\u51fb\"],\n    [\"\u2581against\", null],\n    [\"\u2581HIV\", \"\u827e\u6ecb\u75c5\u6bd2\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nDeborah Gold, chief executive at NAT (National Aids Trust), said: 'This is a pivotal moment in the fight against HIV.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Deb', 'or', 'ah', '\u2581Gold', ',', '\u2581chief', '\u2581executive', '\u2581at', '\u2581NAT', '\u2581(', 'National', '\u2581A', 'ids', '\u2581Trust', '),', '\u2581said', ':', \"\u2581'\", 'This', '\u2581is', '\u2581a', '\u2581p', 'iv', 'otal', '\u2581moment', '\u2581in', '\u2581the', '\u2581fight', '\u2581against', '\u2581HIV', '.']"}], "[\n  [\"\u2581Deb\", \"\u0414\u0435\u0431\u043e\u0440\u0430\"],\n  [\"or\", null],\n  [\"ah\", null],\n  [\"\u2581Gold\", \"\u0413\u043e\u043b\u0434\"],\n  [\",\", null],\n  [\"\u2581chief\", \"\u0431\u0430\u0441\"],\n  [\"\u2581executive\", \"\u0430\u0442\u049b\u0430\u0440\u0443\u0448\u044b\"],\n  [\"\u2581at\", null],\n  [\"\u2581NAT\", \"\u04b0\u0421\u0422\"],\n  [\"\u2581(\", null],\n  [\"National\", \"\u04b0\u043b\u0442\u0442\u044b\u049b\"],\n  [\"\u2581A\", \"\u0416\u0418\u0422\u0421\"],\n  [\"ids\", null],\n  [\"\u2581Trust\", \"\u049a\u043e\u0440\u044b\"],\n  [\")\", null],\n  [\"\u2581said\", \"\u0434\u0435\u0434\u0456\"],\n  [\":\", null],\n  [\"\u2581'\", null],\n  [\"This\", \"\u0411\u04b1\u043b\"],\n  [\"\u2581is\", null],\n  [\"\u2581a\", null],\n  [\"\u2581p\", \"\u0448\u0435\u0448\u0443\u0448\u0456\"],\n  [\"iv\", null],\n  [\"otal\", null],\n  [\"\u2581moment\", \"\u0441\u04d9\u0442\"],\n  [\"\u2581in\", null],\n  [\"\u2581the\", null],\n  [\"\u2581fight\", \"\u043a\u04af\u0440\u0435\u0441\u0442\u0435\"],\n  [\"\u2581against\", \"\u049b\u0430\u0440\u0441\u044b\"],\n  [\"\u2581HIV\", \"\u0410\u0418\u0422\u0412\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nDeborah Gold, chief executive at NAT (National Aids Trust), said: 'This is a pivotal moment in the fight against HIV.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Deb', 'or', 'ah', '\u2581Gold', ',', '\u2581chief', '\u2581executive', '\u2581at', '\u2581NAT', '\u2581(', 'National', '\u2581A', 'ids', '\u2581Trust', '),', '\u2581said', ':', \"\u2581'\", 'This', '\u2581is', '\u2581a', '\u2581p', 'iv', 'otal', '\u2581moment', '\u2581in', '\u2581the', '\u2581fight', '\u2581against', '\u2581HIV', '.']"}], "[\n  [\"\u2581Deb\", \"\u0414\u0435\u0431\u043e\u0440\u0430\"],\n  [\"or\", null],\n  [\"ah\", null],\n  [\"\u2581Gold\", \"\u0413\u043e\u043b\u0434\"],\n  [\",\", null],\n  [\"\u2581chief\", \"\u0433\u043b\u0430\u0432\u043d\u044b\u0439\"],\n  [\"\u2581executive\", \"\u0438\u0441\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\"],\n  [\"\u2581at\", null],\n  [\"\u2581NAT\", \"\u041d\u0410\u0422\"],\n  [\"\u2581(\", null],\n  [\"National\", \"\u041d\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439\"],\n  [\"\u2581A\", \"\u0421\u041f\u0418\u0414\"],\n  [\"ids\", null],\n  [\"\u2581Trust\", \"\u0422\u0440\u0430\u0441\u0442\"],\n  [\")\", null],\n  [\"\u2581said\", \"\u0441\u043a\u0430\u0437\u0430\u043b\u0430\"],\n  [\":\", null],\n  [\"\u2581'\", null],\n  [\"This\", \"\u042d\u0442\u043e\"],\n  [\"\u2581is\", null],\n  [\"\u2581a\", null],\n  [\"\u2581p\", \"\u043f\u0435\u0440\u0435\u043b\u043e\u043c\u043d\u044b\u0439\"],\n  [\"iv\", null],\n  [\"otal\", null],\n  [\"\u2581moment\", \"\u043c\u043e\u043c\u0435\u043d\u0442\"],\n  [\"\u2581in\", null],\n  [\"\u2581the\", null],\n  [\"\u2581fight\", \"\u0431\u043e\u0440\u044c\u0431\u0435\"],\n  [\"\u2581against\", \"\u043f\u0440\u043e\u0442\u0438\u0432\"],\n  [\"\u2581HIV\", \"\u0412\u0418\u0427\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nAfter the poverty-stricken counties are lifted out of poverty, relevant policies, supports and measures shall be continued for them.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581After', '\u2581the', '\u2581poverty', '-', 'str', 'icken', '\u2581count', 'ies', '\u2581are', '\u2581lifted', '\u2581out', '\u2581of', '\u2581poverty', ',', '\u2581relevant', '\u2581policies', ',', '\u2581supports', '\u2581and', '\u2581measures', '\u2581shall', '\u2581be', '\u2581continued', '\u2581for', '\u2581them', '.']"}], "[\n  [\"\u2581After\", \"\u5728\"],\n  [\"\u2581the\", null],\n  [\"\u2581poverty\", \"\u8d2b\u56f0\"],\n  [\"-\", null],\n  [\"str\", null],\n  [\"icken\", null],\n  [\"\u2581count\", \"\u53bf\"],\n  [\"ies\", null],\n  [\"\u2581are\", null],\n  [\"\u2581lifted\", \"\u6446\u8131\"],\n  [\"\u2581out\", null],\n  [\"\u2581of\", null],\n  [\"\u2581poverty\", \"\u8d2b\u56f0\"],\n  [\",\", null],\n  [\"\u2581relevant\", \"\u76f8\u5173\"],\n  [\"\u2581policies\", \"\u653f\u7b56\"],\n  [\",\", null],\n  [\"\u2581supports\", \"\u652f\u6301\"],\n  [\"\u2581and\", null],\n  [\"\u2581measures\", \"\u63aa\u65bd\"],\n  [\"\u2581shall\", null],\n  [\"\u2581be\", null],\n  [\"\u2581continued\", \"\u7ee7\u7eed\"],\n  [\"\u2581for\", \"\u5bf9\"],\n  [\"\u2581them\", \"\u5b83\u4eec\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nAfter the poverty-stricken counties are lifted out of poverty, relevant policies, supports and measures shall be continued for them.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581After', '\u2581the', '\u2581poverty', '-', 'str', 'icken', '\u2581count', 'ies', '\u2581are', '\u2581lifted', '\u2581out', '\u2581of', '\u2581poverty', ',', '\u2581relevant', '\u2581policies', ',', '\u2581supports', '\u2581and', '\u2581measures', '\u2581shall', '\u2581be', '\u2581continued', '\u2581for', '\u2581them', '.']"}], "[\n[\"\u2581After\", \"\u041a\u0435\u0434\u0435\u0439\u043b\u0456\u043a\u0442\u0435\u043d\"],\n[\"\u2581the\", null],\n[\"\u2581poverty\", null],\n[\"-\", null],\n[\"str\", null],\n[\"icken\", null],\n[\"\u2581count\", null],\n[\"ies\", null],\n[\"\u2581are\", null],\n[\"\u2581lifted\", \"\u0448\u044b\u049b\u049b\u0430\u043d\u043d\u0430\u043d\"],\n[\"\u2581out\", null],\n[\"\u2581of\", null],\n[\"\u2581poverty\", \"\u043a\u0435\u0439\u0456\u043d\"],\n[\",\", null],\n[\"\u2581relevant\", \"\u0442\u0438\u0456\u0441\u0442\u0456\"],\n[\"\u2581policies\", \"\u0441\u0430\u044f\u0441\u0430\u0442\u0442\u0430\u0440\"],\n[\",\", null],\n[\"\u2581supports\", \"\u049b\u043e\u043b\u0434\u0430\u0443\u043b\u0430\u0440\"],\n[\"\u2581and\", \"\u0436\u04d9\u043d\u0435\"],\n[\"\u2581measures\", \"\u0448\u0430\u0440\u0430\u043b\u0430\u0440\"],\n[\"\u2581shall\", null],\n[\"\u2581be\", null],\n[\"\u2581continued\", \"\u0436\u0430\u043b\u0493\u0430\u0441\u0442\u044b\u0440\u044b\u043b\u0443\u044b\"],\n[\"\u2581for\", \"\u04af\u0448\u0456\u043d\"],\n[\"\u2581them\", \"\u043e\u043b\u0430\u0440\u0493\u0430\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nAfter the poverty-stricken counties are lifted out of poverty, relevant policies, supports and measures shall be continued for them.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581After', '\u2581the', '\u2581poverty', '-', 'str', 'icken', '\u2581count', 'ies', '\u2581are', '\u2581lifted', '\u2581out', '\u2581of', '\u2581poverty', ',', '\u2581relevant', '\u2581policies', ',', '\u2581supports', '\u2581and', '\u2581measures', '\u2581shall', '\u2581be', '\u2581continued', '\u2581for', '\u2581them', '.']"}], "[\n  [\"\u2581After\", \"\u041f\u043e\u0441\u043b\u0435\"],\n  [\"\u2581the\", null],\n  [\"\u2581poverty\", \"\u0431\u0435\u0434\u043d\u043e\u0441\u0442\u044c\"],\n  [\"-\", null],\n  [\"str\", null],\n  [\"icken\", null],\n  [\"\u2581count\", null],\n  [\"ies\", null],\n  [\"\u2581are\", null],\n  [\"\u2581lifted\", \"\u0432\u044b\u0432\u0435\u0434\u0435\u043d\u044b\"],\n  [\"\u2581out\", null],\n  [\"\u2581of\", null],\n  [\"\u2581poverty\", \"\u0431\u0435\u0434\u043d\u043e\u0441\u0442\u0438\"],\n  [\",\", null],\n  [\"\u2581relevant\", \"\u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435\"],\n  [\"\u2581policies\", \"\u043f\u043e\u043b\u0438\u0442\u0438\u043a\u0438\"],\n  [\",\", null],\n  [\"\u2581supports\", \"\u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430\"],\n  [\"\u2581and\", null],\n  [\"\u2581measures\", \"\u043c\u0435\u0440\u044b\"],\n  [\"\u2581shall\", null],\n  [\"\u2581be\", null],\n  [\"\u2581continued\", \"\u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0435\u043d\u044b\"],\n  [\"\u2581for\", \"\u0434\u043b\u044f\"],\n  [\"\u2581them\", \"\u043d\u0438\u0445\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nThe day after the Congress was concluded, Zhang Jian, a delegate of the 19th CPC National Congress and prima ballerina of the National Ballet of China,\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581day', '\u2581after', '\u2581the', '\u2581Congress', '\u2581was', '\u2581concluded', ',', '\u2581Z', 'hang', '\u2581J', 'ian', ',', '\u2581a', '\u2581delegate', '\u2581of', '\u2581the', '\u2581', '1', '9', 'th', '\u2581C', 'PC', '\u2581National', '\u2581Congress', '\u2581and', '\u2581prima', '\u2581ball', 'er', 'ina', '\u2581of', '\u2581the', '\u2581National', '\u2581B', 'allet', '\u2581of', '\u2581China', ',']"}], "[\n  [\"\u2581The\", null],\n  [\"\u2581day\", \"\u5927\u4f1a\"],\n  [\"\u2581after\", \"\u7ed3\u675f\u540e\"],\n  [\"\u2581the\", null],\n  [\"\u2581Congress\", \"\u5927\u4f1a\"],\n  [\"\u2581was\", null],\n  [\"\u2581concluded\", null],\n  [\",\", null],\n  [\"\u2581Z\", \"\u5f20\"],\n  [\"hang\", null],\n  [\"\u2581J\", \"\u5efa\"],\n  [\"ian\", null],\n  [\",\", null],\n  [\"\u2581a\", null],\n  [\"\u2581delegate\", \"\u4ee3\u8868\"],\n  [\"\u2581of\", null],\n  [\"\u2581the\", null],\n  [\"\u2581\", null],\n  [\"1\", \"\u7b2c\u5341\u4e5d\"],\n  [\"9\", null],\n  [\"th\", null],\n  [\"\u2581C\", \"\u4e2d\"],\n  [\"PC\", \"\u5171\"],\n  [\"\u2581National\", \"\u5168\u56fd\"],\n  [\"\u2581Congress\", \"\u4ee3\u8868\u5927\u4f1a\"],\n  [\"\u2581and\", \"\u548c\"],\n  [\"\u2581prima\", \"\u9996\u5e2d\"],\n  [\"\u2581ball\", \"\u82ad\u857e\u821e\"],\n  [\"er\", null],\n  [\"ina\", null],\n  [\"\u2581of\", null],\n  [\"\u2581the\", null],\n  [\"\u2581National\", \"\u4e2d\u56fd\"],\n  [\"\u2581B\", \"\u82ad\u857e\u821e\"],\n  [\"allet\", null],\n  [\"\u2581of\", null],\n  [\"\u2581China\", \"\u56e2\"],\n  [\",\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nThe day after the Congress was concluded, Zhang Jian, a delegate of the 19th CPC National Congress and prima ballerina of the National Ballet of China,\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581day', '\u2581after', '\u2581the', '\u2581Congress', '\u2581was', '\u2581concluded', ',', '\u2581Z', 'hang', '\u2581J', 'ian', ',', '\u2581a', '\u2581delegate', '\u2581of', '\u2581the', '\u2581', '1', '9', 'th', '\u2581C', 'PC', '\u2581National', '\u2581Congress', '\u2581and', '\u2581prima', '\u2581ball', 'er', 'ina', '\u2581of', '\u2581the', '\u2581National', '\u2581B', 'allet', '\u2581of', '\u2581China', ',']"}], "[\n[\"\u2581The\", null],\n[\"\u2581day\", \"\u043a\u04af\u043d\"],\n[\"\u2581after\", \"\u043a\u0435\u0439\u0456\u043d\"],\n[\"\u2581the\", null],\n[\"\u2581Congress\", \"\u041a\u043e\u043d\u0433\u0440\u0435\u0441\u0441\"],\n[\"\u2581was\", null],\n[\"\u2581concluded\", \"\u0430\u044f\u049b\u0442\u0430\u043b\u0493\u0430\u043d\u043d\u0430\u043d\"],\n[\",\", null],\n[\"\u2581Z\", \"\u0427\u0436\u0430\u043d\"],\n[\"hang\", null],\n[\"\u2581J\", \"\u0426\u0437\u044f\u043d\u044c\"],\n[\"ian\", null],\n[\",\", null],\n[\"\u2581a\", null],\n[\"\u2581delegate\", \"\u0434\u0435\u043b\u0435\u0433\u0430\u0442\"],\n[\"\u2581of\", null],\n[\"\u2581the\", null],\n[\"\u2581\", null],\n[\"1\", null],\n[\"9\", null],\n[\"th\", null],\n[\"\u2581C\", \"\u049a\u041a\u041f\"],\n[\"PC\", null],\n[\"\u2581National\", \"\u04b0\u043b\u0442\u0442\u044b\u049b\"],\n[\"\u2581Congress\", \"\u041a\u043e\u043d\u0433\u0440\u0435\u0441\u0441\"],\n[\"\u2581and\", null],\n[\"\u2581prima\", \"\u043f\u0440\u0438\u043c\u0430\"],\n[\"\u2581ball\", \"\u0431\u0430\u043b\u0435\u0440\u0438\u043d\u0430\"],\n[\"er\", null],\n[\"ina\", null],\n[\"\u2581of\", null],\n[\"\u2581the\", null],\n[\"\u2581National\", \"\u04b0\u043b\u0442\u0442\u044b\u049b\"],\n[\"\u2581B\", \"\u0411\u0430\u043b\u0435\u0442\"],\n[\"allet\", null],\n[\"\u2581of\", null],\n[\"\u2581China\", \"\u049a\u044b\u0442\u0430\u0439\"],\n[\",\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThe day after the Congress was concluded, Zhang Jian, a delegate of the 19th CPC National Congress and prima ballerina of the National Ballet of China,\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581day', '\u2581after', '\u2581the', '\u2581Congress', '\u2581was', '\u2581concluded', ',', '\u2581Z', 'hang', '\u2581J', 'ian', ',', '\u2581a', '\u2581delegate', '\u2581of', '\u2581the', '\u2581', '1', '9', 'th', '\u2581C', 'PC', '\u2581National', '\u2581Congress', '\u2581and', '\u2581prima', '\u2581ball', 'er', 'ina', '\u2581of', '\u2581the', '\u2581National', '\u2581B', 'allet', '\u2581of', '\u2581China', ',']"}], "[\n[\"\u2581The\", null],\n[\"\u2581day\", \"\u0434\u0435\u043d\u044c\"],\n[\"\u2581after\", \"\u043f\u043e\u0441\u043b\u0435\"],\n[\"\u2581the\", null],\n[\"\u2581Congress\", \"\u0421\u044a\u0435\u0437\u0434\u0430\"],\n[\"\u2581was\", null],\n[\"\u2581concluded\", \"\u0437\u0430\u0432\u0435\u0440\u0448\u0438\u043b\u0441\u044f\"],\n[\",\", null],\n[\"\u2581Z\", \"\u0427\u0436\u0430\u043d\"],\n[\"hang\", null],\n[\"\u2581J\", \"\u0426\u0437\u044f\u043d\u044c\"],\n[\"ian\", null],\n[\",\", null],\n[\"\u2581a\", null],\n[\"\u2581delegate\", \"\u0434\u0435\u043b\u0435\u0433\u0430\u0442\"],\n[\"\u2581of\", null],\n[\"\u2581the\", null],\n[\"\u2581\", null],\n[\"1\", null],\n[\"9\", null],\n[\"th\", null],\n[\"\u2581C\", \"\u041a\u041f\u041a\"],\n[\"PC\", null],\n[\"\u2581National\", \"\u041d\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0433\u043e\"],\n[\"\u2581Congress\", \"\u0441\u044a\u0435\u0437\u0434\u0430\"],\n[\"\u2581and\", null],\n[\"\u2581prima\", \"\u043f\u0440\u0438\u043c\u0430-\u0431\u0430\u043b\u0435\u0440\u0438\u043d\u0430\"],\n[\"\u2581ball\", null],\n[\"er\", null],\n[\"ina\", null],\n[\"\u2581of\", null],\n[\"\u2581the\", null],\n[\"\u2581National\", \"\u041d\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0433\u043e\"],\n[\"\u2581B\", \"\u0431\u0430\u043b\u0435\u0442\u0430\"],\n[\"allet\", null],\n[\"\u2581of\", null],\n[\"\u2581China\", \"\u041a\u0438\u0442\u0430\u044f\"],\n[\",\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nWe will work closely for cooperation within multilateral frameworks such as the Shanghai Cooperation Organization to help seeking advancement while stabilizing cooperation in the region\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581We', '\u2581will', '\u2581work', '\u2581closely', '\u2581for', '\u2581cooperation', '\u2581within', '\u2581mult', 'il', 'ateral', '\u2581framework', 's', '\u2581such', '\u2581as', '\u2581the', '\u2581Shanghai', '\u2581Co', 'operation', '\u2581Organization', '\u2581to', '\u2581help', '\u2581seeking', '\u2581advance', 'ment', '\u2581while', '\u2581stabil', 'izing', '\u2581cooperation', '\u2581in', '\u2581the', '\u2581region']"}], "[\n  [\"\u2581We\", \"\u6211\u4eec\"],\n  [\"\u2581will\", null],\n  [\"\u2581work\", \"\u5de5\u4f5c\"],\n  [\"\u2581closely\", \"\u5bc6\u5207\"],\n  [\"\u2581for\", null],\n  [\"\u2581cooperation\", \"\u5408\u4f5c\"],\n  [\"\u2581within\", \"\u5728\"],\n  [\"\u2581mult\", \"\u591a\u8fb9\"],\n  [\"il\", null],\n  [\"ateral\", null],\n  [\"\u2581framework\", \"\u6846\u67b6\"],\n  [\"s\", null],\n  [\"\u2581such\", null],\n  [\"\u2581as\", null],\n  [\"\u2581the\", null],\n  [\"\u2581Shanghai\", \"\u4e0a\u6d77\"],\n  [\"\u2581Co\", \"\u5408\u4f5c\"],\n  [\"operation\", null],\n  [\"\u2581Organization\", \"\u7ec4\u7ec7\"],\n  [\"\u2581to\", null],\n  [\"\u2581help\", \"\u5e2e\u52a9\"],\n  [\"\u2581seeking\", \"\u5bfb\u6c42\"],\n  [\"\u2581advance\", \"\u8fdb\u5c55\"],\n  [\"ment\", null],\n  [\"\u2581while\", \"\u540c\u65f6\"],\n  [\"\u2581stabil\", \"\u7a33\u5b9a\"],\n  [\"izing\", null],\n  [\"\u2581cooperation\", \"\u5408\u4f5c\"],\n  [\"\u2581in\", null],\n  [\"\u2581the\", null],\n  [\"\u2581region\", \"\u5730\u533a\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nWe will work closely for cooperation within multilateral frameworks such as the Shanghai Cooperation Organization to help seeking advancement while stabilizing cooperation in the region\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581We', '\u2581will', '\u2581work', '\u2581closely', '\u2581for', '\u2581cooperation', '\u2581within', '\u2581mult', 'il', 'ateral', '\u2581framework', 's', '\u2581such', '\u2581as', '\u2581the', '\u2581Shanghai', '\u2581Co', 'operation', '\u2581Organization', '\u2581to', '\u2581help', '\u2581seeking', '\u2581advance', 'ment', '\u2581while', '\u2581stabil', 'izing', '\u2581cooperation', '\u2581in', '\u2581the', '\u2581region']"}], "[\n  [\"\u2581We\", \"\u0411\u0456\u0437\"],\n  [\"\u2581will\", null],\n  [\"\u2581work\", \"\u0436\u04b1\u043c\u044b\u0441 \u0456\u0441\u0442\u0435\u0439\u043c\u0456\u0437\"],\n  [\"\u2581closely\", \"\u0442\u044b\u0493\u044b\u0437\"],\n  [\"\u2581for\", null],\n  [\"\u2581cooperation\", \"\u044b\u043d\u0442\u044b\u043c\u0430\u049b\u0442\u0430\u0441\u0442\u044b\u049b\"],\n  [\"\u2581within\", \"\u0430\u044f\u0441\u044b\u043d\u0434\u0430\"],\n  [\"\u2581mult\", \"\u043a\u04e9\u043f\"],\n  [\"il\", null],\n  [\"ateral\", null],\n  [\"\u2581framework\", \"\u049b\u04b1\u0440\u044b\u043b\u044b\u043c\"],\n  [\"s\", null],\n  [\"\u2581such\", null],\n  [\"\u2581as\", null],\n  [\"\u2581the\", null],\n  [\"\u2581Shanghai\", \"\u0428\u0430\u043d\u0445\u0430\u0439\"],\n  [\"\u2581Co\", \"\u042b\u043d\u0442\u044b\u043c\u0430\u049b\u0442\u0430\u0441\u0442\u044b\u049b\"],\n  [\"operation\", null],\n  [\"\u2581Organization\", \"\u04b0\u0439\u044b\u043c\u044b\"],\n  [\"\u2581to\", null],\n  [\"\u2581help\", \"\u043a\u04e9\u043c\u0435\u043a\u0442\u0435\u0441\u0443\"],\n  [\"\u2581seeking\", \"\u0456\u0437\u0434\u0435\u0443\"],\n  [\"\u2581advance\", \"\u0430\u043b\u0493\u0430\"],\n  [\"ment\", null],\n  [\"\u2581while\", null],\n  [\"\u2581stabil\", \"\u0442\u04b1\u0440\u0430\u049b\u0442\u0430\u043d\u0434\u044b\u0440\u0443\"],\n  [\"izing\", null],\n  [\"\u2581cooperation\", \"\u044b\u043d\u0442\u044b\u043c\u0430\u049b\u0442\u0430\u0441\u0442\u044b\u049b\"],\n  [\"\u2581in\", null],\n  [\"\u2581the\", null],\n  [\"\u2581region\", \"\u0430\u0439\u043c\u0430\u049b\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nWe will work closely for cooperation within multilateral frameworks such as the Shanghai Cooperation Organization to help seeking advancement while stabilizing cooperation in the region\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581We', '\u2581will', '\u2581work', '\u2581closely', '\u2581for', '\u2581cooperation', '\u2581within', '\u2581mult', 'il', 'ateral', '\u2581framework', 's', '\u2581such', '\u2581as', '\u2581the', '\u2581Shanghai', '\u2581Co', 'operation', '\u2581Organization', '\u2581to', '\u2581help', '\u2581seeking', '\u2581advance', 'ment', '\u2581while', '\u2581stabil', 'izing', '\u2581cooperation', '\u2581in', '\u2581the', '\u2581region']"}], "[\n  [\"\u2581We\", \"\u041c\u044b\"],\n  [\"\u2581will\", null],\n  [\"\u2581work\", \"\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c\"],\n  [\"\u2581closely\", \"\u0442\u0435\u0441\u043d\u043e\"],\n  [\"\u2581for\", null],\n  [\"\u2581cooperation\", \"\u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u0447\u0435\u0441\u0442\u0432\u0430\"],\n  [\"\u2581within\", \"\u0432 \u0440\u0430\u043c\u043a\u0430\u0445\"],\n  [\"\u2581mult\", \"\u043c\u043d\u043e\u0433\u043e\u0441\u0442\u043e\u0440\u043e\u043d\u043d\u0438\u0445\"],\n  [\"il\", null],\n  [\"ateral\", null],\n  [\"\u2581framework\", \"\u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\"],\n  [\"s\", null],\n  [\"\u2581such\", null],\n  [\"\u2581as\", null],\n  [\"\u2581the\", null],\n  [\"\u2581Shanghai\", \"\u0428\u0430\u043d\u0445\u0430\u0439\u0441\u043a\u043e\u0439\"],\n  [\"\u2581Co\", \"\u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u0438\"],\n  [\"operation\", null],\n  [\"\u2581Organization\", null],\n  [\"\u2581to\", null],\n  [\"\u2581help\", \"\u043f\u043e\u043c\u043e\u0447\u044c\"],\n  [\"\u2581seeking\", \"\u0434\u043e\u0431\u0438\u0442\u044c\u0441\u044f\"],\n  [\"\u2581advance\", \"\u043f\u0440\u043e\u0433\u0440\u0435\u0441\u0441\u0430\"],\n  [\"ment\", null],\n  [\"\u2581while\", \"\u043f\u0440\u0438\"],\n  [\"\u2581stabil\", \"\u0441\u0442\u0430\u0431\u0438\u043b\u0438\u0437\u0430\u0446\u0438\u0438\"],\n  [\"izing\", null],\n  [\"\u2581cooperation\", \"\u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u0447\u0435\u0441\u0442\u0432\u0430\"],\n  [\"\u2581in\", null],\n  [\"\u2581the\", null],\n  [\"\u2581region\", \"\u0440\u0435\u0433\u0438\u043e\u043d\u0435\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\n\"I just wanted you to make sure you were aware because it's such an unusual high visibility event,\" the commander of HMX-1 emailed his superior, the Marine Deputy Commandant for Aviation, the day before the event.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\"', 'I', '\u2581just', '\u2581wanted', '\u2581you', '\u2581to', '\u2581make', '\u2581sure', '\u2581you', '\u2581were', '\u2581aware', '\u2581because', '\u2581it', \"'\", 's', '\u2581such', '\u2581an', '\u2581unusual', '\u2581high', '\u2581visibility', '\u2581event', ',\"', '\u2581the', '\u2581commander', '\u2581of', '\u2581H', 'MX', '-', '1', '\u2581em', 'ailed', '\u2581his', '\u2581superior', ',', '\u2581the', '\u2581Marine', '\u2581Deputy', '\u2581Command', 'ant', '\u2581for', '\u2581A', 'vi', 'ation', ',', '\u2581the', '\u2581day', '\u2581before', '\u2581the', '\u2581event', '.']"}], "[\n  [\"\u2581\\\"\", null],\n  [\"I\", \"\u6211\"],\n  [\"\u2581just\", null],\n  [\"\u2581wanted\", \"\u60f3\u8981\"],\n  [\"\u2581you\", \"\u4f60\"],\n  [\"\u2581to\", null],\n  [\"\u2581make\", \"\u786e\u4fdd\"],\n  [\"\u2581sure\", null],\n  [\"\u2581you\", \"\u4f60\"],\n  [\"\u2581were\", null],\n  [\"\u2581aware\", \"\u77e5\u9053\"],\n  [\"\u2581because\", \"\u56e0\u4e3a\"],\n  [\"\u2581it\", \"\u5b83\"],\n  [\"'\", null],\n  [\"s\", null],\n  [\"\u2581such\", \"\u5982\u6b64\"],\n  [\"\u2581an\", null],\n  [\"\u2581unusual\", \"\u4e0d\u5bfb\u5e38\u7684\"],\n  [\"\u2581high\", \"\u9ad8\"],\n  [\"\u2581visibility\", \"\u80fd\u89c1\u5ea6\"],\n  [\"\u2581event\", \"\u4e8b\u4ef6\"],\n  [\",\\\"\", null],\n  [\"\u2581the\", null],\n  [\"\u2581commander\", \"\u6307\u6325\u5b98\"],\n  [\"\u2581of\", null],\n  [\"\u2581H\", \"HMX-1\"],\n  [\"MX\", null],\n  [\"-\", null],\n  [\"1\", null],\n  [\"\u2581em\", \"\u53d1\u9001\u7535\u5b50\u90ae\u4ef6\"],\n  [\"ailed\", null],\n  [\"\u2581his\", \"\u4ed6\u7684\"],\n  [\"\u2581superior\", \"\u4e0a\u7ea7\"],\n  [\",\", null],\n  [\"\u2581the\", null],\n  [\"\u2581Marine\", \"\u6d77\u519b\u9646\u6218\u961f\"],\n  [\"\u2581Deputy\", \"\u526f\"],\n  [\"\u2581Command\", \"\u53f8\u4ee4\"],\n  [\"ant\", null],\n  [\"\u2581for\", null],\n  [\"\u2581A\", \"\u822a\u7a7a\"],\n  [\"vi\", null],\n  [\"ation\", null],\n  [\",\", null],\n  [\"\u2581the\", null],\n  [\"\u2581day\", \"\u524d\u4e00\u5929\"],\n  [\"\u2581before\", null],\n  [\"\u2581the\", null],\n  [\"\u2581event\", \"\u6d3b\u52a8\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\n\"I just wanted you to make sure you were aware because it's such an unusual high visibility event,\" the commander of HMX-1 emailed his superior, the Marine Deputy Commandant for Aviation, the day before the event.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\"', 'I', '\u2581just', '\u2581wanted', '\u2581you', '\u2581to', '\u2581make', '\u2581sure', '\u2581you', '\u2581were', '\u2581aware', '\u2581because', '\u2581it', \"'\", 's', '\u2581such', '\u2581an', '\u2581unusual', '\u2581high', '\u2581visibility', '\u2581event', ',\"', '\u2581the', '\u2581commander', '\u2581of', '\u2581H', 'MX', '-', '1', '\u2581em', 'ailed', '\u2581his', '\u2581superior', ',', '\u2581the', '\u2581Marine', '\u2581Deputy', '\u2581Command', 'ant', '\u2581for', '\u2581A', 'vi', 'ation', ',', '\u2581the', '\u2581day', '\u2581before', '\u2581the', '\u2581event', '.']"}], "[\n  [\"\u2581\\\"\", null],\n  [\"I\", \"\u041c\u0435\u043d\"],\n  [\"\u2581just\", null],\n  [\"\u2581wanted\", \"\u049b\u0430\u043b\u0430\u0434\u044b\u043c\"],\n  [\"\u2581you\", \"\u0441\u0456\u0437\"],\n  [\"\u2581to\", null],\n  [\"\u2581make\", \"\u0436\u0430\u0441\u0430\u0443\"],\n  [\"\u2581sure\", \"\u0441\u0435\u043d\u0456\u043c\u0434\u0456\"],\n  [\"\u2581you\", \"\u0441\u0456\u0437\"],\n  [\"\u2581were\", null],\n  [\"\u2581aware\", \"\u0445\u0430\u0431\u0430\u0440\u0434\u0430\u0440\"],\n  [\"\u2581because\", \"\u04e9\u0439\u0442\u043a\u0435\u043d\u0456\"],\n  [\"\u2581it\", \"\u0431\u04b1\u043b\"],\n  [\"'\", null],\n  [\"s\", null],\n  [\"\u2581such\", \"\u0441\u043e\u043d\u0434\u0430\u0439\"],\n  [\"\u2581an\", null],\n  [\"\u2581unusual\", \"\u0435\u0440\u0435\u043a\u0448\u0435\"],\n  [\"\u2581high\", \"\u0436\u043e\u0493\u0430\u0440\u044b\"],\n  [\"\u2581visibility\", \"\u043a\u04e9\u0440\u0456\u043d\u0435\u0442\u0456\u043d\"],\n  [\"\u2581event\", \"\u043e\u049b\u0438\u0493\u0430\"],\n  [\",\\\"\", null],\n  [\"\u2581the\", null],\n  [\"\u2581commander\", \"\u043a\u043e\u043c\u0430\u043d\u0434\u0438\u0440\"],\n  [\"\u2581of\", null],\n  [\"\u2581H\", \"HMX-1\"],\n  [\"MX\", null],\n  [\"-\", null],\n  [\"1\", null],\n  [\"\u2581em\", \"\u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u0434\u044b\u049b\"],\n  [\"ailed\", null],\n  [\"\u2581his\", \"\u043e\u043d\u044b\u04a3\"],\n  [\"\u2581superior\", \"\u0431\u0430\u0441\u0442\u044b\u0493\u044b\u043d\u0430\"],\n  [\",\", null],\n  [\"\u2581the\", null],\n  [\"\u2581Marine\", \"\u0422\u0435\u04a3\u0456\u0437 \u0436\u0430\u044f\u0443 \u04d9\u0441\u043a\u0435\u0440\u0456\"],\n  [\"\u2581Deputy\", \"\u041e\u0440\u044b\u043d\u0431\u0430\u0441\u0430\u0440\"],\n  [\"\u2581Command\", \"\u049a\u043e\u043b\u0431\u0430\u0441\u0448\u044b\"],\n  [\"ant\", null],\n  [\"\u2581for\", null],\n  [\"\u2581A\", \"\u0410\u0432\u0438\u0430\u0446\u0438\u044f\"],\n  [\"vi\", null],\n  [\"ation\", null],\n  [\",\", null],\n  [\"\u2581the\", null],\n  [\"\u2581day\", \"\u043a\u04af\u043d\"],\n  [\"\u2581before\", \"\u0431\u04b1\u0440\u044b\u043d\"],\n  [\"\u2581the\", null],\n  [\"\u2581event\", \"\u043e\u049b\u0438\u0493\u0430\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\n\"I just wanted you to make sure you were aware because it's such an unusual high visibility event,\" the commander of HMX-1 emailed his superior, the Marine Deputy Commandant for Aviation, the day before the event.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\"', 'I', '\u2581just', '\u2581wanted', '\u2581you', '\u2581to', '\u2581make', '\u2581sure', '\u2581you', '\u2581were', '\u2581aware', '\u2581because', '\u2581it', \"'\", 's', '\u2581such', '\u2581an', '\u2581unusual', '\u2581high', '\u2581visibility', '\u2581event', ',\"', '\u2581the', '\u2581commander', '\u2581of', '\u2581H', 'MX', '-', '1', '\u2581em', 'ailed', '\u2581his', '\u2581superior', ',', '\u2581the', '\u2581Marine', '\u2581Deputy', '\u2581Command', 'ant', '\u2581for', '\u2581A', 'vi', 'ation', ',', '\u2581the', '\u2581day', '\u2581before', '\u2581the', '\u2581event', '.']"}], "[\n[\"\u2581\\\"\", null],\n[\"I\", \"\u042f\"],\n[\"\u2581just\", \"\u043f\u0440\u043e\u0441\u0442\u043e\"],\n[\"\u2581wanted\", \"\u0445\u043e\u0442\u0435\u043b\"],\n[\"\u2581you\", \"\u0432\u044b\"],\n[\"\u2581to\", null],\n[\"\u2581make\", \"\u0443\u0434\u043e\u0441\u0442\u043e\u0432\u0435\u0440\u0438\u0442\u044c\u0441\u044f\"],\n[\"\u2581sure\", null],\n[\"\u2581you\", \"\u0432\u044b\"],\n[\"\u2581were\", null],\n[\"\u2581aware\", \"\u0437\u043d\u0430\u043b\u0438\"],\n[\"\u2581because\", \"\u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e\"],\n[\"\u2581it\", \"\u044d\u0442\u043e\"],\n[\"'\", null],\n[\"s\", null],\n[\"\u2581such\", \"\u0442\u0430\u043a\u043e\u0435\"],\n[\"\u2581an\", null],\n[\"\u2581unusual\", \"\u043d\u0435\u043e\u0431\u044b\u0447\u043d\u043e\u0435\"],\n[\"\u2581high\", \"\u0432\u044b\u0441\u043e\u043a\u043e\"],\n[\"\u2581visibility\", \"\u0437\u0430\u043c\u0435\u0442\u043d\u043e\u0435\"],\n[\"\u2581event\", \"\u0441\u043e\u0431\u044b\u0442\u0438\u0435\"],\n[\",\\\"\", null],\n[\"\u2581the\", null],\n[\"\u2581commander\", \"\u043a\u043e\u043c\u0430\u043d\u0434\u0438\u0440\"],\n[\"\u2581of\", null],\n[\"\u2581H\", \"HMX\"],\n[\"MX\", null],\n[\"-\", null],\n[\"1\", null],\n[\"\u2581em\", \"\u043e\u0442\u043f\u0440\u0430\u0432\u0438\u043b\"],\n[\"ailed\", null],\n[\"\u2581his\", \"\u0441\u0432\u043e\u0435\u043c\u0443\"],\n[\"\u2581superior\", \"\u043d\u0430\u0447\u0430\u043b\u044c\u043d\u0438\u043a\u0443\"],\n[\",\", null],\n[\"\u2581the\", null],\n[\"\u2581Marine\", \"\u043c\u043e\u0440\u0441\u043a\u043e\u0439\"],\n[\"\u2581Deputy\", \"\u0437\u0430\u043c\u0435\u0441\u0442\u0438\u0442\u0435\u043b\u044c\"],\n[\"\u2581Command\", \"\u043a\u043e\u043c\u0430\u043d\u0434\u0443\u044e\u0449\u0438\u0439\"],\n[\"ant\", null],\n[\"\u2581for\", null],\n[\"\u2581A\", \"\u0430\u0432\u0438\u0430\u0446\u0438\u0435\u0439\"],\n[\"vi\", null],\n[\"ation\", null],\n[\",\", null],\n[\"\u2581the\", null],\n[\"\u2581day\", \"\u0434\u0435\u043d\u044c\"],\n[\"\u2581before\", \"\u043f\u0435\u0440\u0435\u0434\"],\n[\"\u2581the\", null],\n[\"\u2581event\", \"\u0441\u043e\u0431\u044b\u0442\u0438\u0435\u043c\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nHe suggested if the same works in humans, a combination of L. reuteri and a tryptophan-rich diet may foster a more tolerant, less inflammatory gut environment.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581He', '\u2581suggested', '\u2581if', '\u2581the', '\u2581same', '\u2581works', '\u2581in', '\u2581humans', ',', '\u2581a', '\u2581combination', '\u2581of', '\u2581L', '.', '\u2581re', 'uter', 'i', '\u2581and', '\u2581a', '\u2581try', 'pt', 'oph', 'an', '-', 'rich', '\u2581diet', '\u2581may', '\u2581foster', '\u2581a', '\u2581more', '\u2581toler', 'ant', ',', '\u2581less', '\u2581infl', 'am', 'mat', 'ory', '\u2581gut', '\u2581environment', '.']"}], "[\n    [\"\u2581He\", \"\u4ed6\"],\n    [\"\u2581suggested\", \"\u5efa\u8bae\"],\n    [\"\u2581if\", \"\u5982\u679c\"],\n    [\"\u2581the\", null],\n    [\"\u2581same\", \"\u76f8\u540c\"],\n    [\"\u2581works\", \"\u6709\u6548\"],\n    [\"\u2581in\", \"\u5728\"],\n    [\"\u2581humans\", \"\u4eba\u7c7b\"],\n    [\",\", null],\n    [\"\u2581a\", null],\n    [\"\u2581combination\", \"\u7ec4\u5408\"],\n    [\"\u2581of\", null],\n    [\"\u2581L\", \"\u4e73\u6746\u83cc\"],\n    [\".\", null],\n    [\"\u2581re\", null],\n    [\"uter\", null],\n    [\"i\", null],\n    [\"\u2581and\", \"\u548c\"],\n    [\"\u2581a\", null],\n    [\"\u2581try\", \"\u8272\u6c28\u9178\"],\n    [\"pt\", null],\n    [\"oph\", null],\n    [\"an\", null],\n    [\"-\", null],\n    [\"rich\", \"\u5bcc\u542b\"],\n    [\"\u2581diet\", \"\u996e\u98df\"],\n    [\"\u2581may\", \"\u53ef\u80fd\"],\n    [\"\u2581foster\", \"\u4fc3\u8fdb\"],\n    [\"\u2581a\", null],\n    [\"\u2581more\", \"\u66f4\"],\n    [\"\u2581toler\", \"\u5bbd\u5bb9\"],\n    [\"ant\", null],\n    [\",\", null],\n    [\"\u2581less\", \"\u8f83\u5c11\"],\n    [\"\u2581infl\", \"\u708e\u75c7\"],\n    [\"am\", null],\n    [\"mat\", null],\n    [\"ory\", null],\n    [\"\u2581gut\", \"\u80a0\u9053\"],\n    [\"\u2581environment\", \"\u73af\u5883\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nHe suggested if the same works in humans, a combination of L. reuteri and a tryptophan-rich diet may foster a more tolerant, less inflammatory gut environment.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581He', '\u2581suggested', '\u2581if', '\u2581the', '\u2581same', '\u2581works', '\u2581in', '\u2581humans', ',', '\u2581a', '\u2581combination', '\u2581of', '\u2581L', '.', '\u2581re', 'uter', 'i', '\u2581and', '\u2581a', '\u2581try', 'pt', 'oph', 'an', '-', 'rich', '\u2581diet', '\u2581may', '\u2581foster', '\u2581a', '\u2581more', '\u2581toler', 'ant', ',', '\u2581less', '\u2581infl', 'am', 'mat', 'ory', '\u2581gut', '\u2581environment', '.']"}], "[\n  [\"\u2581He\", \"\u041e\u043b\"],\n  [\"\u2581suggested\", \"\u04b1\u0441\u044b\u043d\u0434\u044b\"],\n  [\"\u2581if\", \"\u0435\u0433\u0435\u0440\"],\n  [\"\u2581the\", null],\n  [\"\u2581same\", \"\u0434\u04d9\u043b \u0441\u043e\u043d\u0434\u0430\u0439\"],\n  [\"\u2581works\", \"\u0436\u04b1\u043c\u044b\u0441 \u0456\u0441\u0442\u0435\u0441\u0435\"],\n  [\"\u2581in\", null],\n  [\"\u2581humans\", \"\u0430\u0434\u0430\u043c\u0434\u0430\u0440\u0434\u0430\"],\n  [\",\", null],\n  [\"\u2581a\", null],\n  [\"\u2581combination\", \"\u04af\u0439\u043b\u0435\u0441\u0456\u043c\u0456\"],\n  [\"\u2581of\", null],\n  [\"\u2581L\", \"L\"],\n  [\".\", null],\n  [\"\u2581re\", \"reuteri\"],\n  [\"uter\", null],\n  [\"i\", null],\n  [\"\u2581and\", \"\u0436\u04d9\u043d\u0435\"],\n  [\"\u2581a\", null],\n  [\"\u2581try\", \"\u0442\u0440\u0438\u043f\u0442\u043e\u0444\u0430\u043d\u0493\u0430\"],\n  [\"pt\", null],\n  [\"oph\", null],\n  [\"an\", null],\n  [\"-\", null],\n  [\"rich\", \"\u0431\u0430\u0439\"],\n  [\"\u2581diet\", \"\u0434\u0438\u0435\u0442\u0430\"],\n  [\"\u2581may\", \"\u043c\u04af\u043c\u043a\u0456\u043d\"],\n  [\"\u2581foster\", \"\u049b\u0430\u043b\u044b\u043f\u0442\u0430\u0441\u0442\u044b\u0440\u0443\u044b\"],\n  [\"\u2581a\", null],\n  [\"\u2581more\", \"\u043a\u04e9\u0431\u0456\u0440\u0435\u043a\"],\n  [\"\u2581toler\", \"\u0442\u04e9\u0437\u0456\u043c\u0434\u0456\"],\n  [\"ant\", null],\n  [\",\", null],\n  [\"\u2581less\", \"\u0430\u0437\u044b\u0440\u0430\u049b\"],\n  [\"\u2581infl\", \"\u049b\u0430\u0431\u044b\u043d\u0443\u0493\u0430\"],\n  [\"am\", null],\n  [\"mat\", null],\n  [\"ory\", null],\n  [\"\u2581gut\", \"\u0456\u0448\u0435\u043a\"],\n  [\"\u2581environment\", \"\u043e\u0440\u0442\u0430\u0441\u044b\u043d\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nHe suggested if the same works in humans, a combination of L. reuteri and a tryptophan-rich diet may foster a more tolerant, less inflammatory gut environment.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581He', '\u2581suggested', '\u2581if', '\u2581the', '\u2581same', '\u2581works', '\u2581in', '\u2581humans', ',', '\u2581a', '\u2581combination', '\u2581of', '\u2581L', '.', '\u2581re', 'uter', 'i', '\u2581and', '\u2581a', '\u2581try', 'pt', 'oph', 'an', '-', 'rich', '\u2581diet', '\u2581may', '\u2581foster', '\u2581a', '\u2581more', '\u2581toler', 'ant', ',', '\u2581less', '\u2581infl', 'am', 'mat', 'ory', '\u2581gut', '\u2581environment', '.']"}], "[\n[\"\u2581He\", \"\u041e\u043d\"],\n[\"\u2581suggested\", \"\u043f\u0440\u0435\u0434\u043f\u043e\u043b\u043e\u0436\u0438\u043b\"],\n[\"\u2581if\", \"\u0435\u0441\u043b\u0438\"],\n[\"\u2581the\", null],\n[\"\u2581same\", \"\u0442\u043e \u0436\u0435 \u0441\u0430\u043c\u043e\u0435\"],\n[\"\u2581works\", \"\u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442\"],\n[\"\u2581in\", \"\u0443\"],\n[\"\u2581humans\", \"\u043b\u044e\u0434\u0435\u0439\"],\n[\",\", null],\n[\"\u2581a\", null],\n[\"\u2581combination\", \"\u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u044f\"],\n[\"\u2581of\", null],\n[\"\u2581L\", \"L\"],\n[\".\", null],\n[\"\u2581re\", null],\n[\"uter\", null],\n[\"i\", null],\n[\"\u2581and\", \"\u0438\"],\n[\"\u2581a\", null],\n[\"\u2581try\", null],\n[\"pt\", null],\n[\"oph\", null],\n[\"an\", null],\n[\"-\", null],\n[\"rich\", null],\n[\"\u2581diet\", \"\u0434\u0438\u0435\u0442\u0430\"],\n[\"\u2581may\", \"\u043c\u043e\u0436\u0435\u0442\"],\n[\"\u2581foster\", \"\u0441\u043f\u043e\u0441\u043e\u0431\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c\"],\n[\"\u2581a\", null],\n[\"\u2581more\", \"\u0431\u043e\u043b\u0435\u0435\"],\n[\"\u2581toler\", null],\n[\"ant\", null],\n[\"\u2581,\", null],\n[\"\u2581less\", \"\u043c\u0435\u043d\u0435\u0435\"],\n[\"\u2581infl\", null],\n[\"am\", null],\n[\"mat\", null],\n[\"ory\", null],\n[\"\u2581gut\", \"\u043a\u0438\u0448\u0435\u0447\u043d\u043e\u0439\"],\n[\"\u2581environment\", \"\u0441\u0440\u0435\u0434\u0435\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nTo maintain the sound and stable development of Sino-Canadian relations, we must comply with the trend of the times,\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581To', '\u2581maintain', '\u2581the', '\u2581sound', '\u2581and', '\u2581stable', '\u2581development', '\u2581of', '\u2581S', 'ino', '-', 'Can', 'ad', 'ian', '\u2581relations', ',', '\u2581we', '\u2581must', '\u2581comply', '\u2581with', '\u2581the', '\u2581trend', '\u2581of', '\u2581the', '\u2581times', ',']"}], "[\n  [\"\u2581To\", \"\u4e3a\u4e86\"],\n  [\"\u2581maintain\", \"\u4fdd\u6301\"],\n  [\"\u2581the\", null],\n  [\"\u2581sound\", \"\u5065\u5eb7\"],\n  [\"\u2581and\", null],\n  [\"\u2581stable\", \"\u7a33\u5b9a\"],\n  [\"\u2581development\", \"\u53d1\u5c55\"],\n  [\"\u2581of\", null],\n  [\"\u2581S\", \"\u4e2d\"],\n  [\"ino\", null],\n  [\"-\", null],\n  [\"Can\", \"\u52a0\"],\n  [\"ad\", null],\n  [\"ian\", null],\n  [\"\u2581relations\", \"\u5173\u7cfb\"],\n  [\",\", null],\n  [\"\u2581we\", \"\u6211\u4eec\"],\n  [\"\u2581must\", \"\u5fc5\u987b\"],\n  [\"\u2581comply\", \"\u987a\u5e94\"],\n  [\"\u2581with\", null],\n  [\"\u2581the\", null],\n  [\"\u2581trend\", \"\u8d8b\u52bf\"],\n  [\"\u2581of\", null],\n  [\"\u2581the\", null],\n  [\"\u2581times\", \"\u65f6\u4ee3\"],\n  [\",\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nTo maintain the sound and stable development of Sino-Canadian relations, we must comply with the trend of the times,\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581To', '\u2581maintain', '\u2581the', '\u2581sound', '\u2581and', '\u2581stable', '\u2581development', '\u2581of', '\u2581S', 'ino', '-', 'Can', 'ad', 'ian', '\u2581relations', ',', '\u2581we', '\u2581must', '\u2581comply', '\u2581with', '\u2581the', '\u2581trend', '\u2581of', '\u2581the', '\u2581times', ',']"}], "[\n  [\"\u2581To\", \"null\"],\n  [\"\u2581maintain\", \"\u0441\u0430\u049b\u0442\u0430\u0443\"],\n  [\"\u2581the\", \"null\"],\n  [\"\u2581sound\", \"\u0434\u04b1\u0440\u044b\u0441\"],\n  [\"\u2581and\", \"null\"],\n  [\"\u2581stable\", \"\u0442\u04b1\u0440\u0430\u049b\u0442\u044b\"],\n  [\"\u2581development\", \"\u0434\u0430\u043c\u0443\"],\n  [\"\u2581of\", \"null\"],\n  [\"\u2581S\", \"\u049b\u044b\u0442\u0430\u0439\"],\n  [\"ino\", \"null\"],\n  [\"-\", \"null\"],\n  [\"Can\", \"\u043a\u0430\u043d\u0430\u0434\u0430\"],\n  [\"ad\", \"null\"],\n  [\"ian\", \"null\"],\n  [\"\u2581relations\", \"\u049b\u0430\u0442\u044b\u043d\u0430\u0441\u0442\u0430\u0440\u044b\"],\n  [\",\", \"null\"],\n  [\"\u2581we\", \"null\"],\n  [\"\u2581must\", \"\u043a\u0435\u0440\u0435\u043a\"],\n  [\"\u2581comply\", \"\u0441\u04d9\u0439\u043a\u0435\u0441\"],\n  [\"\u2581with\", \"null\"],\n  [\"\u2581the\", \"null\"],\n  [\"\u2581trend\", \"\u04af\u0440\u0434\u0456\u0441\u0456\u043d\"],\n  [\"\u2581of\", \"null\"],\n  [\"\u2581the\", \"null\"],\n  [\"\u2581times\", \"\u0443\u0430\u049b\u044b\u0442\"],\n  [\",\", \"null\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nTo maintain the sound and stable development of Sino-Canadian relations, we must comply with the trend of the times,\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581To', '\u2581maintain', '\u2581the', '\u2581sound', '\u2581and', '\u2581stable', '\u2581development', '\u2581of', '\u2581S', 'ino', '-', 'Can', 'ad', 'ian', '\u2581relations', ',', '\u2581we', '\u2581must', '\u2581comply', '\u2581with', '\u2581the', '\u2581trend', '\u2581of', '\u2581the', '\u2581times', ',']"}], "[\n  [\"\u2581To\", \"\u0414\u043b\u044f\"],\n  [\"\u2581maintain\", \"\u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0442\u044c\"],\n  [\"\u2581the\", null],\n  [\"\u2581sound\", \"\u0437\u0434\u043e\u0440\u043e\u0432\u043e\u0435\"],\n  [\"\u2581and\", null],\n  [\"\u2581stable\", \"\u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u043e\u0435\"],\n  [\"\u2581development\", \"\u0440\u0430\u0437\u0432\u0438\u0442\u0438\u0435\"],\n  [\"\u2581of\", null],\n  [\"\u2581S\", \"\u043a\u0438\u0442\u0430\u0439\u0441\u043a\u043e\"],\n  [\"ino\", null],\n  [\"-\", null],\n  [\"Can\", null],\n  [\"ad\", null],\n  [\"ian\", null],\n  [\"\u2581relations\", \"\u043a\u0430\u043d\u0430\u0434\u0441\u043a\u0438\u0445 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0439\"],\n  [\",\", null],\n  [\"\u2581we\", \"\u043c\u044b\"],\n  [\"\u2581must\", \"\u0434\u043e\u043b\u0436\u043d\u044b\"],\n  [\"\u2581comply\", \"\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u044c\"],\n  [\"\u2581with\", null],\n  [\"\u2581the\", null],\n  [\"\u2581trend\", \"\u0442\u0435\u043d\u0434\u0435\u043d\u0446\u0438\u044f\u043c\"],\n  [\"\u2581of\", null],\n  [\"\u2581the\", null],\n  [\"\u2581times\", \"\u0432\u0440\u0435\u043c\u0435\u043d\u0438\"],\n  [\",\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nThey also had an expose on government spending concentrated in the governor's hometown.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581They', '\u2581also', '\u2581had', '\u2581an', '\u2581expose', '\u2581on', '\u2581government', '\u2581spending', '\u2581concentrated', '\u2581in', '\u2581the', '\u2581governor', \"'\", 's', '\u2581hom', 'et', 'own', '.']"}], "[\n  [\"\u2581They\", \"\u4ed6\u4eec\"],\n  [\"\u2581also\", null],\n  [\"\u2581had\", \"\u6709\"],\n  [\"\u2581an\", null],\n  [\"\u2581expose\", \"\u63ed\u9732\"],\n  [\"\u2581on\", null],\n  [\"\u2581government\", \"\u653f\u5e9c\"],\n  [\"\u2581spending\", \"\u652f\u51fa\"],\n  [\"\u2581concentrated\", \"\u96c6\u4e2d\"],\n  [\"\u2581in\", null],\n  [\"\u2581the\", null],\n  [\"\u2581governor\", \"\u5dde\u957f\"],\n  [\"'\", null],\n  [\"s\", null],\n  [\"\u2581hom\", \"\u5bb6\u4e61\"],\n  [\"et\", null],\n  [\"own\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nThey also had an expose on government spending concentrated in the governor's hometown.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581They', '\u2581also', '\u2581had', '\u2581an', '\u2581expose', '\u2581on', '\u2581government', '\u2581spending', '\u2581concentrated', '\u2581in', '\u2581the', '\u2581governor', \"'\", 's', '\u2581hom', 'et', 'own', '.']"}], "[\n  [\"\u2581They\", \"\u041e\u043b\u0430\u0440\"],\n  [\"\u2581also\", null],\n  [\"\u2581had\", \"\u0431\u043e\u043b\u0434\u044b\"],\n  [\"\u2581an\", null],\n  [\"\u2581expose\", \"\u04d9\u0448\u043a\u0435\u0440\u0435\u043b\u0435\u0443\"],\n  [\"\u2581on\", null],\n  [\"\u2581government\", \"\u04af\u043a\u0456\u043c\u0435\u0442\"],\n  [\"\u2581spending\", \"\u0448\u044b\u0493\u044b\u043d\u0434\u0430\u0440\u044b\"],\n  [\"\u2581concentrated\", \"\u0448\u043e\u0493\u044b\u0440\u043b\u0430\u043d\u0493\u0430\u043d\"],\n  [\"\u2581in\", null],\n  [\"\u2581the\", null],\n  [\"\u2581governor\", \"\u04d9\u043a\u0456\u043c\"],\n  [\"'\", null],\n  [\"s\", null],\n  [\"\u2581hom\", \"\u0442\u0443\u0493\u0430\u043d\"],\n  [\"et\", null],\n  [\"own\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThey also had an expose on government spending concentrated in the governor's hometown.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581They', '\u2581also', '\u2581had', '\u2581an', '\u2581expose', '\u2581on', '\u2581government', '\u2581spending', '\u2581concentrated', '\u2581in', '\u2581the', '\u2581governor', \"'\", 's', '\u2581hom', 'et', 'own', '.']"}], "[\n  [\"\u2581They\", \"\u041e\u043d\u0438\"],\n  [\"\u2581also\", null],\n  [\"\u2581had\", \"\u0438\u043c\u0435\u043b\u0438\"],\n  [\"\u2581an\", null],\n  [\"\u2581expose\", \"\u0440\u0430\u0437\u043e\u0431\u043b\u0430\u0447\u0435\u043d\u0438\u0435\"],\n  [\"\u2581on\", null],\n  [\"\u2581government\", \"\u043f\u0440\u0430\u0432\u0438\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0445\"],\n  [\"\u2581spending\", \"\u0440\u0430\u0441\u0445\u043e\u0434\u043e\u0432\"],\n  [\"\u2581concentrated\", \"\u0441\u043e\u0441\u0440\u0435\u0434\u043e\u0442\u043e\u0447\u0435\u043d\u043d\u044b\u0445\"],\n  [\"\u2581in\", null],\n  [\"\u2581the\", null],\n  [\"\u2581governor\", \"\u0433\u0443\u0431\u0435\u0440\u043d\u0430\u0442\u043e\u0440\u0430\"],\n  [\"'\", null],\n  [\"s\", null],\n  [\"\u2581hom\", \"\u0440\u043e\u0434\u043d\u043e\u043c\"],\n  [\"et\", null],\n  [\"own\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nand unexplained explosions have been detected in the vicinity of the watershed, these crew members are very unlikely to have survived.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581and', '\u2581une', 'x', 'pl', 'ained', '\u2581explos', 'ions', '\u2581have', '\u2581been', '\u2581detected', '\u2581in', '\u2581the', '\u2581vic', 'inity', '\u2581of', '\u2581the', '\u2581waters', 'hed', ',', '\u2581these', '\u2581crew', '\u2581members', '\u2581are', '\u2581very', '\u2581unlikely', '\u2581to', '\u2581have', '\u2581survived', '.']"}], "[\n[\"\u2581and\", null],\n[\"\u2581une\", \"\u4e0d\u660e\"],\n[\"x\", null],\n[\"pl\", null],\n[\"ained\", null],\n[\"\u2581explos\", \"\u7206\u70b8\"],\n[\"ions\", null],\n[\"\u2581have\", null],\n[\"\u2581been\", null],\n[\"\u2581detected\", \"\u88ab\u53d1\u73b0\"],\n[\"\u2581in\", null],\n[\"\u2581the\", null],\n[\"\u2581vic\", \"\u9644\u8fd1\"],\n[\"inity\", null],\n[\"\u2581of\", null],\n[\"\u2581the\", null],\n[\"\u2581waters\", \"\u6d41\u57df\"],\n[\"hed\", null],\n[\",\", null],\n[\"\u2581these\", null],\n[\"\u2581crew\", \"\u8239\u5458\"],\n[\"\u2581members\", null],\n[\"\u2581are\", null],\n[\"\u2581very\", null],\n[\"\u2581unlikely\", \"\u4e0d\u592a\u53ef\u80fd\"],\n[\"\u2581to\", null],\n[\"\u2581have\", null],\n[\"\u2581survived\", \"\u5e78\u5b58\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nand unexplained explosions have been detected in the vicinity of the watershed, these crew members are very unlikely to have survived.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581and', '\u2581une', 'x', 'pl', 'ained', '\u2581explos', 'ions', '\u2581have', '\u2581been', '\u2581detected', '\u2581in', '\u2581the', '\u2581vic', 'inity', '\u2581of', '\u2581the', '\u2581waters', 'hed', ',', '\u2581these', '\u2581crew', '\u2581members', '\u2581are', '\u2581very', '\u2581unlikely', '\u2581to', '\u2581have', '\u2581survived', '.']"}], "[\n  [\"\u2581and\", null],\n  [\"\u2581une\", \"\u0442\u04af\u0441\u0456\u043d\u0456\u043a\u0441\u0456\u0437\"],\n  [\"x\", null],\n  [\"pl\", null],\n  [\"ained\", null],\n  [\"\u2581explos\", \"\u0436\u0430\u0440\u044b\u043b\u044b\u0441\u0442\u0430\u0440\"],\n  [\"ions\", null],\n  [\"\u2581have\", null],\n  [\"\u2581been\", null],\n  [\"\u2581detected\", \"\u0430\u043d\u044b\u049b\u0442\u0430\u043b\u0493\u0430\u043d\"],\n  [\"\u2581in\", null],\n  [\"\u2581the\", null],\n  [\"\u2581vic\", \"\u043c\u0430\u04a3\u044b\u043d\u0434\u0430\"],\n  [\"inity\", null],\n  [\"\u2581of\", null],\n  [\"\u2581the\", null],\n  [\"\u2581waters\", \"\u0441\u0443\"],\n  [\"hed\", null],\n  [\",\", null],\n  [\"\u2581these\", null],\n  [\"\u2581crew\", \"\u044d\u043a\u0438\u043f\u0430\u0436\"],\n  [\"\u2581members\", \"\u043c\u04af\u0448\u0435\u043b\u0435\u0440\u0456\"],\n  [\"\u2581are\", null],\n  [\"\u2581very\", null],\n  [\"\u2581unlikely\", \"\u0435\u043a\u0456\u0442\u0430\u043b\u0430\u0439\"],\n  [\"\u2581to\", null],\n  [\"\u2581have\", null],\n  [\"\u2581survived\", \"\u0430\u043c\u0430\u043d \u049b\u0430\u043b\u0443\u044b\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nand unexplained explosions have been detected in the vicinity of the watershed, these crew members are very unlikely to have survived.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581and', '\u2581une', 'x', 'pl', 'ained', '\u2581explos', 'ions', '\u2581have', '\u2581been', '\u2581detected', '\u2581in', '\u2581the', '\u2581vic', 'inity', '\u2581of', '\u2581the', '\u2581waters', 'hed', ',', '\u2581these', '\u2581crew', '\u2581members', '\u2581are', '\u2581very', '\u2581unlikely', '\u2581to', '\u2581have', '\u2581survived', '.']"}], "[\n[\"\u2581and\", null],\n[\"\u2581une\", \"\u043d\u0435\u043e\u0431\u044a\u044f\u0441\u043d\u0438\u043c\u044b\u0435\"],\n[\"x\", null],\n[\"pl\", null],\n[\"ained\", null],\n[\"\u2581explos\", \"\u0432\u0437\u0440\u044b\u0432\u044b\"],\n[\"ions\", null],\n[\"\u2581have\", null],\n[\"\u2581been\", null],\n[\"\u2581detected\", \"\u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u044b\"],\n[\"\u2581in\", null],\n[\"\u2581the\", null],\n[\"\u2581vic\", \"\u043e\u043a\u0440\u0435\u0441\u0442\u043d\u043e\u0441\u0442\u044f\u0445\"],\n[\"inity\", null],\n[\"\u2581of\", null],\n[\"\u2581the\", null],\n[\"\u2581waters\", \"\u0432\u043e\u0434\u043e\u0440\u0430\u0437\u0434\u0435\u043b\u0430\"],\n[\"hed\", null],\n[\",\", null],\n[\"\u2581these\", null],\n[\"\u2581crew\", \"\u0447\u043b\u0435\u043d\u044b\"],\n[\"\u2581members\", \"\u044d\u043a\u0438\u043f\u0430\u0436\u0430\"],\n[\"\u2581are\", null],\n[\"\u2581very\", \"\u043e\u0447\u0435\u043d\u044c\"],\n[\"\u2581unlikely\", \"\u043c\u0430\u043b\u043e\u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\"],\n[\"\u2581to\", null],\n[\"\u2581have\", null],\n[\"\u2581survived\", \"\u0432\u044b\u0436\u0438\u043b\u0438\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nEfforts should be made to fight innovative war and make preparation for war, keep up with evolution in war situation and method of war fighting,\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Eff', 'orts', '\u2581should', '\u2581be', '\u2581made', '\u2581to', '\u2581fight', '\u2581innovative', '\u2581war', '\u2581and', '\u2581make', '\u2581preparation', '\u2581for', '\u2581war', ',', '\u2581keep', '\u2581up', '\u2581with', '\u2581evolution', '\u2581in', '\u2581war', '\u2581situation', '\u2581and', '\u2581method', '\u2581of', '\u2581war', '\u2581fighting', ',']"}], "[\n  [\"\u2581Eff\", \"\u52aa\u529b\"],\n  [\"orts\", null],\n  [\"\u2581should\", null],\n  [\"\u2581be\", null],\n  [\"\u2581made\", null],\n  [\"\u2581to\", null],\n  [\"\u2581fight\", \"\u6253\u51fb\"],\n  [\"\u2581innovative\", \"\u521b\u65b0\"],\n  [\"\u2581war\", \"\u6218\u4e89\"],\n  [\"\u2581and\", null],\n  [\"\u2581make\", \"\u505a\"],\n  [\"\u2581preparation\", \"\u51c6\u5907\"],\n  [\"\u2581for\", null],\n  [\"\u2581war\", \"\u6218\u4e89\"],\n  [\",\", null],\n  [\"\u2581keep\", \"\u4fdd\u6301\"],\n  [\"\u2581up\", null],\n  [\"\u2581with\", null],\n  [\"\u2581evolution\", \"\u6f14\u53d8\"],\n  [\"\u2581in\", null],\n  [\"\u2581war\", \"\u6218\u4e89\"],\n  [\"\u2581situation\", \"\u5c40\u52bf\"],\n  [\"\u2581and\", null],\n  [\"\u2581method\", \"\u65b9\u6cd5\"],\n  [\"\u2581of\", null],\n  [\"\u2581war\", \"\u6218\u4e89\"],\n  [\"\u2581fighting\", \"\u4f5c\u6218\"],\n  [\",\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nEfforts should be made to fight innovative war and make preparation for war, keep up with evolution in war situation and method of war fighting,\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Eff', 'orts', '\u2581should', '\u2581be', '\u2581made', '\u2581to', '\u2581fight', '\u2581innovative', '\u2581war', '\u2581and', '\u2581make', '\u2581preparation', '\u2581for', '\u2581war', ',', '\u2581keep', '\u2581up', '\u2581with', '\u2581evolution', '\u2581in', '\u2581war', '\u2581situation', '\u2581and', '\u2581method', '\u2581of', '\u2581war', '\u2581fighting', ',']"}], "[\n  [\"\u2581Eff\", \"\u041a\u04af\u0448-\u0436\u0456\u0433\u0435\u0440\"],\n  [\"orts\", null],\n  [\"\u2581should\", null],\n  [\"\u2581be\", null],\n  [\"\u2581made\", null],\n  [\"\u2581to\", null],\n  [\"\u2581fight\", \"\u043a\u04af\u0440\u0435\u0441\u0443\"],\n  [\"\u2581innovative\", \"\u0438\u043d\u043d\u043e\u0432\u0430\u0446\u0438\u044f\u043b\u044b\u049b\"],\n  [\"\u2581war\", \"\u0441\u043e\u0493\u044b\u0441\"],\n  [\"\u2581and\", null],\n  [\"\u2581make\", \"\u0434\u0430\u0439\u044b\u043d\u0434\u0430\u0443\"],\n  [\"\u2581preparation\", null],\n  [\"\u2581for\", null],\n  [\"\u2581war\", \"\u0441\u043e\u0493\u044b\u0441\"],\n  [\",\", null],\n  [\"\u2581keep\", \"\u0441\u0430\u049b\u0442\u0430\u0443\"],\n  [\"\u2581up\", null],\n  [\"\u2581with\", null],\n  [\"\u2581evolution\", \"\u044d\u0432\u043e\u043b\u044e\u0446\u0438\u044f\"],\n  [\"\u2581in\", null],\n  [\"\u2581war\", \"\u0441\u043e\u0493\u044b\u0441\"],\n  [\"\u2581situation\", \"\u0436\u0430\u0493\u0434\u0430\u0439\"],\n  [\"\u2581and\", null],\n  [\"\u2581method\", \"\u04d9\u0434\u0456\u0441\"],\n  [\"\u2581of\", null],\n  [\"\u2581war\", \"\u0441\u043e\u0493\u044b\u0441\"],\n  [\"\u2581fighting\", \"\u0436\u04af\u0440\u0433\u0456\u0437\u0443\"],\n  [\",\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nEfforts should be made to fight innovative war and make preparation for war, keep up with evolution in war situation and method of war fighting,\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Eff', 'orts', '\u2581should', '\u2581be', '\u2581made', '\u2581to', '\u2581fight', '\u2581innovative', '\u2581war', '\u2581and', '\u2581make', '\u2581preparation', '\u2581for', '\u2581war', ',', '\u2581keep', '\u2581up', '\u2581with', '\u2581evolution', '\u2581in', '\u2581war', '\u2581situation', '\u2581and', '\u2581method', '\u2581of', '\u2581war', '\u2581fighting', ',']"}], "[\n  [\"\u2581Eff\", \"\u0423\u0441\u0438\u043b\u0438\u044f\"],\n  [\"orts\", null],\n  [\"\u2581should\", null],\n  [\"\u2581be\", null],\n  [\"\u2581made\", null],\n  [\"\u2581to\", null],\n  [\"\u2581fight\", \"\u0431\u043e\u0440\u043e\u0442\u044c\u0441\u044f\"],\n  [\"\u2581innovative\", \"\u0438\u043d\u043d\u043e\u0432\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439\"],\n  [\"\u2581war\", \"\u0432\u043e\u0439\u043d\u043e\u0439\"],\n  [\"\u2581and\", null],\n  [\"\u2581make\", \"\u0434\u0435\u043b\u0430\u0442\u044c\"],\n  [\"\u2581preparation\", \"\u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0443\"],\n  [\"\u2581for\", null],\n  [\"\u2581war\", \"\u0432\u043e\u0439\u043d\u0435\"],\n  [\",\", null],\n  [\"\u2581keep\", \"\u0438\u0434\u0442\u0438\"],\n  [\"\u2581up\", null],\n  [\"\u2581with\", null],\n  [\"\u2581evolution\", \"\u044d\u0432\u043e\u043b\u044e\u0446\u0438\u0435\u0439\"],\n  [\"\u2581in\", null],\n  [\"\u2581war\", \"\u0432\u043e\u0435\u043d\u043d\u043e\u0439\"],\n  [\"\u2581situation\", \"\u0441\u0438\u0442\u0443\u0430\u0446\u0438\u0438\"],\n  [\"\u2581and\", null],\n  [\"\u2581method\", \"\u043c\u0435\u0442\u043e\u0434\u043e\u0432\"],\n  [\"\u2581of\", null],\n  [\"\u2581war\", \"\u0432\u0435\u0434\u0435\u043d\u0438\u044f\"],\n  [\"\u2581fighting\", \"\u0432\u043e\u0439\u043d\u044b\"],\n  [\",\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\n\u201cMade in Sichuan\u201d renewable energy vehicles rose against the tide. Weakening fiscal subsidies, inadequate charging facilities, and ternary lithium batteries suspended from use in renewable energy passenger vehicles; in the face of prevailing upward pressures in the domestic renewable energy automobile market\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u201c', 'M', 'ade', '\u2581in', '\u2581S', 'ich', 'uan', '\u201d', '\u2581renew', 'able', '\u2581energy', '\u2581vehicles', '\u2581rose', '\u2581against', '\u2581the', '\u2581tide', '.', '\u2581We', 'aken', 'ing', '\u2581fiscal', '\u2581subs', 'id', 'ies', ',', '\u2581in', 'ade', 'quate', '\u2581charging', '\u2581facilities', ',', '\u2581and', '\u2581t', 'ern', 'ary', '\u2581l', 'ith', 'ium', '\u2581batteries', '\u2581suspended', '\u2581from', '\u2581use', '\u2581in', '\u2581renew', 'able', '\u2581energy', '\u2581passenger', '\u2581vehicles', ';', '\u2581in', '\u2581the', '\u2581face', '\u2581of', '\u2581prev', 'ailing', '\u2581upward', '\u2581press', 'ures', '\u2581in', '\u2581the', '\u2581domestic', '\u2581renew', 'able', '\u2581energy', '\u2581autom', 'obile', '\u2581market']"}], "[\n  [\"\u2581\\\"\", null],\n  [\"M\", \"\u56db\u5ddd\"],\n  [\"ade\", null],\n  [\"\u2581in\", null],\n  [\"\u2581S\", null],\n  [\"ich\", null],\n  [\"uan\", null],\n  [\"\\\"\", null],\n  [\"\u2581renew\", \"\u53ef\u518d\u751f\"],\n  [\"able\", null],\n  [\"\u2581energy\", \"\u80fd\u6e90\"],\n  [\"\u2581vehicles\", \"\u8f66\u8f86\"],\n  [\"\u2581rose\", \"\u9006\u52bf\u4e0a\u6da8\"],\n  [\"\u2581against\", null],\n  [\"\u2581the\", null],\n  [\"\u2581tide\", null],\n  [\".\", null],\n  [\"\u2581We\", \"\u524a\u5f31\"],\n  [\"aken\", null],\n  [\"ing\", null],\n  [\"\u2581fiscal\", \"\u8d22\u653f\"],\n  [\"\u2581subs\", \"\u8865\u8d34\"],\n  [\"id\", null],\n  [\"ies\", null],\n  [\",\", null],\n  [\"\u2581in\", \"\u4e0d\u8db3\"],\n  [\"ade\", null],\n  [\"quate\", null],\n  [\"\u2581charging\", \"\u5145\u7535\"],\n  [\"\u2581facilities\", \"\u8bbe\u65bd\"],\n  [\",\", null],\n  [\"\u2581and\", null],\n  [\"\u2581t\", \"\u4e09\u5143\"],\n  [\"ern\", null],\n  [\"ary\", null],\n  [\"\u2581l\", \"\u9502\"],\n  [\"ith\", null],\n  [\"ium\", null],\n  [\"\u2581batteries\", \"\u7535\u6c60\"],\n  [\"\u2581suspended\", \"\u6682\u505c\"],\n  [\"\u2581from\", null],\n  [\"\u2581use\", \"\u4f7f\u7528\"],\n  [\"\u2581in\", null],\n  [\"\u2581renew\", \"\u53ef\u518d\u751f\"],\n  [\"able\", null],\n  [\"\u2581energy\", \"\u80fd\u6e90\"],\n  [\"\u2581passenger\", \"\u4e58\u7528\"],\n  [\"\u2581vehicles\", \"\u8f66\"],\n  [\";\", null],\n  [\"\u2581in\", null],\n  [\"\u2581the\", null],\n  [\"\u2581face\", \"\u9762\u5bf9\"],\n  [\"\u2581of\", null],\n  [\"\u2581prev\", \"\u666e\u904d\"],\n  [\"ailing\", null],\n  [\"\u2581upward\", \"\u4e0a\u5347\"],\n  [\"\u2581press\", \"\u538b\u529b\"],\n  [\"ures\", null],\n  [\"\u2581in\", null],\n  [\"\u2581the\", null],\n  [\"\u2581domestic\", \"\u56fd\u5185\"],\n  [\"\u2581renew\", \"\u53ef\u518d\u751f\"],\n  [\"able\", null],\n  [\"\u2581energy\", \"\u80fd\u6e90\"],\n  [\"\u2581autom\", \"\u6c7d\u8f66\"],\n  [\"obile\", null],\n  [\"\u2581market\", \"\u5e02\u573a\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\n\u201cMade in Sichuan\u201d renewable energy vehicles rose against the tide. Weakening fiscal subsidies, inadequate charging facilities, and ternary lithium batteries suspended from use in renewable energy passenger vehicles; in the face of prevailing upward pressures in the domestic renewable energy automobile market\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u201c', 'M', 'ade', '\u2581in', '\u2581S', 'ich', 'uan', '\u201d', '\u2581renew', 'able', '\u2581energy', '\u2581vehicles', '\u2581rose', '\u2581against', '\u2581the', '\u2581tide', '.', '\u2581We', 'aken', 'ing', '\u2581fiscal', '\u2581subs', 'id', 'ies', ',', '\u2581in', 'ade', 'quate', '\u2581charging', '\u2581facilities', ',', '\u2581and', '\u2581t', 'ern', 'ary', '\u2581l', 'ith', 'ium', '\u2581batteries', '\u2581suspended', '\u2581from', '\u2581use', '\u2581in', '\u2581renew', 'able', '\u2581energy', '\u2581passenger', '\u2581vehicles', ';', '\u2581in', '\u2581the', '\u2581face', '\u2581of', '\u2581prev', 'ailing', '\u2581upward', '\u2581press', 'ures', '\u2581in', '\u2581the', '\u2581domestic', '\u2581renew', 'able', '\u2581energy', '\u2581autom', 'obile', '\u2581market']"}], "[\n[\"\u2581\\\"\", null],\n[\"M\", \"\u0421\u044b\u0447\u0443\u0430\u043d\u044c\"],\n[\"ade\", null],\n[\"\u2581in\", null],\n[\"\u2581S\", null],\n[\"ich\", null],\n[\"uan\", null],\n[\"\\\"\", null],\n[\"\u2581renew\", \"\u0436\u0430\u04a3\u0430\u0440\u0442\u044b\u043b\u0430\u0442\u044b\u043d\"],\n[\"able\", null],\n[\"\u2581energy\", \"\u044d\u043d\u0435\u0440\u0433\u0438\u044f\"],\n[\"\u2581vehicles\", \"\u043a\u04e9\u043b\u0456\u043a\u0442\u0435\u0440\"],\n[\"\u2581rose\", \"\u043a\u04e9\u0442\u0435\u0440\u0456\u043b\u0434\u0456\"],\n[\"\u2581against\", \"\u049b\u0430\u0440\u0441\u044b\"],\n[\"\u2581the\", null],\n[\"\u2581tide\", \"\u0430\u0493\u044b\u043c\u0493\u0430\"],\n[\".\", null],\n[\"\u2581We\", \"\u04d8\u043b\u0441\u0456\u0440\u0435\u0433\u0435\u043d\"],\n[\"aken\", null],\n[\"ing\", null],\n[\"\u2581fiscal\", \"\u0444\u0438\u0441\u043a\u0430\u043b\u0434\u044b\u049b\"],\n[\"\u2581subs\", \"\u0441\u0443\u0431\u0441\u0438\u0434\u0438\u044f\u043b\u0430\u0440\"],\n[\"id\", null],\n[\"ies\", null],\n[\",\" , null],\n[\"\u2581in\", \"\u0436\u0435\u0442\u043a\u0456\u043b\u0456\u043a\u0441\u0456\u0437\"],\n[\"ade\", null],\n[\"quate\", null],\n[\"\u2581charging\", \"\u0437\u0430\u0440\u044f\u0434\u0442\u0430\u0443\"],\n[\"\u2581facilities\", \"\u0438\u043d\u0444\u0440\u0430\u049b\u04b1\u0440\u044b\u043b\u044b\u043c\u044b\"],\n[\",\" , null],\n[\"\u2581and\", null],\n[\"\u2581t\", \"\u04af\u0448\u0442\u0456\u043a\"],\n[\"ern\", null],\n[\"ary\", null],\n[\"\u2581l\", \"\u043b\u0438\u0442\u0438\u0439\"],\n[\"ith\", null],\n[\"ium\", null],\n[\"\u2581batteries\", \"\u0431\u0430\u0442\u0430\u0440\u0435\u044f\u043b\u0430\u0440\u044b\"],\n[\"\u2581suspended\", \"\u0442\u043e\u049b\u0442\u0430\u0442\u044b\u043b\u0493\u0430\u043d\"],\n[\"\u2581from\", null],\n[\"\u2581use\", \"\u043f\u0430\u0439\u0434\u0430\u043b\u0430\u043d\u0443\u0434\u0430\u043d\"],\n[\"\u2581in\", null],\n[\"\u2581renew\", \"\u0436\u0430\u04a3\u0430\u0440\u0442\u044b\u043b\u0430\u0442\u044b\u043d\"],\n[\"able\", null],\n[\"\u2581energy\", \"\u044d\u043d\u0435\u0440\u0433\u0438\u044f\"],\n[\"\u2581passenger\", \"\u0436\u043e\u043b\u0430\u0443\u0448\u044b\u043b\u0430\u0440\"],\n[\"\u2581vehicles\", \"\u043a\u04e9\u043b\u0456\u043a\u0442\u0435\u0440\u0456\u043d\u0434\u0435\"],\n[\";\", null],\n[\"\u2581in\", null],\n[\"\u2581the\", null],\n[\"\u2581face\", \"\u0436\u0430\u0493\u0434\u0430\u0439\u044b\u043d\u0434\u0430\"],\n[\"\u2581of\", null],\n[\"\u2581prev\", \"\u04af\u0441\u0442\u0435\u043c\u0434\u0456\u043a\"],\n[\"ailing\", null],\n[\"\u2581upward\", \"\u0436\u043e\u0493\u0430\u0440\u044b\"],\n[\"\u2581press\", \"\u049b\u044b\u0441\u044b\u043c\"],\n[\"ures\", null],\n[\"\u2581in\", null],\n[\"\u2581the\", null],\n[\"\u2581domestic\", \"\u043e\u0442\u0430\u043d\u0434\u044b\u049b\"],\n[\"\u2581renew\", \"\u0436\u0430\u04a3\u0430\u0440\u0442\u044b\u043b\u0430\u0442\u044b\u043d\"],\n[\"able\", null],\n[\"\u2581energy\", \"\u044d\u043d\u0435\u0440\u0433\u0438\u044f\"],\n[\"\u2581autom\", \"\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044c\"],\n[\"obile\", null],\n[\"\u2581market\", \"\u043d\u0430\u0440\u044b\u0493\u044b\u043d\u0434\u0430\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\n\u201cMade in Sichuan\u201d renewable energy vehicles rose against the tide. Weakening fiscal subsidies, inadequate charging facilities, and ternary lithium batteries suspended from use in renewable energy passenger vehicles; in the face of prevailing upward pressures in the domestic renewable energy automobile market\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u201c', 'M', 'ade', '\u2581in', '\u2581S', 'ich', 'uan', '\u201d', '\u2581renew', 'able', '\u2581energy', '\u2581vehicles', '\u2581rose', '\u2581against', '\u2581the', '\u2581tide', '.', '\u2581We', 'aken', 'ing', '\u2581fiscal', '\u2581subs', 'id', 'ies', ',', '\u2581in', 'ade', 'quate', '\u2581charging', '\u2581facilities', ',', '\u2581and', '\u2581t', 'ern', 'ary', '\u2581l', 'ith', 'ium', '\u2581batteries', '\u2581suspended', '\u2581from', '\u2581use', '\u2581in', '\u2581renew', 'able', '\u2581energy', '\u2581passenger', '\u2581vehicles', ';', '\u2581in', '\u2581the', '\u2581face', '\u2581of', '\u2581prev', 'ailing', '\u2581upward', '\u2581press', 'ures', '\u2581in', '\u2581the', '\u2581domestic', '\u2581renew', 'able', '\u2581energy', '\u2581autom', 'obile', '\u2581market']"}], "[\n  [\"\u2581\\\"\", null],\n  [\"M\", \"\u0421\u0434\u0435\u043b\u0430\u043d\u043e\"],\n  [\"ade\", null],\n  [\"\u2581in\", null],\n  [\"\u2581S\", \"\u0421\u044b\u0447\u0443\u0430\u043d\u044c\"],\n  [\"ich\", null],\n  [\"uan\", null],\n  [\"\\\"\", null],\n  [\"\u2581renew\", \"\u0432\u043e\u0437\u043e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u043c\u044b\u0435\"],\n  [\"able\", null],\n  [\"\u2581energy\", \"\u044d\u043d\u0435\u0440\u0433\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435\"],\n  [\"\u2581vehicles\", \"\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u0438\"],\n  [\"\u2581rose\", \"\u0432\u044b\u0440\u043e\u0441\u043b\u0438\"],\n  [\"\u2581against\", \"\u0432\u043e\u043f\u0440\u0435\u043a\u0438\"],\n  [\"\u2581the\", null],\n  [\"\u2581tide\", \"\u0442\u0435\u043d\u0434\u0435\u043d\u0446\u0438\u0438\"],\n  [\".\", null],\n  [\"\u2581We\", \"\u041e\u0441\u043b\u0430\u0431\u043b\u0435\u043d\u0438\u0435\"],\n  [\"aken\", null],\n  [\"ing\", null],\n  [\"\u2581fiscal\", \"\u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u044b\u0445\"],\n  [\"\u2581subs\", \"\u0441\u0443\u0431\u0441\u0438\u0434\u0438\u0439\"],\n  [\"id\", null],\n  [\"ies\", null],\n  [\",\", null],\n  [\"\u2581in\", \"\u043d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u0430\u044f\"],\n  [\"ade\", null],\n  [\"quate\", null],\n  [\"\u2581charging\", \"\u0437\u0430\u0440\u044f\u0434\u043d\u0430\u044f\"],\n  [\"\u2581facilities\", \"\u0438\u043d\u0444\u0440\u0430\u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430\"],\n  [\",\", null],\n  [\"\u2581and\", null],\n  [\"\u2581t\", \"\u0442\u0440\u043e\u0439\u043d\u044b\u0435\"],\n  [\"ern\", null],\n  [\"ary\", null],\n  [\"\u2581l\", \"\u043b\u0438\u0442\u0438\u0439\"],\n  [\"ith\", null],\n  [\"ium\", null],\n  [\"\u2581batteries\", \"\u0431\u0430\u0442\u0430\u0440\u0435\u0438\"],\n  [\"\u2581suspended\", \"\u0437\u0430\u043f\u0440\u0435\u0449\u0435\u043d\u044b\"],\n  [\"\u2581from\", null],\n  [\"\u2581use\", \"\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f\"],\n  [\"\u2581in\", null],\n  [\"\u2581renew\", \"\u0432\u043e\u0437\u043e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u043c\u044b\u0445\"],\n  [\"able\", null],\n  [\"\u2581energy\", \"\u044d\u043d\u0435\u0440\u0433\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445\"],\n  [\"\u2581passenger\", \"\u043f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u0441\u043a\u0438\u0445\"],\n  [\"\u2581vehicles\", \"\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044f\u0445\"],\n  [\";\", null],\n  [\"\u2581in\", null],\n  [\"\u2581the\", null],\n  [\"\u2581face\", \"\u0443\u0441\u043b\u043e\u0432\u0438\u044f\u0445\"],\n  [\"\u2581of\", null],\n  [\"\u2581prev\", \"\u043f\u0440\u0435\u043e\u0431\u043b\u0430\u0434\u0430\u044e\u0449\u0435\u0433\u043e\"],\n  [\"ailing\", null],\n  [\"\u2581upward\", \"\u0432\u043e\u0441\u0445\u043e\u0434\u044f\u0449\u0435\u0433\u043e\"],\n  [\"\u2581press\", \"\u0434\u0430\u0432\u043b\u0435\u043d\u0438\u044f\"],\n  [\"ures\", null],\n  [\"\u2581in\", null],\n  [\"\u2581the\", null],\n  [\"\u2581domestic\", \"\u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0435\u043c\"],\n  [\"\u2581renew\", \"\u0432\u043e\u0437\u043e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u043c\u044b\u0445\"],\n  [\"able\", null],\n  [\"\u2581energy\", \"\u044d\u043d\u0435\u0440\u0433\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445\"],\n  [\"\u2581autom\", \"\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044c\u043d\u043e\u043c\"],\n  [\"obile\", null],\n  [\"\u2581market\", \"\u0440\u044b\u043d\u043a\u0435\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nChronic total occlusion is described as the \u201clast fortress\u201d of interventional therapy in coronary heart disease. The surgery success rate is around 70%.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Ch', 'ronic', '\u2581total', '\u2581oc', 'clusion', '\u2581is', '\u2581described', '\u2581as', '\u2581the', '\u2581\u201c', 'last', '\u2581fort', 'ress', '\u201d', '\u2581of', '\u2581inter', 'vent', 'ional', '\u2581therapy', '\u2581in', '\u2581coron', 'ary', '\u2581heart', '\u2581disease', '.', '\u2581The', '\u2581surgery', '\u2581success', '\u2581rate', '\u2581is', '\u2581around', '\u2581', '7', '0', '%.']"}], "[\n  [\"\u2581Ch\", \"\u6162\u6027\"],\n  [\"ronic\", null],\n  [\"\u2581total\", \"\u5b8c\u5168\"],\n  [\"\u2581oc\", \"\u95ed\u585e\"],\n  [\"clusion\", null],\n  [\"\u2581is\", null],\n  [\"\u2581described\", \"\u88ab\u63cf\u8ff0\u4e3a\"],\n  [\"\u2581as\", null],\n  [\"\u2581the\", null],\n  [\"\u2581\\\"\", null],\n  [\"last\", \"\u6700\u540e\"],\n  [\"\u2581fort\", \"\u5821\u5792\"],\n  [\"ress\", null],\n  [\"\\\"\", null],\n  [\"\u2581of\", null],\n  [\"\u2581inter\", \"\u4ecb\u5165\"],\n  [\"vent\", null],\n  [\"ional\", null],\n  [\"\u2581therapy\", \"\u6cbb\u7597\"],\n  [\"\u2581in\", null],\n  [\"\u2581coron\", \"\u51a0\u72b6\"],\n  [\"ary\", null],\n  [\"\u2581heart\", \"\u5fc3\u810f\"],\n  [\"\u2581disease\", \"\u75be\u75c5\"],\n  [\".\", null],\n  [\"\u2581The\", null],\n  [\"\u2581surgery\", \"\u624b\u672f\"],\n  [\"\u2581success\", \"\u6210\u529f\"],\n  [\"\u2581rate\", \"\u7387\"],\n  [\"\u2581is\", null],\n  [\"\u2581around\", \"\u7ea6\u4e3a\"],\n  [\"\u2581\", null],\n  [\"7\", \"7\"],\n  [\"0\", \"0\"],\n  [\"%\", \"%\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nChronic total occlusion is described as the \u201clast fortress\u201d of interventional therapy in coronary heart disease. The surgery success rate is around 70%.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Ch', 'ronic', '\u2581total', '\u2581oc', 'clusion', '\u2581is', '\u2581described', '\u2581as', '\u2581the', '\u2581\u201c', 'last', '\u2581fort', 'ress', '\u201d', '\u2581of', '\u2581inter', 'vent', 'ional', '\u2581therapy', '\u2581in', '\u2581coron', 'ary', '\u2581heart', '\u2581disease', '.', '\u2581The', '\u2581surgery', '\u2581success', '\u2581rate', '\u2581is', '\u2581around', '\u2581', '7', '0', '%.']"}], "[\n  [\"\u2581Ch\", \"\u0421\u043e\u0437\u044b\u043b\u043c\u0430\u043b\u044b\"],\n  [\"ronic\", null],\n  [\"\u2581total\", \"\u0442\u043e\u043b\u044b\u049b\"],\n  [\"\u2581oc\", \"\u043e\u043a\u043a\u043b\u044e\u0437\u0438\u044f\"],\n  [\"clusion\", null],\n  [\"\u2581is\", null],\n  [\"\u2581described\", \"\u0441\u0438\u043f\u0430\u0442\u0442\u0430\u043b\u0430\u0434\u044b\"],\n  [\"\u2581as\", null],\n  [\"\u2581the\", null],\n  [\"\u2581\\\"\", null],\n  [\"last\", \"\u0441\u043e\u04a3\u0493\u044b\"],\n  [\"\u2581fort\", \"\u049b\u0430\u043c\u0430\u043b\"],\n  [\"ress\", null],\n  [\"\\\"\", null],\n  [\"\u2581of\", null],\n  [\"\u2581inter\", \"\u0438\u043d\u0442\u0435\u0440\u0432\u0435\u043d\u0446\u0438\u044f\u043b\u044b\u049b\"],\n  [\"vent\", null],\n  [\"ional\", null],\n  [\"\u2581therapy\", \"\u0442\u0435\u0440\u0430\u043f\u0438\u044f\"],\n  [\"\u2581in\", null],\n  [\"\u2581coron\", \"\u0436\u04af\u0440\u0435\u043a\"],\n  [\"ary\", null],\n  [\"\u2581heart\", \"\u049b\u0430\u043d\u0442\u0430\u043c\u044b\u0440\"],\n  [\"\u2581disease\", \"\u0430\u0443\u0440\u0443\u044b\"],\n  [\".\", null],\n  [\"\u2581The\", null],\n  [\"\u2581surgery\", \"\u041e\u043f\u0435\u0440\u0430\u0446\u0438\u044f\u043d\u044b\u04a3\"],\n  [\"\u2581success\", \"\u0441\u04d9\u0442\u0442\u0456\u043b\u0456\u043a\"],\n  [\"\u2581rate\", \"\u043a\u04e9\u0440\u0441\u0435\u0442\u043a\u0456\u0448\u0456\"],\n  [\"\u2581is\", null],\n  [\"\u2581around\", \"\u0448\u0430\u043c\u0430\u043c\u0435\u043d\"],\n  [\"\u2581\", null],\n  [\"7\", \"70\"],\n  [\"0\", null],\n  [\"%.\", \"%\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nChronic total occlusion is described as the \u201clast fortress\u201d of interventional therapy in coronary heart disease. The surgery success rate is around 70%.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Ch', 'ronic', '\u2581total', '\u2581oc', 'clusion', '\u2581is', '\u2581described', '\u2581as', '\u2581the', '\u2581\u201c', 'last', '\u2581fort', 'ress', '\u201d', '\u2581of', '\u2581inter', 'vent', 'ional', '\u2581therapy', '\u2581in', '\u2581coron', 'ary', '\u2581heart', '\u2581disease', '.', '\u2581The', '\u2581surgery', '\u2581success', '\u2581rate', '\u2581is', '\u2581around', '\u2581', '7', '0', '%.']"}], "[\n[\"\u2581Ch\", \"\u0425\u0440\u043e\u043d\u0438\u0447\u0435\u0441\u043a\u0430\u044f\"],\n[\"ronic\", null],\n[\"\u2581total\", \"\u043f\u043e\u043b\u043d\u0430\u044f\"],\n[\"\u2581oc\", \"\u043e\u043a\u043a\u043b\u044e\u0437\u0438\u044f\"],\n[\"clusion\", null],\n[\"\u2581is\", null],\n[\"\u2581described\", \"\u043e\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u0442\u0441\u044f\"],\n[\"\u2581as\", null],\n[\"\u2581the\", null],\n[\"\u2581\\\"\", null],\n[\"last\", \"\u043f\u043e\u0441\u043b\u0435\u0434\u043d\u044f\u044f\"],\n[\"\u2581fort\", \"\u043a\u0440\u0435\u043f\u043e\u0441\u0442\u044c\"],\n[\"ress\", null],\n[\"\\\"\", null],\n[\"\u2581of\", null],\n[\"\u2581inter\", \"\u0438\u043d\u0442\u0435\u0440\u0432\u0435\u043d\u0446\u0438\u043e\u043d\u043d\u043e\u0439\"],\n[\"vent\", null],\n[\"ional\", null],\n[\"\u2581therapy\", \"\u0442\u0435\u0440\u0430\u043f\u0438\u0438\"],\n[\"\u2581in\", null],\n[\"\u2581coron\", \"\u043a\u043e\u0440\u043e\u043d\u0430\u0440\u043d\u043e\u0439\"],\n[\"ary\", null],\n[\"\u2581heart\", \"\u0441\u0435\u0440\u0434\u0435\u0447\u043d\u043e\u0439\"],\n[\"\u2581disease\", \"\u0431\u043e\u043b\u0435\u0437\u043d\u0438\"],\n[\".\", null],\n[\"\u2581The\", null],\n[\"\u2581surgery\", \"\u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438\"],\n[\"\u2581success\", \"\u0443\u0441\u043f\u0435\u0445\u0430\"],\n[\"\u2581rate\", \"\u0443\u0440\u043e\u0432\u0435\u043d\u044c\"],\n[\"\u2581is\", null],\n[\"\u2581around\", \"\u043e\u043a\u043e\u043b\u043e\"],\n[\"\u2581\", null],\n[\"7\", \"70\"],\n[\"0\", null],\n[\"%.\", \"%\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nNominations for the 90th Academy Awards will be officially announced on January 23, after which Hollywood\u2019s annual most eye-catching \u201cOscar Time\u201d will commence.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Nom', 'inations', '\u2581for', '\u2581the', '\u2581', '9', '0', 'th', '\u2581Academy', '\u2581Awards', '\u2581will', '\u2581be', '\u2581officially', '\u2581announced', '\u2581on', '\u2581January', '\u2581', '2', '3', ',', '\u2581after', '\u2581which', '\u2581Hollywood', '\u2019', 's', '\u2581annual', '\u2581most', '\u2581eye', '-', 'catch', 'ing', '\u2581\u201c', 'O', 'scar', '\u2581Time', '\u201d', '\u2581will', '\u2581commence', '.']"}], "[\n[\"\u2581Nom\", \"\u63d0\u540d\"],\n[\"inations\", null],\n[\"\u2581for\", null],\n[\"\u2581the\", null],\n[\"\u2581\", null],\n[\"9\", null],\n[\"0\", null],\n[\"th\", \"\u7b2c90\u5c4a\"],\n[\"\u2581Academy\", \"\u5965\u65af\u5361\"],\n[\"\u2581Awards\", \"\u9881\u5956\u5178\u793c\"],\n[\"\u2581will\", null],\n[\"\u2581be\", null],\n[\"\u2581officially\", \"\u6b63\u5f0f\"],\n[\"\u2581announced\", \"\u5ba3\u5e03\"],\n[\"\u2581on\", null],\n[\"\u2581January\", \"1\u6708\"],\n[\"\u2581\", null],\n[\"2\", null],\n[\"3\", \"23\u65e5\"],\n[\",\", null],\n[\"\u2581after\", \"\u4e4b\u540e\"],\n[\"\u2581which\", null],\n[\"\u2581Hollywood\", \"\u597d\u83b1\u575e\"],\n[\"'\", null],\n[\"s\", null],\n[\"\u2581annual\", \"\u5e74\u5ea6\"],\n[\"\u2581most\", \"\u6700\"],\n[\"\u2581eye\", \"\u5f15\u4eba\u6ce8\u76ee\"],\n[\"-\", null],\n[\"catch\", null],\n[\"ing\", null],\n[\"\u2581\\\"\", null],\n[\"O\", \"\u5965\u65af\u5361\"],\n[\"scar\", null],\n[\"\u2581Time\", \"\u65f6\u95f4\"],\n[\"\\\"\", null],\n[\"\u2581will\", null],\n[\"\u2581commence\", \"\u5f00\u59cb\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nNominations for the 90th Academy Awards will be officially announced on January 23, after which Hollywood\u2019s annual most eye-catching \u201cOscar Time\u201d will commence.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Nom', 'inations', '\u2581for', '\u2581the', '\u2581', '9', '0', 'th', '\u2581Academy', '\u2581Awards', '\u2581will', '\u2581be', '\u2581officially', '\u2581announced', '\u2581on', '\u2581January', '\u2581', '2', '3', ',', '\u2581after', '\u2581which', '\u2581Hollywood', '\u2019', 's', '\u2581annual', '\u2581most', '\u2581eye', '-', 'catch', 'ing', '\u2581\u201c', 'O', 'scar', '\u2581Time', '\u201d', '\u2581will', '\u2581commence', '.']"}], "[\n[\"\u2581Nom\", \"\u041d\u043e\u043c\u0438\u043d\u0430\u0446\u0438\u044f\u043b\u0430\u0440\"],\n[\"inations\", null],\n[\"\u2581for\", null],\n[\"\u2581the\", null],\n[\"\u2581\", null],\n[\"9\", null],\n[\"0\", null],\n[\"th\", null],\n[\"\u2581Academy\", \"\u0410\u043a\u0430\u0434\u0435\u043c\u0438\u044f\"],\n[\"\u2581Awards\", \"\u041c\u0430\u0440\u0430\u043f\u0430\u0442\u0442\u0430\u0440\u044b\"],\n[\"\u2581will\", null],\n[\"\u2581be\", null],\n[\"\u2581officially\", \"\u0440\u0435\u0441\u043c\u0438\"],\n[\"\u2581announced\", \"\u0436\u0430\u0440\u0438\u044f\u043b\u0430\u043d\u0430\u0434\u044b\"],\n[\"\u2581on\", null],\n[\"\u2581January\", \"\u049b\u0430\u04a3\u0442\u0430\u0440\"],\n[\"\u2581\", null],\n[\"2\", null],\n[\"3\", null],\n[\",\", null],\n[\"\u2581after\", null],\n[\"\u2581which\", null],\n[\"\u2581Hollywood\", \"\u0413\u043e\u043b\u043b\u0438\u0432\u0443\u0434\u0442\u044b\u04a3\"],\n[\"'\", null],\n[\"s\", null],\n[\"\u2581annual\", \"\u0436\u044b\u043b\u0434\u044b\u049b\"],\n[\"\u2581most\", null],\n[\"\u2581eye\", \"\u043a\u04e9\u0437\"],\n[\"-\", null],\n[\"catch\", \"\u0442\u0430\u0440\u0442\u0430\u0440\u043b\u044b\u049b\"],\n[\"ing\", null],\n[\"\u2581\\\"\", null],\n[\"O\", \"\u041e\u0441\u043a\u0430\u0440\"],\n[\"scar\", null],\n[\"\u2581Time\", \"\u0423\u0430\u049b\u044b\u0442\u044b\"],\n[\"\\\"\", null],\n[\"\u2581will\", null],\n[\"\u2581commence\", \"\u0431\u0430\u0441\u0442\u0430\u043b\u0430\u0434\u044b\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nNominations for the 90th Academy Awards will be officially announced on January 23, after which Hollywood\u2019s annual most eye-catching \u201cOscar Time\u201d will commence.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581Nom', 'inations', '\u2581for', '\u2581the', '\u2581', '9', '0', 'th', '\u2581Academy', '\u2581Awards', '\u2581will', '\u2581be', '\u2581officially', '\u2581announced', '\u2581on', '\u2581January', '\u2581', '2', '3', ',', '\u2581after', '\u2581which', '\u2581Hollywood', '\u2019', 's', '\u2581annual', '\u2581most', '\u2581eye', '-', 'catch', 'ing', '\u2581\u201c', 'O', 'scar', '\u2581Time', '\u201d', '\u2581will', '\u2581commence', '.']"}], "[\n[\"\u2581Nom\", \"\u041d\u043e\u043c\u0438\u043d\u0430\u0446\u0438\u0438\"],\n[\"inations\", null],\n[\"\u2581for\", null],\n[\"\u2581the\", null],\n[\"\u2581\", null],\n[\"9\", null],\n[\"0\", null],\n[\"th\", null],\n[\"\u2581Academy\", \"\u0410\u043a\u0430\u0434\u0435\u043c\u0438\u0438\"],\n[\"\u2581Awards\", \"\u043a\u0438\u043d\u043e\u043f\u0440\u0435\u043c\u0438\u0438\"],\n[\"\u2581will\", null],\n[\"\u2581be\", null],\n[\"\u2581officially\", \"\u043e\u0444\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u043e\"],\n[\"\u2581announced\", \"\u043e\u0431\u044a\u044f\u0432\u043b\u0435\u043d\u044b\"],\n[\"\u2581on\", null],\n[\"\u2581January\", \"\u044f\u043d\u0432\u0430\u0440\u044f\"],\n[\"\u2581\", null],\n[\"2\", null],\n[\"3\", null],\n[\",\", null],\n[\"\u2581after\", \"\u043f\u043e\u0441\u043b\u0435\"],\n[\"\u2581which\", \"\u0447\u0435\u0433\u043e\"],\n[\"\u2581Hollywood\", \"\u0413\u043e\u043b\u043b\u0438\u0432\u0443\u0434\u0430\"],\n[\"'\", null],\n[\"s\", null],\n[\"\u2581annual\", \"\u0435\u0436\u0435\u0433\u043e\u0434\u043d\u043e\u0435\"],\n[\"\u2581most\", \"\u0441\u0430\u043c\u043e\u0435\"],\n[\"\u2581eye\", \"\u043f\u0440\u0438\u0432\u043b\u0435\u043a\u0430\u044e\u0449\u0435\u0435\"],\n[\"-\", null],\n[\"catch\", null],\n[\"ing\", null],\n[\"\u2581\\\"\", null],\n[\"O\", \"\u041e\u0441\u043a\u0430\u0440\u043e\u0432\u0441\u043a\u043e\u0435\"],\n[\"scar\", null],\n[\"\u2581Time\", \"\u0432\u0440\u0435\u043c\u044f\"],\n[\"\\\"\", null],\n[\"\u2581will\", null],\n[\"\u2581commence\", \"\u043d\u0430\u0447\u043d\u0435\u0442\u0441\u044f\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nGuglielmi said police received a call from the Grand Plaza building manager who had just received a cryptic tip that said something had occurred in apartment 1008.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581G', 'ug', 'l', 'iel', 'mi', '\u2581said', '\u2581police', '\u2581received', '\u2581a', '\u2581call', '\u2581from', '\u2581the', '\u2581Grand', '\u2581Pl', 'aza', '\u2581building', '\u2581manager', '\u2581who', '\u2581had', '\u2581just', '\u2581received', '\u2581a', '\u2581crypt', 'ic', '\u2581tip', '\u2581that', '\u2581said', '\u2581something', '\u2581had', '\u2581occurred', '\u2581in', '\u2581apartment', '\u2581', '1', '0', '0', '8', '.']"}], "[\n  [\"\u2581G\", \"\u53e4\"],\n  [\"ug\", null],\n  [\"l\", null],\n  [\"iel\", null],\n  [\"mi\", null],\n  [\"\u2581said\", \"\u8bf4\"],\n  [\"\u2581police\", \"\u8b66\u65b9\"],\n  [\"\u2581received\", \"\u6536\u5230\"],\n  [\"\u2581a\", null],\n  [\"\u2581call\", \"\u7535\u8bdd\"],\n  [\"\u2581from\", \"\u6765\u81ea\"],\n  [\"\u2581the\", null],\n  [\"\u2581Grand\", \"\u5927\"],\n  [\"\u2581Pl\", \"\u5e7f\"],\n  [\"aza\", null],\n  [\"\u2581building\", \"\u5927\u697c\"],\n  [\"\u2581manager\", \"\u7ecf\u7406\"],\n  [\"\u2581who\", \"\u4ed6\"],\n  [\"\u2581had\", null],\n  [\"\u2581just\", \"\u521a\u521a\"],\n  [\"\u2581received\", \"\u6536\u5230\"],\n  [\"\u2581a\", null],\n  [\"\u2581crypt\", \"\u795e\u79d8\"],\n  [\"ic\", null],\n  [\"\u2581tip\", \"\u63d0\u793a\"],\n  [\"\u2581that\", null],\n  [\"\u2581said\", \"\u8bf4\"],\n  [\"\u2581something\", \"\u67d0\u4e8b\"],\n  [\"\u2581had\", null],\n  [\"\u2581occurred\", \"\u53d1\u751f\"],\n  [\"\u2581in\", \"\u5728\"],\n  [\"\u2581apartment\", \"\u516c\u5bd3\"],\n  [\"\u2581\", null],\n  [\"1\", \"1008\"],\n  [\"0\", null],\n  [\"0\", null],\n  [\"8\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nGuglielmi said police received a call from the Grand Plaza building manager who had just received a cryptic tip that said something had occurred in apartment 1008.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581G', 'ug', 'l', 'iel', 'mi', '\u2581said', '\u2581police', '\u2581received', '\u2581a', '\u2581call', '\u2581from', '\u2581the', '\u2581Grand', '\u2581Pl', 'aza', '\u2581building', '\u2581manager', '\u2581who', '\u2581had', '\u2581just', '\u2581received', '\u2581a', '\u2581crypt', 'ic', '\u2581tip', '\u2581that', '\u2581said', '\u2581something', '\u2581had', '\u2581occurred', '\u2581in', '\u2581apartment', '\u2581', '1', '0', '0', '8', '.']"}], "[\n[\"\u2581G\", \"\u0413\u0443\u043b\u044c\u0435\u043b\u044c\u043c\u0438\"],\n[\"ug\", null],\n[\"l\", null],\n[\"iel\", null],\n[\"mi\", null],\n[\"\u2581said\", \"\u0430\u0439\u0442\u0442\u044b\"],\n[\"\u2581police\", \"\u043f\u043e\u043b\u0438\u0446\u0438\u044f\"],\n[\"\u2581received\", \"\u0430\u043b\u0434\u044b\"],\n[\"\u2581a\", null],\n[\"\u2581call\", \"\u049b\u043e\u04a3\u044b\u0440\u0430\u0443\"],\n[\"\u2581from\", null],\n[\"\u2581the\", null],\n[\"\u2581Grand\", \"\u0413\u0440\u0430\u043d\u0434\"],\n[\"\u2581Pl\", \"\u041f\u043b\u0430\u0437\u0430\"],\n[\"aza\", null],\n[\"\u2581building\", \"\u0493\u0438\u043c\u0430\u0440\u0430\u0442\u044b\u043d\u044b\u04a3\"],\n[\"\u2581manager\", \"\u043c\u0435\u043d\u0435\u0434\u0436\u0435\u0440\u0456\u043d\u0435\u043d\"],\n[\"\u2581who\", null],\n[\"\u2581had\", null],\n[\"\u2581just\", \"\u0436\u0430\u04a3\u0430 \u0493\u0430\u043d\u0430\"],\n[\"\u2581received\", \"\u0430\u043b\u0493\u0430\u043d\"],\n[\"\u2581a\", null],\n[\"\u2581crypt\", \"\u049b\u04b1\u043f\u0438\u044f\"],\n[\"ic\", null],\n[\"\u2581tip\", \"\u0430\u049b\u043f\u0430\u0440\u0430\u0442\"],\n[\"\u2581that\", null],\n[\"\u2581said\", \"\u0430\u0439\u0442\u049b\u0430\u043d\"],\n[\"\u2581something\", \"\u0431\u0456\u0440\u0434\u0435\u04a3\u0435\"],\n[\"\u2581had\", null],\n[\"\u2581occurred\", \"\u0431\u043e\u043b\u0493\u0430\u043d\u044b\u043d\"],\n[\"\u2581in\", null],\n[\"\u2581apartment\", \"\u043f\u04d9\u0442\u0435\u0440\u0434\u0435\"],\n[\"\u2581\", null],\n[\"1\", null],\n[\"0\", null],\n[\"0\", null],\n[\"8\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nGuglielmi said police received a call from the Grand Plaza building manager who had just received a cryptic tip that said something had occurred in apartment 1008.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581G', 'ug', 'l', 'iel', 'mi', '\u2581said', '\u2581police', '\u2581received', '\u2581a', '\u2581call', '\u2581from', '\u2581the', '\u2581Grand', '\u2581Pl', 'aza', '\u2581building', '\u2581manager', '\u2581who', '\u2581had', '\u2581just', '\u2581received', '\u2581a', '\u2581crypt', 'ic', '\u2581tip', '\u2581that', '\u2581said', '\u2581something', '\u2581had', '\u2581occurred', '\u2581in', '\u2581apartment', '\u2581', '1', '0', '0', '8', '.']"}], "[\n  [\"\u2581G\", \"\u0413\u0443\u043b\u044c\u0435\u043b\u044c\u043c\u0438\"],\n  [\"ug\", null],\n  [\"l\", null],\n  [\"iel\", null],\n  [\"mi\", null],\n  [\"\u2581said\", \"\u0441\u043a\u0430\u0437\u0430\u043b\"],\n  [\"\u2581police\", \"\u043f\u043e\u043b\u0438\u0446\u0438\u044f\"],\n  [\"\u2581received\", \"\u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0430\"],\n  [\"\u2581a\", null],\n  [\"\u2581call\", \"\u0437\u0432\u043e\u043d\u043e\u043a\"],\n  [\"\u2581from\", \"\u043e\u0442\"],\n  [\"\u2581the\", null],\n  [\"\u2581Grand\", \"\u0413\u0440\u0430\u043d\u0434\"],\n  [\"\u2581Pl\", \"\u041f\u043b\u0430\"],\n  [\"aza\", null],\n  [\"\u2581building\", \"\u0437\u0434\u0430\u043d\u0438\u044f\"],\n  [\"\u2581manager\", \"\u0443\u043f\u0440\u0430\u0432\u043b\u044f\u044e\u0449\u0435\u0433\u043e\"],\n  [\"\u2581who\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u0439\"],\n  [\"\u2581had\", null],\n  [\"\u2581just\", \"\u0442\u043e\u043b\u044c\u043a\u043e \u0447\u0442\u043e\"],\n  [\"\u2581received\", \"\u043f\u043e\u043b\u0443\u0447\u0438\u043b\"],\n  [\"\u2581a\", null],\n  [\"\u2581crypt\", \"\u0437\u0430\u0433\u0430\u0434\u043e\u0447\u043d\u0443\u044e\"],\n  [\"ic\", null],\n  [\"\u2581tip\", \"\u043d\u0430\u0432\u043e\u0434\u043a\u0443\"],\n  [\"\u2581that\", \"\u043a\u043e\u0442\u043e\u0440\u0430\u044f\"],\n  [\"\u2581said\", \"\u0441\u043e\u043e\u0431\u0449\u0430\u043b\u0430\"],\n  [\"\u2581something\", \"\u0447\u0442\u043e-\u0442\u043e\"],\n  [\"\u2581had\", null],\n  [\"\u2581occurred\", \"\u043f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u043e\"],\n  [\"\u2581in\", \"\u0432\"],\n  [\"\u2581apartment\", \"\u043a\u0432\u0430\u0440\u0442\u0438\u0440\u0435\"],\n  [\"\u2581\", null],\n  [\"1\", null],\n  [\"0\", null],\n  [\"0\", null],\n  [\"8\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nDuring his meeting with Sagintayev, Li Keqiang said he hopes both China and Kazakhstan will continue to work together\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581During', '\u2581his', '\u2581meeting', '\u2581with', '\u2581S', 'ag', 'int', 'ay', 'ev', ',', '\u2581Li', '\u2581K', 'eq', 'i', 'ang', '\u2581said', '\u2581he', '\u2581hopes', '\u2581both', '\u2581China', '\u2581and', '\u2581Kaz', 'akh', 'stan', '\u2581will', '\u2581continue', '\u2581to', '\u2581work', '\u2581together']"}], "[\n  [\"\u2581During\", \"\u5728\"],\n  [\"\u2581his\", null],\n  [\"\u2581meeting\", \"\u4f1a\u89c1\"],\n  [\"\u2581with\", null],\n  [\"\u2581S\", \"\u8428\u91d1\u5854\u8036\u592b\"],\n  [\"ag\", null],\n  [\"int\", null],\n  [\"ay\", null],\n  [\"ev\", null],\n  [\",\", null],\n  [\"\u2581Li\", \"\u674e\"],\n  [\"\u2581K\", \"\u514b\u5f3a\"],\n  [\"eq\", null],\n  [\"i\", null],\n  [\"ang\", null],\n  [\"\u2581said\", \"\u8868\u793a\"],\n  [\"\u2581he\", \"\u4ed6\"],\n  [\"\u2581hopes\", \"\u5e0c\u671b\"],\n  [\"\u2581both\", \"\u4e24\u56fd\"],\n  [\"\u2581China\", \"\u4e2d\u56fd\"],\n  [\"\u2581and\", null],\n  [\"\u2581Kaz\", \"\u54c8\u8428\u514b\u65af\u5766\"],\n  [\"akh\", null],\n  [\"stan\", null],\n  [\"\u2581will\", null],\n  [\"\u2581continue\", \"\u7ee7\u7eed\"],\n  [\"\u2581to\", null],\n  [\"\u2581work\", \"\u5408\u4f5c\"],\n  [\"\u2581together\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nDuring his meeting with Sagintayev, Li Keqiang said he hopes both China and Kazakhstan will continue to work together\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581During', '\u2581his', '\u2581meeting', '\u2581with', '\u2581S', 'ag', 'int', 'ay', 'ev', ',', '\u2581Li', '\u2581K', 'eq', 'i', 'ang', '\u2581said', '\u2581he', '\u2581hopes', '\u2581both', '\u2581China', '\u2581and', '\u2581Kaz', 'akh', 'stan', '\u2581will', '\u2581continue', '\u2581to', '\u2581work', '\u2581together']"}], "[\n  [\"\u2581During\", \"\u041a\u0435\u0437\u0456\u043d\u0434\u0435\"],\n  [\"\u2581his\", null],\n  [\"\u2581meeting\", \"\u043a\u0435\u0437\u0434\u0435\u0441\u0443\u0456\"],\n  [\"\u2581with\", null],\n  [\"\u2581S\", \"\u0421\u0430\u0493\u044b\u043d\u0442\u0430\u0435\u0432\"],\n  [\"ag\", null],\n  [\"int\", null],\n  [\"ay\", null],\n  [\"ev\", null],\n  [\",\", null],\n  [\"\u2581Li\", \"\u041b\u0438\"],\n  [\"\u2581K\", \"\u041a\u044d\u0446\u044f\u043d\"],\n  [\"eq\", null],\n  [\"i\", null],\n  [\"ang\", null],\n  [\"\u2581said\", \"\u0430\u0439\u0442\u0442\u044b\"],\n  [\"\u2581he\", null],\n  [\"\u2581hopes\", \"\u04af\u043c\u0456\u0442\u0442\u0435\u043d\u0435\u0434\u0456\"],\n  [\"\u2581both\", null],\n  [\"\u2581China\", \"\u049a\u044b\u0442\u0430\u0439\"],\n  [\"\u2581and\", null],\n  [\"\u2581Kaz\", \"\u049a\u0430\u0437\u0430\u049b\u0441\u0442\u0430\u043d\"],\n  [\"akh\", null],\n  [\"stan\", null],\n  [\"\u2581will\", null],\n  [\"\u2581continue\", \"\u0436\u0430\u043b\u0493\u0430\u0441\u0442\u044b\u0440\u0430\u0434\u044b\"],\n  [\"\u2581to\", null],\n  [\"\u2581work\", \"\u0436\u04b1\u043c\u044b\u0441 \u0456\u0441\u0442\u0435\u0443\u0434\u0456\"],\n  [\"\u2581together\", \"\u0431\u0456\u0440\u0433\u0435\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nDuring his meeting with Sagintayev, Li Keqiang said he hopes both China and Kazakhstan will continue to work together\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581During', '\u2581his', '\u2581meeting', '\u2581with', '\u2581S', 'ag', 'int', 'ay', 'ev', ',', '\u2581Li', '\u2581K', 'eq', 'i', 'ang', '\u2581said', '\u2581he', '\u2581hopes', '\u2581both', '\u2581China', '\u2581and', '\u2581Kaz', 'akh', 'stan', '\u2581will', '\u2581continue', '\u2581to', '\u2581work', '\u2581together']"}], "[\n    [\"\u2581During\", \"\u0412\u043e \u0432\u0440\u0435\u043c\u044f\"],\n    [\"\u2581his\", null],\n    [\"\u2581meeting\", \"\u0432\u0441\u0442\u0440\u0435\u0447\u0438\"],\n    [\"\u2581with\", null],\n    [\"\u2581S\", \"\u0421\u0430\u0433\u0438\u043d\u0442\u0430\u0435\u0432\u044b\u043c\"],\n    [\"ag\", null],\n    [\"int\", null],\n    [\"ay\", null],\n    [\"ev\", null],\n    [\",\", null],\n    [\"\u2581Li\", \"\u041b\u0438\"],\n    [\"\u2581K\", \"\u041a\u044d\u0446\u044f\u043d\"],\n    [\"eq\", null],\n    [\"i\", null],\n    [\"ang\", null],\n    [\"\u2581said\", \"\u0441\u043a\u0430\u0437\u0430\u043b\"],\n    [\"\u2581he\", null],\n    [\"\u2581hopes\", \"\u043d\u0430\u0434\u0435\u0435\u0442\u0441\u044f\"],\n    [\"\u2581both\", null],\n    [\"\u2581China\", \"\u041a\u0438\u0442\u0430\u0439\"],\n    [\"\u2581and\", null],\n    [\"\u2581Kaz\", \"\u041a\u0430\u0437\u0430\u0445\u0441\u0442\u0430\u043d\"],\n    [\"akh\", null],\n    [\"stan\", null],\n    [\"\u2581will\", null],\n    [\"\u2581continue\", \"\u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0430\u0442\"],\n    [\"\u2581to\", null],\n    [\"\u2581work\", \"\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c\"],\n    [\"\u2581together\", \"\u0432\u043c\u0435\u0441\u0442\u0435\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nThe U.S. Dollar Index, which measures the US$ against six major currencies, ended the day down 0.10%, falling to 96.479 at the closing session.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581U', '.', 'S', '.', '\u2581D', 'ollar', '\u2581Index', ',', '\u2581which', '\u2581measures', '\u2581the', '\u2581US', '$', '\u2581against', '\u2581six', '\u2581major', '\u2581cur', 'ren', 'cies', ',', '\u2581ended', '\u2581the', '\u2581day', '\u2581down', '\u2581', '0', '.', '1', '0', '%,', '\u2581falling', '\u2581to', '\u2581', '9', '6', '.', '4', '7', '9', '\u2581at', '\u2581the', '\u2581closing', '\u2581session', '.']"}], "[\n[\"\u2581The\", null],\n[\"\u2581U\", \"\u7f8e\"],\n[\".\", null],\n[\"S\", null],\n[\".\", null],\n[\"\u2581D\", \"\u7f8e\u5143\"],\n[\"ollar\", null],\n[\"\u2581Index\", \"\u6307\u6570\"],\n[\",\", null],\n[\"\u2581which\", null],\n[\"\u2581measures\", \"\u8861\u91cf\"],\n[\"\u2581the\", null],\n[\"\u2581US\", \"\u7f8e\u56fd\"],\n[\"$\", null],\n[\"\u2581against\", \"\u5bf9\u6bd4\"],\n[\"\u2581six\", \"\u516d\u79cd\"],\n[\"\u2581major\", \"\u4e3b\u8981\"],\n[\"\u2581cur\", \"\u8d27\"],\n[\"ren\", null],\n[\"cies\", null],\n[\",\", null],\n[\"\u2581ended\", \"\u7ed3\u675f\"],\n[\"\u2581the\", null],\n[\"\u2581day\", \"\u5f53\u5929\"],\n[\"\u2581down\", \"\u4e0b\u8dcc\"],\n[\"\u2581\", null],\n[\"0\", \"0\"],\n[\".\", null],\n[\"1\", \"1\"],\n[\"0\", \"0\"],\n[\"%,\", null],\n[\"\u2581falling\", \"\u4e0b\u964d\"],\n[\"\u2581to\", \"\u81f3\"],\n[\"\u2581\", null],\n[\"9\", \"96\"],\n[\"6\", null],\n[\".\", null],\n[\"4\", \"479\"],\n[\"7\", null],\n[\"9\", null],\n[\"\u2581at\", \"\u5728\"],\n[\"\u2581the\", null],\n[\"\u2581closing\", \"\u6536\u76d8\"],\n[\"\u2581session\", \"\u65f6\u6bb5\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nThe U.S. Dollar Index, which measures the US$ against six major currencies, ended the day down 0.10%, falling to 96.479 at the closing session.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581U', '.', 'S', '.', '\u2581D', 'ollar', '\u2581Index', ',', '\u2581which', '\u2581measures', '\u2581the', '\u2581US', '$', '\u2581against', '\u2581six', '\u2581major', '\u2581cur', 'ren', 'cies', ',', '\u2581ended', '\u2581the', '\u2581day', '\u2581down', '\u2581', '0', '.', '1', '0', '%,', '\u2581falling', '\u2581to', '\u2581', '9', '6', '.', '4', '7', '9', '\u2581at', '\u2581the', '\u2581closing', '\u2581session', '.']"}], "[\n[\"\u2581The\", null],\n[\"\u2581U\", \"\u0410\u049a\u0428\"],\n[\".\", null],\n[\"S\", null],\n[\".\", null],\n[\"\u2581D\", \"\u0414\u043e\u043b\u043b\u0430\u0440\"],\n[\"ollar\", null],\n[\"\u2581Index\", \"\u0418\u043d\u0434\u0435\u043a\u0441\u0456\"],\n[\",\", null],\n[\"\u2581which\", null],\n[\"\u2581measures\", \"\u04e9\u043b\u0448\u0435\u0439\u0442\u0456\u043d\"],\n[\"\u2581the\", null],\n[\"\u2581US\", \"\u0410\u049a\u0428\"],\n[\"$\", null],\n[\"\u2581against\", \"\u049b\u0430\u0440\u0441\u044b\"],\n[\"\u2581six\", \"\u0430\u043b\u0442\u044b\"],\n[\"\u2581major\", \"\u043d\u0435\u0433\u0456\u0437\u0433\u0456\"],\n[\"\u2581cur\", \"\u0432\u0430\u043b\u044e\u0442\u0430\"],\n[\"ren\", null],\n[\"cies\", null],\n[\",\", null],\n[\"\u2581ended\", \"\u0430\u044f\u049b\u0442\u0430\u043b\u0434\u044b\"],\n[\"\u2581the\", null],\n[\"\u2581day\", \"\u043a\u04af\u043d\u0434\u0456\"],\n[\"\u2581down\", \"\u0442\u04e9\u043c\u0435\u043d\u0434\u0435\u043f\"],\n[\"\u2581\", null],\n[\"0\", null],\n[\".\", null],\n[\"1\", null],\n[\"0\", null],\n[\"%,\", null],\n[\"\u2581falling\", \"\u049b\u04b1\u043b\u0434\u044b\u0440\u0430\u043f\"],\n[\"\u2581to\", null],\n[\"\u2581\", null],\n[\"9\", null],\n[\"6\", null],\n[\".\", null],\n[\"4\", null],\n[\"7\", null],\n[\"9\", null],\n[\"\u2581at\", null],\n[\"\u2581the\", null],\n[\"\u2581closing\", \"\u0436\u0430\u0431\u044b\u043b\u0443\"],\n[\"\u2581session\", \"\u0441\u0435\u0441\u0441\u0438\u044f\u0441\u044b\u043d\u0434\u0430\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThe U.S. Dollar Index, which measures the US$ against six major currencies, ended the day down 0.10%, falling to 96.479 at the closing session.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581U', '.', 'S', '.', '\u2581D', 'ollar', '\u2581Index', ',', '\u2581which', '\u2581measures', '\u2581the', '\u2581US', '$', '\u2581against', '\u2581six', '\u2581major', '\u2581cur', 'ren', 'cies', ',', '\u2581ended', '\u2581the', '\u2581day', '\u2581down', '\u2581', '0', '.', '1', '0', '%,', '\u2581falling', '\u2581to', '\u2581', '9', '6', '.', '4', '7', '9', '\u2581at', '\u2581the', '\u2581closing', '\u2581session', '.']"}], "[\n[\"\u2581The\", null],\n[\"\u2581U\", \"\u0418\u043d\u0434\u0435\u043a\u0441\"],\n[\".\", null],\n[\"S\", null],\n[\".\", null],\n[\"\u2581D\", null],\n[\"ollar\", null],\n[\"\u2581Index\", null],\n[\",\", null],\n[\"\u2581which\", null],\n[\"\u2581measures\", \"\u0438\u0437\u043c\u0435\u0440\u044f\u044e\u0449\u0438\u0439\"],\n[\"\u2581the\", null],\n[\"\u2581US\", \"\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0441\u043a\u0438\u0439\"],\n[\"$\", null],\n[\"\u2581against\", \"\u043f\u043e\"],\n[\"\u2581six\", \"\u0448\u0435\u0441\u0442\u0438\"],\n[\"\u2581major\", \"\u043e\u0441\u043d\u043e\u0432\u043d\u044b\u043c\"],\n[\"\u2581cur\", \"\u0432\u0430\u043b\u044e\"],\n[\"ren\", null],\n[\"cies\", null],\n[\",\", null],\n[\"\u2581ended\", \"\u0437\u0430\u043a\u043e\u043d\u0447\u0438\u043b\"],\n[\"\u2581the\", null],\n[\"\u2581day\", \"\u0434\u0435\u043d\u044c\"],\n[\"\u2581down\", \"\u0441\u043d\u0438\u0436\u0435\u043d\u0438\u0435\u043c\"],\n[\"\u2581\", null],\n[\"0\", null],\n[\".\", null],\n[\"1\", null],\n[\"0\", null],\n[\"%,\", null],\n[\"\u2581falling\", \"\u0443\u043f\u0430\u0432\"],\n[\"\u2581to\", \"\u0434\u043e\"],\n[\"\u2581\", null],\n[\"9\", null],\n[\"6\", null],\n[\".\", null],\n[\"4\", null],\n[\"7\", null],\n[\"9\", null],\n[\"\u2581at\", \"\u043d\u0430\"],\n[\"\u2581the\", null],\n[\"\u2581closing\", \"\u0437\u0430\u043a\u0440\u044b\u0442\u0438\u0438\"],\n[\"\u2581session\", \"\u0442\u043e\u0440\u0433\u043e\u0432\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nThe RMB middle price is registered at 6.4401, a depreciation of 66 basis points. The rising slows down.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581R', 'MB', '\u2581middle', '\u2581price', '\u2581is', '\u2581registered', '\u2581at', '\u2581', '6', '.', '4', '4', '0', '1', ',', '\u2581a', '\u2581de', 'pre', 'ci', 'ation', '\u2581of', '\u2581', '6', '6', '\u2581basis', '\u2581points', '.', '\u2581The', '\u2581rising', '\u2581sl', 'ows', '\u2581down', '.']"}], "[\n  [\"\u2581The\", null],\n  [\"\u2581R\", \"\u4eba\u6c11\u5e01\"],\n  [\"MB\", null],\n  [\"\u2581middle\", \"\u4e2d\u95f4\"],\n  [\"\u2581price\", \"\u4ef7\u683c\"],\n  [\"\u2581is\", null],\n  [\"\u2581registered\", \"\u62a5\u4ef7\"],\n  [\"\u2581at\", null],\n  [\"\u2581\", null],\n  [\"6\", null],\n  [\".\", null],\n  [\"4\", null],\n  [\"4\", null],\n  [\"0\", null],\n  [\"1\", null],\n  [\",\", null],\n  [\"\u2581a\", null],\n  [\"\u2581de\", \"\u8d2c\u503c\"],\n  [\"pre\", null],\n  [\"ci\", null],\n  [\"ation\", null],\n  [\"\u2581of\", null],\n  [\"\u2581\", null],\n  [\"6\", null],\n  [\"6\", null],\n  [\"\u2581basis\", \"\u57fa\u70b9\"],\n  [\"\u2581points\", null],\n  [\".\", null],\n  [\"\u2581The\", null],\n  [\"\u2581rising\", \"\u4e0a\u6da8\"],\n  [\"\u2581sl\", \"\u653e\u7f13\"],\n  [\"ows\", null],\n  [\"\u2581down\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nThe RMB middle price is registered at 6.4401, a depreciation of 66 basis points. The rising slows down.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581R', 'MB', '\u2581middle', '\u2581price', '\u2581is', '\u2581registered', '\u2581at', '\u2581', '6', '.', '4', '4', '0', '1', ',', '\u2581a', '\u2581de', 'pre', 'ci', 'ation', '\u2581of', '\u2581', '6', '6', '\u2581basis', '\u2581points', '.', '\u2581The', '\u2581rising', '\u2581sl', 'ows', '\u2581down', '.']"}], "[\n  [\"\u2581The\", null],\n  [\"\u2581R\", \"RMB\"],\n  [\"MB\", null],\n  [\"\u2581middle\", \"\u043e\u0440\u0442\u0430\u0448\u0430\"],\n  [\"\u2581price\", \"\u0431\u0430\u0493\u0430\"],\n  [\"\u2581is\", null],\n  [\"\u2581registered\", \"\u0442\u0456\u0440\u043a\u0435\u043b\u0433\u0435\u043d\"],\n  [\"\u2581at\", null],\n  [\"\u2581\", null],\n  [\"6\", null],\n  [\".\", null],\n  [\"4\", null],\n  [\"4\", null],\n  [\"0\", null],\n  [\"1\", null],\n  [\",\", null],\n  [\"\u2581a\", null],\n  [\"\u2581de\", \"\u049b\u04b1\u043d\u0441\u044b\u0437\u0434\u0430\u043d\u0443\"],\n  [\"pre\", null],\n  [\"ci\", null],\n  [\"ation\", null],\n  [\"\u2581of\", null],\n  [\"\u2581\", null],\n  [\"6\", null],\n  [\"6\", null],\n  [\"\u2581basis\", \"\u0431\u0430\u0437\u0438\u0441\u0442\u0456\u043a\"],\n  [\"\u2581points\", \"\u043f\u0443\u043d\u043a\u0442\"],\n  [\".\", null],\n  [\"\u2581The\", null],\n  [\"\u2581rising\", \"\u04e9\u0441\u0443\"],\n  [\"\u2581sl\", \"\u0431\u0430\u044f\u0443\u043b\u0430\u0439\u0434\u044b\"],\n  [\"ows\", null],\n  [\"\u2581down\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThe RMB middle price is registered at 6.4401, a depreciation of 66 basis points. The rising slows down.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581The', '\u2581R', 'MB', '\u2581middle', '\u2581price', '\u2581is', '\u2581registered', '\u2581at', '\u2581', '6', '.', '4', '4', '0', '1', ',', '\u2581a', '\u2581de', 'pre', 'ci', 'ation', '\u2581of', '\u2581', '6', '6', '\u2581basis', '\u2581points', '.', '\u2581The', '\u2581rising', '\u2581sl', 'ows', '\u2581down', '.']"}], "[\n  [\"\u2581The\", null],\n  [\"\u2581R\", \"\u042e\u0430\u043d\u044c\"],\n  [\"MB\", null],\n  [\"\u2581middle\", \"\u0441\u0440\u0435\u0434\u043d\u0438\u0439\"],\n  [\"\u2581price\", \"\u043a\u0443\u0440\u0441\"],\n  [\"\u2581is\", null],\n  [\"\u2581registered\", \"\u0437\u0430\u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043d\"],\n  [\"\u2581at\", \"\u043d\u0430\"],\n  [\"\u2581\", null],\n  [\"6\", null],\n  [\".\", null],\n  [\"4\", null],\n  [\"4\", null],\n  [\"0\", null],\n  [\"1\", null],\n  [\",\", null],\n  [\"\u2581a\", null],\n  [\"\u2581de\", \"\u043e\u0431\u0435\u0441\u0446\u0435\u043d\u0438\u0432\u0430\u043d\u0438\u0435\"],\n  [\"pre\", null],\n  [\"ci\", null],\n  [\"ation\", null],\n  [\"\u2581of\", null],\n  [\"\u2581\", null],\n  [\"6\", null],\n  [\"6\", null],\n  [\"\u2581basis\", \"\u0431\u0430\u0437\u0438\u0441\u043d\u044b\u0445\"],\n  [\"\u2581points\", \"\u043f\u0443\u043d\u043a\u0442\u043e\u0432\"],\n  [\".\", null],\n  [\"\u2581The\", null],\n  [\"\u2581rising\", \"\u0440\u043e\u0441\u0442\"],\n  [\"\u2581sl\", \"\u0437\u0430\u043c\u0435\u0434\u043b\u044f\u0435\u0442\u0441\u044f\"],\n  [\"ows\", null],\n  [\"\u2581down\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u6d3b\u52a8\u524d\u4e00\u5929\uff0cHMX-1 \u6307\u6325\u5b98\u5728\u5411\u4e0a\u7ea7\u6d77\u519b\u9646\u6218\u961f\u98de\u884c\u526f\u53f8\u4ee4\u53d1\u9001\u7684\u7535\u5b50\u90ae\u4ef6\u4e2d\u8868\u793a\u201c\u6211\u53ea\u60f3\u786e\u8ba4\u60a8\u77e5\u6653\uff0c\u56e0\u4e3a\u8fd9\u6b64\u6b21\u6d3b\u52a8\u53d7\u5230\u9ad8\u5ea6\u91cd\u89c6\u3002\u201d\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u6d3b', '\u52a8', '\u524d', '\u4e00', '\u5929', '\uff0c', 'H', 'MX', '-', '1', '\u2581', '\u6307', '<0xE6>', '<0x8C>', '<0xA5>', '\u5b98', '\u5728', '\u5411', '\u4e0a', '\u7ea7', '\u6d77', '\u519b', '\u9646', '\u6218', '\u961f', '\u98de', '\u884c', '\u526f', '\u53f8', '\u4ee4', '\u53d1', '\u9001', '\u7684', '\u7535', '\u5b50', '\u90ae', '\u4ef6', '\u4e2d', '\u8868', '\u793a', '\u201c', '\u6211', '\u53ea', '\u60f3', '\u786e', '\u8ba4', '\u60a8', '\u77e5', '<0xE6>', '<0x99>', '<0x93>', '\uff0c', '\u56e0', '\u4e3a', '\u8fd9', '\u6b64', '\u6b21', '\u6d3b', '\u52a8', '\u53d7', '\u5230', '\u9ad8', '\u5ea6', '\u91cd', '\u89c6', '\u3002', '\u201d']"}], "[\n  [\"\u2581\", null],\n  [\"\u6d3b\", \"activity\"],\n  [\"\u52a8\", null],\n  [\"\u524d\", \"before\"],\n  [\"\u4e00\", \"one\"],\n  [\"\u5929\", \"day\"],\n  [\"\uff0c\", null],\n  [\"H\", \"H\"],\n  [\"MX\", \"MX\"],\n  [\"-\", null],\n  [\"1\", \"1\"],\n  [\"\u2581\", null],\n  [\"\u6307\", \"commander\"],\n  [\"<0xE6>\", null],\n  [\"<0x8C>\", null],\n  [\"<0xA5>\", null],\n  [\"\u5b98\", null],\n  [\"\u5728\", null],\n  [\"\u5411\", \"to\"],\n  [\"\u4e0a\", \"superior\"],\n  [\"\u7ea7\", null],\n  [\"\u6d77\", \"Marine\"],\n  [\"\u519b\", null],\n  [\"\u9646\", null],\n  [\"\u6218\", null],\n  [\"\u961f\", null],\n  [\"\u98de\", \"Aviation\"],\n  [\"\u884c\", null],\n  [\"\u526f\", \"Deputy\"],\n  [\"\u53f8\", \"Commander\"],\n  [\"\u4ee4\", null],\n  [\"\u53d1\", \"send\"],\n  [\"\u9001\", null],\n  [\"\u7684\", null],\n  [\"\u7535\", \"email\"],\n  [\"\u5b50\", null],\n  [\"\u90ae\", null],\n  [\"\u4ef6\", null],\n  [\"\u4e2d\", \"in\"],\n  [\"\u8868\", \"stated\"],\n  [\"\u793a\", null],\n  [\"\\\"\", null],\n  [\"\u6211\", \"I\"],\n  [\"\u53ea\", \"just\"],\n  [\"\u60f3\", \"want\"],\n  [\"\u786e\", \"confirm\"],\n  [\"\u8ba4\", null],\n  [\"\u60a8\", \"you\"],\n  [\"\u77e5\", \"know\"],\n  [\"<0xE6>\", null],\n  [\"<0x99>\", null],\n  [\"<0x93>\", null],\n  [\"\uff0c\", null],\n  [\"\u56e0\", \"because\"],\n  [\"\u4e3a\", null],\n  [\"\u8fd9\", \"this\"],\n  [\"\u6b64\", null],\n  [\"\u6b21\", null],\n  [\"\u6d3b\", \"activity\"],\n  [\"\u52a8\", null],\n  [\"\u53d7\", \"receiving\"],\n  [\"\u5230\", null],\n  [\"\u9ad8\", \"high\"],\n  [\"\u5ea6\", \"degree\"],\n  [\"\u91cd\", \"importance\"],\n  [\"\u89c6\", null],\n  [\"\u3002\", null],\n  [\"\\\"\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u6d3b\u52a8\u524d\u4e00\u5929\uff0cHMX-1 \u6307\u6325\u5b98\u5728\u5411\u4e0a\u7ea7\u6d77\u519b\u9646\u6218\u961f\u98de\u884c\u526f\u53f8\u4ee4\u53d1\u9001\u7684\u7535\u5b50\u90ae\u4ef6\u4e2d\u8868\u793a\u201c\u6211\u53ea\u60f3\u786e\u8ba4\u60a8\u77e5\u6653\uff0c\u56e0\u4e3a\u8fd9\u6b64\u6b21\u6d3b\u52a8\u53d7\u5230\u9ad8\u5ea6\u91cd\u89c6\u3002\u201d\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u6d3b', '\u52a8', '\u524d', '\u4e00', '\u5929', '\uff0c', 'H', 'MX', '-', '1', '\u2581', '\u6307', '<0xE6>', '<0x8C>', '<0xA5>', '\u5b98', '\u5728', '\u5411', '\u4e0a', '\u7ea7', '\u6d77', '\u519b', '\u9646', '\u6218', '\u961f', '\u98de', '\u884c', '\u526f', '\u53f8', '\u4ee4', '\u53d1', '\u9001', '\u7684', '\u7535', '\u5b50', '\u90ae', '\u4ef6', '\u4e2d', '\u8868', '\u793a', '\u201c', '\u6211', '\u53ea', '\u60f3', '\u786e', '\u8ba4', '\u60a8', '\u77e5', '<0xE6>', '<0x99>', '<0x93>', '\uff0c', '\u56e0', '\u4e3a', '\u8fd9', '\u6b64', '\u6b21', '\u6d3b', '\u52a8', '\u53d7', '\u5230', '\u9ad8', '\u5ea6', '\u91cd', '\u89c6', '\u3002', '\u201d']"}], "[\n[\"\u2581\", null],\n[\"\u6d3b\", \"\u0456\u0441-\u0448\u0430\u0440\u0430\"],\n[\"\u52a8\", null],\n[\"\u524d\", \"\u0430\u043b\u0434\u044b\u043d\u0434\u0430\u0493\u044b\"],\n[\"\u4e00\", \"\u0431\u0456\u0440\"],\n[\"\u5929\", \"\u043a\u04af\u043d\"],\n[\"\uff0c\", null],\n[\"H\", null],\n[\"MX\", null],\n[\"-\", null],\n[\"1\", null],\n[\"\u2581\", null],\n[\"\u6307\", \"\u043a\u043e\u043c\u0430\u043d\u0434\u0438\u0440\"],\n[\"<0xE6>\", null],\n[\"<0x8C>\", null],\n[\"<0xA5>\", null],\n[\"\u5b98\", null],\n[\"\u5728\", null],\n[\"\u5411\", null],\n[\"\u4e0a\", \"\u0436\u043e\u0493\u0430\u0440\u044b\"],\n[\"\u7ea7\", null],\n[\"\u6d77\", \"\u0442\u0435\u04a3\u0456\u0437\"],\n[\"\u519b\", null],\n[\"\u9646\", \"\u049b\u04b1\u0440\u043b\u044b\u049b\"],\n[\"\u6218\", null],\n[\"\u961f\", \"\u04d9\u0441\u043a\u0435\u0440\u0456\"],\n[\"\u98de\", \"\u04b1\u0448\u0443\"],\n[\"\u884c\", null],\n[\"\u526f\", \"\u043e\u0440\u044b\u043d\u0431\u0430\u0441\u0430\u0440\"],\n[\"\u53f8\", \"\u043a\u043e\u043c\u0430\u043d\u0434\u0438\u0440\"],\n[\"\u4ee4\", null],\n[\"\u53d1\", \"\u0436\u0456\u0431\u0435\u0440\u0443\"],\n[\"\u9001\", null],\n[\"\u7684\", null],\n[\"\u7535\", \"\u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u0434\u044b\"],\n[\"\u5b50\", null],\n[\"\u90ae\", \"\u043f\u043e\u0448\u0442\u0430\"],\n[\"\u4ef6\", null],\n[\"\u4e2d\", null],\n[\"\u8868\", \"\u0431\u0456\u043b\u0434\u0456\u0440\u0443\"],\n[\"\u793a\", null],\n[\"\"\", null],\n[\"\u6211\", \"\u043c\u0435\u043d\"],\n[\"\u53ea\", \"\u0442\u0435\u043a\"],\n[\"\u60f3\", \"\u049b\u0430\u043b\u0430\u0439\u043c\u044b\u043d\"],\n[\"\u786e\", \"\u0440\u0430\u0441\u0442\u0430\u0443\"],\n[\"\u8ba4\", null],\n[\"\u60a8\", \"\u0441\u0456\u0437\"],\n[\"\u77e5\", \"\u0431\u0456\u043b\u0443\"],\n[\"<0xE6>\", null],\n[\"<0x99>\", null],\n[\"<0x93>\", null],\n[\"\uff0c\", null],\n[\"\u56e0\", \"\u0441\u0435\u0431\u0435\u0431\u0456\"],\n[\"\u4e3a\", null],\n[\"\u8fd9\", \"\u0431\u04b1\u043b\"],\n[\"\u6b64\", null],\n[\"\u6b21\", \"\u0440\u0435\u0442\"],\n[\"\u6d3b\", \"\u0456\u0441-\u0448\u0430\u0440\u0430\"],\n[\"\u52a8\", null],\n[\"\u53d7\", \"\u049b\u0430\u0431\u044b\u043b\u0434\u0430\u0443\"],\n[\"\u5230\", null],\n[\"\u9ad8\", \"\u0436\u043e\u0493\u0430\u0440\u044b\"],\n[\"\u5ea6\", null],\n[\"\u91cd\", \"\u043c\u0430\u04a3\u044b\u0437\u0434\u044b\"],\n[\"\u89c6\", null],\n[\"\u3002\", null],\n[\"\"\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u6d3b\u52a8\u524d\u4e00\u5929\uff0cHMX-1 \u6307\u6325\u5b98\u5728\u5411\u4e0a\u7ea7\u6d77\u519b\u9646\u6218\u961f\u98de\u884c\u526f\u53f8\u4ee4\u53d1\u9001\u7684\u7535\u5b50\u90ae\u4ef6\u4e2d\u8868\u793a\u201c\u6211\u53ea\u60f3\u786e\u8ba4\u60a8\u77e5\u6653\uff0c\u56e0\u4e3a\u8fd9\u6b64\u6b21\u6d3b\u52a8\u53d7\u5230\u9ad8\u5ea6\u91cd\u89c6\u3002\u201d\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u6d3b', '\u52a8', '\u524d', '\u4e00', '\u5929', '\uff0c', 'H', 'MX', '-', '1', '\u2581', '\u6307', '<0xE6>', '<0x8C>', '<0xA5>', '\u5b98', '\u5728', '\u5411', '\u4e0a', '\u7ea7', '\u6d77', '\u519b', '\u9646', '\u6218', '\u961f', '\u98de', '\u884c', '\u526f', '\u53f8', '\u4ee4', '\u53d1', '\u9001', '\u7684', '\u7535', '\u5b50', '\u90ae', '\u4ef6', '\u4e2d', '\u8868', '\u793a', '\u201c', '\u6211', '\u53ea', '\u60f3', '\u786e', '\u8ba4', '\u60a8', '\u77e5', '<0xE6>', '<0x99>', '<0x93>', '\uff0c', '\u56e0', '\u4e3a', '\u8fd9', '\u6b64', '\u6b21', '\u6d3b', '\u52a8', '\u53d7', '\u5230', '\u9ad8', '\u5ea6', '\u91cd', '\u89c6', '\u3002', '\u201d']"}], "[\n[\"\u2581\", null],\n[\"\u6d3b\", \"\u043c\u0435\u0440\u043e\u043f\u0440\u0438\u044f\u0442\u0438\u0435\"],\n[\"\u52a8\", null],\n[\"\u524d\", \"\u043f\u0435\u0440\u0435\u0434\"],\n[\"\u4e00\", \"\u043e\u0434\u0438\u043d\"],\n[\"\u5929\", \"\u0434\u0435\u043d\u044c\"],\n[\"\uff0c\", null],\n[\"H\", null],\n[\"MX\", null],\n[\"-\", null],\n[\"1\", null],\n[\"\u2581\", null],\n[\"\u6307\", \"\u043a\u043e\u043c\u0430\u043d\u0434\u0438\u0440\"],\n[\"<0xE6>\", null],\n[\"<0x8C>\", null],\n[\"<0xA5>\", null],\n[\"\u5b98\", null],\n[\"\u5728\", null],\n[\"\u5411\", null],\n[\"\u4e0a\", \"\u0432\u044b\u0448\u0435\u0441\u0442\u043e\u044f\u0449\u0435\u043c\u0443\"],\n[\"\u7ea7\", null],\n[\"\u6d77\", \"\u043c\u043e\u0440\u0441\u043a\u043e\u0439\"],\n[\"\u519b\", null],\n[\"\u9646\", \"\u0441\u0443\u0445\u043e\u043f\u0443\u0442\u043d\u044b\u0439\"],\n[\"\u6218\", null],\n[\"\u961f\", null],\n[\"\u98de\", \"\u0430\u0432\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439\"],\n[\"\u884c\", null],\n[\"\u526f\", \"\u0437\u0430\u043c\u0435\u0441\u0442\u0438\u0442\u0435\u043b\u044c\"],\n[\"\u53f8\", \"\u043a\u043e\u043c\u0430\u043d\u0434\u0443\u044e\u0449\u0438\u0439\"],\n[\"\u4ee4\", null],\n[\"\u53d1\", \"\u043e\u0442\u043f\u0440\u0430\u0432\u0438\u0442\u044c\"],\n[\"\u9001\", null],\n[\"\u7684\", null],\n[\"\u7535\", \"\u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u043d\u043e\u0435\"],\n[\"\u5b50\", null],\n[\"\u90ae\", \"\u043f\u0438\u0441\u044c\u043c\u043e\"],\n[\"\u4ef6\", null],\n[\"\u4e2d\", null],\n[\"\u8868\", \"\u0432\u044b\u0440\u0430\u0437\u0438\u0442\u044c\"],\n[\"\u793a\", null],\n[\"\"\", null],\n[\"\u6211\", \"\u044f\"],\n[\"\u53ea\", \"\u0442\u043e\u043b\u044c\u043a\u043e\"],\n[\"\u60f3\", \"\u0445\u043e\u0442\u0435\u0442\u044c\"],\n[\"\u786e\", \"\u043f\u043e\u0434\u0442\u0432\u0435\u0440\u0434\u0438\u0442\u044c\"],\n[\"\u8ba4\", null],\n[\"\u60a8\", \"\u0432\u044b\"],\n[\"\u77e5\", \"\u0437\u043d\u0430\u0442\u044c\"],\n[\"<0xE6>\", null],\n[\"<0x99>\", null],\n[\"<0x93>\", null],\n[\"\uff0c\", null],\n[\"\u56e0\", \"\u043f\u043e\u0442\u043e\u043c\u0443\"],\n[\"\u4e3a\", null],\n[\"\u8fd9\", \"\u044d\u0442\u043e\"],\n[\"\u6b64\", \"\u0434\u0430\u043d\u043d\u043e\u0435\"],\n[\"\u6b21\", null],\n[\"\u6d3b\", \"\u043c\u0435\u0440\u043e\u043f\u0440\u0438\u044f\u0442\u0438\u0435\"],\n[\"\u52a8\", null],\n[\"\u53d7\", \"\u043f\u043e\u043b\u0443\u0447\u0430\u0442\u044c\"],\n[\"\u5230\", null],\n[\"\u9ad8\", \"\u0432\u044b\u0441\u043e\u043a\u0438\u0439\"],\n[\"\u5ea6\", null],\n[\"\u91cd\", \"\u0432\u0430\u0436\u043d\u044b\u0439\"],\n[\"\u89c6\", null],\n[\"\u3002\", null],\n[\"\"\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u201c\u56db\u5ddd\u9020\u201d\u65b0\u80fd\u6e90\u6c7d\u8f66\u9006\u52bf\u4e0a\u884c\u8d22\u653f\u8865\u8d34\u529b\u5ea6\u51cf\u5f31\u3001\u5145\u7535\u8bbe\u65bd\u666e\u53ca\u7387\u4e0d\u9ad8\u3001\u4e09\u5143\u9502\u7535\u6c60\u6682\u505c\u5728\u65b0\u80fd\u6e90\u5ba2\u8f66\u4e0a\u4f7f\u7528\uff0c\u9762\u5bf9\u56fd\u5185\u65b0\u80fd\u6e90\u6c7d\u8f66\u5e02\u573a\u5f53\u524d\u7684\u4e0a\u884c\u538b\u529b\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u201c', '\u56db', '\u5ddd', '\u9020', '\u201d', '\u65b0', '\u80fd', '\u6e90', '<0xE6>', '<0xB1>', '<0xBD>', '\u8f66', '\u9006', '\u52bf', '\u4e0a', '\u884c', '\u8d22', '\u653f', '\u8865', '\u8d34', '\u529b', '\u5ea6', '\u51cf', '<0xE5>', '<0xBC>', '<0xB1>', '\u3001', '\u5145', '\u7535', '\u8bbe', '\u65bd', '\u666e', '\u53ca', '\u7387', '\u4e0d', '\u9ad8', '\u3001', '\u4e09', '\u5143', '<0xE9>', '<0x94>', '<0x82>', '\u7535', '\u6c60', '\u6682', '\u505c', '\u5728', '\u65b0', '\u80fd', '\u6e90', '\u5ba2', '\u8f66', '\u4e0a', '\u4f7f', '\u7528', '\uff0c', '\u9762', '\u5bf9', '\u56fd', '\u5185', '\u65b0', '\u80fd', '\u6e90', '<0xE6>', '<0xB1>', '<0xBD>', '\u8f66', '\u5e02', '\u573a', '\u5f53', '\u524d', '\u7684', '\u4e0a', '\u884c', '\u538b', '\u529b']"}], "[\n[\"\u2581\\\"\", null],\n[\"\u56db\", \"Sichuan\"],\n[\"\u5ddd\", null],\n[\"\u9020\", \"made\"],\n[\"\\\"\", null],\n[\"\u65b0\", \"new\"],\n[\"\u80fd\", null],\n[\"\u6e90\", null],\n[\"<0xE6>\", null],\n[\"<0xB1>\", null],\n[\"<0xBD>\", null],\n[\"\u8f66\", \"vehicles\"],\n[\"\u9006\", \"against\"],\n[\"\u52bf\", null],\n[\"\u4e0a\", \"rise\"],\n[\"\u884c\", null],\n[\"\u8d22\", \"financial\"],\n[\"\u653f\", null],\n[\"\u8865\", \"subsidies\"],\n[\"\u8d34\", null],\n[\"\u529b\", \"strength\"],\n[\"\u5ea6\", null],\n[\"\u51cf\", \"decrease\"],\n[\"<0xE5>\", null],\n[\"<0xBC>\", null],\n[\"<0xB1>\", null],\n[\"\u3001\", null],\n[\"\u5145\", \"charging\"],\n[\"\u7535\", null],\n[\"\u8bbe\", \"facilities\"],\n[\"\u65bd\", null],\n[\"\u666e\", \"prevalence\"],\n[\"\u53ca\", null],\n[\"\u7387\", \"rate\"],\n[\"\u4e0d\", \"not\"],\n[\"\u9ad8\", \"high\"],\n[\"\u3001\", null],\n[\"\u4e09\", \"ternary\"],\n[\"\u5143\", null],\n[\"<0xE9>\", null],\n[\"<0x94>\", null],\n[\"<0x82>\", null],\n[\"\u7535\", \"lithium\"],\n[\"\u6c60\", null],\n[\"\u6682\", \"temporarily\"],\n[\"\u505c\", \"suspended\"],\n[\"\u5728\", \"in\"],\n[\"\u65b0\", \"new\"],\n[\"\u80fd\", null],\n[\"\u6e90\", null],\n[\"\u5ba2\", \"passenger\"],\n[\"\u8f66\", null],\n[\"\u4e0a\", \"on\"],\n[\"\u4f7f\", \"use\"],\n[\"\u7528\", null],\n[\"\uff0c\", null],\n[\"\u9762\", \"facing\"],\n[\"\u5bf9\", null],\n[\"\u56fd\", \"domestic\"],\n[\"\u5185\", null],\n[\"\u65b0\", \"new\"],\n[\"\u80fd\", null],\n[\"\u6e90\", null],\n[\"<0xE6>\", null],\n[\"<0xB1>\", null],\n[\"<0xBD>\", null],\n[\"\u8f66\", \"vehicle\"],\n[\"\u5e02\", \"market\"],\n[\"\u573a\", null],\n[\"\u5f53\", \"current\"],\n[\"\u524d\", null],\n[\"\u7684\", null],\n[\"\u4e0a\", \"upward\"],\n[\"\u884c\", null],\n[\"\u538b\", \"pressure\"],\n[\"\u529b\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u201c\u56db\u5ddd\u9020\u201d\u65b0\u80fd\u6e90\u6c7d\u8f66\u9006\u52bf\u4e0a\u884c\u8d22\u653f\u8865\u8d34\u529b\u5ea6\u51cf\u5f31\u3001\u5145\u7535\u8bbe\u65bd\u666e\u53ca\u7387\u4e0d\u9ad8\u3001\u4e09\u5143\u9502\u7535\u6c60\u6682\u505c\u5728\u65b0\u80fd\u6e90\u5ba2\u8f66\u4e0a\u4f7f\u7528\uff0c\u9762\u5bf9\u56fd\u5185\u65b0\u80fd\u6e90\u6c7d\u8f66\u5e02\u573a\u5f53\u524d\u7684\u4e0a\u884c\u538b\u529b\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u201c', '\u56db', '\u5ddd', '\u9020', '\u201d', '\u65b0', '\u80fd', '\u6e90', '<0xE6>', '<0xB1>', '<0xBD>', '\u8f66', '\u9006', '\u52bf', '\u4e0a', '\u884c', '\u8d22', '\u653f', '\u8865', '\u8d34', '\u529b', '\u5ea6', '\u51cf', '<0xE5>', '<0xBC>', '<0xB1>', '\u3001', '\u5145', '\u7535', '\u8bbe', '\u65bd', '\u666e', '\u53ca', '\u7387', '\u4e0d', '\u9ad8', '\u3001', '\u4e09', '\u5143', '<0xE9>', '<0x94>', '<0x82>', '\u7535', '\u6c60', '\u6682', '\u505c', '\u5728', '\u65b0', '\u80fd', '\u6e90', '\u5ba2', '\u8f66', '\u4e0a', '\u4f7f', '\u7528', '\uff0c', '\u9762', '\u5bf9', '\u56fd', '\u5185', '\u65b0', '\u80fd', '\u6e90', '<0xE6>', '<0xB1>', '<0xBD>', '\u8f66', '\u5e02', '\u573a', '\u5f53', '\u524d', '\u7684', '\u4e0a', '\u884c', '\u538b', '\u529b']"}], "[\n  [\"\u2581\\\"\", null],\n  [\"\u56db\", \"\u0421\u044b\u0447\u0443\u0430\u043d\u044c\"],\n  [\"\u5ddd\", null],\n  [\"\u9020\", null],\n  [\"\\\"\", null],\n  [\"\u65b0\", \"\u0436\u0430\u04a3\u0430\"],\n  [\"\u80fd\", null],\n  [\"\u6e90\", null],\n  [\"<0xE6>\", null],\n  [\"<0xB1>\", null],\n  [\"<0xBD>\", null],\n  [\"\u8f66\", \"\u043a\u04e9\u043b\u0456\u043a\"],\n  [\"\u9006\", \"\u049b\u0430\u0440\u0441\u044b\"],\n  [\"\u52bf\", null],\n  [\"\u4e0a\", \"\u0436\u043e\u0493\u0430\u0440\u044b\"],\n  [\"\u884c\", null],\n  [\"\u8d22\", \"\u049b\u0430\u0440\u0436\u044b\u043b\u044b\u049b\"],\n  [\"\u653f\", null],\n  [\"\u8865\", \"\u0441\u0443\u0431\u0441\u0438\u0434\u0438\u044f\"],\n  [\"\u8d34\", null],\n  [\"\u529b\", \"\u043a\u04af\u0448\"],\n  [\"\u5ea6\", null],\n  [\"\u51cf\", \"\u0430\u0437\u0430\u0439\u0442\u0443\"],\n  [\"<0xE5>\", null],\n  [\"<0xBC>\", null],\n  [\"<0xB1>\", null],\n  [\"\u3001\", null],\n  [\"\u5145\", \"\u0437\u0430\u0440\u044f\u0434\u0442\u0430\u0443\"],\n  [\"\u7535\", null],\n  [\"\u8bbe\", \"\u049b\u04b1\u0440\u044b\u043b\u0493\u044b\"],\n  [\"\u65bd\", null],\n  [\"\u666e\", \"\u0436\u0430\u043b\u043f\u044b\"],\n  [\"\u53ca\", null],\n  [\"\u7387\", \"\u0434\u0435\u04a3\u0433\u0435\u0439\"],\n  [\"\u4e0d\", \"\u0435\u043c\u0435\u0441\"],\n  [\"\u9ad8\", \"\u0436\u043e\u0493\u0430\u0440\u044b\"],\n  [\"\u3001\", null],\n  [\"\u4e09\", \"\u04af\u0448\"],\n  [\"\u5143\", null],\n  [\"<0xE9>\", null],\n  [\"<0x94>\", null],\n  [\"<0x82>\", null],\n  [\"\u7535\", \"\u0431\u0430\u0442\u0430\u0440\u0435\u044f\"],\n  [\"\u6c60\", null],\n  [\"\u6682\", \"\u0443\u0430\u049b\u044b\u0442\u0448\u0430\"],\n  [\"\u505c\", \"\u0442\u043e\u049b\u0442\u0430\u0442\u0443\"],\n  [\"\u5728\", null],\n  [\"\u65b0\", \"\u0436\u0430\u04a3\u0430\"],\n  [\"\u80fd\", null],\n  [\"\u6e90\", null],\n  [\"\u5ba2\", \"\u0436\u043e\u043b\u0430\u0443\u0448\u044b\"],\n  [\"\u8f66\", null],\n  [\"\u4e0a\", null],\n  [\"\u4f7f\", \"\u043f\u0430\u0439\u0434\u0430\u043b\u0430\u043d\u0443\"],\n  [\"\u7528\", null],\n  [\"\uff0c\", null],\n  [\"\u9762\", \"\u0431\u0435\u0442\"],\n  [\"\u5bf9\", null],\n  [\"\u56fd\", \"\u0435\u043b\"],\n  [\"\u5185\", null],\n  [\"\u65b0\", \"\u0436\u0430\u04a3\u0430\"],\n  [\"\u80fd\", null],\n  [\"\u6e90\", null],\n  [\"<0xE6>\", null],\n  [\"<0xB1>\", null],\n  [\"<0xBD>\", null],\n  [\"\u8f66\", \"\u043a\u04e9\u043b\u0456\u043a\"],\n  [\"\u5e02\", \"\u043d\u0430\u0440\u044b\u049b\"],\n  [\"\u573a\", null],\n  [\"\u5f53\", \"\u0430\u0493\u044b\u043c\u0434\u0430\u0493\u044b\"],\n  [\"\u524d\", null],\n  [\"\u7684\", null],\n  [\"\u4e0a\", \"\u0436\u043e\u0493\u0430\u0440\u044b\"],\n  [\"\u884c\", null],\n  [\"\u538b\", \"\u049b\u044b\u0441\u044b\u043c\"],\n  [\"\u529b\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u201c\u56db\u5ddd\u9020\u201d\u65b0\u80fd\u6e90\u6c7d\u8f66\u9006\u52bf\u4e0a\u884c\u8d22\u653f\u8865\u8d34\u529b\u5ea6\u51cf\u5f31\u3001\u5145\u7535\u8bbe\u65bd\u666e\u53ca\u7387\u4e0d\u9ad8\u3001\u4e09\u5143\u9502\u7535\u6c60\u6682\u505c\u5728\u65b0\u80fd\u6e90\u5ba2\u8f66\u4e0a\u4f7f\u7528\uff0c\u9762\u5bf9\u56fd\u5185\u65b0\u80fd\u6e90\u6c7d\u8f66\u5e02\u573a\u5f53\u524d\u7684\u4e0a\u884c\u538b\u529b\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u201c', '\u56db', '\u5ddd', '\u9020', '\u201d', '\u65b0', '\u80fd', '\u6e90', '<0xE6>', '<0xB1>', '<0xBD>', '\u8f66', '\u9006', '\u52bf', '\u4e0a', '\u884c', '\u8d22', '\u653f', '\u8865', '\u8d34', '\u529b', '\u5ea6', '\u51cf', '<0xE5>', '<0xBC>', '<0xB1>', '\u3001', '\u5145', '\u7535', '\u8bbe', '\u65bd', '\u666e', '\u53ca', '\u7387', '\u4e0d', '\u9ad8', '\u3001', '\u4e09', '\u5143', '<0xE9>', '<0x94>', '<0x82>', '\u7535', '\u6c60', '\u6682', '\u505c', '\u5728', '\u65b0', '\u80fd', '\u6e90', '\u5ba2', '\u8f66', '\u4e0a', '\u4f7f', '\u7528', '\uff0c', '\u9762', '\u5bf9', '\u56fd', '\u5185', '\u65b0', '\u80fd', '\u6e90', '<0xE6>', '<0xB1>', '<0xBD>', '\u8f66', '\u5e02', '\u573a', '\u5f53', '\u524d', '\u7684', '\u4e0a', '\u884c', '\u538b', '\u529b']"}], "[\n[\"\u2581\\\"\", null],\n[\"\u56db\", \"\u0421\u044b\u0447\u0443\u0430\u043d\u044c\"],\n[\"\u5ddd\", null],\n[\"\u9020\", null],\n[\"\\\"\", null],\n[\"\u65b0\", \"\u041d\u043e\u0432\u044b\u0435\"],\n[\"\u80fd\", null],\n[\"\u6e90\", null],\n[\"<0xE6>\", null],\n[\"<0xB1>\", null],\n[\"<0xBD>\", null],\n[\"\u8f66\", \"\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u0438\"],\n[\"\u9006\", \"\u043f\u0440\u043e\u0442\u0438\u0432\"],\n[\"\u52bf\", null],\n[\"\u4e0a\", null],\n[\"\u884c\", \"\u0442\u0435\u043d\u0434\u0435\u043d\u0446\u0438\u0438\"],\n[\"\u8d22\", \"\u0424\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u044b\u0435\"],\n[\"\u653f\", null],\n[\"\u8865\", \"\u0441\u0443\u0431\u0441\u0438\u0434\u0438\u0438\"],\n[\"\u8d34\", null],\n[\"\u529b\", null],\n[\"\u5ea6\", null],\n[\"\u51cf\", \"\u0443\u043c\u0435\u043d\u044c\u0448\u0430\u044e\u0442\u0441\u044f\"],\n[\"<0xE5>\", null],\n[\"<0xBC>\", null],\n[\"<0xB1>\", null],\n[\"\u3001\", null],\n[\"\u5145\", \"\u0437\u0430\u0440\u044f\u0434\u043d\u0430\u044f\"],\n[\"\u7535\", null],\n[\"\u8bbe\", \"\u0438\u043d\u0444\u0440\u0430\u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430\"],\n[\"\u65bd\", null],\n[\"\u666e\", \"\u043d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e\"],\n[\"\u53ca\", null],\n[\"\u7387\", \"\u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u0430\"],\n[\"\u4e0d\", null],\n[\"\u9ad8\", null],\n[\"\u3001\", null],\n[\"\u4e09\", \"\u0422\u0440\u0435\u0445\u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u043d\u044b\u0435\"],\n[\"\u5143\", null],\n[\"<0xE9>\", null],\n[\"<0x94>\", null],\n[\"<0x82>\", null],\n[\"\u7535\", \"\u043b\u0438\u0442\u0438\u0435\u0432\u044b\u0435\"],\n[\"\u6c60\", \"\u0431\u0430\u0442\u0430\u0440\u0435\u0438\"],\n[\"\u6682\", \"\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\"],\n[\"\u505c\", \"\u043f\u0440\u0435\u043a\u0440\u0430\u0449\u0435\u043d\u043e\"],\n[\"\u5728\", null],\n[\"\u65b0\", \"\u043d\u043e\u0432\u044b\u0445\"],\n[\"\u80fd\", null],\n[\"\u6e90\", null],\n[\"\u5ba2\", \"\u043f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u0441\u043a\u0438\u0445\"],\n[\"\u8f66\", \"\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044f\u0445\"],\n[\"\u4e0a\", null],\n[\"\u4f7f\", \"\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\"],\n[\"\u7528\", null],\n[\"\uff0c\", null],\n[\"\u9762\", \"\u041f\u0435\u0440\u0435\u0434\"],\n[\"\u5bf9\", null],\n[\"\u56fd\", \"\u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0438\u043c\"],\n[\"\u5185\", null],\n[\"\u65b0\", \"\u043d\u043e\u0432\u044b\u0445\"],\n[\"\u80fd\", null],\n[\"\u6e90\", null],\n[\"<0xE6>\", null],\n[\"<0xB1>\", null],\n[\"<0xBD>\", null],\n[\"\u8f66\", \"\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u0435\u0439\"],\n[\"\u5e02\", \"\u0440\u044b\u043d\u043a\u043e\u043c\"],\n[\"\u573a\", null],\n[\"\u5f53\", \"\u0432\"],\n[\"\u524d\", \"\u043d\u0430\u0441\u0442\u043e\u044f\u0449\u0435\u0435\"],\n[\"\u7684\", null],\n[\"\u4e0a\", \"\u0440\u0430\u0441\u0442\u0443\u0449\u0435\u0435\"],\n[\"\u884c\", null],\n[\"\u538b\", \"\u0434\u0430\u0432\u043b\u0435\u043d\u0438\u0435\"],\n[\"\u529b\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u65b0\u95fb\u62a5\u7eb8 Svenska Dagbladet \u8c03\u67e5\u4e86\u6b64\u6b21\u516c\u5e03\u7684 581 \u4f8b\u6d4b\u8bd5\u7ed3\u679c\uff0c\u53d1\u73b0\u5176\u4e2d 243 \u4f8b\u53d7\u8bd5\u8005\u5df2\u516c\u5f00\u8868\u793a\u4ed6\u4eec\u4eca\u5e74\u4f1a\u5e74\u6ee1 18 \u5c81\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u65b0', '\u95fb', '\u62a5', '<0xE7>', '<0xBA>', '<0xB8>', '\u2581S', 'vens', 'ka', '\u2581D', 'ag', 'bl', 'ad', 'et', '\u2581', '\u8c03', '\u67e5', '\u4e86', '\u6b64', '\u6b21', '\u516c', '\u5e03', '\u7684', '\u2581', '5', '8', '1', '\u2581', '\u4f8b', '\u6d4b', '\u8bd5', '\u7ed3', '\u679c', '\uff0c', '\u53d1', '\u73b0', '\u5176', '\u4e2d', '\u2581', '2', '4', '3', '\u2581', '\u4f8b', '\u53d7', '\u8bd5', '\u8005', '\u5df2', '\u516c', '\u5f00', '\u8868', '\u793a', '\u4ed6', '\u4eec', '\u4eca', '\u5e74', '\u4f1a', '\u5e74', '\u6ee1', '\u2581', '1', '8', '\u2581', '<0xE5>', '<0xB2>', '<0x81>', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u65b0\", \"News\"],\n  [\"\u95fb\", null],\n  [\"\u62a5\", null],\n  [\"<0xE7>\", null],\n  [\"<0xBA>\", null],\n  [\"<0xB8>\", null],\n  [\"\u2581S\", \"Svenska\"],\n  [\"vens\", null],\n  [\"ka\", null],\n  [\"\u2581D\", \"Dagbladet\"],\n  [\"ag\", null],\n  [\"bl\", null],\n  [\"ad\", null],\n  [\"et\", null],\n  [\"\u2581\", null],\n  [\"\u8c03\", \"investigated\"],\n  [\"\u67e5\", null],\n  [\"\u4e86\", null],\n  [\"\u6b64\", \"this\"],\n  [\"\u6b21\", null],\n  [\"\u516c\", \"published\"],\n  [\"\u5e03\", null],\n  [\"\u7684\", null],\n  [\"\u2581\", null],\n  [\"5\", \"581\"],\n  [\"8\", null],\n  [\"1\", null],\n  [\"\u2581\", null],\n  [\"\u4f8b\", \"cases\"],\n  [\"\u6d4b\", \"test\"],\n  [\"\u8bd5\", null],\n  [\"\u7ed3\", \"results\"],\n  [\"\u679c\", null],\n  [\"\uff0c\", null],\n  [\"\u53d1\", \"found\"],\n  [\"\u73b0\", null],\n  [\"\u5176\", \"among\"],\n  [\"\u4e2d\", null],\n  [\"\u2581\", null],\n  [\"2\", \"243\"],\n  [\"4\", null],\n  [\"3\", null],\n  [\"\u2581\", null],\n  [\"\u4f8b\", \"cases\"],\n  [\"\u53d7\", \"subjects\"],\n  [\"\u8bd5\", null],\n  [\"\u8005\", null],\n  [\"\u5df2\", \"have\"],\n  [\"\u516c\", \"publicly\"],\n  [\"\u5f00\", null],\n  [\"\u8868\", \"stated\"],\n  [\"\u793a\", null],\n  [\"\u4ed6\", \"they\"],\n  [\"\u4eec\", null],\n  [\"\u4eca\", \"this\"],\n  [\"\u5e74\", \"year\"],\n  [\"\u4f1a\", \"will\"],\n  [\"\u5e74\", \"turn\"],\n  [\"\u6ee1\", null],\n  [\"\u2581\", null],\n  [\"1\", \"18\"],\n  [\"8\", null],\n  [\"\u2581\", null],\n  [\"<0xE5>\", null],\n  [\"<0xB2>\", null],\n  [\"<0x81>\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u65b0\u95fb\u62a5\u7eb8 Svenska Dagbladet \u8c03\u67e5\u4e86\u6b64\u6b21\u516c\u5e03\u7684 581 \u4f8b\u6d4b\u8bd5\u7ed3\u679c\uff0c\u53d1\u73b0\u5176\u4e2d 243 \u4f8b\u53d7\u8bd5\u8005\u5df2\u516c\u5f00\u8868\u793a\u4ed6\u4eec\u4eca\u5e74\u4f1a\u5e74\u6ee1 18 \u5c81\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u65b0', '\u95fb', '\u62a5', '<0xE7>', '<0xBA>', '<0xB8>', '\u2581S', 'vens', 'ka', '\u2581D', 'ag', 'bl', 'ad', 'et', '\u2581', '\u8c03', '\u67e5', '\u4e86', '\u6b64', '\u6b21', '\u516c', '\u5e03', '\u7684', '\u2581', '5', '8', '1', '\u2581', '\u4f8b', '\u6d4b', '\u8bd5', '\u7ed3', '\u679c', '\uff0c', '\u53d1', '\u73b0', '\u5176', '\u4e2d', '\u2581', '2', '4', '3', '\u2581', '\u4f8b', '\u53d7', '\u8bd5', '\u8005', '\u5df2', '\u516c', '\u5f00', '\u8868', '\u793a', '\u4ed6', '\u4eec', '\u4eca', '\u5e74', '\u4f1a', '\u5e74', '\u6ee1', '\u2581', '1', '8', '\u2581', '<0xE5>', '<0xB2>', '<0x81>', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"\u65b0\", \"\u0436\u0430\u04a3\u0430\"],\n[\"\u95fb\", null],\n[\"\u62a5\", \"\u0433\u0430\u0437\u0435\u0442\"],\n[\"<0xE7>\", null],\n[\"<0xBA>\", null],\n[\"<0xB8>\", null],\n[\"\u2581S\", \"Svenska\"],\n[\"vens\", null],\n[\"ka\", null],\n[\"\u2581D\", \"Dagbladet\"],\n[\"ag\", null],\n[\"bl\", null],\n[\"ad\", null],\n[\"et\", null],\n[\"\u2581\", null],\n[\"\u8c03\", \"\u0437\u0435\u0440\u0442\u0442\u0435\u0443\"],\n[\"\u67e5\", null],\n[\"\u4e86\", null],\n[\"\u6b64\", \"\u0431\u04b1\u043b\"],\n[\"\u6b21\", null],\n[\"\u516c\", \"\u0436\u0430\u0440\u0438\u044f\u043b\u0430\u043d\u0493\u0430\u043d\"],\n[\"\u5e03\", null],\n[\"\u7684\", null],\n[\"\u2581\", null],\n[\"5\", null],\n[\"8\", null],\n[\"1\", null],\n[\"\u2581\", null],\n[\"\u4f8b\", \"\u043d\u04d9\u0442\u0438\u0436\u0435\"],\n[\"\u6d4b\", \"\u0442\u0435\u0441\u0442\"],\n[\"\u8bd5\", null],\n[\"\u7ed3\", \"\u043d\u04d9\u0442\u0438\u0436\u0435\"],\n[\"\u679c\", null],\n[\"\uff0c\", null],\n[\"\u53d1\", \"\u0430\u043d\u044b\u049b\u0442\u0430\u0434\u044b\"],\n[\"\u73b0\", null],\n[\"\u5176\", \"\u043e\u043d\u044b\u04a3\"],\n[\"\u4e2d\", null],\n[\"\u2581\", null],\n[\"2\", null],\n[\"4\", null],\n[\"3\", null],\n[\"\u2581\", null],\n[\"\u4f8b\", \"\u0430\u0434\u0430\u043c\"],\n[\"\u53d7\", \"\u0441\u044b\u043d\u0430\u049b\"],\n[\"\u8bd5\", null],\n[\"\u8005\", null],\n[\"\u5df2\", null],\n[\"\u516c\", \"\u0436\u0430\u0440\u0438\u044f\"],\n[\"\u5f00\", null],\n[\"\u8868\", \"\u043c\u04d9\u043b\u0456\u043c\u0434\u0435\u0434\u0456\"],\n[\"\u793a\", null],\n[\"\u4ed6\", \"\u043e\u043b\u0430\u0440\"],\n[\"\u4eec\", null],\n[\"\u4eca\", \"\u0431\u0438\u044b\u043b\"],\n[\"\u5e74\", null],\n[\"\u4f1a\", null],\n[\"\u5e74\", \"\u0436\u0430\u0441\"],\n[\"\u6ee1\", null],\n[\"\u2581\", null],\n[\"1\", null],\n[\"8\", null],\n[\"\u2581\", null],\n[\"<0xE5>\", null],\n[\"<0xB2>\", null],\n[\"<0x81>\", null],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u65b0\u95fb\u62a5\u7eb8 Svenska Dagbladet \u8c03\u67e5\u4e86\u6b64\u6b21\u516c\u5e03\u7684 581 \u4f8b\u6d4b\u8bd5\u7ed3\u679c\uff0c\u53d1\u73b0\u5176\u4e2d 243 \u4f8b\u53d7\u8bd5\u8005\u5df2\u516c\u5f00\u8868\u793a\u4ed6\u4eec\u4eca\u5e74\u4f1a\u5e74\u6ee1 18 \u5c81\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u65b0', '\u95fb', '\u62a5', '<0xE7>', '<0xBA>', '<0xB8>', '\u2581S', 'vens', 'ka', '\u2581D', 'ag', 'bl', 'ad', 'et', '\u2581', '\u8c03', '\u67e5', '\u4e86', '\u6b64', '\u6b21', '\u516c', '\u5e03', '\u7684', '\u2581', '5', '8', '1', '\u2581', '\u4f8b', '\u6d4b', '\u8bd5', '\u7ed3', '\u679c', '\uff0c', '\u53d1', '\u73b0', '\u5176', '\u4e2d', '\u2581', '2', '4', '3', '\u2581', '\u4f8b', '\u53d7', '\u8bd5', '\u8005', '\u5df2', '\u516c', '\u5f00', '\u8868', '\u793a', '\u4ed6', '\u4eec', '\u4eca', '\u5e74', '\u4f1a', '\u5e74', '\u6ee1', '\u2581', '1', '8', '\u2581', '<0xE5>', '<0xB2>', '<0x81>', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u65b0\", \"\u0413\u0430\u0437\u0435\u0442\u0430\"],\n  [\"\u95fb\", null],\n  [\"\u62a5\", null],\n  [\"<0xE7>\", null],\n  [\"<0xBA>\", null],\n  [\"<0xB8>\", null],\n  [\"\u2581S\", \"Svenska\"],\n  [\"vens\", null],\n  [\"ka\", null],\n  [\"\u2581D\", \"Dagbladet\"],\n  [\"ag\", null],\n  [\"bl\", null],\n  [\"ad\", null],\n  [\"et\", null],\n  [\"\u2581\", null],\n  [\"\u8c03\", \"\u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043b\u0430\"],\n  [\"\u67e5\", null],\n  [\"\u4e86\", null],\n  [\"\u6b64\", \"\u044d\u0442\u0438\"],\n  [\"\u6b21\", null],\n  [\"\u516c\", \"\u043e\u043f\u0443\u0431\u043b\u0438\u043a\u043e\u0432\u0430\u043d\u043d\u044b\u0435\"],\n  [\"\u5e03\", null],\n  [\"\u7684\", null],\n  [\"\u2581\", null],\n  [\"5\", \"581\"],\n  [\"8\", null],\n  [\"1\", null],\n  [\"\u2581\", null],\n  [\"\u4f8b\", \"\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\"],\n  [\"\u6d4b\", \"\u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f\"],\n  [\"\u8bd5\", null],\n  [\"\u7ed3\", null],\n  [\"\u679c\", null],\n  [\"\uff0c\", null],\n  [\"\u53d1\", \"\u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0438\u043b\u0430\"],\n  [\"\u73b0\", null],\n  [\"\u5176\", \"\u0441\u0440\u0435\u0434\u0438\"],\n  [\"\u4e2d\", \"\u043d\u0438\u0445\"],\n  [\"\u2581\", null],\n  [\"2\", \"243\"],\n  [\"4\", null],\n  [\"3\", null],\n  [\"\u2581\", null],\n  [\"\u4f8b\", \"\u0443\u0447\u0430\u0441\u0442\u043d\u0438\u043a\u0430\"],\n  [\"\u53d7\", null],\n  [\"\u8bd5\", null],\n  [\"\u8005\", null],\n  [\"\u5df2\", \"\u0443\u0436\u0435\"],\n  [\"\u516c\", \"\u043f\u0443\u0431\u043b\u0438\u0447\u043d\u043e\"],\n  [\"\u5f00\", null],\n  [\"\u8868\", \"\u0437\u0430\u044f\u0432\u0438\u043b\u0438\"],\n  [\"\u793a\", null],\n  [\"\u4ed6\", \"\u0447\u0442\u043e\"],\n  [\"\u4eec\", null],\n  [\"\u4eca\", \"\u0432\"],\n  [\"\u5e74\", \"\u044d\u0442\u043e\u043c\"],\n  [\"\u4f1a\", \"\u0433\u043e\u0434\u0443\"],\n  [\"\u5e74\", \"\u0438\u0441\u043f\u043e\u043b\u043d\u0438\u0442\u0441\u044f\"],\n  [\"\u6ee1\", null],\n  [\"\u2581\", null],\n  [\"1\", \"18\"],\n  [\"8\", null],\n  [\"\u2581\", null],\n  [\"<0xE5>\", null],\n  [\"<0xB2>\", null],\n  [\"<0x81>\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u66ff\u5361\u7279\u8fa9\u62a4\u7684\u4eba\u4e3a\u6570\u4e0d\u591a\uff0c\u963f\u66fc\u8fbe?\u8bfa\u514b\u65af (Amanda Knox) \u5c31\u662f\u5176\u4e2d\u4e4b\u4e00\uff0c\u5979\u4e8e 2007 \u5e74\u88ab\u6307\u63a7\u5728\u610f\u5927\u5229\u8c0b\u6740\u5979\u7684\u82f1\u56fd\u5ba4\u53cb\uff0c\u4e24\u5e74\u540e\u88ab\u5ba3\u5e03\u65e0\u7f6a\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u66ff', '\u5361', '\u7279', '<0xE8>', '<0xBE>', '<0xA9>', '\u62a4', '\u7684', '\u4eba', '\u4e3a', '\u6570', '\u4e0d', '\u591a', '\uff0c', '\u963f', '<0xE6>', '<0x9B>', '<0xBC>', '\u8fbe', '?', '<0xE8>', '<0xAF>', '<0xBA>', '\u514b', '\u65af', '\u2581(', 'Am', 'anda', '\u2581Kn', 'ox', ')', '\u2581', '\u5c31', '\u662f', '\u5176', '\u4e2d', '\u4e4b', '\u4e00', '\uff0c', '\u5979', '\u4e8e', '\u2581', '2', '0', '0', '7', '\u2581', '\u5e74', '\u88ab', '\u6307', '\u63a7', '\u5728', '\u610f', '\u5927', '\u5229', '<0xE8>', '<0xB0>', '<0x8B>', '\u6740', '\u5979', '\u7684', '\u82f1', '\u56fd', '\u5ba4', '\u53cb', '\uff0c', '\u4e24', '\u5e74', '\u540e', '\u88ab', '\u5ba3', '\u5e03', '\u65e0', '<0xE7>', '<0xBD>', '<0xAA>', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u66ff\", \"Defending\"],\n  [\"\u5361\", null],\n  [\"\u7279\", null],\n  [\"<0xE8>\", null],\n  [\"<0xBE>\", null],\n  [\"<0xA9>\", null],\n  [\"\u62a4\", null],\n  [\"\u7684\", null],\n  [\"\u4eba\", \"people\"],\n  [\"\u4e3a\", null],\n  [\"\u6570\", null],\n  [\"\u4e0d\", null],\n  [\"\u591a\", \"few\"],\n  [\"\uff0c\", null],\n  [\"\u963f\", null],\n  [\"<0xE6>\", null],\n  [\"<0x9B>\", null],\n  [\"<0xBC>\", null],\n  [\"\u8fbe\", null],\n  [\"?\", null],\n  [\"<0xE8>\", null],\n  [\"<0xAF>\", null],\n  [\"<0xBA>\", null],\n  [\"\u514b\", null],\n  [\"\u65af\", null],\n  [\"\u2581(\", null],\n  [\"Am\", null],\n  [\"anda\", null],\n  [\"\u2581Kn\", null],\n  [\"ox\", null],\n  [\")\", null],\n  [\"\u2581\", null],\n  [\"\u5c31\", null],\n  [\"\u662f\", \"is\"],\n  [\"\u5176\", null],\n  [\"\u4e2d\", null],\n  [\"\u4e4b\", null],\n  [\"\u4e00\", \"one\"],\n  [\"\uff0c\", null],\n  [\"\u5979\", \"She\"],\n  [\"\u4e8e\", null],\n  [\"\u2581\", null],\n  [\"2\", null],\n  [\"0\", null],\n  [\"0\", null],\n  [\"7\", null],\n  [\"\u2581\", null],\n  [\"\u5e74\", \"year\"],\n  [\"\u88ab\", null],\n  [\"\u6307\", null],\n  [\"\u63a7\", \"accused\"],\n  [\"\u5728\", \"in\"],\n  [\"\u610f\", null],\n  [\"\u5927\", null],\n  [\"\u5229\", \"Italy\"],\n  [\"<0xE8>\", null],\n  [\"<0xB0>\", null],\n  [\"<0x8B>\", null],\n  [\"\u6740\", \"murdering\"],\n  [\"\u5979\", \"her\"],\n  [\"\u7684\", null],\n  [\"\u82f1\", null],\n  [\"\u56fd\", \"British\"],\n  [\"\u5ba4\", null],\n  [\"\u53cb\", \"roommate\"],\n  [\"\uff0c\", null],\n  [\"\u4e24\", null],\n  [\"\u5e74\", \"years\"],\n  [\"\u540e\", \"later\"],\n  [\"\u88ab\", null],\n  [\"\u5ba3\", null],\n  [\"\u5e03\", \"declared\"],\n  [\"\u65e0\", null],\n  [\"<0xE7>\", null],\n  [\"<0xBD>\", null],\n  [\"<0xAA>\", \"innocent\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u66ff\u5361\u7279\u8fa9\u62a4\u7684\u4eba\u4e3a\u6570\u4e0d\u591a\uff0c\u963f\u66fc\u8fbe?\u8bfa\u514b\u65af (Amanda Knox) \u5c31\u662f\u5176\u4e2d\u4e4b\u4e00\uff0c\u5979\u4e8e 2007 \u5e74\u88ab\u6307\u63a7\u5728\u610f\u5927\u5229\u8c0b\u6740\u5979\u7684\u82f1\u56fd\u5ba4\u53cb\uff0c\u4e24\u5e74\u540e\u88ab\u5ba3\u5e03\u65e0\u7f6a\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u66ff', '\u5361', '\u7279', '<0xE8>', '<0xBE>', '<0xA9>', '\u62a4', '\u7684', '\u4eba', '\u4e3a', '\u6570', '\u4e0d', '\u591a', '\uff0c', '\u963f', '<0xE6>', '<0x9B>', '<0xBC>', '\u8fbe', '?', '<0xE8>', '<0xAF>', '<0xBA>', '\u514b', '\u65af', '\u2581(', 'Am', 'anda', '\u2581Kn', 'ox', ')', '\u2581', '\u5c31', '\u662f', '\u5176', '\u4e2d', '\u4e4b', '\u4e00', '\uff0c', '\u5979', '\u4e8e', '\u2581', '2', '0', '0', '7', '\u2581', '\u5e74', '\u88ab', '\u6307', '\u63a7', '\u5728', '\u610f', '\u5927', '\u5229', '<0xE8>', '<0xB0>', '<0x8B>', '\u6740', '\u5979', '\u7684', '\u82f1', '\u56fd', '\u5ba4', '\u53cb', '\uff0c', '\u4e24', '\u5e74', '\u540e', '\u88ab', '\u5ba3', '\u5e03', '\u65e0', '<0xE7>', '<0xBD>', '<0xAA>', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u66ff\", \"\u0422\u0438\u043a\u0430\u0442\u0442\u044b\"],\n  [\"\u5361\", null],\n  [\"\u7279\", null],\n  [\"<0xE8>\", null],\n  [\"<0xBE>\", null],\n  [\"<0xA9>\", null],\n  [\"\u62a4\", \"\u049b\u043e\u0440\u0493\u0430\u0439\u0442\u044b\u043d\"],\n  [\"\u7684\", null],\n  [\"\u4eba\", \"\u0430\u0434\u0430\u043c\u0434\u0430\u0440\"],\n  [\"\u4e3a\", null],\n  [\"\u6570\", \"\u0441\u0430\u043d\u044b\"],\n  [\"\u4e0d\", null],\n  [\"\u591a\", \"\u0430\u0437\"],\n  [\"\uff0c\", null],\n  [\"\u963f\", \"\u0410\u043c\u0430\u043d\u0434\u0430\"],\n  [\"<0xE6>\", null],\n  [\"<0x9B>\", null],\n  [\"<0xBC>\", null],\n  [\"\u8fbe\", null],\n  [\"?\", null],\n  [\"<0xE8>\", null],\n  [\"<0xAF>\", null],\n  [\"<0xBA>\", null],\n  [\"\u514b\", \"\u041d\u043e\u043a\u0441\"],\n  [\"\u65af\", null],\n  [\"\u2581(\", null],\n  [\"Am\", null],\n  [\"anda\", null],\n  [\"\u2581Kn\", null],\n  [\"ox\", null],\n  [\")\", null],\n  [\"\u2581\", null],\n  [\"\u5c31\", null],\n  [\"\u662f\", null],\n  [\"\u5176\", \"\u0441\u043e\u043b\u0430\u0440\u0434\u044b\u04a3\"],\n  [\"\u4e2d\", null],\n  [\"\u4e4b\", null],\n  [\"\u4e00\", \"\u0431\u0456\u0440\u0456\"],\n  [\"\uff0c\", null],\n  [\"\u5979\", \"\u043e\u043b\"],\n  [\"\u4e8e\", null],\n  [\"\u2581\", null],\n  [\"2\", null],\n  [\"0\", null],\n  [\"0\", null],\n  [\"7\", null],\n  [\"\u2581\", null],\n  [\"\u5e74\", \"\u0436\u044b\u043b\u044b\"],\n  [\"\u88ab\", null],\n  [\"\u6307\", \"\u0430\u0439\u044b\u043f\u0442\u0430\u043b\u0493\u0430\u043d\"],\n  [\"\u63a7\", null],\n  [\"\u5728\", null],\n  [\"\u610f\", \"\u0418\u0442\u0430\u043b\u0438\u044f\u0434\u0430\"],\n  [\"\u5927\", null],\n  [\"\u5229\", null],\n  [\"<0xE8>\", null],\n  [\"<0xB0>\", null],\n  [\"<0x8B>\", null],\n  [\"\u6740\", \"\u04e9\u043b\u0442\u0456\u0440\u0443\"],\n  [\"\u5979\", \"\u043e\u043d\u044b\u04a3\"],\n  [\"\u7684\", null],\n  [\"\u82f1\", \"\u0430\u0493\u044b\u043b\u0448\u044b\u043d\"],\n  [\"\u56fd\", null],\n  [\"\u5ba4\", \"\u0431\u04e9\u043b\u043c\u0435\u043b\u0435\u0441\u0456\u043d\"],\n  [\"\u53cb\", null],\n  [\"\uff0c\", null],\n  [\"\u4e24\", \"\u0435\u043a\u0456\"],\n  [\"\u5e74\", \"\u0436\u044b\u043b\u0434\u0430\u043d\"],\n  [\"\u540e\", \"\u043a\u0435\u0439\u0456\u043d\"],\n  [\"\u88ab\", null],\n  [\"\u5ba3\", \"\u0430\u049b\u0442\u0430\u043b\u0493\u0430\u043d\"],\n  [\"\u5e03\", null],\n  [\"\u65e0\", null],\n  [\"<0xE7>\", null],\n  [\"<0xBD>\", null],\n  [\"<0xAA>\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u66ff\u5361\u7279\u8fa9\u62a4\u7684\u4eba\u4e3a\u6570\u4e0d\u591a\uff0c\u963f\u66fc\u8fbe?\u8bfa\u514b\u65af (Amanda Knox) \u5c31\u662f\u5176\u4e2d\u4e4b\u4e00\uff0c\u5979\u4e8e 2007 \u5e74\u88ab\u6307\u63a7\u5728\u610f\u5927\u5229\u8c0b\u6740\u5979\u7684\u82f1\u56fd\u5ba4\u53cb\uff0c\u4e24\u5e74\u540e\u88ab\u5ba3\u5e03\u65e0\u7f6a\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u66ff', '\u5361', '\u7279', '<0xE8>', '<0xBE>', '<0xA9>', '\u62a4', '\u7684', '\u4eba', '\u4e3a', '\u6570', '\u4e0d', '\u591a', '\uff0c', '\u963f', '<0xE6>', '<0x9B>', '<0xBC>', '\u8fbe', '?', '<0xE8>', '<0xAF>', '<0xBA>', '\u514b', '\u65af', '\u2581(', 'Am', 'anda', '\u2581Kn', 'ox', ')', '\u2581', '\u5c31', '\u662f', '\u5176', '\u4e2d', '\u4e4b', '\u4e00', '\uff0c', '\u5979', '\u4e8e', '\u2581', '2', '0', '0', '7', '\u2581', '\u5e74', '\u88ab', '\u6307', '\u63a7', '\u5728', '\u610f', '\u5927', '\u5229', '<0xE8>', '<0xB0>', '<0x8B>', '\u6740', '\u5979', '\u7684', '\u82f1', '\u56fd', '\u5ba4', '\u53cb', '\uff0c', '\u4e24', '\u5e74', '\u540e', '\u88ab', '\u5ba3', '\u5e03', '\u65e0', '<0xE7>', '<0xBD>', '<0xAA>', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u66ff\", \"\u0417\u0430\u0449\u0438\u0449\u0430\u0442\u044c\"],\n  [\"\u5361\", null],\n  [\"\u7279\", null],\n  [\"<0xE8>\", null],\n  [\"<0xBE>\", null],\n  [\"<0xA9>\", null],\n  [\"\u62a4\", null],\n  [\"\u7684\", null],\n  [\"\u4eba\", \"\u043b\u044e\u0434\u0438\"],\n  [\"\u4e3a\", null],\n  [\"\u6570\", \"\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e\"],\n  [\"\u4e0d\", null],\n  [\"\u591a\", null],\n  [\"\uff0c\", null],\n  [\"\u963f\", \"\u0410\u043c\u0430\u043d\u0434\u0430\"],\n  [\"<0xE6>\", null],\n  [\"<0x9B>\", null],\n  [\"<0xBC>\", null],\n  [\"\u8fbe\", null],\n  [\"?\", null],\n  [\"<0xE8>\", null],\n  [\"<0xAF>\", null],\n  [\"<0xBA>\", null],\n  [\"\u514b\", \"\u041d\u043e\u043a\u0441\"],\n  [\"\u65af\", null],\n  [\"\u2581(\", null],\n  [\"Am\", null],\n  [\"anda\", null],\n  [\"\u2581Kn\", null],\n  [\"ox\", null],\n  [\")\", null],\n  [\"\u2581\", null],\n  [\"\u5c31\", null],\n  [\"\u662f\", null],\n  [\"\u5176\", null],\n  [\"\u4e2d\", null],\n  [\"\u4e4b\", null],\n  [\"\u4e00\", \"\u043e\u0434\u043d\u0430\"],\n  [\"\uff0c\", null],\n  [\"\u5979\", \"\u041e\u043d\u0430\"],\n  [\"\u4e8e\", null],\n  [\"\u2581\", null],\n  [\"2\", null],\n  [\"0\", null],\n  [\"0\", null],\n  [\"7\", null],\n  [\"\u2581\", null],\n  [\"\u5e74\", \"\u0433\u043e\u0434\u0443\"],\n  [\"\u88ab\", null],\n  [\"\u6307\", \"\u043e\u0431\u0432\u0438\u043d\u0435\u043d\u0430\"],\n  [\"\u63a7\", null],\n  [\"\u5728\", \"\u0432\"],\n  [\"\u610f\", \"\u0418\u0442\u0430\u043b\u0438\u0438\"],\n  [\"\u5927\", null],\n  [\"\u5229\", null],\n  [\"<0xE8>\", null],\n  [\"<0xB0>\", null],\n  [\"<0x8B>\", null],\n  [\"\u6740\", \"\u0443\u0431\u0438\u0439\u0441\u0442\u0432\u0435\"],\n  [\"\u5979\", \"\u0435\u0451\"],\n  [\"\u7684\", null],\n  [\"\u82f1\", \"\u0431\u0440\u0438\u0442\u0430\u043d\u0441\u043a\u043e\u0439\"],\n  [\"\u56fd\", null],\n  [\"\u5ba4\", \"\u0441\u043e\u0441\u0435\u0434\u043a\u0438\"],\n  [\"\u53cb\", null],\n  [\"\uff0c\", null],\n  [\"\u4e24\", \"\u0434\u0432\u0430\"],\n  [\"\u5e74\", \"\u0433\u043e\u0434\u0430\"],\n  [\"\u540e\", \"\u0441\u043f\u0443\u0441\u0442\u044f\"],\n  [\"\u88ab\", null],\n  [\"\u5ba3\", \"\u043f\u0440\u0438\u0437\u043d\u0430\u043d\u0430\"],\n  [\"\u5e03\", null],\n  [\"\u65e0\", \"\u043d\u0435\u0432\u0438\u043d\u043e\u0432\u043d\u043e\u0439\"],\n  [\"<0xE7>\", null],\n  [\"<0xBD>\", null],\n  [\"<0xAA>\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u4eca\u5e74\u7684\u6f14\u51fa\u4e8e\u516d\u6708\u5f00\u59cb\uff0c\u7b2c\u4e00\u5929\u6709\u5c06\u8fd1 30 \u540d\u6f14\u5458 - \u5341\u51e0\u4f4d\u767d\u4eba\uff0c\u5176\u4f59\u4e3a\u9ed1\u4eba - \u6324\u5728\u4e00\u95f4\u5c4b\u5b50\u91cc\uff0c\u542c\u53d6\u683c\u6797\u53d1\u5e03\u6392\u7ec3\u597d\u7684\u6f14\u51fa\u6982\u89c8\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u4eca', '\u5e74', '\u7684', '\u6f14', '\u51fa', '\u4e8e', '\u516d', '\u6708', '\u5f00', '\u59cb', '\uff0c', '\u7b2c', '\u4e00', '\u5929', '\u6709', '\u5c06', '\u8fd1', '\u2581', '3', '0', '\u2581', '\u540d', '\u6f14', '\u5458', '\u2581-', '\u2581', '\u5341', '\u51e0', '\u4f4d', '\u767d', '\u4eba', '\uff0c', '\u5176', '\u4f59', '\u4e3a', '\u9ed1', '\u4eba', '\u2581-', '\u2581', '<0xE6>', '<0x8C>', '<0xA4>', '\u5728', '\u4e00', '\u95f4', '\u5c4b', '\u5b50', '\u91cc', '\uff0c', '\u542c', '\u53d6', '\u683c', '\u6797', '\u53d1', '\u5e03', '\u6392', '\u7ec3', '\u597d', '\u7684', '\u6f14', '\u51fa', '\u6982', '\u89c8', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"\u4eca\", \"This\"],\n[\"\u5e74\", null],\n[\"\u7684\", null],\n[\"\u6f14\", \"performance\"],\n[\"\u51fa\", null],\n[\"\u4e8e\", null],\n[\"\u516d\", \"June\"],\n[\"\u6708\", null],\n[\"\u5f00\", \"began\"],\n[\"\u59cb\", null],\n[\"\uff0c\", null],\n[\"\u7b2c\", \"the\"],\n[\"\u4e00\", null],\n[\"\u5929\", \"first day\"],\n[\"\u6709\", null],\n[\"\u5c06\", \"nearly\"],\n[\"\u8fd1\", null],\n[\"\u2581\", null],\n[\"3\", null],\n[\"0\", null],\n[\"\u2581\", null],\n[\"\u540d\", \"actors\"],\n[\"\u6f14\", null],\n[\"\u5458\", null],\n[\"\u2581-\", null],\n[\"\u2581\", null],\n[\"\u5341\", \"a dozen\"],\n[\"\u51e0\", null],\n[\"\u4f4d\", null],\n[\"\u767d\", \"white\"],\n[\"\u4eba\", null],\n[\"\uff0c\", null],\n[\"\u5176\", \"the\"],\n[\"\u4f59\", \"rest\"],\n[\"\u4e3a\", null],\n[\"\u9ed1\", \"black\"],\n[\"\u4eba\", null],\n[\"\u2581-\", null],\n[\"\u2581\", null],\n[\"<0xE6>\", \"crowded\"],\n[\"<0x8C>\", null],\n[\"<0xA4>\", null],\n[\"\u5728\", \"into\"],\n[\"\u4e00\", \"a\"],\n[\"\u95f4\", null],\n[\"\u5c4b\", \"room\"],\n[\"\u5b50\", null],\n[\"\u91cc\", null],\n[\"\uff0c\", null],\n[\"\u542c\", \"to listen to\"],\n[\"\u53d6\", null],\n[\"\u683c\", \"Green\"],\n[\"\u6797\", null],\n[\"\u53d1\", \"deliver\"],\n[\"\u5e03\", null],\n[\"\u6392\", \"rehearsed\"],\n[\"\u7ec3\", null],\n[\"\u597d\", null],\n[\"\u7684\", null],\n[\"\u6f14\", \"performance\"],\n[\"\u51fa\", null],\n[\"\u6982\", \"overview\"],\n[\"\u89c8\", null],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u4eca\u5e74\u7684\u6f14\u51fa\u4e8e\u516d\u6708\u5f00\u59cb\uff0c\u7b2c\u4e00\u5929\u6709\u5c06\u8fd1 30 \u540d\u6f14\u5458 - \u5341\u51e0\u4f4d\u767d\u4eba\uff0c\u5176\u4f59\u4e3a\u9ed1\u4eba - \u6324\u5728\u4e00\u95f4\u5c4b\u5b50\u91cc\uff0c\u542c\u53d6\u683c\u6797\u53d1\u5e03\u6392\u7ec3\u597d\u7684\u6f14\u51fa\u6982\u89c8\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u4eca', '\u5e74', '\u7684', '\u6f14', '\u51fa', '\u4e8e', '\u516d', '\u6708', '\u5f00', '\u59cb', '\uff0c', '\u7b2c', '\u4e00', '\u5929', '\u6709', '\u5c06', '\u8fd1', '\u2581', '3', '0', '\u2581', '\u540d', '\u6f14', '\u5458', '\u2581-', '\u2581', '\u5341', '\u51e0', '\u4f4d', '\u767d', '\u4eba', '\uff0c', '\u5176', '\u4f59', '\u4e3a', '\u9ed1', '\u4eba', '\u2581-', '\u2581', '<0xE6>', '<0x8C>', '<0xA4>', '\u5728', '\u4e00', '\u95f4', '\u5c4b', '\u5b50', '\u91cc', '\uff0c', '\u542c', '\u53d6', '\u683c', '\u6797', '\u53d1', '\u5e03', '\u6392', '\u7ec3', '\u597d', '\u7684', '\u6f14', '\u51fa', '\u6982', '\u89c8', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"\u4eca\", \"\u0431\u0438\u044b\u043b\u0493\u044b\"],\n[\"\u5e74\", null],\n[\"\u7684\", null],\n[\"\u6f14\", \"\u049b\u043e\u0439\u044b\u043b\u044b\u043c\"],\n[\"\u51fa\", null],\n[\"\u4e8e\", null],\n[\"\u516d\", \"\u043c\u0430\u0443\u0441\u044b\u043c\"],\n[\"\u6708\", null],\n[\"\u5f00\", \"\u0431\u0430\u0441\u0442\u0430\u043b\u0434\u044b\"],\n[\"\u59cb\", null],\n[\"\uff0c\", null],\n[\"\u7b2c\", \"\u0431\u0456\u0440\u0456\u043d\u0448\u0456\"],\n[\"\u4e00\", null],\n[\"\u5929\", \"\u043a\u04af\u043d\u0456\"],\n[\"\u6709\", null],\n[\"\u5c06\", \"\u0448\u0430\u043c\u0430\u043c\u0435\u043d\"],\n[\"\u8fd1\", null],\n[\"\u2581\", null],\n[\"3\", null],\n[\"0\", null],\n[\"\u2581\", null],\n[\"\u540d\", \"\u0430\u043a\u0442\u0435\u0440\"],\n[\"\u6f14\", null],\n[\"\u5458\", null],\n[\"\u2581-\", null],\n[\"\u2581\", null],\n[\"\u5341\", \"\u043e\u043d\"],\n[\"\u51e0\", \"\u0448\u0430\u049b\u0442\u044b\"],\n[\"\u4f4d\", null],\n[\"\u767d\", \"\u0430\u049b\"],\n[\"\u4eba\", \"\u0430\u0434\u0430\u043c\"],\n[\"\uff0c\", null],\n[\"\u5176\", \"\u049b\u0430\u043b\u0493\u0430\u043d\u0434\u0430\u0440\u044b\"],\n[\"\u4f59\", null],\n[\"\u4e3a\", null],\n[\"\u9ed1\", \"\u049b\u0430\u0440\u0430\"],\n[\"\u4eba\", \"\u0430\u0434\u0430\u043c\"],\n[\"\u2581-\", null],\n[\"\u2581\", null],\n[\"<0xE6>\", null],\n[\"<0x8C>\", null],\n[\"<0xA4>\", null],\n[\"\u5728\", null],\n[\"\u4e00\", \"\u0431\u0456\u0440\"],\n[\"\u95f4\", null],\n[\"\u5c4b\", \"\u0431\u04e9\u043b\u043c\u0435\u0434\u0435\"],\n[\"\u5b50\", null],\n[\"\u91cc\", null],\n[\"\uff0c\", null],\n[\"\u542c\", \"\u0442\u044b\u04a3\u0434\u0430\u0434\u044b\"],\n[\"\u53d6\", null],\n[\"\u683c\", \"\u0413\u0440\u0438\u043d\"],\n[\"\u6797\", null],\n[\"\u53d1\", \"\u0436\u0430\u0440\u0438\u044f\u043b\u0430\u0493\u0430\u043d\"],\n[\"\u5e03\", null],\n[\"\u6392\", \"\u0434\u0430\u0439\u044b\u043d\u0434\u0430\u043b\u0493\u0430\u043d\"],\n[\"\u7ec3\", null],\n[\"\u597d\", null],\n[\"\u7684\", null],\n[\"\u6f14\", \"\u049b\u043e\u0439\u044b\u043b\u044b\u043c\u043d\u044b\u04a3\"],\n[\"\u51fa\", null],\n[\"\u6982\", \"\u0448\u043e\u043b\u0443\u044b\u043d\"],\n[\"\u89c8\", null],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u4eca\u5e74\u7684\u6f14\u51fa\u4e8e\u516d\u6708\u5f00\u59cb\uff0c\u7b2c\u4e00\u5929\u6709\u5c06\u8fd1 30 \u540d\u6f14\u5458 - \u5341\u51e0\u4f4d\u767d\u4eba\uff0c\u5176\u4f59\u4e3a\u9ed1\u4eba - \u6324\u5728\u4e00\u95f4\u5c4b\u5b50\u91cc\uff0c\u542c\u53d6\u683c\u6797\u53d1\u5e03\u6392\u7ec3\u597d\u7684\u6f14\u51fa\u6982\u89c8\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u4eca', '\u5e74', '\u7684', '\u6f14', '\u51fa', '\u4e8e', '\u516d', '\u6708', '\u5f00', '\u59cb', '\uff0c', '\u7b2c', '\u4e00', '\u5929', '\u6709', '\u5c06', '\u8fd1', '\u2581', '3', '0', '\u2581', '\u540d', '\u6f14', '\u5458', '\u2581-', '\u2581', '\u5341', '\u51e0', '\u4f4d', '\u767d', '\u4eba', '\uff0c', '\u5176', '\u4f59', '\u4e3a', '\u9ed1', '\u4eba', '\u2581-', '\u2581', '<0xE6>', '<0x8C>', '<0xA4>', '\u5728', '\u4e00', '\u95f4', '\u5c4b', '\u5b50', '\u91cc', '\uff0c', '\u542c', '\u53d6', '\u683c', '\u6797', '\u53d1', '\u5e03', '\u6392', '\u7ec3', '\u597d', '\u7684', '\u6f14', '\u51fa', '\u6982', '\u89c8', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u4eca\", \"\u0412\"],\n  [\"\u5e74\", \"\u044d\u0442\u043e\u043c\"],\n  [\"\u7684\", null],\n  [\"\u6f14\", \"\u0432\u044b\u0441\u0442\u0443\u043f\u043b\u0435\u043d\u0438\u044f\"],\n  [\"\u51fa\", null],\n  [\"\u4e8e\", null],\n  [\"\u516d\", \"\u0438\u044e\u043d\u0435\"],\n  [\"\u6708\", null],\n  [\"\u5f00\", \"\u043d\u0430\u0447\u0430\u043b\u0438\u0441\u044c\"],\n  [\"\u59cb\", null],\n  [\"\uff0c\", null],\n  [\"\u7b2c\", \"\u0412\"],\n  [\"\u4e00\", \"\u043f\u0435\u0440\u0432\u044b\u0439\"],\n  [\"\u5929\", \"\u0434\u0435\u043d\u044c\"],\n  [\"\u6709\", null],\n  [\"\u5c06\", \"\u043e\u043a\u043e\u043b\u043e\"],\n  [\"\u8fd1\", null],\n  [\"\u2581\", null],\n  [\"3\", null],\n  [\"0\", null],\n  [\"\u2581\", null],\n  [\"\u540d\", null],\n  [\"\u6f14\", \"\u0430\u043a\u0442\u0435\u0440\u043e\u0432\"],\n  [\"\u5458\", null],\n  [\"\u2581-\", null],\n  [\"\u2581\", null],\n  [\"\u5341\", \"\u0434\u0435\u0441\u044f\u0442\u043e\u043a\"],\n  [\"\u51e0\", null],\n  [\"\u4f4d\", null],\n  [\"\u767d\", \"\u0431\u0435\u043b\u044b\u0445\"],\n  [\"\u4eba\", null],\n  [\"\uff0c\", null],\n  [\"\u5176\", \"\u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0435\"],\n  [\"\u4f59\", null],\n  [\"\u4e3a\", null],\n  [\"\u9ed1\", \"\u0447\u0435\u0440\u043d\u043e\u043a\u043e\u0436\u0438\u0435\"],\n  [\"\u4eba\", null],\n  [\"\u2581-\", null],\n  [\"\u2581\", null],\n  [\"<0xE6>\", null],\n  [\"<0x8C>\", null],\n  [\"<0xA4>\", null],\n  [\"\u5728\", null],\n  [\"\u4e00\", \"\u043e\u0434\u043d\u043e\u0439\"],\n  [\"\u95f4\", null],\n  [\"\u5c4b\", \"\u043a\u043e\u043c\u043d\u0430\u0442\u0435\"],\n  [\"\u5b50\", null],\n  [\"\u91cc\", null],\n  [\"\uff0c\", null],\n  [\"\u542c\", \"\u0441\u043b\u0443\u0448\u0430\u043b\u0438\"],\n  [\"\u53d6\", null],\n  [\"\u683c\", \"\u0413\u0440\u0438\u043d\u0430\"],\n  [\"\u6797\", null],\n  [\"\u53d1\", \"\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0432\u0448\u0435\u0433\u043e\"],\n  [\"\u5e03\", null],\n  [\"\u6392\", \"\u043e\u0442\u0440\u0435\u043f\u0435\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439\"],\n  [\"\u7ec3\", null],\n  [\"\u597d\", null],\n  [\"\u7684\", null],\n  [\"\u6f14\", \"\u043e\u0431\u0437\u043e\u0440\"],\n  [\"\u51fa\", null],\n  [\"\u6982\", \"\u0432\u044b\u0441\u0442\u0443\u043f\u043b\u0435\u043d\u0438\u044f\"],\n  [\"\u89c8\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u6bd4\u5982\uff0c\u4e9a\u9a6c\u900a\u5728\u7f8e\u56fd\u5728\u7ebf\u96f6\u552e\u5e02\u573a\u7684\u4efd\u989d\u5df2\u8fbe\uff13\uff14\uff05\uff0c\uff12\uff10\uff12\uff10\u5e74\u66f4\u6709\u671b\u5347\u81f3\uff15\uff10\uff05\uff0c\u4f46\u5728\u4e2d\u56fd\uff0c\u4e9a\u9a6c\u900a\u7684\u5e02\u573a\u4efd\u989d\u8fd8\u4e0d\u5230\uff11\uff05\uff0c\u8fdc\u8fdc\u843d\u540e\u4e8e\u963f\u91cc\u5df4\u5df4\u548c\u4eac\u4e1c\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u6bd4', '\u5982', '\uff0c', '\u4e9a', '\u9a6c', '<0xE9>', '<0x80>', '<0x8A>', '\u5728', '\u7f8e', '\u56fd', '\u5728', '\u7ebf', '\u96f6', '\u552e', '\u5e02', '\u573a', '\u7684', '\u4efd', '\u989d', '\u5df2', '\u8fbe', '<0xEF>', '<0xBC>', '<0x93>', '<0xEF>', '<0xBC>', '<0x94>', '<0xEF>', '<0xBC>', '<0x85>', '\uff0c', '<0xEF>', '<0xBC>', '<0x92>', '<0xEF>', '<0xBC>', '<0x90>', '<0xEF>', '<0xBC>', '<0x92>', '<0xEF>', '<0xBC>', '<0x90>', '\u5e74', '\u66f4', '\u6709', '\u671b', '\u5347', '\u81f3', '<0xEF>', '<0xBC>', '<0x95>', '<0xEF>', '<0xBC>', '<0x90>', '<0xEF>', '<0xBC>', '<0x85>', '\uff0c', '\u4f46', '\u5728', '\u4e2d', '\u56fd', '\uff0c', '\u4e9a', '\u9a6c', '<0xE9>', '<0x80>', '<0x8A>', '\u7684', '\u5e02', '\u573a', '\u4efd', '\u989d', '\u8fd8', '\u4e0d', '\u5230', '<0xEF>', '<0xBC>', '<0x91>', '<0xEF>', '<0xBC>', '<0x85>', '\uff0c', '\u8fdc', '\u8fdc', '\u843d', '\u540e', '\u4e8e', '\u963f', '\u91cc', '\u5df4', '\u5df4', '\u548c', '\u4eac', '\u4e1c', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u6bd4\", \"For\"],\n  [\"\u5982\", null],\n  [\"\uff0c\", null],\n  [\"\u4e9a\", \"Amazon\"],\n  [\"\u9a6c\", null],\n  [\"<0xE9>\", null],\n  [\"<0x80>\", null],\n  [\"<0x8A>\", null],\n  [\"\u5728\", null],\n  [\"\u7f8e\", \"United\"],\n  [\"\u56fd\", null],\n  [\"\u5728\", null],\n  [\"\u7ebf\", \"online\"],\n  [\"\u96f6\", \"retail\"],\n  [\"\u552e\", null],\n  [\"\u5e02\", \"market\"],\n  [\"\u573a\", null],\n  [\"\u7684\", null],\n  [\"\u4efd\", \"share\"],\n  [\"\u989d\", null],\n  [\"\u5df2\", null],\n  [\"\u8fbe\", \"reached\"],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x93>\", null],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x94>\", null],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x85>\", null],\n  [\"\uff0c\", null],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x92>\", null],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x90>\", null],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x92>\", null],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x90>\", null],\n  [\"\u5e74\", \"year\"],\n  [\"\u66f4\", null],\n  [\"\u6709\", null],\n  [\"\u671b\", \"expected\"],\n  [\"\u5347\", \"rise\"],\n  [\"\u81f3\", null],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x95>\", null],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x90>\", null],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x85>\", null],\n  [\"\uff0c\", null],\n  [\"\u4f46\", \"However\"],\n  [\"\u5728\", \"in\"],\n  [\"\u4e2d\", \"China\"],\n  [\"\u56fd\", null],\n  [\"\uff0c\", null],\n  [\"\u4e9a\", \"Amazon's\"],\n  [\"\u9a6c\", null],\n  [\"<0xE9>\", null],\n  [\"<0x80>\", null],\n  [\"<0x8A>\", null],\n  [\"\u7684\", null],\n  [\"\u5e02\", \"market\"],\n  [\"\u573a\", null],\n  [\"\u4efd\", \"share\"],\n  [\"\u989d\", null],\n  [\"\u8fd8\", null],\n  [\"\u4e0d\", \"less\"],\n  [\"\u5230\", null],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x91>\", null],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x85>\", null],\n  [\"\uff0c\", null],\n  [\"\u8fdc\", \"far\"],\n  [\"\u8fdc\", null],\n  [\"\u843d\", \"behind\"],\n  [\"\u540e\", null],\n  [\"\u4e8e\", null],\n  [\"\u963f\", \"Alibaba\"],\n  [\"\u91cc\", null],\n  [\"\u5df4\", null],\n  [\"\u5df4\", null],\n  [\"\u548c\", \"and\"],\n  [\"\u4eac\", \"JD.com\"],\n  [\"\u4e1c\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u6bd4\u5982\uff0c\u4e9a\u9a6c\u900a\u5728\u7f8e\u56fd\u5728\u7ebf\u96f6\u552e\u5e02\u573a\u7684\u4efd\u989d\u5df2\u8fbe\uff13\uff14\uff05\uff0c\uff12\uff10\uff12\uff10\u5e74\u66f4\u6709\u671b\u5347\u81f3\uff15\uff10\uff05\uff0c\u4f46\u5728\u4e2d\u56fd\uff0c\u4e9a\u9a6c\u900a\u7684\u5e02\u573a\u4efd\u989d\u8fd8\u4e0d\u5230\uff11\uff05\uff0c\u8fdc\u8fdc\u843d\u540e\u4e8e\u963f\u91cc\u5df4\u5df4\u548c\u4eac\u4e1c\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u6bd4', '\u5982', '\uff0c', '\u4e9a', '\u9a6c', '<0xE9>', '<0x80>', '<0x8A>', '\u5728', '\u7f8e', '\u56fd', '\u5728', '\u7ebf', '\u96f6', '\u552e', '\u5e02', '\u573a', '\u7684', '\u4efd', '\u989d', '\u5df2', '\u8fbe', '<0xEF>', '<0xBC>', '<0x93>', '<0xEF>', '<0xBC>', '<0x94>', '<0xEF>', '<0xBC>', '<0x85>', '\uff0c', '<0xEF>', '<0xBC>', '<0x92>', '<0xEF>', '<0xBC>', '<0x90>', '<0xEF>', '<0xBC>', '<0x92>', '<0xEF>', '<0xBC>', '<0x90>', '\u5e74', '\u66f4', '\u6709', '\u671b', '\u5347', '\u81f3', '<0xEF>', '<0xBC>', '<0x95>', '<0xEF>', '<0xBC>', '<0x90>', '<0xEF>', '<0xBC>', '<0x85>', '\uff0c', '\u4f46', '\u5728', '\u4e2d', '\u56fd', '\uff0c', '\u4e9a', '\u9a6c', '<0xE9>', '<0x80>', '<0x8A>', '\u7684', '\u5e02', '\u573a', '\u4efd', '\u989d', '\u8fd8', '\u4e0d', '\u5230', '<0xEF>', '<0xBC>', '<0x91>', '<0xEF>', '<0xBC>', '<0x85>', '\uff0c', '\u8fdc', '\u8fdc', '\u843d', '\u540e', '\u4e8e', '\u963f', '\u91cc', '\u5df4', '\u5df4', '\u548c', '\u4eac', '\u4e1c', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"\u6bd4\", \"\u041c\u044b\u0441\u0430\u043b\u044b\"],\n[\"\u5982\", null],\n[\"\uff0c\", null],\n[\"\u4e9a\", \"Amazon\"],\n[\"\u9a6c\", null],\n[\"<0xE9>\", null],\n[\"<0x80>\", null],\n[\"<0x8A>\", null],\n[\"\u5728\", null],\n[\"\u7f8e\", \"\u0410\u049a\u0428\"],\n[\"\u56fd\", null],\n[\"\u5728\", null],\n[\"\u7ebf\", \"\u043e\u043d\u043b\u0430\u0439\u043d\"],\n[\"\u96f6\", \"\u0431\u04e9\u043b\u0448\u0435\u043a\"],\n[\"\u552e\", null],\n[\"\u5e02\", \"\u0441\u0430\u0443\u0434\u0430\"],\n[\"\u573a\", null],\n[\"\u7684\", null],\n[\"\u4efd\", \"\u04af\u043b\u0435\u0441\u0456\"],\n[\"\u989d\", null],\n[\"\u5df2\", null],\n[\"\u8fbe\", \"\u0436\u0435\u0442\u0442\u0456\"],\n[\"<0xEF>\", null],\n[\"<0xBC>\", null],\n[\"<0x93>\", null],\n[\"<0xEF>\", null],\n[\"<0xBC>\", null],\n[\"<0x94>\", null],\n[\"<0xEF>\", null],\n[\"<0xBC>\", null],\n[\"<0x85>\", null],\n[\"\uff0c\", null],\n[\"<0xEF>\", null],\n[\"<0xBC>\", null],\n[\"<0x92>\", null],\n[\"<0xEF>\", null],\n[\"<0xBC>\", null],\n[\"<0x90>\", null],\n[\"<0xEF>\", null],\n[\"<0xBC>\", null],\n[\"<0x92>\", null],\n[\"<0xEF>\", null],\n[\"<0xBC>\", null],\n[\"<0x90>\", null],\n[\"\u5e74\", \"\u0436\u044b\u043b\u044b\"],\n[\"\u66f4\", null],\n[\"\u6709\", null],\n[\"\u671b\", \"\u043a\u04af\u0442\u0456\u043b\u0443\u0434\u0435\"],\n[\"\u5347\", \"\u043a\u04e9\u0442\u0435\u0440\u0456\u043b\u0443\u0456\"],\n[\"\u81f3\", null],\n[\"<0xEF>\", null],\n[\"<0xBC>\", null],\n[\"<0x95>\", null],\n[\"<0xEF>\", null],\n[\"<0xBC>\", null],\n[\"<0x90>\", null],\n[\"<0xEF>\", null],\n[\"<0xBC>\", null],\n[\"<0x85>\", null],\n[\"\uff0c\", null],\n[\"\u4f46\", \"\u0431\u0456\u0440\u0430\u049b\"],\n[\"\u5728\", null],\n[\"\u4e2d\", \"\u049a\u044b\u0442\u0430\u0439\u0434\u0430\"],\n[\"\u56fd\", null],\n[\"\uff0c\", null],\n[\"\u4e9a\", \"Amazon\"],\n[\"\u9a6c\", null],\n[\"<0xE9>\", null],\n[\"<0x80>\", null],\n[\"<0x8A>\", null],\n[\"\u7684\", null],\n[\"\u5e02\", \"\u043d\u0430\u0440\u044b\u049b\"],\n[\"\u573a\", null],\n[\"\u4efd\", \"\u04af\u043b\u0435\u0441\u0456\"],\n[\"\u989d\", null],\n[\"\u8fd8\", null],\n[\"\u4e0d\", null],\n[\"\u5230\", \"\u0436\u0435\u0442\u043f\u0435\u0439\u0434\u0456\"],\n[\"<0xEF>\", null],\n[\"<0xBC>\", null],\n[\"<0x91>\", null],\n[\"<0xEF>\", null],\n[\"<0xBC>\", null],\n[\"<0x85>\", null],\n[\"\uff0c\", null],\n[\"\u8fdc\", \"\u04d9\u043b\u0434\u0435\u049b\u0430\u0439\u0434\u0430\"],\n[\"\u8fdc\", null],\n[\"\u843d\", \"\u0430\u0440\u0442\u0442\u0430\"],\n[\"\u540e\", null],\n[\"\u4e8e\", null],\n[\"\u963f\", \"Alibaba\"],\n[\"\u91cc\", null],\n[\"\u5df4\", null],\n[\"\u5df4\", null],\n[\"\u548c\", \"\u0436\u04d9\u043d\u0435\"],\n[\"\u4eac\", \"JD.com\"],\n[\"\u4e1c\", null],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u6bd4\u5982\uff0c\u4e9a\u9a6c\u900a\u5728\u7f8e\u56fd\u5728\u7ebf\u96f6\u552e\u5e02\u573a\u7684\u4efd\u989d\u5df2\u8fbe\uff13\uff14\uff05\uff0c\uff12\uff10\uff12\uff10\u5e74\u66f4\u6709\u671b\u5347\u81f3\uff15\uff10\uff05\uff0c\u4f46\u5728\u4e2d\u56fd\uff0c\u4e9a\u9a6c\u900a\u7684\u5e02\u573a\u4efd\u989d\u8fd8\u4e0d\u5230\uff11\uff05\uff0c\u8fdc\u8fdc\u843d\u540e\u4e8e\u963f\u91cc\u5df4\u5df4\u548c\u4eac\u4e1c\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u6bd4', '\u5982', '\uff0c', '\u4e9a', '\u9a6c', '<0xE9>', '<0x80>', '<0x8A>', '\u5728', '\u7f8e', '\u56fd', '\u5728', '\u7ebf', '\u96f6', '\u552e', '\u5e02', '\u573a', '\u7684', '\u4efd', '\u989d', '\u5df2', '\u8fbe', '<0xEF>', '<0xBC>', '<0x93>', '<0xEF>', '<0xBC>', '<0x94>', '<0xEF>', '<0xBC>', '<0x85>', '\uff0c', '<0xEF>', '<0xBC>', '<0x92>', '<0xEF>', '<0xBC>', '<0x90>', '<0xEF>', '<0xBC>', '<0x92>', '<0xEF>', '<0xBC>', '<0x90>', '\u5e74', '\u66f4', '\u6709', '\u671b', '\u5347', '\u81f3', '<0xEF>', '<0xBC>', '<0x95>', '<0xEF>', '<0xBC>', '<0x90>', '<0xEF>', '<0xBC>', '<0x85>', '\uff0c', '\u4f46', '\u5728', '\u4e2d', '\u56fd', '\uff0c', '\u4e9a', '\u9a6c', '<0xE9>', '<0x80>', '<0x8A>', '\u7684', '\u5e02', '\u573a', '\u4efd', '\u989d', '\u8fd8', '\u4e0d', '\u5230', '<0xEF>', '<0xBC>', '<0x91>', '<0xEF>', '<0xBC>', '<0x85>', '\uff0c', '\u8fdc', '\u8fdc', '\u843d', '\u540e', '\u4e8e', '\u963f', '\u91cc', '\u5df4', '\u5df4', '\u548c', '\u4eac', '\u4e1c', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u6bd4\", \"\u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440\"],\n  [\"\u5982\", null],\n  [\"\uff0c\", null],\n  [\"\u4e9a\", \"\u0410\u043c\u0430\u0437\u043e\u043d\"],\n  [\"\u9a6c\", null],\n  [\"<0xE9>\", null],\n  [\"<0x80>\", null],\n  [\"<0x8A>\", null],\n  [\"\u5728\", null],\n  [\"\u7f8e\", \"\u0421\u0428\u0410\"],\n  [\"\u56fd\", null],\n  [\"\u5728\", null],\n  [\"\u7ebf\", \"\u043e\u043d\u043b\u0430\u0439\u043d\"],\n  [\"\u96f6\", \"\u0440\u043e\u0437\u043d\u0438\u0447\u043d\u044b\u0439\"],\n  [\"\u552e\", null],\n  [\"\u5e02\", \"\u0440\u044b\u043d\u043e\u043a\"],\n  [\"\u573a\", null],\n  [\"\u7684\", null],\n  [\"\u4efd\", \"\u0434\u043e\u043b\u044f\"],\n  [\"\u989d\", null],\n  [\"\u5df2\", null],\n  [\"\u8fbe\", \"\u0434\u043e\u0441\u0442\u0438\u0433\u043b\u0430\"],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x93>\", null],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x94>\", null],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x85>\", null],\n  [\"\uff0c\", null],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x92>\", null],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x90>\", null],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x92>\", null],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x90>\", null],\n  [\"\u5e74\", \"\u0433\u043e\u0434\"],\n  [\"\u66f4\", null],\n  [\"\u6709\", null],\n  [\"\u671b\", \"\u043e\u0436\u0438\u0434\u0430\u0435\u0442\u0441\u044f\"],\n  [\"\u5347\", \"\u0432\u044b\u0440\u0430\u0441\u0442\u0438\"],\n  [\"\u81f3\", \"\u0434\u043e\"],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x95>\", null],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x90>\", null],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x85>\", null],\n  [\"\uff0c\", null],\n  [\"\u4f46\", \"\u043d\u043e\"],\n  [\"\u5728\", \"\u0432\"],\n  [\"\u4e2d\", \"\u041a\u0438\u0442\u0430\u0435\"],\n  [\"\u56fd\", null],\n  [\"\uff0c\", null],\n  [\"\u4e9a\", \"\u0410\u043c\u0430\u0437\u043e\u043d\"],\n  [\"\u9a6c\", null],\n  [\"<0xE9>\", null],\n  [\"<0x80>\", null],\n  [\"<0x8A>\", null],\n  [\"\u7684\", null],\n  [\"\u5e02\", \"\u0440\u044b\u043d\u043e\u0447\u043d\u0430\u044f\"],\n  [\"\u573a\", null],\n  [\"\u4efd\", \"\u0434\u043e\u043b\u044f\"],\n  [\"\u989d\", null],\n  [\"\u8fd8\", null],\n  [\"\u4e0d\", null],\n  [\"\u5230\", \"\u043c\u0435\u043d\u0435\u0435\"],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x91>\", null],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x85>\", null],\n  [\"\uff0c\", null],\n  [\"\u8fdc\", \"\u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e\"],\n  [\"\u8fdc\", null],\n  [\"\u843d\", \"\u043e\u0442\u0441\u0442\u0430\u0435\u0442\"],\n  [\"\u540e\", null],\n  [\"\u4e8e\", \"\u043e\u0442\"],\n  [\"\u963f\", \"\u0410\u043b\u0438\u0431\u0430\u0431\u0430\"],\n  [\"\u91cc\", null],\n  [\"\u5df4\", null],\n  [\"\u5df4\", null],\n  [\"\u548c\", \"\u0438\"],\n  [\"\u4eac\", \"JD.com\"],\n  [\"\u4e1c\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u7136\u800c\uff0c\u8fd8\u6709\u4e00\u4e2a\u66f4\u52a0\u4ee4\u4eba\u62c5\u5fe7\u7684\u957f\u671f\u573a\u666f\uff0c\u7279\u6717\u666e\u8981\u201c\u6b63\u5e38\u5316\u201d\u7684\u662f\u4e0e\u53f3\u7ffc\u5171\u548c\u515a\u4eba\u7684\u5173\u7cfb\uff0c\u8fd9\u4e9b\u5171\u548c\u515a\u4eba\u4e4b\u524d\u66fe\u53cd\u5bf9\u603b\u7edf\u5148\u751f\u7684\u6807\u51c6\uff0c\u73b0\u5728\u5219\u7a81\u7136\u95f4\u770b\u8d77\u6765\u6210\u4e86\u7406\u6027\u7684\u8c61\u5f81\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u7136', '\u800c', '\uff0c', '\u8fd8', '\u6709', '\u4e00\u4e2a', '\u66f4', '\u52a0', '\u4ee4', '\u4eba', '\u62c5', '<0xE5>', '<0xBF>', '<0xA7>', '\u7684', '\u957f', '\u671f', '\u573a', '\u666f', '\uff0c', '\u7279', '<0xE6>', '<0x9C>', '<0x97>', '\u666e', '\u8981', '\u201c', '\u6b63', '\u5e38', '\u5316', '\u201d', '\u7684', '\u662f', '\u4e0e', '\u53f3', '<0xE7>', '<0xBF>', '<0xBC>', '\u5171', '\u548c', '\u515a', '\u4eba', '\u7684', '\u5173', '\u7cfb', '\uff0c', '\u8fd9', '\u4e9b', '\u5171', '\u548c', '\u515a', '\u4eba', '\u4e4b', '\u524d', '\u66fe', '\u53cd', '\u5bf9', '\u603b', '\u7edf', '\u5148', '\u751f', '\u7684', '\u6807', '\u51c6', '\uff0c', '\u73b0', '\u5728', '\u5219', '\u7a81', '\u7136', '\u95f4', '\u770b', '\u8d77', '\u6765', '\u6210', '\u4e86', '\u7406', '\u6027', '\u7684', '\u8c61', '\u5f81', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u7136\", \"However\"],\n  [\"\u800c\", null],\n  [\"\uff0c\", null],\n  [\"\u8fd8\", null],\n  [\"\u6709\", null],\n  [\"\u4e00\u4e2a\", \"a\"],\n  [\"\u66f4\", \"more\"],\n  [\"\u52a0\", null],\n  [\"\u4ee4\", null],\n  [\"\u4eba\", null],\n  [\"\u62c5\", \"worrying\"],\n  [\"<0xE5>\", null],\n  [\"<0xBF>\", null],\n  [\"<0xA7>\", null],\n  [\"\u7684\", null],\n  [\"\u957f\", \"long\"],\n  [\"\u671f\", null],\n  [\"\u573a\", \"scenario\"],\n  [\"\u666f\", null],\n  [\"\uff0c\", null],\n  [\"\u7279\", \"Trump\"],\n  [\"<0xE6>\", null],\n  [\"<0x9C>\", null],\n  [\"<0x97>\", null],\n  [\"\u666e\", null],\n  [\"\u8981\", null],\n  [\"\"\", null],\n  [\"\u6b63\", \"normalize\"],\n  [\"\u5e38\", null],\n  [\"\u5316\", null],\n  [\"\"\", null],\n  [\"\u7684\", null],\n  [\"\u662f\", null],\n  [\"\u4e0e\", null],\n  [\"\u53f3\", \"right-wing\"],\n  [\"<0xE7>\", null],\n  [\"<0xBF>\", null],\n  [\"<0xBC>\", null],\n  [\"\u5171\", \"Republicans\"],\n  [\"\u548c\", null],\n  [\"\u515a\", null],\n  [\"\u4eba\", null],\n  [\"\u7684\", null],\n  [\"\u5173\", \"relationship\"],\n  [\"\u7cfb\", null],\n  [\"\uff0c\", null],\n  [\"\u8fd9\", \"these\"],\n  [\"\u4e9b\", null],\n  [\"\u5171\", \"Republicans\"],\n  [\"\u548c\", null],\n  [\"\u515a\", null],\n  [\"\u4eba\", null],\n  [\"\u4e4b\", null],\n  [\"\u524d\", null],\n  [\"\u66fe\", null],\n  [\"\u53cd\", \"opposed\"],\n  [\"\u5bf9\", null],\n  [\"\u603b\", \"President\"],\n  [\"\u7edf\", null],\n  [\"\u5148\", null],\n  [\"\u751f\", null],\n  [\"\u7684\", null],\n  [\"\u6807\", \"standards\"],\n  [\"\u51c6\", null],\n  [\"\uff0c\", null],\n  [\"\u73b0\", \"now\"],\n  [\"\u5728\", null],\n  [\"\u5219\", null],\n  [\"\u7a81\", \"suddenly\"],\n  [\"\u7136\", null],\n  [\"\u95f4\", null],\n  [\"\u770b\", \"appear\"],\n  [\"\u8d77\", null],\n  [\"\u6765\", null],\n  [\"\u6210\", \"become\"],\n  [\"\u4e86\", null],\n  [\"\u7406\", \"rational\"],\n  [\"\u6027\", null],\n  [\"\u7684\", null],\n  [\"\u8c61\", \"symbols\"],\n  [\"\u5f81\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u7136\u800c\uff0c\u8fd8\u6709\u4e00\u4e2a\u66f4\u52a0\u4ee4\u4eba\u62c5\u5fe7\u7684\u957f\u671f\u573a\u666f\uff0c\u7279\u6717\u666e\u8981\u201c\u6b63\u5e38\u5316\u201d\u7684\u662f\u4e0e\u53f3\u7ffc\u5171\u548c\u515a\u4eba\u7684\u5173\u7cfb\uff0c\u8fd9\u4e9b\u5171\u548c\u515a\u4eba\u4e4b\u524d\u66fe\u53cd\u5bf9\u603b\u7edf\u5148\u751f\u7684\u6807\u51c6\uff0c\u73b0\u5728\u5219\u7a81\u7136\u95f4\u770b\u8d77\u6765\u6210\u4e86\u7406\u6027\u7684\u8c61\u5f81\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u7136', '\u800c', '\uff0c', '\u8fd8', '\u6709', '\u4e00\u4e2a', '\u66f4', '\u52a0', '\u4ee4', '\u4eba', '\u62c5', '<0xE5>', '<0xBF>', '<0xA7>', '\u7684', '\u957f', '\u671f', '\u573a', '\u666f', '\uff0c', '\u7279', '<0xE6>', '<0x9C>', '<0x97>', '\u666e', '\u8981', '\u201c', '\u6b63', '\u5e38', '\u5316', '\u201d', '\u7684', '\u662f', '\u4e0e', '\u53f3', '<0xE7>', '<0xBF>', '<0xBC>', '\u5171', '\u548c', '\u515a', '\u4eba', '\u7684', '\u5173', '\u7cfb', '\uff0c', '\u8fd9', '\u4e9b', '\u5171', '\u548c', '\u515a', '\u4eba', '\u4e4b', '\u524d', '\u66fe', '\u53cd', '\u5bf9', '\u603b', '\u7edf', '\u5148', '\u751f', '\u7684', '\u6807', '\u51c6', '\uff0c', '\u73b0', '\u5728', '\u5219', '\u7a81', '\u7136', '\u95f4', '\u770b', '\u8d77', '\u6765', '\u6210', '\u4e86', '\u7406', '\u6027', '\u7684', '\u8c61', '\u5f81', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u7136\", \"\u0430\u043b\u0430\u0439\u0434\u0430\"],\n  [\"\u800c\", null],\n  [\"\uff0c\", null],\n  [\"\u8fd8\", null],\n  [\"\u6709\", null],\n  [\"\u4e00\u4e2a\", \"\u0431\u0456\u0440\"],\n  [\"\u66f4\", null],\n  [\"\u52a0\", null],\n  [\"\u4ee4\", null],\n  [\"\u4eba\", null],\n  [\"\u62c5\", \"\u0430\u043b\u0430\u04a3\u0434\u0430\u0442\u0430\u0440\u043b\u044b\u049b\"],\n  [\"<0xE5>\", null],\n  [\"<0xBF>\", null],\n  [\"<0xA7>\", null],\n  [\"\u7684\", null],\n  [\"\u957f\", \"\u04b1\u0437\u0430\u049b\"],\n  [\"\u671f\", null],\n  [\"\u573a\", \"\u0441\u0446\u0435\u043d\u0430\u0440\u0438\u0439\"],\n  [\"\u666f\", null],\n  [\"\uff0c\", null],\n  [\"\u7279\", \"\u0422\u0440\u0430\u043c\u043f\"],\n  [\"<0xE6>\", null],\n  [\"<0x9C>\", null],\n  [\"<0x97>\", null],\n  [\"\u666e\", null],\n  [\"\u8981\", null],\n  [\"\"\", null],\n  [\"\u6b63\", \"\u049b\u0430\u043b\u044b\u043f\u049b\u0430\"],\n  [\"\u5e38\", null],\n  [\"\u5316\", null],\n  [\"\"\", null],\n  [\"\u7684\", null],\n  [\"\u662f\", null],\n  [\"\u4e0e\", null],\n  [\"\u53f3\", \"\u043e\u04a3\u0448\u044b\u043b\"],\n  [\"<0xE7>\", null],\n  [\"<0xBF>\", null],\n  [\"<0xBC>\", null],\n  [\"\u5171\", \"\u0440\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430\u0448\u044b\u043b\"],\n  [\"\u548c\", null],\n  [\"\u515a\", null],\n  [\"\u4eba\", null],\n  [\"\u7684\", null],\n  [\"\u5173\", \"\u049b\u0430\u0440\u044b\u043c-\u049b\u0430\u0442\u044b\u043d\u0430\u0441\"],\n  [\"\u7cfb\", null],\n  [\"\uff0c\", null],\n  [\"\u8fd9\", null],\n  [\"\u4e9b\", null],\n  [\"\u5171\", \"\u0440\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430\u0448\u044b\u043b\u0434\u0430\u0440\"],\n  [\"\u548c\", null],\n  [\"\u515a\", null],\n  [\"\u4eba\", null],\n  [\"\u4e4b\", null],\n  [\"\u524d\", null],\n  [\"\u66fe\", null],\n  [\"\u53cd\", \"\u049b\u0430\u0440\u0441\u044b\"],\n  [\"\u5bf9\", null],\n  [\"\u603b\", \"\u043f\u0440\u0435\u0437\u0438\u0434\u0435\u043d\u0442\"],\n  [\"\u7edf\", null],\n  [\"\u5148\", null],\n  [\"\u751f\", null],\n  [\"\u7684\", null],\n  [\"\u6807\", \"\u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0442\u0430\u0440\"],\n  [\"\u51c6\", null],\n  [\"\uff0c\", null],\n  [\"\u73b0\", \"\u049b\u0430\u0437\u0456\u0440\"],\n  [\"\u5728\", null],\n  [\"\u5219\", null],\n  [\"\u7a81\", \"\u043a\u0435\u043d\u0435\u0442\u0442\u0435\u043d\"],\n  [\"\u7136\", null],\n  [\"\u95f4\", null],\n  [\"\u770b\", null],\n  [\"\u8d77\", null],\n  [\"\u6765\", null],\n  [\"\u6210\", null],\n  [\"\u4e86\", null],\n  [\"\u7406\", \"\u0430\u049b\u044b\u043b\u0434\u044b\u043b\u044b\u049b\u0442\u044b\u04a3\"],\n  [\"\u6027\", null],\n  [\"\u7684\", null],\n  [\"\u8c61\", \"\u0441\u0438\u043c\u0432\u043e\u043b\u044b\"],\n  [\"\u5f81\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u7136\u800c\uff0c\u8fd8\u6709\u4e00\u4e2a\u66f4\u52a0\u4ee4\u4eba\u62c5\u5fe7\u7684\u957f\u671f\u573a\u666f\uff0c\u7279\u6717\u666e\u8981\u201c\u6b63\u5e38\u5316\u201d\u7684\u662f\u4e0e\u53f3\u7ffc\u5171\u548c\u515a\u4eba\u7684\u5173\u7cfb\uff0c\u8fd9\u4e9b\u5171\u548c\u515a\u4eba\u4e4b\u524d\u66fe\u53cd\u5bf9\u603b\u7edf\u5148\u751f\u7684\u6807\u51c6\uff0c\u73b0\u5728\u5219\u7a81\u7136\u95f4\u770b\u8d77\u6765\u6210\u4e86\u7406\u6027\u7684\u8c61\u5f81\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u7136', '\u800c', '\uff0c', '\u8fd8', '\u6709', '\u4e00\u4e2a', '\u66f4', '\u52a0', '\u4ee4', '\u4eba', '\u62c5', '<0xE5>', '<0xBF>', '<0xA7>', '\u7684', '\u957f', '\u671f', '\u573a', '\u666f', '\uff0c', '\u7279', '<0xE6>', '<0x9C>', '<0x97>', '\u666e', '\u8981', '\u201c', '\u6b63', '\u5e38', '\u5316', '\u201d', '\u7684', '\u662f', '\u4e0e', '\u53f3', '<0xE7>', '<0xBF>', '<0xBC>', '\u5171', '\u548c', '\u515a', '\u4eba', '\u7684', '\u5173', '\u7cfb', '\uff0c', '\u8fd9', '\u4e9b', '\u5171', '\u548c', '\u515a', '\u4eba', '\u4e4b', '\u524d', '\u66fe', '\u53cd', '\u5bf9', '\u603b', '\u7edf', '\u5148', '\u751f', '\u7684', '\u6807', '\u51c6', '\uff0c', '\u73b0', '\u5728', '\u5219', '\u7a81', '\u7136', '\u95f4', '\u770b', '\u8d77', '\u6765', '\u6210', '\u4e86', '\u7406', '\u6027', '\u7684', '\u8c61', '\u5f81', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u7136\", \"\u041e\u0434\u043d\u0430\u043a\u043e\"],\n  [\"\u800c\", null],\n  [\"\uff0c\", null],\n  [\"\u8fd8\", \"\u0435\u0449\u0435\"],\n  [\"\u6709\", null],\n  [\"\u4e00\u4e2a\", \"\u043e\u0434\u0438\u043d\"],\n  [\"\u66f4\", \"\u0431\u043e\u043b\u0435\u0435\"],\n  [\"\u52a0\", null],\n  [\"\u4ee4\", null],\n  [\"\u4eba\", null],\n  [\"\u62c5\", \"\u0442\u0440\u0435\u0432\u043e\u0436\u043d\u044b\u0439\"],\n  [\"<0xE5>\", null],\n  [\"<0xBF>\", null],\n  [\"<0xA7>\", null],\n  [\"\u7684\", null],\n  [\"\u957f\", \"\u0434\u043e\u043b\u0433\u043e\u0441\u0440\u043e\u0447\u043d\u044b\u0439\"],\n  [\"\u671f\", null],\n  [\"\u573a\", \"\u0441\u0446\u0435\u043d\u0430\u0440\u0438\u0439\"],\n  [\"\u666f\", null],\n  [\"\uff0c\", null],\n  [\"\u7279\", \"\u0422\u0440\u0430\u043c\u043f\"],\n  [\"<0xE6>\", null],\n  [\"<0x9C>\", null],\n  [\"<0x97>\", null],\n  [\"\u666e\", null],\n  [\"\u8981\", \"\u0445\u043e\u0447\u0435\u0442\"],\n  [\"\"\", null],\n  [\"\u6b63\", \"\u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c\"],\n  [\"\u5e38\", null],\n  [\"\u5316\", null],\n  [\"\"\", null],\n  [\"\u7684\", null],\n  [\"\u662f\", null],\n  [\"\u4e0e\", \"\u0441\"],\n  [\"\u53f3\", \"\u043f\u0440\u0430\u0432\u044b\u043c\u0438\"],\n  [\"<0xE7>\", null],\n  [\"<0xBF>\", null],\n  [\"<0xBC>\", null],\n  [\"\u5171\", \"\u0440\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430\u043d\u0446\u0430\u043c\u0438\"],\n  [\"\u548c\", null],\n  [\"\u515a\", null],\n  [\"\u4eba\", null],\n  [\"\u7684\", null],\n  [\"\u5173\", \"\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044f\"],\n  [\"\u7cfb\", null],\n  [\"\uff0c\", null],\n  [\"\u8fd9\", \"\u044d\u0442\u0438\"],\n  [\"\u4e9b\", null],\n  [\"\u5171\", \"\u0440\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430\u043d\u0446\u044b\"],\n  [\"\u548c\", null],\n  [\"\u515a\", null],\n  [\"\u4eba\", null],\n  [\"\u4e4b\", \"\u0440\u0430\u043d\u0435\u0435\"],\n  [\"\u524d\", null],\n  [\"\u66fe\", null],\n  [\"\u53cd\", \"\u0432\u044b\u0441\u0442\u0443\u043f\u0430\u043b\u0438\"],\n  [\"\u5bf9\", null],\n  [\"\u603b\", \"\u043f\u0440\u0435\u0437\u0438\u0434\u0435\u043d\u0442\u0430\"],\n  [\"\u7edf\", null],\n  [\"\u5148\", null],\n  [\"\u751f\", null],\n  [\"\u7684\", null],\n  [\"\u6807\", \"\u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u044b\"],\n  [\"\u51c6\", null],\n  [\"\uff0c\", null],\n  [\"\u73b0\", \"\u0442\u0435\u043f\u0435\u0440\u044c\"],\n  [\"\u5728\", null],\n  [\"\u5219\", null],\n  [\"\u7a81\", \"\u0432\u043d\u0435\u0437\u0430\u043f\u043d\u043e\"],\n  [\"\u7136\", null],\n  [\"\u95f4\", null],\n  [\"\u770b\", \"\u043a\u0430\u0436\u0443\u0442\u0441\u044f\"],\n  [\"\u8d77\", null],\n  [\"\u6765\", null],\n  [\"\u6210\", \"\u0441\u0438\u043c\u0432\u043e\u043b\u043e\u043c\"],\n  [\"\u4e86\", null],\n  [\"\u7406\", \"\u0440\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438\"],\n  [\"\u6027\", null],\n  [\"\u7684\", null],\n  [\"\u8c61\", null],\n  [\"\u5f81\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u56e0\u6b64\uff0c\u4ece\u5c31\u4e1a\u3001\u85aa\u916c\u548c\u7ecf\u6d4e\u7e41\u8363\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u5728\u82f1\u56fd\u8131\u6b27\uff0c\u751f\u6d3b\u6210\u672c\u4e0a\u6da8\u548c\u5236\u9020\u4e1a\u957f\u671f\u8870\u9000\u7684\u80cc\u666f\u4e0b\uff0c\u683c\u62c9\u65af\u54e5\u4e1c\u90e8\u5730\u533a\u8feb\u5207\u9700\u8981\u9ea6\u7ef4\u4ed6\u4e0d\u65ad\u6269\u5927\u751f\u4ea7\u7f51\u7edc\u3002\u201d\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u56e0', '\u6b64', '\uff0c', '\u4ece', '\u5c31', '\u4e1a', '\u3001', '<0xE8>', '<0x96>', '<0xAA>', '<0xE9>', '<0x85>', '<0xAC>', '\u548c', '\u7ecf', '\u6d4e', '\u7e41', '<0xE8>', '<0x8D>', '<0xA3>', '\u7684', '\u89d2', '\u5ea6', '\u6765', '\u770b', '\uff0c', '\u5728', '\u82f1', '\u56fd', '<0xE8>', '<0x84>', '<0xB1>', '<0xE6>', '<0xAC>', '<0xA7>', '\uff0c', '\u751f', '\u6d3b', '\u6210', '\u672c', '\u4e0a', '<0xE6>', '<0xB6>', '<0xA8>', '\u548c', '\u5236', '\u9020', '\u4e1a', '\u957f', '\u671f', '<0xE8>', '<0xA1>', '<0xB0>', '\u9000', '\u7684', '\u80cc', '\u666f', '\u4e0b', '\uff0c', '\u683c', '\u62c9', '\u65af', '\u54e5', '\u4e1c', '\u90e8', '\u5730', '\u533a', '<0xE8>', '<0xBF>', '<0xAB>', '\u5207', '\u9700', '\u8981', '<0xE9>', '<0xBA>', '<0xA6>', '\u7ef4', '\u4ed6', '\u4e0d', '\u65ad', '\u6269', '\u5927', '\u751f', '\u4ea7', '\u7f51', '\u7edc', '\u3002', '\u201d']"}], "[\n[\"\u2581\", null],\n[\"\u56e0\", \"Therefore\"],\n[\"\u6b64\", null],\n[\"\uff0c\", null],\n[\"\u4ece\", \"from\"],\n[\"\u5c31\", null],\n[\"\u4e1a\", \"employment\"],\n[\"\u3001\", null],\n[\"<0xE8>\", null],\n[\"<0x96>\", null],\n[\"<0xAA>\", null],\n[\"<0xE9>\", null],\n[\"<0x85>\", null],\n[\"<0xAC>\", null],\n[\"\u548c\", null],\n[\"\u7ecf\", null],\n[\"\u6d4e\", \"economic\"],\n[\"\u7e41\", null],\n[\"<0xE8>\", null],\n[\"<0x8D>\", null],\n[\"<0xA3>\", null],\n[\"\u7684\", null],\n[\"\u89d2\", null],\n[\"\u5ea6\", \"perspective\"],\n[\"\u6765\", null],\n[\"\u770b\", \"view\"],\n[\"\uff0c\", null],\n[\"\u5728\", \"in\"],\n[\"\u82f1\", null],\n[\"\u56fd\", \"UK\"],\n[\"<0xE8>\", null],\n[\"<0x84>\", null],\n[\"<0xB1>\", null],\n[\"<0xE6>\", null],\n[\"<0xAC>\", null],\n[\"<0xA7>\", null],\n[\"\uff0c\", null],\n[\"\u751f\", null],\n[\"\u6d3b\", \"living\"],\n[\"\u6210\", null],\n[\"\u672c\", \"costs\"],\n[\"\u4e0a\", null],\n[\"<0xE6>\", null],\n[\"<0xB6>\", null],\n[\"<0xA8>\", null],\n[\"\u548c\", null],\n[\"\u5236\", null],\n[\"\u9020\", null],\n[\"\u4e1a\", \"manufacturing\"],\n[\"\u957f\", null],\n[\"\u671f\", \"long-term\"],\n[\"<0xE8>\", null],\n[\"<0xA1>\", null],\n[\"<0xB0>\", null],\n[\"\u9000\", \"decline\"],\n[\"\u7684\", null],\n[\"\u80cc\", null],\n[\"\u666f\", \"context\"],\n[\"\u4e0b\", null],\n[\"\uff0c\", null],\n[\"\u683c\", null],\n[\"\u62c9\", null],\n[\"\u65af\", null],\n[\"\u54e5\", \"Glasgow\"],\n[\"\u4e1c\", null],\n[\"\u90e8\", \"East\"],\n[\"\u5730\", null],\n[\"\u533a\", \"area\"],\n[\"<0xE8>\", null],\n[\"<0xBF>\", null],\n[\"<0xAB>\", null],\n[\"\u5207\", \"urgently\"],\n[\"\u9700\", null],\n[\"\u8981\", \"needs\"],\n[\"<0xE9>\", null],\n[\"<0xBA>\", null],\n[\"<0xA6>\", null],\n[\"\u7ef4\", null],\n[\"\u4ed6\", \"McVitie's\"],\n[\"\u4e0d\", null],\n[\"\u65ad\", \"continuously\"],\n[\"\u6269\", null],\n[\"\u5927\", \"expand\"],\n[\"\u751f\", null],\n[\"\u4ea7\", \"production\"],\n[\"\u7f51\", null],\n[\"\u7edc\", \"network\"],\n[\"\u3002\", null],\n[\"\"\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u56e0\u6b64\uff0c\u4ece\u5c31\u4e1a\u3001\u85aa\u916c\u548c\u7ecf\u6d4e\u7e41\u8363\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u5728\u82f1\u56fd\u8131\u6b27\uff0c\u751f\u6d3b\u6210\u672c\u4e0a\u6da8\u548c\u5236\u9020\u4e1a\u957f\u671f\u8870\u9000\u7684\u80cc\u666f\u4e0b\uff0c\u683c\u62c9\u65af\u54e5\u4e1c\u90e8\u5730\u533a\u8feb\u5207\u9700\u8981\u9ea6\u7ef4\u4ed6\u4e0d\u65ad\u6269\u5927\u751f\u4ea7\u7f51\u7edc\u3002\u201d\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u56e0', '\u6b64', '\uff0c', '\u4ece', '\u5c31', '\u4e1a', '\u3001', '<0xE8>', '<0x96>', '<0xAA>', '<0xE9>', '<0x85>', '<0xAC>', '\u548c', '\u7ecf', '\u6d4e', '\u7e41', '<0xE8>', '<0x8D>', '<0xA3>', '\u7684', '\u89d2', '\u5ea6', '\u6765', '\u770b', '\uff0c', '\u5728', '\u82f1', '\u56fd', '<0xE8>', '<0x84>', '<0xB1>', '<0xE6>', '<0xAC>', '<0xA7>', '\uff0c', '\u751f', '\u6d3b', '\u6210', '\u672c', '\u4e0a', '<0xE6>', '<0xB6>', '<0xA8>', '\u548c', '\u5236', '\u9020', '\u4e1a', '\u957f', '\u671f', '<0xE8>', '<0xA1>', '<0xB0>', '\u9000', '\u7684', '\u80cc', '\u666f', '\u4e0b', '\uff0c', '\u683c', '\u62c9', '\u65af', '\u54e5', '\u4e1c', '\u90e8', '\u5730', '\u533a', '<0xE8>', '<0xBF>', '<0xAB>', '\u5207', '\u9700', '\u8981', '<0xE9>', '<0xBA>', '<0xA6>', '\u7ef4', '\u4ed6', '\u4e0d', '\u65ad', '\u6269', '\u5927', '\u751f', '\u4ea7', '\u7f51', '\u7edc', '\u3002', '\u201d']"}], "[\n[\"\u2581\", null],\n[\"\u56e0\", \"\u0421\u043e\u043d\u0434\u044b\u049b\u0442\u0430\u043d\"],\n[\"\u6b64\", null],\n[\"\uff0c\", null],\n[\"\u4ece\", null],\n[\"\u5c31\", \"\u0436\u04b1\u043c\u044b\u0441\u043f\u0435\u043d\"],\n[\"\u4e1a\", null],\n[\"\u3001\", null],\n[\"<0xE8>\", null],\n[\"<0x96>\", null],\n[\"<0xAA>\", null],\n[\"<0xE9>\", null],\n[\"<0x85>\", null],\n[\"<0xAC>\", null],\n[\"\u548c\", \"\u0436\u04d9\u043d\u0435\"],\n[\"\u7ecf\", \"\u044d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0430\u043b\u044b\u049b\"],\n[\"\u6d4e\", null],\n[\"\u7e41\", \"\u04e9\u0440\u043a\u0435\u043d\u0434\u0435\u0443\"],\n[\"<0xE8>\", null],\n[\"<0x8D>\", null],\n[\"<0xA3>\", null],\n[\"\u7684\", null],\n[\"\u89d2\", \"\u0442\u04b1\u0440\u0493\u044b\u0441\u044b\u043d\u0430\u043d\"],\n[\"\u5ea6\", null],\n[\"\u6765\", null],\n[\"\u770b\", \"\u049b\u0430\u0440\u0430\u0493\u0430\u043d\u0434\u0430\"],\n[\"\uff0c\", null],\n[\"\u5728\", null],\n[\"\u82f1\", \"\u04b0\u043b\u044b\u0431\u0440\u0438\u0442\u0430\u043d\u0438\u044f\u043d\u044b\u04a3\"],\n[\"\u56fd\", null],\n[\"<0xE8>\", null],\n[\"<0x84>\", null],\n[\"<0xB1>\", null],\n[\"<0xE6>\", null],\n[\"<0xAC>\", null],\n[\"<0xA7>\", null],\n[\"\uff0c\", null],\n[\"\u751f\", \"\u0442\u04b1\u0440\u043c\u044b\u0441\u0442\u044b\u049b\"],\n[\"\u6d3b\", null],\n[\"\u6210\", \"\u0448\u044b\u0493\u044b\u043d\u0434\u0430\u0440\u0434\u044b\u04a3\"],\n[\"\u672c\", null],\n[\"\u4e0a\", \"\u04e9\u0441\u0443\u0456\"],\n[\"<0xE6>\", null],\n[\"<0xB6>\", null],\n[\"<0xA8>\", null],\n[\"\u548c\", \"\u0436\u04d9\u043d\u0435\"],\n[\"\u5236\", \"\u04e9\u043d\u0434\u0456\u0440\u0456\u0441\"],\n[\"\u9020\", null],\n[\"\u4e1a\", null],\n[\"\u957f\", \"\u04b1\u0437\u0430\u049b\"],\n[\"\u671f\", \"\u043c\u0435\u0440\u0437\u0456\u043c\u0434\u0456\"],\n[\"<0xE8>\", null],\n[\"<0xA1>\", null],\n[\"<0xB0>\", null],\n[\"\u9000\", \"\u049b\u04b1\u043b\u0434\u044b\u0440\u0430\u0443\u044b\"],\n[\"\u7684\", null],\n[\"\u80cc\", \"\u0436\u0430\u0493\u0434\u0430\u0439\u044b\u043d\u0434\u0430\"],\n[\"\u666f\", null],\n[\"\u4e0b\", null],\n[\"\uff0c\", null],\n[\"\u683c\", \"\u0413\u043b\u0430\u0437\u0433\u043e\u043d\u044b\u04a3\"],\n[\"\u62c9\", null],\n[\"\u65af\", null],\n[\"\u54e5\", null],\n[\"\u4e1c\", \"\u0448\u044b\u0493\u044b\u0441\"],\n[\"\u90e8\", null],\n[\"\u5730\", \"\u0430\u0439\u043c\u0430\u0493\u044b\u043d\u0430\"],\n[\"\u533a\", null],\n[\"<0xE8>\", null],\n[\"<0xBF>\", null],\n[\"<0xAB>\", null],\n[\"\u5207\", \"\u049b\u0430\u0436\u0435\u0442\"],\n[\"\u9700\", null],\n[\"\u8981\", null],\n[\"<0xE9>\", null],\n[\"<0xBA>\", null],\n[\"<0xA6>\", null],\n[\"\u7ef4\", \"McVitie's\"],\n[\"\u4ed6\", null],\n[\"\u4e0d\", \"\u04e9\u043d\u0434\u0456\u0440\u0456\u0441\"],\n[\"\u65ad\", null],\n[\"\u6269\", \"\u0436\u0435\u043b\u0456\u0441\u0456\u043d\"],\n[\"\u5927\", null],\n[\"\u751f\", \"\u04af\u0437\u0434\u0456\u043a\u0441\u0456\u0437\"],\n[\"\u4ea7\", null],\n[\"\u7f51\", \"\u043a\u0435\u04a3\u0435\u0439\u0442\u0443\"],\n[\"\u7edc\", null],\n[\"\u3002\", null],\n[\"\"\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u56e0\u6b64\uff0c\u4ece\u5c31\u4e1a\u3001\u85aa\u916c\u548c\u7ecf\u6d4e\u7e41\u8363\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u5728\u82f1\u56fd\u8131\u6b27\uff0c\u751f\u6d3b\u6210\u672c\u4e0a\u6da8\u548c\u5236\u9020\u4e1a\u957f\u671f\u8870\u9000\u7684\u80cc\u666f\u4e0b\uff0c\u683c\u62c9\u65af\u54e5\u4e1c\u90e8\u5730\u533a\u8feb\u5207\u9700\u8981\u9ea6\u7ef4\u4ed6\u4e0d\u65ad\u6269\u5927\u751f\u4ea7\u7f51\u7edc\u3002\u201d\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u56e0', '\u6b64', '\uff0c', '\u4ece', '\u5c31', '\u4e1a', '\u3001', '<0xE8>', '<0x96>', '<0xAA>', '<0xE9>', '<0x85>', '<0xAC>', '\u548c', '\u7ecf', '\u6d4e', '\u7e41', '<0xE8>', '<0x8D>', '<0xA3>', '\u7684', '\u89d2', '\u5ea6', '\u6765', '\u770b', '\uff0c', '\u5728', '\u82f1', '\u56fd', '<0xE8>', '<0x84>', '<0xB1>', '<0xE6>', '<0xAC>', '<0xA7>', '\uff0c', '\u751f', '\u6d3b', '\u6210', '\u672c', '\u4e0a', '<0xE6>', '<0xB6>', '<0xA8>', '\u548c', '\u5236', '\u9020', '\u4e1a', '\u957f', '\u671f', '<0xE8>', '<0xA1>', '<0xB0>', '\u9000', '\u7684', '\u80cc', '\u666f', '\u4e0b', '\uff0c', '\u683c', '\u62c9', '\u65af', '\u54e5', '\u4e1c', '\u90e8', '\u5730', '\u533a', '<0xE8>', '<0xBF>', '<0xAB>', '\u5207', '\u9700', '\u8981', '<0xE9>', '<0xBA>', '<0xA6>', '\u7ef4', '\u4ed6', '\u4e0d', '\u65ad', '\u6269', '\u5927', '\u751f', '\u4ea7', '\u7f51', '\u7edc', '\u3002', '\u201d']"}], "[\n[\"\u2581\", null],\n[\"\u56e0\", \"\u041f\u043e\u044d\u0442\u043e\u043c\u0443\"],\n[\"\u6b64\", null],\n[\"\uff0c\", null],\n[\"\u4ece\", null],\n[\"\u5c31\", \"\u0442\u0440\u0443\u0434\u043e\u0443\u0441\u0442\u0440\u043e\u0439\u0441\u0442\u0432\u0430\"],\n[\"\u4e1a\", null],\n[\"\u3001\", null],\n[\"<0xE8>\", null],\n[\"<0x96>\", null],\n[\"<0xAA>\", null],\n[\"<0xE9>\", null],\n[\"<0x85>\", null],\n[\"<0xAC>\", null],\n[\"\u548c\", \"\u0438\"],\n[\"\u7ecf\", \"\u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e\"],\n[\"\u6d4e\", null],\n[\"\u7e41\", \"\u043f\u0440\u043e\u0446\u0432\u0435\u0442\u0430\u043d\u0438\u044f\"],\n[\"<0xE8>\", null],\n[\"<0x8D>\", null],\n[\"<0xA3>\", null],\n[\"\u7684\", null],\n[\"\u89d2\", \"\u0442\u043e\u0447\u043a\u0438\"],\n[\"\u5ea6\", null],\n[\"\u6765\", null],\n[\"\u770b\", \"\u0437\u0440\u0435\u043d\u0438\u044f\"],\n[\"\uff0c\", null],\n[\"\u5728\", \"\u043d\u0430\"],\n[\"\u82f1\", \"\u0411\u0440\u0435\u043a\u0441\u0438\u0442\u0430\"],\n[\"\u56fd\", null],\n[\"<0xE8>\", null],\n[\"<0x84>\", null],\n[\"<0xB1>\", null],\n[\"<0xE6>\", null],\n[\"<0xAC>\", null],\n[\"<0xA7>\", null],\n[\"\uff0c\", null],\n[\"\u751f\", \"\u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u0438\"],\n[\"\u6d3b\", null],\n[\"\u6210\", null],\n[\"\u672c\", null],\n[\"\u4e0a\", \"\u0436\u0438\u0437\u043d\u0438\"],\n[\"<0xE6>\", null],\n[\"<0xB6>\", null],\n[\"<0xA8>\", null],\n[\"\u548c\", \"\u0438\"],\n[\"\u5236\", \"\u0434\u043e\u043b\u0433\u043e\u0441\u0440\u043e\u0447\u043d\u043e\u0433\u043e\"],\n[\"\u9020\", null],\n[\"\u4e1a\", null],\n[\"\u957f\", null],\n[\"\u671f\", null],\n[\"<0xE8>\", null],\n[\"<0xA1>\", null],\n[\"<0xB0>\", null],\n[\"\u9000\", \"\u0441\u043f\u0430\u0434\u0430\"],\n[\"\u7684\", null],\n[\"\u80cc\", \"\u0444\u043e\u043d\u0435\"],\n[\"\u666f\", null],\n[\"\u4e0b\", null],\n[\"\uff0c\", null],\n[\"\u683c\", \"\u0412\u043e\u0441\u0442\u043e\u0447\u043d\u043e\u043c\u0443\"],\n[\"\u62c9\", null],\n[\"\u65af\", null],\n[\"\u54e5\", null],\n[\"\u4e1c\", null],\n[\"\u90e8\", null],\n[\"\u5730\", \"\u0440\u0430\u0439\u043e\u043d\u0443\"],\n[\"\u533a\", null],\n[\"<0xE8>\", null],\n[\"<0xBF>\", null],\n[\"<0xAB>\", null],\n[\"\u5207\", \"\u0441\u0440\u043e\u0447\u043d\u043e\"],\n[\"\u9700\", \"\u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f\"],\n[\"\u8981\", null],\n[\"<0xE9>\", null],\n[\"<0xBA>\", null],\n[\"<0xA6>\", null],\n[\"\u7ef4\", \"McVitie's\"],\n[\"\u4ed6\", null],\n[\"\u4e0d\", \"\u043f\u043e\u0441\u0442\u043e\u044f\u043d\u043d\u043e\"],\n[\"\u65ad\", null],\n[\"\u6269\", \"\u0440\u0430\u0441\u0448\u0438\u0440\u044f\u0442\u044c\"],\n[\"\u5927\", null],\n[\"\u751f\", \"\u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0441\u0442\u0432\u0435\u043d\u043d\u0443\u044e\"],\n[\"\u4ea7\", null],\n[\"\u7f51\", \"\u0441\u0435\u0442\u044c\"],\n[\"\u7edc\", null],\n[\"\u3002\", null],\n[\"\"\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u526f\u603b\u7406\u7a46\u7279\u79d1\u5728\u8bc4\u8bba\u4e16\u754c\u53cd\u5174\u594b\u5242\u673a\u6784\u8def\u7ebf\u56fe\u300a\u9075\u5b88\u89c4\u5b9a\u300b\u7684\u7b2c\u4e8c\u90e8\u5206\u65f6\u505a\u51fa\u4e0a\u8ff0\u8868\u8ff0\u3002\u8be5\u90e8\u5206\u89c4\u5b9a\u4e86\u6062\u590d RUSADA \u6743\u5229\u7684\u6807\u51c6\uff0c\u5e76\u4e8e\u6700\u8fd1\u5728\u8be5\u7ec4\u7ec7\u7f51\u7ad9\u4e0a\u53d1\u5e03\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u526f', '\u603b', '\u7406', '<0xE7>', '<0xA9>', '<0x86>', '\u7279', '\u79d1', '\u5728', '\u8bc4', '\u8bba', '\u4e16', '\u754c', '\u53cd', '\u5174', '<0xE5>', '<0xA5>', '<0x8B>', '<0xE5>', '<0x89>', '<0x82>', '\u673a', '\u6784', '\u8def', '\u7ebf', '\u56fe', '\u300a', '<0xE9>', '<0x81>', '<0xB5>', '\u5b88', '\u89c4', '\u5b9a', '\u300b', '\u7684', '\u7b2c', '\u4e8c', '\u90e8', '\u5206', '\u65f6', '\u505a', '\u51fa', '\u4e0a', '\u8ff0', '\u8868', '\u8ff0', '\u3002', '\u8be5', '\u90e8', '\u5206', '\u89c4', '\u5b9a', '\u4e86', '\u6062', '\u590d', '\u2581R', 'US', 'AD', 'A', '\u2581', '\u6743', '\u5229', '\u7684', '\u6807', '\u51c6', '\uff0c', '\u5e76', '\u4e8e', '\u6700', '\u8fd1', '\u5728', '\u8be5', '\u7ec4', '\u7ec7', '\u7f51', '\u7ad9', '\u4e0a', '\u53d1', '\u5e03', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u526f\", \"Deputy\"],\n  [\"\u603b\", null],\n  [\"\u7406\", null],\n  [\"<0xE7>\", null],\n  [\"<0xA9>\", null],\n  [\"<0x86>\", null],\n  [\"\u7279\", null],\n  [\"\u79d1\", null],\n  [\"\u5728\", null],\n  [\"\u8bc4\", \"commented\"],\n  [\"\u8bba\", null],\n  [\"\u4e16\", \"World\"],\n  [\"\u754c\", null],\n  [\"\u53cd\", \"Anti\"],\n  [\"\u5174\", null],\n  [\"<0xE5>\", null],\n  [\"<0xA5>\", null],\n  [\"<0x8B>\", null],\n  [\"<0xE5>\", null],\n  [\"<0x89>\", null],\n  [\"<0x82>\", null],\n  [\"\u673a\", \"Agency\"],\n  [\"\u6784\", null],\n  [\"\u8def\", \"roadmap\"],\n  [\"\u7ebf\", null],\n  [\"\u56fe\", null],\n  [\"\u300a\", null],\n  [\"<0xE9>\", null],\n  [\"<0x81>\", null],\n  [\"<0xB5>\", null],\n  [\"\u5b88\", \"Compliance\"],\n  [\"\u89c4\", null],\n  [\"\u5b9a\", null],\n  [\"\u300b\", null],\n  [\"\u7684\", null],\n  [\"\u7b2c\", \"second\"],\n  [\"\u4e8c\", null],\n  [\"\u90e8\", \"part\"],\n  [\"\u5206\", null],\n  [\"\u65f6\", null],\n  [\"\u505a\", \"made\"],\n  [\"\u51fa\", null],\n  [\"\u4e0a\", \"above\"],\n  [\"\u8ff0\", null],\n  [\"\u8868\", \"statement\"],\n  [\"\u8ff0\", null],\n  [\"\u3002\", null],\n  [\"\u8be5\", \"This\"],\n  [\"\u90e8\", \"section\"],\n  [\"\u5206\", null],\n  [\"\u89c4\", \"stipulates\"],\n  [\"\u5b9a\", null],\n  [\"\u4e86\", null],\n  [\"\u6062\", \"restoration\"],\n  [\"\u590d\", null],\n  [\"\u2581R\", null],\n  [\"US\", null],\n  [\"AD\", null],\n  [\"A\", null],\n  [\"\u2581\", null],\n  [\"\u6743\", \"rights\"],\n  [\"\u5229\", null],\n  [\"\u7684\", null],\n  [\"\u6807\", \"criteria\"],\n  [\"\u51c6\", null],\n  [\"\uff0c\", null],\n  [\"\u5e76\", \"and\"],\n  [\"\u4e8e\", null],\n  [\"\u6700\", \"recently\"],\n  [\"\u8fd1\", null],\n  [\"\u5728\", null],\n  [\"\u8be5\", \"the\"],\n  [\"\u7ec4\", \"organization's\"],\n  [\"\u7ec7\", null],\n  [\"\u7f51\", \"website\"],\n  [\"\u7ad9\", null],\n  [\"\u4e0a\", null],\n  [\"\u53d1\", \"published\"],\n  [\"\u5e03\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u526f\u603b\u7406\u7a46\u7279\u79d1\u5728\u8bc4\u8bba\u4e16\u754c\u53cd\u5174\u594b\u5242\u673a\u6784\u8def\u7ebf\u56fe\u300a\u9075\u5b88\u89c4\u5b9a\u300b\u7684\u7b2c\u4e8c\u90e8\u5206\u65f6\u505a\u51fa\u4e0a\u8ff0\u8868\u8ff0\u3002\u8be5\u90e8\u5206\u89c4\u5b9a\u4e86\u6062\u590d RUSADA \u6743\u5229\u7684\u6807\u51c6\uff0c\u5e76\u4e8e\u6700\u8fd1\u5728\u8be5\u7ec4\u7ec7\u7f51\u7ad9\u4e0a\u53d1\u5e03\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u526f', '\u603b', '\u7406', '<0xE7>', '<0xA9>', '<0x86>', '\u7279', '\u79d1', '\u5728', '\u8bc4', '\u8bba', '\u4e16', '\u754c', '\u53cd', '\u5174', '<0xE5>', '<0xA5>', '<0x8B>', '<0xE5>', '<0x89>', '<0x82>', '\u673a', '\u6784', '\u8def', '\u7ebf', '\u56fe', '\u300a', '<0xE9>', '<0x81>', '<0xB5>', '\u5b88', '\u89c4', '\u5b9a', '\u300b', '\u7684', '\u7b2c', '\u4e8c', '\u90e8', '\u5206', '\u65f6', '\u505a', '\u51fa', '\u4e0a', '\u8ff0', '\u8868', '\u8ff0', '\u3002', '\u8be5', '\u90e8', '\u5206', '\u89c4', '\u5b9a', '\u4e86', '\u6062', '\u590d', '\u2581R', 'US', 'AD', 'A', '\u2581', '\u6743', '\u5229', '\u7684', '\u6807', '\u51c6', '\uff0c', '\u5e76', '\u4e8e', '\u6700', '\u8fd1', '\u5728', '\u8be5', '\u7ec4', '\u7ec7', '\u7f51', '\u7ad9', '\u4e0a', '\u53d1', '\u5e03', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u526f\", \"\u0412\u0438\u0446\u0435\"],\n  [\"\u603b\", null],\n  [\"\u7406\", null],\n  [\"<0xE7>\", null],\n  [\"<0xA9>\", null],\n  [\"<0x86>\", null],\n  [\"\u7279\", null],\n  [\"\u79d1\", null],\n  [\"\u5728\", null],\n  [\"\u8bc4\", \"\u0431\u0430\u0493\u0430\u043b\u0430\u0443\"],\n  [\"\u8bba\", null],\n  [\"\u4e16\", \"\u04d9\u043b\u0435\u043c\u0434\u0456\u043a\"],\n  [\"\u754c\", null],\n  [\"\u53cd\", \"\u049b\u0430\u0440\u0441\u044b\"],\n  [\"\u5174\", null],\n  [\"<0xE5>\", null],\n  [\"<0xA5>\", null],\n  [\"<0x8B>\", null],\n  [\"<0xE5>\", null],\n  [\"<0x89>\", null],\n  [\"<0x82>\", null],\n  [\"\u673a\", \"\u0430\u0433\u0435\u043d\u0442\u0442\u0456\u0433\u0456\"],\n  [\"\u6784\", null],\n  [\"\u8def\", \"\u0436\u043e\u043b\"],\n  [\"\u7ebf\", null],\n  [\"\u56fe\", \"\u043a\u0430\u0440\u0442\u0430\u0441\u044b\"],\n  [\"\u300a\", null],\n  [\"<0xE9>\", null],\n  [\"<0x81>\", null],\n  [\"<0xB5>\", null],\n  [\"\u5b88\", \"\u0441\u0430\u049b\u0442\u0430\u0443\"],\n  [\"\u89c4\", null],\n  [\"\u5b9a\", null],\n  [\"\u300b\", null],\n  [\"\u7684\", null],\n  [\"\u7b2c\", null],\n  [\"\u4e8c\", \"\u0435\u043a\u0456\u043d\u0448\u0456\"],\n  [\"\u90e8\", \"\u0431\u04e9\u043b\u0456\u043c\"],\n  [\"\u5206\", null],\n  [\"\u65f6\", null],\n  [\"\u505a\", \"\u0436\u0430\u0441\u0430\u0434\u044b\"],\n  [\"\u51fa\", null],\n  [\"\u4e0a\", \"\u0436\u043e\u0493\u0430\u0440\u044b\u0434\u0430\u0493\u044b\"],\n  [\"\u8ff0\", null],\n  [\"\u8868\", \"\u043c\u04d9\u043b\u0456\u043c\u0434\u0435\u043c\u0435\"],\n  [\"\u8ff0\", null],\n  [\"\u3002\", null],\n  [\"\u8be5\", null],\n  [\"\u90e8\", \"\u0431\u04e9\u043b\u0456\u043c\"],\n  [\"\u5206\", null],\n  [\"\u89c4\", \"\u0431\u0435\u043b\u0433\u0456\u043b\u0435\u0439\u0434\u0456\"],\n  [\"\u5b9a\", null],\n  [\"\u4e86\", null],\n  [\"\u6062\", \"\u049b\u0430\u043b\u043f\u044b\u043d\u0430\"],\n  [\"\u590d\", null],\n  [\"\u2581R\", null],\n  [\"US\", null],\n  [\"AD\", null],\n  [\"A\", null],\n  [\"\u2581\", null],\n  [\"\u6743\", \"\u049b\u04b1\u049b\u044b\u049b\u0442\u0430\u0440\u044b\u043d\"],\n  [\"\u5229\", null],\n  [\"\u7684\", null],\n  [\"\u6807\", \"\u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0442\u0430\u0440\u044b\u043d\"],\n  [\"\u51c6\", null],\n  [\"\uff0c\", null],\n  [\"\u5e76\", null],\n  [\"\u4e8e\", null],\n  [\"\u6700\", \"\u0436\u0430\u049b\u044b\u043d\u0434\u0430\"],\n  [\"\u8fd1\", null],\n  [\"\u5728\", null],\n  [\"\u8be5\", null],\n  [\"\u7ec4\", \"\u04b1\u0439\u044b\u043c\u043d\u044b\u04a3\"],\n  [\"\u7ec7\", null],\n  [\"\u7f51\", \"\u0432\u0435\u0431-\u0441\u0430\u0439\u0442\u044b\u043d\u0434\u0430\"],\n  [\"\u7ad9\", null],\n  [\"\u4e0a\", null],\n  [\"\u53d1\", \"\u0436\u0430\u0440\u0438\u044f\u043b\u0430\u043d\u0434\u044b\"],\n  [\"\u5e03\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u526f\u603b\u7406\u7a46\u7279\u79d1\u5728\u8bc4\u8bba\u4e16\u754c\u53cd\u5174\u594b\u5242\u673a\u6784\u8def\u7ebf\u56fe\u300a\u9075\u5b88\u89c4\u5b9a\u300b\u7684\u7b2c\u4e8c\u90e8\u5206\u65f6\u505a\u51fa\u4e0a\u8ff0\u8868\u8ff0\u3002\u8be5\u90e8\u5206\u89c4\u5b9a\u4e86\u6062\u590d RUSADA \u6743\u5229\u7684\u6807\u51c6\uff0c\u5e76\u4e8e\u6700\u8fd1\u5728\u8be5\u7ec4\u7ec7\u7f51\u7ad9\u4e0a\u53d1\u5e03\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u526f', '\u603b', '\u7406', '<0xE7>', '<0xA9>', '<0x86>', '\u7279', '\u79d1', '\u5728', '\u8bc4', '\u8bba', '\u4e16', '\u754c', '\u53cd', '\u5174', '<0xE5>', '<0xA5>', '<0x8B>', '<0xE5>', '<0x89>', '<0x82>', '\u673a', '\u6784', '\u8def', '\u7ebf', '\u56fe', '\u300a', '<0xE9>', '<0x81>', '<0xB5>', '\u5b88', '\u89c4', '\u5b9a', '\u300b', '\u7684', '\u7b2c', '\u4e8c', '\u90e8', '\u5206', '\u65f6', '\u505a', '\u51fa', '\u4e0a', '\u8ff0', '\u8868', '\u8ff0', '\u3002', '\u8be5', '\u90e8', '\u5206', '\u89c4', '\u5b9a', '\u4e86', '\u6062', '\u590d', '\u2581R', 'US', 'AD', 'A', '\u2581', '\u6743', '\u5229', '\u7684', '\u6807', '\u51c6', '\uff0c', '\u5e76', '\u4e8e', '\u6700', '\u8fd1', '\u5728', '\u8be5', '\u7ec4', '\u7ec7', '\u7f51', '\u7ad9', '\u4e0a', '\u53d1', '\u5e03', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u526f\", \"\u0412\u0438\u0446\u0435\"],\n  [\"\u603b\", null],\n  [\"\u7406\", null],\n  [\"<0xE7>\", null],\n  [\"<0xA9>\", null],\n  [\"<0x86>\", null],\n  [\"\u7279\", null],\n  [\"\u79d1\", \"\u041c\u0443\u0442\u043a\u043e\"],\n  [\"\u5728\", null],\n  [\"\u8bc4\", \"\u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0438\u0440\u0443\u044f\"],\n  [\"\u8bba\", null],\n  [\"\u4e16\", \"\u0412\u0441\u0435\u043c\u0438\u0440\u043d\u043e\u0433\u043e\"],\n  [\"\u754c\", null],\n  [\"\u53cd\", \"\u0430\u043d\u0442\u0438\"],\n  [\"\u5174\", null],\n  [\"<0xE5>\", null],\n  [\"<0xA5>\", null],\n  [\"<0x8B>\", null],\n  [\"<0xE5>\", null],\n  [\"<0x89>\", null],\n  [\"<0x82>\", null],\n  [\"\u673a\", \"\u0430\u0433\u0435\u043d\u0442\u0441\u0442\u0432\u0430\"],\n  [\"\u6784\", null],\n  [\"\u8def\", \"\u0434\u043e\u0440\u043e\u0436\u043d\u0443\u044e\"],\n  [\"\u7ebf\", null],\n  [\"\u56fe\", \"\u043a\u0430\u0440\u0442\u0443\"],\n  [\"\u300a\", null],\n  [\"<0xE9>\", null],\n  [\"<0x81>\", null],\n  [\"<0xB5>\", null],\n  [\"\u5b88\", \"\u0421\u043e\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u0435\"],\n  [\"\u89c4\", null],\n  [\"\u5b9a\", null],\n  [\"\u300b\", null],\n  [\"\u7684\", null],\n  [\"\u7b2c\", \"\u0432\u0442\u043e\u0440\u043e\u0439\"],\n  [\"\u4e8c\", null],\n  [\"\u90e8\", \"\u0447\u0430\u0441\u0442\u0438\"],\n  [\"\u5206\", null],\n  [\"\u65f6\", null],\n  [\"\u505a\", \"\u0441\u0434\u0435\u043b\u0430\u043b\"],\n  [\"\u51fa\", null],\n  [\"\u4e0a\", \"\u0432\u044b\u0448\u0435\u0443\u043f\u043e\u043c\u044f\u043d\u0443\u0442\u043e\u0435\"],\n  [\"\u8ff0\", null],\n  [\"\u8868\", \"\u0437\u0430\u044f\u0432\u043b\u0435\u043d\u0438\u0435\"],\n  [\"\u8ff0\", null],\n  [\"\u3002\", null],\n  [\"\u8be5\", \"\u042d\u0442\u0430\"],\n  [\"\u90e8\", \"\u0447\u0430\u0441\u0442\u044c\"],\n  [\"\u5206\", null],\n  [\"\u89c4\", \"\u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u0442\"],\n  [\"\u5b9a\", null],\n  [\"\u4e86\", null],\n  [\"\u6062\", \"\u0432\u043e\u0441\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u044f\"],\n  [\"\u590d\", null],\n  [\"\u2581R\", null],\n  [\"US\", null],\n  [\"AD\", null],\n  [\"A\", null],\n  [\"\u2581\", null],\n  [\"\u6743\", \"\u043f\u0440\u0430\u0432\"],\n  [\"\u5229\", null],\n  [\"\u7684\", null],\n  [\"\u6807\", \"\u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0438\"],\n  [\"\u51c6\", null],\n  [\"\uff0c\", null],\n  [\"\u5e76\", \"\u0438\"],\n  [\"\u4e8e\", null],\n  [\"\u6700\", \"\u043d\u0435\u0434\u0430\u0432\u043d\u043e\"],\n  [\"\u8fd1\", null],\n  [\"\u5728\", null],\n  [\"\u8be5\", \"\u044d\u0442\u043e\u0439\"],\n  [\"\u7ec4\", \"\u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u0438\"],\n  [\"\u7ec7\", null],\n  [\"\u7f51\", \"\u0432\u0435\u0431-\u0441\u0430\u0439\u0442\u0435\"],\n  [\"\u7ad9\", null],\n  [\"\u4e0a\", null],\n  [\"\u53d1\", \"\u043e\u043f\u0443\u0431\u043b\u0438\u043a\u043e\u0432\u0430\u043d\u0430\"],\n  [\"\u5e03\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u8be5\u4fe1\u606f\u8868\u660e\u8be5\u76f4\u5347\u673a\u5c06\u4e8e 7 \u6708 17 \u65e5\u5927\u7ea6\u4e0a\u5348 7 \u70b9\u62b5\u8fbe\u5357\u8349\u576a\u53c2\u52a0\u4e0b\u5348 3 \u70b9\u7684\u6d3b\u52a8\uff0c\u5728\u5f53\u5929\u665a\u4e0a 6 \u70b9\u81f3 9 \u70b9\u8349\u576a\u6e05\u7a7a\u540e\u624d\u79bb\u5f00\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u8be5', '\u4fe1', '\u606f', '\u8868', '\u660e', '\u8be5', '\u76f4', '\u5347', '\u673a', '\u5c06', '\u4e8e', '\u2581', '7', '\u2581', '\u6708', '\u2581', '1', '7', '\u2581', '\u65e5', '\u5927', '\u7ea6', '\u4e0a', '\u5348', '\u2581', '7', '\u2581', '\u70b9', '<0xE6>', '<0x8A>', '<0xB5>', '\u8fbe', '\u5357', '\u8349', '<0xE5>', '<0x9D>', '<0xAA>', '\u53c2', '\u52a0', '\u4e0b', '\u5348', '\u2581', '3', '\u2581', '\u70b9', '\u7684', '\u6d3b', '\u52a8', '\uff0c', '\u5728', '\u5f53', '\u5929', '\u665a', '\u4e0a', '\u2581', '6', '\u2581', '\u70b9', '\u81f3', '\u2581', '9', '\u2581', '\u70b9', '\u8349', '<0xE5>', '<0x9D>', '<0xAA>', '\u6e05', '\u7a7a', '\u540e', '\u624d', '\u79bb', '\u5f00', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u8be5\", \"This\"],\n  [\"\u4fe1\", null],\n  [\"\u606f\", \"information\"],\n  [\"\u8868\", null],\n  [\"\u660e\", \"indicates\"],\n  [\"\u8be5\", \"the\"],\n  [\"\u76f4\", null],\n  [\"\u5347\", null],\n  [\"\u673a\", \"helicopter\"],\n  [\"\u5c06\", \"will\"],\n  [\"\u4e8e\", null],\n  [\"\u2581\", null],\n  [\"7\", null],\n  [\"\u2581\", null],\n  [\"\u6708\", \"July\"],\n  [\"\u2581\", null],\n  [\"1\", null],\n  [\"7\", null],\n  [\"\u2581\", null],\n  [\"\u65e5\", \"day\"],\n  [\"\u5927\", null],\n  [\"\u7ea6\", \"approximately\"],\n  [\"\u4e0a\", null],\n  [\"\u5348\", \"morning\"],\n  [\"\u2581\", null],\n  [\"7\", null],\n  [\"\u2581\", null],\n  [\"\u70b9\", \"o'clock\"],\n  [\"<0xE6>\", null],\n  [\"<0x8A>\", null],\n  [\"<0xB5>\", null],\n  [\"\u8fbe\", \"arrive\"],\n  [\"\u5357\", \"South\"],\n  [\"\u8349\", null],\n  [\"<0xE5>\", null],\n  [\"<0x9D>\", null],\n  [\"<0xAA>\", \"Lawn\"],\n  [\"\u53c2\", null],\n  [\"\u52a0\", \"attend\"],\n  [\"\u4e0b\", null],\n  [\"\u5348\", \"afternoon\"],\n  [\"\u2581\", null],\n  [\"3\", null],\n  [\"\u2581\", null],\n  [\"\u70b9\", \"o'clock\"],\n  [\"\u7684\", null],\n  [\"\u6d3b\", null],\n  [\"\u52a8\", \"event\"],\n  [\"\uff0c\", null],\n  [\"\u5728\", null],\n  [\"\u5f53\", null],\n  [\"\u5929\", \"day\"],\n  [\"\u665a\", null],\n  [\"\u4e0a\", \"evening\"],\n  [\"\u2581\", null],\n  [\"6\", null],\n  [\"\u2581\", null],\n  [\"\u70b9\", \"o'clock\"],\n  [\"\u81f3\", \"to\"],\n  [\"\u2581\", null],\n  [\"9\", null],\n  [\"\u2581\", null],\n  [\"\u70b9\", \"o'clock\"],\n  [\"\u8349\", null],\n  [\"<0xE5>\", null],\n  [\"<0x9D>\", null],\n  [\"<0xAA>\", \"lawn\"],\n  [\"\u6e05\", null],\n  [\"\u7a7a\", \"cleared\"],\n  [\"\u540e\", \"after\"],\n  [\"\u624d\", \"only\"],\n  [\"\u79bb\", null],\n  [\"\u5f00\", \"leave\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u8be5\u4fe1\u606f\u8868\u660e\u8be5\u76f4\u5347\u673a\u5c06\u4e8e 7 \u6708 17 \u65e5\u5927\u7ea6\u4e0a\u5348 7 \u70b9\u62b5\u8fbe\u5357\u8349\u576a\u53c2\u52a0\u4e0b\u5348 3 \u70b9\u7684\u6d3b\u52a8\uff0c\u5728\u5f53\u5929\u665a\u4e0a 6 \u70b9\u81f3 9 \u70b9\u8349\u576a\u6e05\u7a7a\u540e\u624d\u79bb\u5f00\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u8be5', '\u4fe1', '\u606f', '\u8868', '\u660e', '\u8be5', '\u76f4', '\u5347', '\u673a', '\u5c06', '\u4e8e', '\u2581', '7', '\u2581', '\u6708', '\u2581', '1', '7', '\u2581', '\u65e5', '\u5927', '\u7ea6', '\u4e0a', '\u5348', '\u2581', '7', '\u2581', '\u70b9', '<0xE6>', '<0x8A>', '<0xB5>', '\u8fbe', '\u5357', '\u8349', '<0xE5>', '<0x9D>', '<0xAA>', '\u53c2', '\u52a0', '\u4e0b', '\u5348', '\u2581', '3', '\u2581', '\u70b9', '\u7684', '\u6d3b', '\u52a8', '\uff0c', '\u5728', '\u5f53', '\u5929', '\u665a', '\u4e0a', '\u2581', '6', '\u2581', '\u70b9', '\u81f3', '\u2581', '9', '\u2581', '\u70b9', '\u8349', '<0xE5>', '<0x9D>', '<0xAA>', '\u6e05', '\u7a7a', '\u540e', '\u624d', '\u79bb', '\u5f00', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"\u8be5\", \"\u0431\u04b1\u043b\"],\n[\"\u4fe1\", null],\n[\"\u606f\", \"\u0430\u049b\u043f\u0430\u0440\u0430\u0442\"],\n[\"\u8868\", null],\n[\"\u660e\", \"\u043a\u04e9\u0440\u0441\u0435\u0442\u0435\u0434\u0456\"],\n[\"\u8be5\", \"\u0431\u04b1\u043b\"],\n[\"\u76f4\", null],\n[\"\u5347\", null],\n[\"\u673a\", \"\u0442\u0456\u043a\u04b1\u0448\u0430\u049b\"],\n[\"\u5c06\", null],\n[\"\u4e8e\", null],\n[\"\u2581\", null],\n[\"7\", \"7\"],\n[\"\u2581\", null],\n[\"\u6708\", \"\u0430\u0439\"],\n[\"\u2581\", null],\n[\"1\", null],\n[\"7\", \"17\"],\n[\"\u2581\", null],\n[\"\u65e5\", \"\u043a\u04af\u043d\u0456\"],\n[\"\u5927\", null],\n[\"\u7ea6\", \"\u0448\u0430\u043c\u0430\u043c\u0435\u043d\"],\n[\"\u4e0a\", null],\n[\"\u5348\", \"\u0442\u0430\u04a3\u0493\u044b\"],\n[\"\u2581\", null],\n[\"7\", \"7\"],\n[\"\u2581\", null],\n[\"\u70b9\", \"\u0441\u0430\u0493\u0430\u0442\"],\n[\"<0xE6>\", null],\n[\"<0x8A>\", null],\n[\"<0xB5>\", null],\n[\"\u8fbe\", \"\u043a\u0435\u043b\u0456\u043f\"],\n[\"\u5357\", \"\u043e\u04a3\u0442\u04af\u0441\u0442\u0456\u043a\"],\n[\"\u8349\", null],\n[\"<0xE5>\", null],\n[\"<0x9D>\", null],\n[\"<0xAA>\", \"\u0430\u043b\u0430\u04a3\u044b\u043d\u0430\"],\n[\"\u53c2\", null],\n[\"\u52a0\", \"\u049b\u0430\u0442\u044b\u0441\u0430\u0434\u044b\"],\n[\"\u4e0b\", null],\n[\"\u5348\", \"\u0442\u04af\u0441\u0442\u0435\u043d\"],\n[\"\u2581\", null],\n[\"3\", \"3\"],\n[\"\u2581\", null],\n[\"\u70b9\", \"\u0441\u0430\u0493\u0430\u0442\u0442\u0430\u0493\u044b\"],\n[\"\u7684\", null],\n[\"\u6d3b\", null],\n[\"\u52a8\", \"\u0456\u0441-\u0448\u0430\u0440\u0430\u0493\u0430\"],\n[\"\uff0c\", null],\n[\"\u5728\", null],\n[\"\u5f53\", \"\u0441\u043e\u043b\"],\n[\"\u5929\", \"\u043a\u04af\u043d\u0456\"],\n[\"\u665a\", \"\u043a\u0435\u0448\u043a\u0456\"],\n[\"\u4e0a\", null],\n[\"\u2581\", null],\n[\"6\", \"6\"],\n[\"\u2581\", null],\n[\"\u70b9\", \"\u0441\u0430\u0493\u0430\u0442\u0442\u0430\u043d\"],\n[\"\u81f3\", null],\n[\"\u2581\", null],\n[\"9\", \"9\"],\n[\"\u2581\", null],\n[\"\u70b9\", \"\u0441\u0430\u0493\u0430\u0442\u049b\u0430\"],\n[\"\u8349\", null],\n[\"<0xE5>\", null],\n[\"<0x9D>\", null],\n[\"<0xAA>\", \"\u0430\u043b\u0430\u04a3\"],\n[\"\u6e05\", null],\n[\"\u7a7a\", \"\u0431\u043e\u0441\u0430\u0442\u044b\u043b\u0493\u0430\u043d\u043d\u0430\u043d\"],\n[\"\u540e\", \"\u043a\u0435\u0439\u0456\u043d\"],\n[\"\u624d\", \"\u0493\u0430\u043d\u0430\"],\n[\"\u79bb\", null],\n[\"\u5f00\", \"\u043a\u0435\u0442\u0435\u0434\u0456\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u8be5\u4fe1\u606f\u8868\u660e\u8be5\u76f4\u5347\u673a\u5c06\u4e8e 7 \u6708 17 \u65e5\u5927\u7ea6\u4e0a\u5348 7 \u70b9\u62b5\u8fbe\u5357\u8349\u576a\u53c2\u52a0\u4e0b\u5348 3 \u70b9\u7684\u6d3b\u52a8\uff0c\u5728\u5f53\u5929\u665a\u4e0a 6 \u70b9\u81f3 9 \u70b9\u8349\u576a\u6e05\u7a7a\u540e\u624d\u79bb\u5f00\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u8be5', '\u4fe1', '\u606f', '\u8868', '\u660e', '\u8be5', '\u76f4', '\u5347', '\u673a', '\u5c06', '\u4e8e', '\u2581', '7', '\u2581', '\u6708', '\u2581', '1', '7', '\u2581', '\u65e5', '\u5927', '\u7ea6', '\u4e0a', '\u5348', '\u2581', '7', '\u2581', '\u70b9', '<0xE6>', '<0x8A>', '<0xB5>', '\u8fbe', '\u5357', '\u8349', '<0xE5>', '<0x9D>', '<0xAA>', '\u53c2', '\u52a0', '\u4e0b', '\u5348', '\u2581', '3', '\u2581', '\u70b9', '\u7684', '\u6d3b', '\u52a8', '\uff0c', '\u5728', '\u5f53', '\u5929', '\u665a', '\u4e0a', '\u2581', '6', '\u2581', '\u70b9', '\u81f3', '\u2581', '9', '\u2581', '\u70b9', '\u8349', '<0xE5>', '<0x9D>', '<0xAA>', '\u6e05', '\u7a7a', '\u540e', '\u624d', '\u79bb', '\u5f00', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"\u8be5\", \"\u042d\u0442\u0430\"],\n[\"\u4fe1\", null],\n[\"\u606f\", \"\u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f\"],\n[\"\u8868\", null],\n[\"\u660e\", \"\u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442\"],\n[\"\u8be5\", \"\u0447\u0442\u043e\"],\n[\"\u76f4\", null],\n[\"\u5347\", null],\n[\"\u673a\", \"\u0432\u0435\u0440\u0442\u043e\u043b\u0435\u0442\"],\n[\"\u5c06\", null],\n[\"\u4e8e\", null],\n[\"\u2581\", null],\n[\"7\", \"7\"],\n[\"\u2581\", null],\n[\"\u6708\", \"\u0438\u044e\u043b\u044f\"],\n[\"\u2581\", null],\n[\"1\", \"1\"],\n[\"7\", \"7\"],\n[\"\u2581\", null],\n[\"\u65e5\", null],\n[\"\u5927\", null],\n[\"\u7ea6\", \"\u043e\u043a\u043e\u043b\u043e\"],\n[\"\u4e0a\", null],\n[\"\u5348\", \"\u0443\u0442\u0440\u0430\"],\n[\"\u2581\", null],\n[\"7\", \"7\"],\n[\"\u2581\", null],\n[\"\u70b9\", \"\u0447\u0430\u0441\u043e\u0432\"],\n[\"<0xE6>\", null],\n[\"<0x8A>\", null],\n[\"<0xB5>\", null],\n[\"\u8fbe\", \"\u043f\u0440\u0438\u0431\u0443\u0434\u0435\u0442\"],\n[\"\u5357\", null],\n[\"\u8349\", null],\n[\"<0xE5>\", null],\n[\"<0x9D>\", null],\n[\"<0xAA>\", \"\u043b\u0443\u0436\u0430\u0439\u043a\u0443\"],\n[\"\u53c2\", null],\n[\"\u52a0\", \"\u0434\u043b\u044f\"],\n[\"\u4e0b\", null],\n[\"\u5348\", \"\u043c\u0435\u0440\u043e\u043f\u0440\u0438\u044f\u0442\u0438\u044f\"],\n[\"\u2581\", null],\n[\"3\", \"3\"],\n[\"\u2581\", null],\n[\"\u70b9\", \"\u0447\u0430\u0441\u0430\"],\n[\"\u7684\", null],\n[\"\u6d3b\", null],\n[\"\u52a8\", null],\n[\"\uff0c\", null],\n[\"\u5728\", \"\u0438\"],\n[\"\u5f53\", null],\n[\"\u5929\", \"\u0432\u0435\u0447\u0435\u0440\u043e\u043c\"],\n[\"\u665a\", null],\n[\"\u4e0a\", null],\n[\"\u2581\", null],\n[\"6\", \"6\"],\n[\"\u2581\", null],\n[\"\u70b9\", \"\u0447\u0430\u0441\u043e\u0432\"],\n[\"\u81f3\", \"\u0434\u043e\"],\n[\"\u2581\", null],\n[\"9\", \"9\"],\n[\"\u2581\", null],\n[\"\u70b9\", \"\u0447\u0430\u0441\u043e\u0432\"],\n[\"\u8349\", null],\n[\"<0xE5>\", null],\n[\"<0x9D>\", null],\n[\"<0xAA>\", \"\u043b\u0443\u0436\u0430\u0439\u043a\u0430\"],\n[\"\u6e05\", null],\n[\"\u7a7a\", \"\u043e\u0441\u0432\u043e\u0431\u043e\u0434\u0438\u0442\u0441\u044f\"],\n[\"\u540e\", null],\n[\"\u624d\", \"\u0442\u043e\u043b\u044c\u043a\u043e\"],\n[\"\u79bb\", null],\n[\"\u5f00\", \"\u0443\u043b\u0435\u0442\u0438\u0442\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u622a\u81f32017\u5e741\u6708\uff0c\u91cd\u5e86\u5168\u5e02\u5171\u5b8c\u94bb\u9875\u5ca9\u6c14\u4e95368\u53e3(\u4e0d\u542b\u53c2\u6570\u4e95)\uff0c\u6295\u4ea7261\u53e3\uff0c\u8fd1\u671f\uff0c\u968f\u7740\u4e2d\u56fd\u77f3\u5316\u5357\u5ddd\u533a\u5e73\u6865\u5357\u533a\u57df\u51854\u53e3\u9875\u5ca9\u6c14\u63a2\u4e95\u6d4b\u8bd5\u65e5\u4ea7\u91cf\u5747\u8d85\u8fc720\u4e07\u7acb\u65b9\u7c73\u7684\u597d\u6d88\u606f\u4f20\u6765\uff0c\u5168\u7403\u5317\u7f8e\u4ee5\u5916\u9996\u4e2a\u5546\u4e1a\u5316\u5f00\u91c7\u7684\u5927\u578b\u9875\u5ca9\u6c14\u7530\u4e2d\u56fd\u77f3\u5316\u6daa\u9675\u9875\u5ca9\u6c14\u7530\u6709\u671b\u6309\u7167\u8ba1\u5212\u5728\u4eca\u5e74\u5efa\u6210100\u4ebf\u7acb\u65b9\u7c73\u4ea7\u80fd\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u622a', '\u81f3', '2', '0', '1', '7', '\u5e74', '1', '\u6708', '\uff0c', '\u91cd', '\u5e86', '\u5168', '\u5e02', '\u5171', '\u5b8c', '<0xE9>', '<0x92>', '<0xBB>', '\u9875', '\u5ca9', '\u6c14', '\u4e95', '3', '6', '8', '\u53e3', '(', '\u4e0d', '\u542b', '\u53c2', '\u6570', '\u4e95', ')', '\uff0c', '\u6295', '\u4ea7', '2', '6', '1', '\u53e3', '\uff0c', '\u8fd1', '\u671f', '\uff0c', '\u968f', '\u7740', '\u4e2d', '\u56fd', '\u77f3', '\u5316', '\u5357', '\u5ddd', '\u533a', '\u5e73', '\u6865', '\u5357', '\u533a', '\u57df', '\u5185', '4', '\u53e3', '\u9875', '\u5ca9', '\u6c14', '\u63a2', '\u4e95', '\u6d4b', '\u8bd5', '\u65e5', '\u4ea7', '\u91cf', '\u5747', '\u8d85', '\u8fc7', '2', '0', '\u4e07', '\u7acb', '\u65b9', '\u7c73', '\u7684', '\u597d', '\u6d88', '\u606f', '\u4f20', '\u6765', '\uff0c', '\u5168', '\u7403', '\u5317', '\u7f8e', '\u4ee5', '\u5916', '\u9996', '\u4e2a', '\u5546', '\u4e1a', '\u5316', '\u5f00', '\u91c7', '\u7684', '\u5927', '\u578b', '\u9875', '\u5ca9', '\u6c14', '\u7530', '\u4e2d', '\u56fd', '\u77f3', '\u5316', '<0xE6>', '<0xB6>', '<0xAA>', '\u9675', '\u9875', '\u5ca9', '\u6c14', '\u7530', '\u6709', '\u671b', '\u6309', '\u7167', '\u8ba1', '\u5212', '\u5728', '\u4eca', '\u5e74', '\u5efa', '\u6210', '1', '0', '0', '<0xE4>', '<0xBA>', '<0xBF>', '\u7acb', '\u65b9', '\u7c73', '\u4ea7', '\u80fd', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"\u622a\", \"As\"],\n[\"\u81f3\", null],\n[\"2\", null],\n[\"0\", null],\n[\"1\", null],\n[\"7\", null],\n[\"\u5e74\", \"of\"],\n[\"1\", null],\n[\"\u6708\", \"January\"],\n[\"\uff0c\", null],\n[\"\u91cd\", \"Chongqing\"],\n[\"\u5e86\", null],\n[\"\u5168\", \"entire\"],\n[\"\u5e02\", \"city\"],\n[\"\u5171\", \"total\"],\n[\"\u5b8c\", \"completed\"],\n[\"<0xE9>\", null],\n[\"<0x92>\", null],\n[\"<0xBB>\", null],\n[\"\u9875\", \"shale\"],\n[\"\u5ca9\", null],\n[\"\u6c14\", \"gas\"],\n[\"\u4e95\", \"wells\"],\n[\"3\", null],\n[\"6\", null],\n[\"8\", null],\n[\"\u53e3\", null],\n[\"(\", null],\n[\"\u4e0d\", \"excluding\"],\n[\"\u542b\", null],\n[\"\u53c2\", \"parameter\"],\n[\"\u6570\", null],\n[\"\u4e95\", \"wells\"],\n[\")\", null],\n[\"\uff0c\", null],\n[\"\u6295\", \"put\"],\n[\"\u4ea7\", \"into production\"],\n[\"2\", null],\n[\"6\", null],\n[\"1\", null],\n[\"\u53e3\", null],\n[\"\uff0c\", null],\n[\"\u8fd1\", \"Recently\"],\n[\"\u671f\", null],\n[\"\uff0c\", null],\n[\"\u968f\", \"following\"],\n[\"\u7740\", null],\n[\"\u4e2d\", \"Sinopec\"],\n[\"\u56fd\", null],\n[\"\u77f3\", null],\n[\"\u5316\", null],\n[\"\u5357\", \"Nanchuan\"],\n[\"\u5ddd\", null],\n[\"\u533a\", \"District\"],\n[\"\u5e73\", \"Pingqiao\"],\n[\"\u6865\", null],\n[\"\u5357\", \"South\"],\n[\"\u533a\", \"area\"],\n[\"\u57df\", null],\n[\"\u5185\", \"within\"],\n[\"4\", null],\n[\"\u53e3\", null],\n[\"\u9875\", \"shale\"],\n[\"\u5ca9\", null],\n[\"\u6c14\", \"gas\"],\n[\"\u63a2\", \"exploration\"],\n[\"\u4e95\", \"wells\"],\n[\"\u6d4b\", \"test\"],\n[\"\u8bd5\", null],\n[\"\u65e5\", \"daily\"],\n[\"\u4ea7\", \"production\"],\n[\"\u91cf\", null],\n[\"\u5747\", \"all\"],\n[\"\u8d85\", \"exceeding\"],\n[\"\u8fc7\", null],\n[\"2\", null],\n[\"0\", null],\n[\"\u4e07\", \"200,000\"],\n[\"\u7acb\", \"cubic\"],\n[\"\u65b9\", null],\n[\"\u7c73\", \"meters\"],\n[\"\u7684\", null],\n[\"\u597d\", \"good\"],\n[\"\u6d88\", \"news\"],\n[\"\u606f\", null],\n[\"\u4f20\", \"reported\"],\n[\"\u6765\", null],\n[\"\uff0c\", null],\n[\"\u5168\", \"world's\"],\n[\"\u7403\", null],\n[\"\u5317\", \"North\"],\n[\"\u7f8e\", \"America\"],\n[\"\u4ee5\", \"outside\"],\n[\"\u5916\", null],\n[\"\u9996\", \"first\"],\n[\"\u4e2a\", null],\n[\"\u5546\", \"commercialized\"],\n[\"\u4e1a\", null],\n[\"\u5316\", null],\n[\"\u5f00\", \"developed\"],\n[\"\u91c7\", null],\n[\"\u7684\", null],\n[\"\u5927\", \"large-scale\"],\n[\"\u578b\", null],\n[\"\u9875\", \"shale\"],\n[\"\u5ca9\", null],\n[\"\u6c14\", \"gas\"],\n[\"\u7530\", \"field\"],\n[\"\u4e2d\", \"Sinopec\"],\n[\"\u56fd\", null],\n[\"\u77f3\", null],\n[\"\u5316\", null],\n[\"<0xE6>\", null],\n[\"<0xB6>\", null],\n[\"<0xAA>\", null],\n[\"\u9675\", \"Fuling\"],\n[\"\u9875\", \"Shale\"],\n[\"\u5ca9\", null],\n[\"\u6c14\", \"Gas\"],\n[\"\u7530\", \"Field\"],\n[\"\u6709\", \"is expected\"],\n[\"\u671b\", null],\n[\"\u6309\", \"according\"],\n[\"\u7167\", null],\n[\"\u8ba1\", \"plan\"],\n[\"\u5212\", null],\n[\"\u5728\", \"this\"],\n[\"\u4eca\", null],\n[\"\u5e74\", \"year\"],\n[\"\u5efa\", \"build\"],\n[\"\u6210\", null],\n[\"1\", null],\n[\"0\", null],\n[\"0\", null],\n[\"<0xE4>\", null],\n[\"<0xBA>\", null],\n[\"<0xBF>\", null],\n[\"\u7acb\", \"cubic\"],\n[\"\u65b9\", null],\n[\"\u7c73\", \"meters\"],\n[\"\u4ea7\", \"production\"],\n[\"\u80fd\", \"capacity\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u622a\u81f32017\u5e741\u6708\uff0c\u91cd\u5e86\u5168\u5e02\u5171\u5b8c\u94bb\u9875\u5ca9\u6c14\u4e95368\u53e3(\u4e0d\u542b\u53c2\u6570\u4e95)\uff0c\u6295\u4ea7261\u53e3\uff0c\u8fd1\u671f\uff0c\u968f\u7740\u4e2d\u56fd\u77f3\u5316\u5357\u5ddd\u533a\u5e73\u6865\u5357\u533a\u57df\u51854\u53e3\u9875\u5ca9\u6c14\u63a2\u4e95\u6d4b\u8bd5\u65e5\u4ea7\u91cf\u5747\u8d85\u8fc720\u4e07\u7acb\u65b9\u7c73\u7684\u597d\u6d88\u606f\u4f20\u6765\uff0c\u5168\u7403\u5317\u7f8e\u4ee5\u5916\u9996\u4e2a\u5546\u4e1a\u5316\u5f00\u91c7\u7684\u5927\u578b\u9875\u5ca9\u6c14\u7530\u4e2d\u56fd\u77f3\u5316\u6daa\u9675\u9875\u5ca9\u6c14\u7530\u6709\u671b\u6309\u7167\u8ba1\u5212\u5728\u4eca\u5e74\u5efa\u6210100\u4ebf\u7acb\u65b9\u7c73\u4ea7\u80fd\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u622a', '\u81f3', '2', '0', '1', '7', '\u5e74', '1', '\u6708', '\uff0c', '\u91cd', '\u5e86', '\u5168', '\u5e02', '\u5171', '\u5b8c', '<0xE9>', '<0x92>', '<0xBB>', '\u9875', '\u5ca9', '\u6c14', '\u4e95', '3', '6', '8', '\u53e3', '(', '\u4e0d', '\u542b', '\u53c2', '\u6570', '\u4e95', ')', '\uff0c', '\u6295', '\u4ea7', '2', '6', '1', '\u53e3', '\uff0c', '\u8fd1', '\u671f', '\uff0c', '\u968f', '\u7740', '\u4e2d', '\u56fd', '\u77f3', '\u5316', '\u5357', '\u5ddd', '\u533a', '\u5e73', '\u6865', '\u5357', '\u533a', '\u57df', '\u5185', '4', '\u53e3', '\u9875', '\u5ca9', '\u6c14', '\u63a2', '\u4e95', '\u6d4b', '\u8bd5', '\u65e5', '\u4ea7', '\u91cf', '\u5747', '\u8d85', '\u8fc7', '2', '0', '\u4e07', '\u7acb', '\u65b9', '\u7c73', '\u7684', '\u597d', '\u6d88', '\u606f', '\u4f20', '\u6765', '\uff0c', '\u5168', '\u7403', '\u5317', '\u7f8e', '\u4ee5', '\u5916', '\u9996', '\u4e2a', '\u5546', '\u4e1a', '\u5316', '\u5f00', '\u91c7', '\u7684', '\u5927', '\u578b', '\u9875', '\u5ca9', '\u6c14', '\u7530', '\u4e2d', '\u56fd', '\u77f3', '\u5316', '<0xE6>', '<0xB6>', '<0xAA>', '\u9675', '\u9875', '\u5ca9', '\u6c14', '\u7530', '\u6709', '\u671b', '\u6309', '\u7167', '\u8ba1', '\u5212', '\u5728', '\u4eca', '\u5e74', '\u5efa', '\u6210', '1', '0', '0', '<0xE4>', '<0xBA>', '<0xBF>', '\u7acb', '\u65b9', '\u7c73', '\u4ea7', '\u80fd', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u622a\", \"\u0434\u0435\u0439\u0456\u043d\"],\n  [\"\u81f3\", null],\n  [\"2\", null],\n  [\"0\", null],\n  [\"1\", null],\n  [\"7\", null],\n  [\"\u5e74\", \"\u0436\u044b\u043b\"],\n  [\"1\", null],\n  [\"\u6708\", \"\u0430\u0439\"],\n  [\"\uff0c\", null],\n  [\"\u91cd\", \"\u0427\u0443\u043d\u0446\u0438\u043d\"],\n  [\"\u5e86\", null],\n  [\"\u5168\", \"\u0431\u04af\u043a\u0456\u043b\"],\n  [\"\u5e02\", \"\u049b\u0430\u043b\u0430\"],\n  [\"\u5171\", \"\u0431\u0430\u0440\u043b\u044b\u0493\u044b\"],\n  [\"\u5b8c\", null],\n  [\"<0xE9>\", null],\n  [\"<0x92>\", null],\n  [\"<0xBB>\", null],\n  [\"\u9875\", \"\u0442\u0430\u049b\u0442\u0430\u0442\u0430\u0441\"],\n  [\"\u5ca9\", null],\n  [\"\u6c14\", \"\u0433\u0430\u0437\"],\n  [\"\u4e95\", \"\u04b1\u04a3\u0493\u044b\u043c\u0430\"],\n  [\"3\", null],\n  [\"6\", null],\n  [\"8\", null],\n  [\"\u53e3\", null],\n  [\"(\", null],\n  [\"\u4e0d\", \"\u049b\u043e\u0441\u043f\u0430\u0493\u0430\u043d\u0434\u0430\"],\n  [\"\u542b\", null],\n  [\"\u53c2\", \"\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043b\u0456\u043a\"],\n  [\"\u6570\", null],\n  [\"\u4e95\", \"\u04b1\u04a3\u0493\u044b\u043c\u0430\"],\n  [\")\", null],\n  [\"\uff0c\", null],\n  [\"\u6295\", \"\u04e9\u043d\u0434\u0456\u0440\u0456\u0441\u043a\u0435\"],\n  [\"\u4ea7\", \"\u049b\u043e\u0441\u044b\u043b\u0493\u0430\u043d\"],\n  [\"2\", null],\n  [\"6\", null],\n  [\"1\", null],\n  [\"\u53e3\", null],\n  [\"\uff0c\", null],\n  [\"\u8fd1\", \"\u0436\u0430\u049b\u044b\u043d\u0434\u0430\"],\n  [\"\u671f\", null],\n  [\"\uff0c\", null],\n  [\"\u968f\", null],\n  [\"\u7740\", null],\n  [\"\u4e2d\", \"\u049a\u044b\u0442\u0430\u0439\"],\n  [\"\u56fd\", null],\n  [\"\u77f3\", \"\u043c\u04b1\u043d\u0430\u0439-\u0445\u0438\u043c\u0438\u044f\"],\n  [\"\u5316\", null],\n  [\"\u5357\", \"\u041d\u0430\u043d\u044c\u0447\u0443\u0430\u043d\u044c\"],\n  [\"\u5ddd\", null],\n  [\"\u533a\", \"\u0430\u0443\u0434\u0430\u043d\u044b\"],\n  [\"\u5e73\", \"\u041f\u0438\u043d\u0446\u044f\u043e\"],\n  [\"\u6865\", null],\n  [\"\u5357\", \"\u043e\u04a3\u0442\u04af\u0441\u0442\u0456\u043a\"],\n  [\"\u533a\", \"\u0430\u0439\u043c\u0430\u0493\u044b\"],\n  [\"\u57df\", null],\n  [\"\u5185\", \"\u0456\u0448\u0456\u043d\u0434\u0435\"],\n  [\"4\", null],\n  [\"\u53e3\", null],\n  [\"\u9875\", \"\u0442\u0430\u049b\u0442\u0430\u0442\u0430\u0441\"],\n  [\"\u5ca9\", null],\n  [\"\u6c14\", \"\u0433\u0430\u0437\"],\n  [\"\u63a2\", \"\u0431\u0430\u0440\u043b\u0430\u0443\"],\n  [\"\u4e95\", \"\u04b1\u04a3\u0493\u044b\u043c\u0430\u0441\u044b\"],\n  [\"\u6d4b\", \"\u0441\u044b\u043d\u0430\u049b\"],\n  [\"\u8bd5\", null],\n  [\"\u65e5\", \"\u043a\u04af\u043d\u0434\u0456\u043a\"],\n  [\"\u4ea7\", \"\u04e9\u043d\u0434\u0456\u0440\u0456\u0441\"],\n  [\"\u91cf\", \"\u043a\u04e9\u043b\u0435\u043c\u0456\"],\n  [\"\u5747\", \"\u043e\u0440\u0442\u0430\u0448\u0430\"],\n  [\"\u8d85\", \"\u0430\u0441\u044b\u043f\"],\n  [\"\u8fc7\", \"\u0442\u04af\u0441\u0442\u0456\"],\n  [\"2\", null],\n  [\"0\", null],\n  [\"\u4e07\", \"\u043c\u044b\u04a3\"],\n  [\"\u7acb\", \"\u0442\u0435\u043a\u0448\u0435\"],\n  [\"\u65b9\", null],\n  [\"\u7c73\", \"\u043c\u0435\u0442\u0440\"],\n  [\"\u7684\", null],\n  [\"\u597d\", \"\u0436\u0430\u049b\u0441\u044b\"],\n  [\"\u6d88\", \"\u0436\u0430\u04a3\u0430\u043b\u044b\u049b\"],\n  [\"\u606f\", null],\n  [\"\u4f20\", \"\u043a\u0435\u043b\u0434\u0456\"],\n  [\"\u6765\", null],\n  [\"\uff0c\", null],\n  [\"\u5168\", \"\u04d9\u043b\u0435\u043c\u0434\u0435\u0433\u0456\"],\n  [\"\u7403\", null],\n  [\"\u5317\", \"\u0421\u043e\u043b\u0442\u04af\u0441\u0442\u0456\u043a\"],\n  [\"\u7f8e\", \"\u0410\u043c\u0435\u0440\u0438\u043a\u0430\"],\n  [\"\u4ee5\", \"\u0431\u0430\u0441\u049b\u0430\"],\n  [\"\u5916\", null],\n  [\"\u9996\", \"\u0430\u043b\u0493\u0430\u0448\u049b\u044b\"],\n  [\"\u4e2a\", null],\n  [\"\u5546\", \"\u043a\u043e\u043c\u043c\u0435\u0440\u0446\u0438\u044f\u043b\u044b\u049b\"],\n  [\"\u4e1a\", null],\n  [\"\u5316\", null],\n  [\"\u5f00\", \"\u0438\u0433\u0435\u0440\u0456\u043b\u0433\u0435\u043d\"],\n  [\"\u91c7\", null],\n  [\"\u7684\", null],\n  [\"\u5927\", \"\u0456\u0440\u0456\"],\n  [\"\u578b\", null],\n  [\"\u9875\", \"\u0442\u0430\u049b\u0442\u0430\u0442\u0430\u0441\"],\n  [\"\u5ca9\", null],\n  [\"\u6c14\", \"\u0433\u0430\u0437\"],\n  [\"\u7530\", \"\u043a\u0435\u043d \u043e\u0440\u043d\u044b\"],\n  [\"\u4e2d\", \"\u049a\u044b\u0442\u0430\u0439\"],\n  [\"\u56fd\", null],\n  [\"\u77f3\", \"\u043c\u04b1\u043d\u0430\u0439-\u0445\u0438\u043c\u0438\u044f\"],\n  [\"\u5316\", null],\n  [\"<0xE6>\", null],\n  [\"<0xB6>\", null],\n  [\"<0xAA>\", null],\n  [\"\u9675\", \"\u0424\u0443\u043b\u0438\u043d\"],\n  [\"\u9875\", \"\u0442\u0430\u049b\u0442\u0430\u0442\u0430\u0441\"],\n  [\"\u5ca9\", null],\n  [\"\u6c14\", \"\u0433\u0430\u0437\"],\n  [\"\u7530\", \"\u043a\u0435\u043d \u043e\u0440\u043d\u044b\"],\n  [\"\u6709\", \"\u04af\u043c\u0456\u0442\"],\n  [\"\u671b\", \"\u0431\u0430\u0440\"],\n  [\"\u6309\", \"\u0436\u043e\u0441\u043f\u0430\u0440\"],\n  [\"\u7167\", \"\u0431\u043e\u0439\u044b\u043d\u0448\u0430\"],\n  [\"\u8ba1\", null],\n  [\"\u5212\", null],\n  [\"\u5728\", null],\n  [\"\u4eca\", \"\u0431\u0438\u044b\u043b\"],\n  [\"\u5e74\", null],\n  [\"\u5efa\", \"\u0441\u0430\u043b\u044b\u043d\u044b\u043f\"],\n  [\"\u6210\", \"\u0431\u0456\u0442\u0435\u0434\u0456\"],\n  [\"1\", null],\n  [\"0\", null],\n  [\"0\", null],\n  [\"<0xE4>\", null],\n  [\"<0xBA>\", null],\n  [\"<0xBF>\", null],\n  [\"\u7acb\", \"\u0442\u0435\u043a\u0448\u0435\"],\n  [\"\u65b9\", null],\n  [\"\u7c73\", \"\u043c\u0435\u0442\u0440\"],\n  [\"\u4ea7\", \"\u04e9\u043d\u0434\u0456\u0440\u0443\"],\n  [\"\u80fd\", \"\u049b\u0443\u0430\u0442\u044b\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u622a\u81f32017\u5e741\u6708\uff0c\u91cd\u5e86\u5168\u5e02\u5171\u5b8c\u94bb\u9875\u5ca9\u6c14\u4e95368\u53e3(\u4e0d\u542b\u53c2\u6570\u4e95)\uff0c\u6295\u4ea7261\u53e3\uff0c\u8fd1\u671f\uff0c\u968f\u7740\u4e2d\u56fd\u77f3\u5316\u5357\u5ddd\u533a\u5e73\u6865\u5357\u533a\u57df\u51854\u53e3\u9875\u5ca9\u6c14\u63a2\u4e95\u6d4b\u8bd5\u65e5\u4ea7\u91cf\u5747\u8d85\u8fc720\u4e07\u7acb\u65b9\u7c73\u7684\u597d\u6d88\u606f\u4f20\u6765\uff0c\u5168\u7403\u5317\u7f8e\u4ee5\u5916\u9996\u4e2a\u5546\u4e1a\u5316\u5f00\u91c7\u7684\u5927\u578b\u9875\u5ca9\u6c14\u7530\u4e2d\u56fd\u77f3\u5316\u6daa\u9675\u9875\u5ca9\u6c14\u7530\u6709\u671b\u6309\u7167\u8ba1\u5212\u5728\u4eca\u5e74\u5efa\u6210100\u4ebf\u7acb\u65b9\u7c73\u4ea7\u80fd\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u622a', '\u81f3', '2', '0', '1', '7', '\u5e74', '1', '\u6708', '\uff0c', '\u91cd', '\u5e86', '\u5168', '\u5e02', '\u5171', '\u5b8c', '<0xE9>', '<0x92>', '<0xBB>', '\u9875', '\u5ca9', '\u6c14', '\u4e95', '3', '6', '8', '\u53e3', '(', '\u4e0d', '\u542b', '\u53c2', '\u6570', '\u4e95', ')', '\uff0c', '\u6295', '\u4ea7', '2', '6', '1', '\u53e3', '\uff0c', '\u8fd1', '\u671f', '\uff0c', '\u968f', '\u7740', '\u4e2d', '\u56fd', '\u77f3', '\u5316', '\u5357', '\u5ddd', '\u533a', '\u5e73', '\u6865', '\u5357', '\u533a', '\u57df', '\u5185', '4', '\u53e3', '\u9875', '\u5ca9', '\u6c14', '\u63a2', '\u4e95', '\u6d4b', '\u8bd5', '\u65e5', '\u4ea7', '\u91cf', '\u5747', '\u8d85', '\u8fc7', '2', '0', '\u4e07', '\u7acb', '\u65b9', '\u7c73', '\u7684', '\u597d', '\u6d88', '\u606f', '\u4f20', '\u6765', '\uff0c', '\u5168', '\u7403', '\u5317', '\u7f8e', '\u4ee5', '\u5916', '\u9996', '\u4e2a', '\u5546', '\u4e1a', '\u5316', '\u5f00', '\u91c7', '\u7684', '\u5927', '\u578b', '\u9875', '\u5ca9', '\u6c14', '\u7530', '\u4e2d', '\u56fd', '\u77f3', '\u5316', '<0xE6>', '<0xB6>', '<0xAA>', '\u9675', '\u9875', '\u5ca9', '\u6c14', '\u7530', '\u6709', '\u671b', '\u6309', '\u7167', '\u8ba1', '\u5212', '\u5728', '\u4eca', '\u5e74', '\u5efa', '\u6210', '1', '0', '0', '<0xE4>', '<0xBA>', '<0xBF>', '\u7acb', '\u65b9', '\u7c73', '\u4ea7', '\u80fd', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u622a\", \"\u041f\u043e\"],\n  [\"\u81f3\", null],\n  [\"2\", null],\n  [\"0\", null],\n  [\"1\", null],\n  [\"7\", null],\n  [\"\u5e74\", \"\u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u044e\"],\n  [\"1\", null],\n  [\"\u6708\", \"\u044f\u043d\u0432\u0430\u0440\u044f\"],\n  [\"\uff0c\", null],\n  [\"\u91cd\", \"\u0427\u0443\u043d\u0446\u0438\u043d\"],\n  [\"\u5e86\", null],\n  [\"\u5168\", null],\n  [\"\u5e02\", null],\n  [\"\u5171\", \"\u0432\u0441\u0435\u0433\u043e\"],\n  [\"\u5b8c\", \"\u0437\u0430\u0432\u0435\u0440\u0448\u0438\u043b\"],\n  [\"<0xE9>\", null],\n  [\"<0x92>\", null],\n  [\"<0xBB>\", null],\n  [\"\u9875\", \"\u0441\u043b\u0430\u043d\u0446\u0435\u0432\u044b\u0445\"],\n  [\"\u5ca9\", null],\n  [\"\u6c14\", null],\n  [\"\u4e95\", \"\u0441\u043a\u0432\u0430\u0436\u0438\u043d\"],\n  [\"3\", null],\n  [\"6\", null],\n  [\"8\", null],\n  [\"\u53e3\", null],\n  [\"(\", null],\n  [\"\u4e0d\", \"\u043d\u0435\"],\n  [\"\u542b\", \"\u0432\u043a\u043b\u044e\u0447\u0430\u044f\"],\n  [\"\u53c2\", \"\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u0435\"],\n  [\"\u6570\", null],\n  [\"\u4e95\", \"\u0441\u043a\u0432\u0430\u0436\u0438\u043d\u044b\"],\n  [\")\", null],\n  [\"\uff0c\", null],\n  [\"\u6295\", \"\u0432\u0432\u0435\u0434\u0435\u043d\u043e\"],\n  [\"\u4ea7\", null],\n  [\"2\", null],\n  [\"6\", null],\n  [\"1\", null],\n  [\"\u53e3\", null],\n  [\"\uff0c\", null],\n  [\"\u8fd1\", \"\u041d\u0435\u0434\u0430\u0432\u043d\u043e\"],\n  [\"\u671f\", null],\n  [\"\uff0c\", null],\n  [\"\u968f\", \"\u043f\u043e\u0441\u043b\u0435\"],\n  [\"\u7740\", null],\n  [\"\u4e2d\", \"\u041a\u0438\u0442\u0430\u0439\u0441\u043a\u043e\u0439\"],\n  [\"\u56fd\", null],\n  [\"\u77f3\", \"\u043d\u0435\u0444\u0442\u0435\u0445\u0438\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0439\"],\n  [\"\u5316\", null],\n  [\"\u5357\", \"\u041d\u0430\u043d\u044c\u0447\u0443\u0430\u043d\u044c\"],\n  [\"\u5ddd\", null],\n  [\"\u533a\", null],\n  [\"\u5e73\", \"\u041f\u0438\u043d\u0446\u044f\u043e\"],\n  [\"\u6865\", null],\n  [\"\u5357\", null],\n  [\"\u533a\", \"\u0440\u0430\u0439\u043e\u043d\u0435\"],\n  [\"\u57df\", null],\n  [\"\u5185\", null],\n  [\"4\", null],\n  [\"\u53e3\", null],\n  [\"\u9875\", \"\u0441\u043b\u0430\u043d\u0446\u0435\u0432\u043e\u0433\u043e\"],\n  [\"\u5ca9\", null],\n  [\"\u6c14\", null],\n  [\"\u63a2\", \"\u0440\u0430\u0437\u0432\u0435\u0434\u043e\u0447\u043d\u044b\u0445\"],\n  [\"\u4e95\", \"\u0441\u043a\u0432\u0430\u0436\u0438\u043d\"],\n  [\"\u6d4b\", \"\u0438\u0441\u043f\u044b\u0442\u0430\u043d\u0438\u044f\"],\n  [\"\u8bd5\", null],\n  [\"\u65e5\", \"\u0441\u0443\u0442\u043e\u0447\u043d\u044b\u0439\"],\n  [\"\u4ea7\", \"\u043e\u0431\u044a\u0435\u043c\"],\n  [\"\u91cf\", null],\n  [\"\u5747\", \"\u043f\u0440\u0435\u0432\u044b\u0441\u0438\u043b\"],\n  [\"\u8d85\", null],\n  [\"\u8fc7\", null],\n  [\"2\", null],\n  [\"0\", null],\n  [\"\u4e07\", \"200000\"],\n  [\"\u7acb\", \"\u043a\u0443\u0431\u043e\u043c\u0435\u0442\u0440\u043e\u0432\"],\n  [\"\u65b9\", null],\n  [\"\u7c73\", null],\n  [\"\u7684\", null],\n  [\"\u597d\", \"\u0445\u043e\u0440\u043e\u0448\u0430\u044f\"],\n  [\"\u6d88\", \"\u043d\u043e\u0432\u043e\u0441\u0442\u044c\"],\n  [\"\u606f\", null],\n  [\"\u4f20\", \"\u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0438\u043b\u0430\u0441\u044c\"],\n  [\"\u6765\", null],\n  [\"\uff0c\", null],\n  [\"\u5168\", \"\u043c\u0438\u0440\u0435\"],\n  [\"\u7403\", null],\n  [\"\u5317\", \"\u0421\u0435\u0432\u0435\u0440\u043d\u043e\u0439\"],\n  [\"\u7f8e\", \"\u0410\u043c\u0435\u0440\u0438\u043a\u0438\"],\n  [\"\u4ee5\", \"\u0437\u0430\"],\n  [\"\u5916\", \"\u043f\u0440\u0435\u0434\u0435\u043b\u0430\u043c\u0438\"],\n  [\"\u9996\", \"\u043f\u0435\u0440\u0432\u043e\u0435\"],\n  [\"\u4e2a\", null],\n  [\"\u5546\", \"\u043a\u043e\u043c\u043c\u0435\u0440\u0447\u0435\u0441\u043a\u043e\u0439\"],\n  [\"\u4e1a\", null],\n  [\"\u5316\", null],\n  [\"\u5f00\", \"\u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438\"],\n  [\"\u91c7\", null],\n  [\"\u7684\", null],\n  [\"\u5927\", \"\u043a\u0440\u0443\u043f\u043d\u043e\u0435\"],\n  [\"\u578b\", null],\n  [\"\u9875\", \"\u0441\u043b\u0430\u043d\u0446\u0435\u0432\u043e\u0435\"],\n  [\"\u5ca9\", null],\n  [\"\u6c14\", null],\n  [\"\u7530\", \"\u043c\u0435\u0441\u0442\u043e\u0440\u043e\u0436\u0434\u0435\u043d\u0438\u0435\"],\n  [\"\u4e2d\", \"\u041a\u0438\u0442\u0430\u0439\u0441\u043a\u043e\u0439\"],\n  [\"\u56fd\", null],\n  [\"\u77f3\", \"\u043d\u0435\u0444\u0442\u0435\u0445\u0438\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0439\"],\n  [\"\u5316\", null],\n  [\"<0xE6>\", null],\n  [\"<0xB6>\", null],\n  [\"<0xAA>\", null],\n  [\"\u9675\", \"\u0424\u0443\u043b\u0438\u043d\"],\n  [\"\u9875\", \"\u0441\u043b\u0430\u043d\u0446\u0435\u0432\u043e\u0435\"],\n  [\"\u5ca9\", null],\n  [\"\u6c14\", null],\n  [\"\u7530\", \"\u043c\u0435\u0441\u0442\u043e\u0440\u043e\u0436\u0434\u0435\u043d\u0438\u0435\"],\n  [\"\u6709\", \"\u043f\u043b\u0430\u043d\u0438\u0440\u0443\u0435\u0442\"],\n  [\"\u671b\", null],\n  [\"\u6309\", \"\u0441\u043e\u0433\u043b\u0430\u0441\u043d\u043e\"],\n  [\"\u7167\", null],\n  [\"\u8ba1\", \"\u043f\u043b\u0430\u043d\u0443\"],\n  [\"\u5212\", null],\n  [\"\u5728\", null],\n  [\"\u4eca\", \"\u044d\u0442\u043e\u043c\"],\n  [\"\u5e74\", \"\u0433\u043e\u0434\u0443\"],\n  [\"\u5efa\", \"\u0434\u043e\u0441\u0442\u0438\u0447\u044c\"],\n  [\"\u6210\", null],\n  [\"1\", null],\n  [\"0\", null],\n  [\"0\", null],\n  [\"<0xE4>\", null],\n  [\"<0xBA>\", null],\n  [\"<0xBF>\", null],\n  [\"\u7acb\", \"\u043a\u0443\u0431\u043e\u043c\u0435\u0442\u0440\u043e\u0432\"],\n  [\"\u65b9\", null],\n  [\"\u7c73\", null],\n  [\"\u4ea7\", \"\u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0439\"],\n  [\"\u80fd\", \"\u043c\u043e\u0449\u043d\u043e\u0441\u0442\u0438\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u7eff\u8272\u4ee3\u8868\u6bd4\u7279\u5e01\u7684\u5408\u6cd5\u5e02\u573a\uff1b\u6a58\u9ec4\u8272\u4ee3\u8868\u4e2d\u6027\u5e02\u573a\uff0c\u867d\u5c1a\u672a\u516c\u5f00\u5408\u6cd5\u5316\u6bd4\u7279\u5e01\u5730\u4f4d\uff0c\u4f46\u4e5f\u6ca1\u6709\u4e3b\u8981\u7684\u4f7f\u7528\u9650\u5236\uff1b\u6de1\u7c89\u8272\u4ee3\u8868\u53d7\u9650\u5236\u7684\u5e02\u573a\uff0c\u5373\u5f53\u5730\u653f\u5e9c\u5c1d\u8bd5\u901a\u8fc7\u7ea2\u7ebf\u653f\u7b56\u6216\u76d1\u7ba1\u6765\u65b9\u6cd5\u6570\u5b57\u8d27\u5e01\u7684\u4f7f\u7528\uff1b\u6df1\u7c89\u8272\u4ee3\u8868\u6bd4\u7279\u5e01\u88ab\u5b9a\u4e49\u4e3a\u975e\u6cd5\u7684\u56fd\u5bb6\uff1b\u7070\u8272\u662f\u5c1a\u672a\u7f6e\u8bc4\u7684\u56fd\u5bb6\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u5982', '\u4e0b', '\u56fe', '\u6240', '\u793a', '\uff0c', '<0xE7>', '<0xBB>', '<0xBF>', '\u8272', '\u4ee3', '\u8868', '\u6bd4', '\u7279', '\u5e01', '\u7684', '\u5408', '\u6cd5', '\u5e02', '\u573a', '\uff1b', '<0xE6>', '<0xA9>', '<0x98>', '\u9ec4', '\u8272', '\u4ee3', '\u8868', '\u4e2d', '\u6027', '\u5e02', '\u573a', '\uff0c', '<0xE8>', '<0x99>', '<0xBD>', '\u5c1a', '\u672a', '\u516c', '\u5f00', '\u5408', '\u6cd5', '\u5316', '\u6bd4', '\u7279', '\u5e01', '\u5730', '\u4f4d', '\uff0c', '\u4f46', '\u4e5f', '\u6ca1', '\u6709', '\u4e3b', '\u8981', '\u7684', '\u4f7f', '\u7528', '\u9650', '\u5236', '\uff1b', '<0xE6>', '<0xB7>', '<0xA1>', '\u7c89', '\u8272', '\u4ee3', '\u8868', '\u53d7', '\u9650', '\u5236', '\u7684', '\u5e02', '\u573a', '\uff0c', '\u5373', '\u5f53', '\u5730', '\u653f', '\u5e9c', '\u5c1d', '\u8bd5', '\u901a', '\u8fc7', '\u7ea2', '\u7ebf', '\u653f', '\u7b56', '\u6216', '\u76d1', '\u7ba1', '\u6765', '\u65b9', '\u6cd5', '\u6570', '\u5b57', '\u8d27', '\u5e01', '\u7684', '\u4f7f', '\u7528', '\uff1b', '\u6df1', '\u7c89', '\u8272', '\u4ee3', '\u8868', '\u6bd4', '\u7279', '\u5e01', '\u88ab', '\u5b9a', '\u4e49', '\u4e3a', '\u975e', '\u6cd5', '\u7684', '\u56fd', '\u5bb6', '\uff1b', '\u7070', '\u8272', '\u662f', '\u5c1a', '\u672a', '\u7f6e', '\u8bc4', '\u7684', '\u56fd', '\u5bb6', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u5982\", \"As\"],\n  [\"\u4e0b\", null],\n  [\"\u56fe\", \"shown\"],\n  [\"\u6240\", null],\n  [\"\u793a\", null],\n  [\"\uff0c\", null],\n  [\"<0xE7>\", null],\n  [\"<0xBB>\", null],\n  [\"<0xBF>\", null],\n  [\"\u8272\", \"color\"],\n  [\"\u4ee3\", null],\n  [\"\u8868\", null],\n  [\"\u6bd4\", \"Bitcoin\"],\n  [\"\u7279\", null],\n  [\"\u5e01\", null],\n  [\"\u7684\", null],\n  [\"\u5408\", \"legal\"],\n  [\"\u6cd5\", null],\n  [\"\u5e02\", \"market\"],\n  [\"\u573a\", null],\n  [\"\uff1b\", null],\n  [\"<0xE6>\", null],\n  [\"<0xA9>\", null],\n  [\"<0x98>\", null],\n  [\"\u9ec4\", \"orange\"],\n  [\"\u8272\", null],\n  [\"\u4ee3\", \"represents\"],\n  [\"\u8868\", null],\n  [\"\u4e2d\", \"neutral\"],\n  [\"\u6027\", null],\n  [\"\u5e02\", \"market\"],\n  [\"\u573a\", null],\n  [\"\uff0c\", null],\n  [\"<0xE8>\", null],\n  [\"<0x99>\", null],\n  [\"<0xBD>\", null],\n  [\"\u5c1a\", \"although\"],\n  [\"\u672a\", null],\n  [\"\u516c\", \"legalized\"],\n  [\"\u5f00\", null],\n  [\"\u5408\", null],\n  [\"\u6cd5\", null],\n  [\"\u5316\", null],\n  [\"\u6bd4\", \"Bitcoin\"],\n  [\"\u7279\", null],\n  [\"\u5e01\", null],\n  [\"\u5730\", \"status\"],\n  [\"\u4f4d\", null],\n  [\"\uff0c\", null],\n  [\"\u4f46\", \"but\"],\n  [\"\u4e5f\", null],\n  [\"\u6ca1\", \"no\"],\n  [\"\u6709\", null],\n  [\"\u4e3b\", \"major\"],\n  [\"\u8981\", null],\n  [\"\u7684\", null],\n  [\"\u4f7f\", \"usage\"],\n  [\"\u7528\", null],\n  [\"\u9650\", \"restrictions\"],\n  [\"\u5236\", null],\n  [\"\uff1b\", null],\n  [\"<0xE6>\", null],\n  [\"<0xB7>\", null],\n  [\"<0xA1>\", null],\n  [\"\u7c89\", \"pink\"],\n  [\"\u8272\", null],\n  [\"\u4ee3\", \"represents\"],\n  [\"\u8868\", null],\n  [\"\u53d7\", \"restricted\"],\n  [\"\u9650\", null],\n  [\"\u5236\", null],\n  [\"\u7684\", null],\n  [\"\u5e02\", \"market\"],\n  [\"\u573a\", null],\n  [\"\uff0c\", null],\n  [\"\u5373\", \"where\"],\n  [\"\u5f53\", \"local\"],\n  [\"\u5730\", null],\n  [\"\u653f\", \"government\"],\n  [\"\u5e9c\", null],\n  [\"\u5c1d\", \"attempts\"],\n  [\"\u8bd5\", null],\n  [\"\u901a\", \"through\"],\n  [\"\u8fc7\", null],\n  [\"\u7ea2\", \"red\"],\n  [\"\u7ebf\", \"line\"],\n  [\"\u653f\", \"policies\"],\n  [\"\u7b56\", null],\n  [\"\u6216\", \"or\"],\n  [\"\u76d1\", \"regulations\"],\n  [\"\u7ba1\", null],\n  [\"\u6765\", \"to\"],\n  [\"\u65b9\", \"prevent\"],\n  [\"\u6cd5\", null],\n  [\"\u6570\", \"digital\"],\n  [\"\u5b57\", null],\n  [\"\u8d27\", \"currency\"],\n  [\"\u5e01\", null],\n  [\"\u7684\", null],\n  [\"\u4f7f\", \"use\"],\n  [\"\u7528\", null],\n  [\"\uff1b\", null],\n  [\"\u6df1\", \"dark\"],\n  [\"\u7c89\", \"pink\"],\n  [\"\u8272\", null],\n  [\"\u4ee3\", \"represents\"],\n  [\"\u8868\", null],\n  [\"\u6bd4\", \"Bitcoin\"],\n  [\"\u7279\", null],\n  [\"\u5e01\", null],\n  [\"\u88ab\", \"is\"],\n  [\"\u5b9a\", \"defined\"],\n  [\"\u4e49\", null],\n  [\"\u4e3a\", \"as\"],\n  [\"\u975e\", \"illegal\"],\n  [\"\u6cd5\", null],\n  [\"\u7684\", null],\n  [\"\u56fd\", \"countries\"],\n  [\"\u5bb6\", null],\n  [\"\uff1b\", null],\n  [\"\u7070\", \"gray\"],\n  [\"\u8272\", null],\n  [\"\u662f\", \"is\"],\n  [\"\u5c1a\", \"yet\"],\n  [\"\u672a\", null],\n  [\"\u7f6e\", \"commented\"],\n  [\"\u8bc4\", null],\n  [\"\u7684\", null],\n  [\"\u56fd\", \"countries\"],\n  [\"\u5bb6\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u7eff\u8272\u4ee3\u8868\u6bd4\u7279\u5e01\u7684\u5408\u6cd5\u5e02\u573a\uff1b\u6a58\u9ec4\u8272\u4ee3\u8868\u4e2d\u6027\u5e02\u573a\uff0c\u867d\u5c1a\u672a\u516c\u5f00\u5408\u6cd5\u5316\u6bd4\u7279\u5e01\u5730\u4f4d\uff0c\u4f46\u4e5f\u6ca1\u6709\u4e3b\u8981\u7684\u4f7f\u7528\u9650\u5236\uff1b\u6de1\u7c89\u8272\u4ee3\u8868\u53d7\u9650\u5236\u7684\u5e02\u573a\uff0c\u5373\u5f53\u5730\u653f\u5e9c\u5c1d\u8bd5\u901a\u8fc7\u7ea2\u7ebf\u653f\u7b56\u6216\u76d1\u7ba1\u6765\u65b9\u6cd5\u6570\u5b57\u8d27\u5e01\u7684\u4f7f\u7528\uff1b\u6df1\u7c89\u8272\u4ee3\u8868\u6bd4\u7279\u5e01\u88ab\u5b9a\u4e49\u4e3a\u975e\u6cd5\u7684\u56fd\u5bb6\uff1b\u7070\u8272\u662f\u5c1a\u672a\u7f6e\u8bc4\u7684\u56fd\u5bb6\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u5982', '\u4e0b', '\u56fe', '\u6240', '\u793a', '\uff0c', '<0xE7>', '<0xBB>', '<0xBF>', '\u8272', '\u4ee3', '\u8868', '\u6bd4', '\u7279', '\u5e01', '\u7684', '\u5408', '\u6cd5', '\u5e02', '\u573a', '\uff1b', '<0xE6>', '<0xA9>', '<0x98>', '\u9ec4', '\u8272', '\u4ee3', '\u8868', '\u4e2d', '\u6027', '\u5e02', '\u573a', '\uff0c', '<0xE8>', '<0x99>', '<0xBD>', '\u5c1a', '\u672a', '\u516c', '\u5f00', '\u5408', '\u6cd5', '\u5316', '\u6bd4', '\u7279', '\u5e01', '\u5730', '\u4f4d', '\uff0c', '\u4f46', '\u4e5f', '\u6ca1', '\u6709', '\u4e3b', '\u8981', '\u7684', '\u4f7f', '\u7528', '\u9650', '\u5236', '\uff1b', '<0xE6>', '<0xB7>', '<0xA1>', '\u7c89', '\u8272', '\u4ee3', '\u8868', '\u53d7', '\u9650', '\u5236', '\u7684', '\u5e02', '\u573a', '\uff0c', '\u5373', '\u5f53', '\u5730', '\u653f', '\u5e9c', '\u5c1d', '\u8bd5', '\u901a', '\u8fc7', '\u7ea2', '\u7ebf', '\u653f', '\u7b56', '\u6216', '\u76d1', '\u7ba1', '\u6765', '\u65b9', '\u6cd5', '\u6570', '\u5b57', '\u8d27', '\u5e01', '\u7684', '\u4f7f', '\u7528', '\uff1b', '\u6df1', '\u7c89', '\u8272', '\u4ee3', '\u8868', '\u6bd4', '\u7279', '\u5e01', '\u88ab', '\u5b9a', '\u4e49', '\u4e3a', '\u975e', '\u6cd5', '\u7684', '\u56fd', '\u5bb6', '\uff1b', '\u7070', '\u8272', '\u662f', '\u5c1a', '\u672a', '\u7f6e', '\u8bc4', '\u7684', '\u56fd', '\u5bb6', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u5982\", \"\u0442\u04e9\u043c\u0435\u043d\u0434\u0435\u0433\u0456\u0434\u0435\u0439\"],\n  [\"\u4e0b\", null],\n  [\"\u56fe\", \"\u0441\u0443\u0440\u0435\u0442\u0442\u0435\"],\n  [\"\u6240\", null],\n  [\"\u793a\", \"\u043a\u04e9\u0440\u0441\u0435\u0442\u0456\u043b\u0433\u0435\u043d\u0434\u0435\u0439\"],\n  [\"\uff0c\", null],\n  [\"<0xE7>\", null],\n  [\"<0xBB>\", null],\n  [\"<0xBF>\", null],\n  [\"\u8272\", \"\u0436\u0430\u0441\u044b\u043b\"],\n  [\"\u4ee3\", null],\n  [\"\u8868\", \"\u0431\u0456\u043b\u0434\u0456\u0440\u0435\u0434\u0456\"],\n  [\"\u6bd4\", null],\n  [\"\u7279\", null],\n  [\"\u5e01\", \"\u0431\u0438\u0442\u043a\u043e\u0438\u043d\"],\n  [\"\u7684\", null],\n  [\"\u5408\", null],\n  [\"\u6cd5\", \"\u0437\u0430\u04a3\u0434\u044b\"],\n  [\"\u5e02\", null],\n  [\"\u573a\", \"\u043d\u0430\u0440\u044b\u0493\u044b\u043d\"],\n  [\"\uff1b\", null],\n  [\"<0xE6>\", null],\n  [\"<0xA9>\", null],\n  [\"<0x98>\", null],\n  [\"\u9ec4\", \"\u049b\u044b\u0437\u0493\u044b\u043b\u0442\"],\n  [\"\u8272\", \"\u0441\u0430\u0440\u044b\"],\n  [\"\u4ee3\", null],\n  [\"\u8868\", \"\u0431\u0456\u043b\u0434\u0456\u0440\u0435\u0434\u0456\"],\n  [\"\u4e2d\", null],\n  [\"\u6027\", \"\u0431\u0435\u0439\u0442\u0430\u0440\u0430\u043f\"],\n  [\"\u5e02\", null],\n  [\"\u573a\", \"\u043d\u0430\u0440\u044b\u049b\u0442\u044b\"],\n  [\"\uff0c\", null],\n  [\"<0xE8>\", null],\n  [\"<0x99>\", null],\n  [\"<0xBD>\", null],\n  [\"\u5c1a\", \"\u04d9\u043b\u0456\"],\n  [\"\u672a\", null],\n  [\"\u516c\", null],\n  [\"\u5f00\", \"\u0430\u0448\u044b\u049b\"],\n  [\"\u5408\", null],\n  [\"\u6cd5\", \"\u0437\u0430\u04a3\u0434\u0430\u0441\u0442\u044b\u0440\u044b\u043b\u043c\u0430\u0493\u0430\u043d\"],\n  [\"\u5316\", null],\n  [\"\u6bd4\", null],\n  [\"\u7279\", null],\n  [\"\u5e01\", \"\u0431\u0438\u0442\u043a\u043e\u0438\u043d\"],\n  [\"\u5730\", null],\n  [\"\u4f4d\", \"\u043c\u04d9\u0440\u0442\u0435\u0431\u0435\u0441\u0456\"],\n  [\"\uff0c\", null],\n  [\"\u4f46\", \"\u0431\u0456\u0440\u0430\u049b\"],\n  [\"\u4e5f\", null],\n  [\"\u6ca1\", null],\n  [\"\u6709\", \"\u0436\u043e\u049b\"],\n  [\"\u4e3b\", null],\n  [\"\u8981\", \"\u043d\u0435\u0433\u0456\u0437\u0433\u0456\"],\n  [\"\u7684\", null],\n  [\"\u4f7f\", null],\n  [\"\u7528\", \"\u043f\u0430\u0439\u0434\u0430\u043b\u0430\u043d\u0443\"],\n  [\"\u9650\", null],\n  [\"\u5236\", \"\u0448\u0435\u043a\u0442\u0435\u0443\u043b\u0435\u0440\u0456\"],\n  [\"\uff1b\", null],\n  [\"<0xE6>\", null],\n  [\"<0xB7>\", null],\n  [\"<0xA1>\", null],\n  [\"\u7c89\", \"\u0430\u049b\u0448\u044b\u043b\"],\n  [\"\u8272\", \"\u049b\u044b\u0437\u0493\u044b\u043b\u0442\"],\n  [\"\u4ee3\", null],\n  [\"\u8868\", \"\u0431\u0456\u043b\u0434\u0456\u0440\u0435\u0434\u0456\"],\n  [\"\u53d7\", null],\n  [\"\u9650\", null],\n  [\"\u5236\", \"\u0448\u0435\u043a\u0442\u0435\u0443\u043b\u0456\"],\n  [\"\u7684\", null],\n  [\"\u5e02\", null],\n  [\"\u573a\", \"\u043d\u0430\u0440\u044b\u049b\u0442\u044b\"],\n  [\"\uff0c\", null],\n  [\"\u5373\", \"\u044f\u0493\u043d\u0438\"],\n  [\"\u5f53\", null],\n  [\"\u5730\", \"\u0436\u0435\u0440\u0433\u0456\u043b\u0456\u043a\u0442\u0456\"],\n  [\"\u653f\", null],\n  [\"\u5e9c\", \"\u04af\u043a\u0456\u043c\u0435\u0442\"],\n  [\"\u5c1d\", null],\n  [\"\u8bd5\", \"\u04d9\u0440\u0435\u043a\u0435\u0442\"],\n  [\"\u901a\", null],\n  [\"\u8fc7\", \"\u0430\u0440\u049b\u044b\u043b\u044b\"],\n  [\"\u7ea2\", \"\u049b\u044b\u0437\u044b\u043b\"],\n  [\"\u7ebf\", \"\u0441\u044b\u0437\u044b\u049b\"],\n  [\"\u653f\", null],\n  [\"\u7b56\", \"\u0441\u0430\u044f\u0441\u0430\u0442\u044b\"],\n  [\"\u6216\", \"\u043d\u0435\u043c\u0435\u0441\u0435\"],\n  [\"\u76d1\", null],\n  [\"\u7ba1\", \"\u0440\u0435\u0442\u0442\u0435\u0443\"],\n  [\"\u6765\", null],\n  [\"\u65b9\", null],\n  [\"\u6cd5\", \"\u04d9\u0434\u0456\u0441\u0456\u043c\u0435\u043d\"],\n  [\"\u6570\", null],\n  [\"\u5b57\", \"\u0441\u0430\u043d\u0434\u044b\u049b\"],\n  [\"\u8d27\", null],\n  [\"\u5e01\", \"\u0430\u049b\u0448\u0430\u043d\u044b\"],\n  [\"\u7684\", null],\n  [\"\u4f7f\", null],\n  [\"\u7528\", \"\u043f\u0430\u0439\u0434\u0430\u043b\u0430\u043d\u0443\u0434\u044b\"],\n  [\"\uff1b\", null],\n  [\"\u6df1\", \"\u049b\u043e\u044e\"],\n  [\"\u7c89\", \"\u049b\u044b\u0437\u0493\u044b\u043b\u0442\"],\n  [\"\u8272\", null],\n  [\"\u4ee3\", null],\n  [\"\u8868\", \"\u0431\u0456\u043b\u0434\u0456\u0440\u0435\u0434\u0456\"],\n  [\"\u6bd4\", null],\n  [\"\u7279\", null],\n  [\"\u5e01\", \"\u0431\u0438\u0442\u043a\u043e\u0438\u043d\"],\n  [\"\u88ab\", null],\n  [\"\u5b9a\", null],\n  [\"\u4e49\", \"\u0430\u043d\u044b\u049b\u0442\u0430\u043b\u0493\u0430\u043d\"],\n  [\"\u4e3a\", null],\n  [\"\u975e\", null],\n  [\"\u6cd5\", \"\u0437\u0430\u04a3\u0441\u044b\u0437\"],\n  [\"\u7684\", null],\n  [\"\u56fd\", null],\n  [\"\u5bb6\", \"\u0435\u043b\u0434\u0435\u0440\"],\n  [\"\uff1b\", null],\n  [\"\u7070\", \"\u0441\u04b1\u0440\"],\n  [\"\u8272\", null],\n  [\"\u662f\", null],\n  [\"\u5c1a\", \"\u04d9\u043b\u0456\"],\n  [\"\u672a\", null],\n  [\"\u7f6e\", null],\n  [\"\u8bc4\", \"\u0431\u0430\u0493\u0430\u043b\u0430\u043d\u0431\u0430\u0493\u0430\u043d\"],\n  [\"\u7684\", null],\n  [\"\u56fd\", null],\n  [\"\u5bb6\", \"\u0435\u043b\u0434\u0435\u0440\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u7eff\u8272\u4ee3\u8868\u6bd4\u7279\u5e01\u7684\u5408\u6cd5\u5e02\u573a\uff1b\u6a58\u9ec4\u8272\u4ee3\u8868\u4e2d\u6027\u5e02\u573a\uff0c\u867d\u5c1a\u672a\u516c\u5f00\u5408\u6cd5\u5316\u6bd4\u7279\u5e01\u5730\u4f4d\uff0c\u4f46\u4e5f\u6ca1\u6709\u4e3b\u8981\u7684\u4f7f\u7528\u9650\u5236\uff1b\u6de1\u7c89\u8272\u4ee3\u8868\u53d7\u9650\u5236\u7684\u5e02\u573a\uff0c\u5373\u5f53\u5730\u653f\u5e9c\u5c1d\u8bd5\u901a\u8fc7\u7ea2\u7ebf\u653f\u7b56\u6216\u76d1\u7ba1\u6765\u65b9\u6cd5\u6570\u5b57\u8d27\u5e01\u7684\u4f7f\u7528\uff1b\u6df1\u7c89\u8272\u4ee3\u8868\u6bd4\u7279\u5e01\u88ab\u5b9a\u4e49\u4e3a\u975e\u6cd5\u7684\u56fd\u5bb6\uff1b\u7070\u8272\u662f\u5c1a\u672a\u7f6e\u8bc4\u7684\u56fd\u5bb6\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u5982', '\u4e0b', '\u56fe', '\u6240', '\u793a', '\uff0c', '<0xE7>', '<0xBB>', '<0xBF>', '\u8272', '\u4ee3', '\u8868', '\u6bd4', '\u7279', '\u5e01', '\u7684', '\u5408', '\u6cd5', '\u5e02', '\u573a', '\uff1b', '<0xE6>', '<0xA9>', '<0x98>', '\u9ec4', '\u8272', '\u4ee3', '\u8868', '\u4e2d', '\u6027', '\u5e02', '\u573a', '\uff0c', '<0xE8>', '<0x99>', '<0xBD>', '\u5c1a', '\u672a', '\u516c', '\u5f00', '\u5408', '\u6cd5', '\u5316', '\u6bd4', '\u7279', '\u5e01', '\u5730', '\u4f4d', '\uff0c', '\u4f46', '\u4e5f', '\u6ca1', '\u6709', '\u4e3b', '\u8981', '\u7684', '\u4f7f', '\u7528', '\u9650', '\u5236', '\uff1b', '<0xE6>', '<0xB7>', '<0xA1>', '\u7c89', '\u8272', '\u4ee3', '\u8868', '\u53d7', '\u9650', '\u5236', '\u7684', '\u5e02', '\u573a', '\uff0c', '\u5373', '\u5f53', '\u5730', '\u653f', '\u5e9c', '\u5c1d', '\u8bd5', '\u901a', '\u8fc7', '\u7ea2', '\u7ebf', '\u653f', '\u7b56', '\u6216', '\u76d1', '\u7ba1', '\u6765', '\u65b9', '\u6cd5', '\u6570', '\u5b57', '\u8d27', '\u5e01', '\u7684', '\u4f7f', '\u7528', '\uff1b', '\u6df1', '\u7c89', '\u8272', '\u4ee3', '\u8868', '\u6bd4', '\u7279', '\u5e01', '\u88ab', '\u5b9a', '\u4e49', '\u4e3a', '\u975e', '\u6cd5', '\u7684', '\u56fd', '\u5bb6', '\uff1b', '\u7070', '\u8272', '\u662f', '\u5c1a', '\u672a', '\u7f6e', '\u8bc4', '\u7684', '\u56fd', '\u5bb6', '\u3002']"}], "[[\"\u2581\",null],[\"\u5982\",\"\u041a\u0430\u043a\"],[\"\u4e0b\",null],[\"\u56fe\",null],[\"\u6240\",null],[\"\u793a\",null],[\",\",null],[\"<0xE7>\",null],[\"<0xBB>\",null],[\"<0xBF>\",null],[\"\u8272\",\"\u0417\u0435\u043b\u0435\u043d\u044b\u0439\"],[\"\u4ee3\",null],[\"\u8868\",null],[\"\u6bd4\",null],[\"\u7279\",null],[\"\u5e01\",\"\u0431\u0438\u0442\u043a\u043e\u0438\u043d\"],[\"\u7684\",null],[\"\u5408\",null],[\"\u6cd5\",null],[\"\u5e02\",null],[\"\u573a\",\"\u0440\u044b\u043d\u043e\u043a\"],[\"\uff1b\",null],[\"<0xE6>\",null],[\"<0xA9>\",null],[\"<0x98>\",null],[\"\u9ec4\",\"\u041e\u0440\u0430\u043d\u0436\u0435\u0432\u044b\u0439\"],[\"\u8272\",null],[\"\u4ee3\",null],[\"\u8868\",null],[\"\u4e2d\",null],[\"\u6027\",null],[\"\u5e02\",null],[\"\u573a\",\"\u0440\u044b\u043d\u043e\u043a\"],[\"\uff0c\",null],[\"<0xE8>\",null],[\"<0x99>\",null],[\"<0xBD>\",null],[\"\u5c1a\",null],[\"\u672a\",null],[\"\u516c\",null],[\"\u5f00\",null],[\"\u5408\",null],[\"\u6cd5\",null],[\"\u5316\",null],[\"\u6bd4\",null],[\"\u7279\",null],[\"\u5e01\",\"\u0431\u0438\u0442\u043a\u043e\u0438\u043d\"],[\"\u5730\",null],[\"\u4f4d\",null],[\"\uff0c\",null],[\"\u4f46\",null],[\"\u4e5f\",null],[\"\u6ca1\",null],[\"\u6709\",null],[\"\u4e3b\",null],[\"\u8981\",null],[\"\u7684\",null],[\"\u4f7f\",null],[\"\u7528\",null],[\"\u9650\",null],[\"\u5236\",\"\u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u044f\"],[\"\uff1b\",null],[\"<0xE6>\",null],[\"<0xB7>\",null],[\"<0xA1>\",null],[\"\u7c89\",\"\u0421\u0432\u0435\u0442\u043b\u043e-\u0440\u043e\u0437\u043e\u0432\u044b\u0439\"],[\"\u8272\",null],[\"\u4ee3\",null],[\"\u8868\",null],[\"\u53d7\",null],[\"\u9650\",null],[\"\u5236\",null],[\"\u7684\",null],[\"\u5e02\",null],[\"\u573a\",\"\u0440\u044b\u043d\u043e\u043a\"],[\"\uff0c\",null],[\"\u5373\",null],[\"\u5f53\",null],[\"\u5730\",null],[\"\u653f\",null],[\"\u5e9c\",\"\u043f\u0440\u0430\u0432\u0438\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u043e\"],[\"\u5c1d\",null],[\"\u8bd5\",null],[\"\u901a\",null],[\"\u8fc7\",null],[\"\u7ea2\",null],[\"\u7ebf\",null],[\"\u653f\",null],[\"\u7b56\",\"\u043f\u043e\u043b\u0438\u0442\u0438\u043a\u0430\"],[\"\u6216\",null],[\"\u76d1\",null],[\"\u7ba1\",null],[\"\u6765\",null],[\"\u65b9\",null],[\"\u6cd5\",null],[\"\u6570\",null],[\"\u5b57\",null],[\"\u8d27\",null],[\"\u5e01\",\"\u0432\u0430\u043b\u044e\u0442\u0430\"],[\"\u7684\",null],[\"\u4f7f\",null],[\"\u7528\",null],[\"\uff1b\",null],[\"\u6df1\",\"\u0422\u0435\u043c\u043d\u043e\"],[\"\u7c89\",null],[\"\u8272\",\"\u0440\u043e\u0437\u043e\u0432\u044b\u0439\"],[\"\u4ee3\",null],[\"\u8868\",null],[\"\u6bd4\",null],[\"\u7279\",null],[\"\u5e01\",\"\u0431\u0438\u0442\u043a\u043e\u0438\u043d\"],[\"\u88ab\",null],[\"\u5b9a\",null],[\"\u4e49\",null],[\"\u4e3a\",null],[\"\u975e\",null],[\"\u6cd5\",null],[\"\u7684\",null],[\"\u56fd\",null],[\"\u5bb6\",\"\u0441\u0442\u0440\u0430\u043d\u044b\"],[\"\uff1b\",null],[\"\u7070\",\"\u0421\u0435\u0440\u044b\u0439\"],[\"\u8272\",null],[\"\u662f\",null],[\"\u5c1a\",null],[\"\u672a\",null],[\"\u7f6e\",null],[\"\u8bc4\",null],[\"\u7684\",null],[\"\u56fd\",null],[\"\u5bb6\",\"\u0441\u0442\u0440\u0430\u043d\u044b\"],[\"\u3002\",null]]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u800c\u9f99\u6e56\u6631\u6e56\u58f9\u53f7\uff0c\u4e34\u8fd1\u91cd\u5e86\u9996\u4e2a\u4f4e\u5bc6\u5ea6\u6ee8\u6c34\u5546\u5708\u2014\u2014\u793c\u5609\u6ee8\u6c34\u5546\u5708\uff0c\u88ab\u4e5d\u4e2a\u516c\u56ed\u73af\u7ed5\uff0c\u5546\u4e1a\u8857\u533a\u5bc6\u5ea6\u4f4e\uff0c\u5f00\u653e\u7a0b\u5ea6\u9ad8\uff0c\u4f7f\u5f97\u4eba\u4eec\u8d2d\u7269\u65f6\u66f4\u613f\u610f\u505c\u7559\u3001\u9a7b\u8db3\u3001\u6162\u6162\u4eab\u53d7\u8d2d\u7269\u7684\u4e50\u8da3\uff0c\u800c\u4e0d\u662f\u88ab\u540e\u9762\u7684\u4eba\u6d41\u88f9\u631f\u7740\u6025\u6025\u5fd9\u5fd9\u5730\u5411\u524d\u8d76\uff0c\u771f\u6b63\u5b9e\u73b0\u4e86\u201c\u6563\u6b65\u5f0f\u8d2d\u7269\u201d\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u800c', '\u9f99', '\u6e56', '<0xE6>', '<0x98>', '<0xB1>', '\u6e56', '<0xE5>', '<0xA3>', '<0xB9>', '\u53f7', '\uff0c', '\u4e34', '\u8fd1', '\u91cd', '\u5e86', '\u9996', '\u4e2a', '\u4f4e', '\u5bc6', '\u5ea6', '<0xE6>', '<0xBB>', '<0xA8>', '\u6c34', '\u5546', '\u5708', '\u2014\u2014', '\u793c', '\u5609', '<0xE6>', '<0xBB>', '<0xA8>', '\u6c34', '\u5546', '\u5708', '\uff0c', '\u88ab', '\u4e5d', '\u4e2a', '\u516c', '\u56ed', '\u73af', '<0xE7>', '<0xBB>', '<0x95>', '\uff0c', '\u5546', '\u4e1a', '\u8857', '\u533a', '\u5bc6', '\u5ea6', '\u4f4e', '\uff0c', '\u5f00', '\u653e', '\u7a0b', '\u5ea6', '\u9ad8', '\uff0c', '\u4f7f', '\u5f97', '\u4eba', '\u4eec', '\u8d2d', '\u7269', '\u65f6', '\u66f4', '<0xE6>', '<0x84>', '<0xBF>', '\u610f', '\u505c', '\u7559', '\u3001', '<0xE9>', '<0xA9>', '<0xBB>', '\u8db3', '\u3001', '\u6162', '\u6162', '\u4eab', '\u53d7', '\u8d2d', '\u7269', '\u7684', '\u4e50', '<0xE8>', '<0xB6>', '<0xA3>', '\uff0c', '\u800c', '\u4e0d', '\u662f', '\u88ab', '\u540e', '\u9762', '\u7684', '\u4eba', '\u6d41', '<0xE8>', '<0xA3>', '<0xB9>', '<0xE6>', '<0x8C>', '<0x9F>', '\u7740', '\u6025', '\u6025', '<0xE5>', '<0xBF>', '<0x99>', '<0xE5>', '<0xBF>', '<0x99>', '\u5730', '\u5411', '\u524d', '<0xE8>', '<0xB5>', '<0xB6>', '\uff0c', '\u771f', '\u6b63', '\u5b9e', '\u73b0', '\u4e86', '\u201c', '\u6563', '\u6b65', '\u5f0f', '\u8d2d', '\u7269', '\u201d', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u800c\", \"Meanwhile\"],\n  [\"\u9f99\", \"Longhu\"],\n  [\"\u6e56\", null],\n  [\"<0xE6>\", null],\n  [\"<0x98>\", null],\n  [\"<0xB1>\", null],\n  [\"\u6e56\", null],\n  [\"<0xE5>\", null],\n  [\"<0xA3>\", null],\n  [\"<0xB9>\", null],\n  [\"\u53f7\", null],\n  [\"\uff0c\", null],\n  [\"\u4e34\", \"near\"],\n  [\"\u8fd1\", null],\n  [\"\u91cd\", \"Chongqing\"],\n  [\"\u5e86\", null],\n  [\"\u9996\", \"first\"],\n  [\"\u4e2a\", null],\n  [\"\u4f4e\", \"low\"],\n  [\"\u5bc6\", \"density\"],\n  [\"\u5ea6\", null],\n  [\"<0xE6>\", null],\n  [\"<0xBB>\", null],\n  [\"<0xA8>\", null],\n  [\"\u6c34\", \"waterfront\"],\n  [\"\u5546\", \"commercial\"],\n  [\"\u5708\", \"district\"],\n  [\"\u2014\u2014\", null],\n  [\"\u793c\", \"Lijia\"],\n  [\"\u5609\", null],\n  [\"<0xE6>\", null],\n  [\"<0xBB>\", null],\n  [\"<0xA8>\", null],\n  [\"\u6c34\", \"waterfront\"],\n  [\"\u5546\", \"commercial\"],\n  [\"\u5708\", \"district\"],\n  [\"\uff0c\", null],\n  [\"\u88ab\", \"surrounded\"],\n  [\"\u4e5d\", \"nine\"],\n  [\"\u4e2a\", null],\n  [\"\u516c\", \"parks\"],\n  [\"\u56ed\", null],\n  [\"\u73af\", null],\n  [\"<0x7B>\", null],\n  [\"<0xBB>\", null],\n  [\"<0x95>\", null],\n  [\"\uff0c\", null],\n  [\"\u5546\", \"commercial\"],\n  [\"\u4e1a\", null],\n  [\"\u8857\", \"streets\"],\n  [\"\u533a\", null],\n  [\"\u5bc6\", \"density\"],\n  [\"\u5ea6\", null],\n  [\"\u4f4e\", \"low\"],\n  [\"\uff0c\", null],\n  [\"\u5f00\", \"openness\"],\n  [\"\u653e\", null],\n  [\"\u7a0b\", \"degree\"],\n  [\"\u5ea6\", null],\n  [\"\u9ad8\", \"high\"],\n  [\"\uff0c\", null],\n  [\"\u4f7f\", \"makes\"],\n  [\"\u5f97\", null],\n  [\"\u4eba\", \"people\"],\n  [\"\u4eec\", null],\n  [\"\u8d2d\", \"shopping\"],\n  [\"\u7269\", null],\n  [\"\u65f6\", \"when\"],\n  [\"\u66f4\", \"more\"],\n  [\"<0xE6>\", null],\n  [\"<0x84>\", null],\n  [\"<0xBF>\", null],\n  [\"\u610f\", \"willing\"],\n  [\"\u505c\", \"stop\"],\n  [\"\u7559\", null],\n  [\"\u3001\", null],\n  [\"<0xE9>\", null],\n  [\"<0xA9>\", null],\n  [\"<0xBB>\", null],\n  [\"\u8db3\", \"linger\"],\n  [\"\u3001\", null],\n  [\"\u6162\", \"slowly\"],\n  [\"\u6162\", null],\n  [\"\u4eab\", \"enjoy\"],\n  [\"\u53d7\", null],\n  [\"\u8d2d\", \"shopping\"],\n  [\"\u7269\", null],\n  [\"\u7684\", null],\n  [\"\u4e50\", \"pleasure\"],\n  [\"<0xE8>\", null],\n  [\"<0xB6>\", null],\n  [\"<0xA3>\", null],\n  [\"\uff0c\", null],\n  [\"\u800c\", \"rather than\"],\n  [\"\u4e0d\", null],\n  [\"\u662f\", null],\n  [\"\u88ab\", \"being\"],\n  [\"\u540e\", \"behind\"],\n  [\"\u9762\", null],\n  [\"\u7684\", null],\n  [\"\u4eba\", \"crowd\"],\n  [\"\u6d41\", null],\n  [\"<0xE8>\", null],\n  [\"<0xA3>\", null],\n  [\"<0xB9>\", null],\n  [\"<0xE6>\", null],\n  [\"<0x8C>\", null],\n  [\"<0x9F>\", null],\n  [\"\u7740\", null],\n  [\"\u6025\", \"hurriedly\"],\n  [\"\u6025\", null],\n  [\"<0xE5>\", null],\n  [\"<0xBF>\", null],\n  [\"<0x99>\", null],\n  [\"<0xE5>\", null],\n  [\"<0xBF>\", null],\n  [\"<0x99>\", null],\n  [\"\u5730\", null],\n  [\"\u5411\", \"forward\"],\n  [\"\u524d\", null],\n  [\"<0xE8>\", null],\n  [\"<0xB5>\", null],\n  [\"<0xB6>\", null],\n  [\"\uff0c\", null],\n  [\"\u771f\", \"truly\"],\n  [\"\u6b63\", null],\n  [\"\u5b9e\", \"realizes\"],\n  [\"\u73b0\", null],\n  [\"\u4e86\", null],\n  [\"\\\"\", null],\n  [\"\u6563\", \"stroll\"],\n  [\"\u6b65\", null],\n  [\"\u5f0f\", \"style\"],\n  [\"\u8d2d\", \"shopping\"],\n  [\"\u7269\", null],\n  [\"\\\"\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u800c\u9f99\u6e56\u6631\u6e56\u58f9\u53f7\uff0c\u4e34\u8fd1\u91cd\u5e86\u9996\u4e2a\u4f4e\u5bc6\u5ea6\u6ee8\u6c34\u5546\u5708\u2014\u2014\u793c\u5609\u6ee8\u6c34\u5546\u5708\uff0c\u88ab\u4e5d\u4e2a\u516c\u56ed\u73af\u7ed5\uff0c\u5546\u4e1a\u8857\u533a\u5bc6\u5ea6\u4f4e\uff0c\u5f00\u653e\u7a0b\u5ea6\u9ad8\uff0c\u4f7f\u5f97\u4eba\u4eec\u8d2d\u7269\u65f6\u66f4\u613f\u610f\u505c\u7559\u3001\u9a7b\u8db3\u3001\u6162\u6162\u4eab\u53d7\u8d2d\u7269\u7684\u4e50\u8da3\uff0c\u800c\u4e0d\u662f\u88ab\u540e\u9762\u7684\u4eba\u6d41\u88f9\u631f\u7740\u6025\u6025\u5fd9\u5fd9\u5730\u5411\u524d\u8d76\uff0c\u771f\u6b63\u5b9e\u73b0\u4e86\u201c\u6563\u6b65\u5f0f\u8d2d\u7269\u201d\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u800c', '\u9f99', '\u6e56', '<0xE6>', '<0x98>', '<0xB1>', '\u6e56', '<0xE5>', '<0xA3>', '<0xB9>', '\u53f7', '\uff0c', '\u4e34', '\u8fd1', '\u91cd', '\u5e86', '\u9996', '\u4e2a', '\u4f4e', '\u5bc6', '\u5ea6', '<0xE6>', '<0xBB>', '<0xA8>', '\u6c34', '\u5546', '\u5708', '\u2014\u2014', '\u793c', '\u5609', '<0xE6>', '<0xBB>', '<0xA8>', '\u6c34', '\u5546', '\u5708', '\uff0c', '\u88ab', '\u4e5d', '\u4e2a', '\u516c', '\u56ed', '\u73af', '<0xE7>', '<0xBB>', '<0x95>', '\uff0c', '\u5546', '\u4e1a', '\u8857', '\u533a', '\u5bc6', '\u5ea6', '\u4f4e', '\uff0c', '\u5f00', '\u653e', '\u7a0b', '\u5ea6', '\u9ad8', '\uff0c', '\u4f7f', '\u5f97', '\u4eba', '\u4eec', '\u8d2d', '\u7269', '\u65f6', '\u66f4', '<0xE6>', '<0x84>', '<0xBF>', '\u610f', '\u505c', '\u7559', '\u3001', '<0xE9>', '<0xA9>', '<0xBB>', '\u8db3', '\u3001', '\u6162', '\u6162', '\u4eab', '\u53d7', '\u8d2d', '\u7269', '\u7684', '\u4e50', '<0xE8>', '<0xB6>', '<0xA3>', '\uff0c', '\u800c', '\u4e0d', '\u662f', '\u88ab', '\u540e', '\u9762', '\u7684', '\u4eba', '\u6d41', '<0xE8>', '<0xA3>', '<0xB9>', '<0xE6>', '<0x8C>', '<0x9F>', '\u7740', '\u6025', '\u6025', '<0xE5>', '<0xBF>', '<0x99>', '<0xE5>', '<0xBF>', '<0x99>', '\u5730', '\u5411', '\u524d', '<0xE8>', '<0xB5>', '<0xB6>', '\uff0c', '\u771f', '\u6b63', '\u5b9e', '\u73b0', '\u4e86', '\u201c', '\u6563', '\u6b65', '\u5f0f', '\u8d2d', '\u7269', '\u201d', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u800c\", \"\u0410\u043b\"],\n  [\"\u9f99\", \"\u041b\u043e\u043d\u0433\"],\n  [\"\u6e56\", \"\u043a\u04e9\u043b\"],\n  [\"<0xE6>\", null],\n  [\"<0x98>\", null],\n  [\"<0xB1>\", null],\n  [\"\u6e56\", null],\n  [\"<0xE5>\", null],\n  [\"<0xA3>\", null],\n  [\"<0xB9>\", null],\n  [\"\u53f7\", null],\n  [\"\uff0c\", null],\n  [\"\u4e34\", \"\u0416\u0430\u049b\u044b\u043d\"],\n  [\"\u8fd1\", null],\n  [\"\u91cd\", \"\u0427\u0443\u043d\u0446\u0438\u043d\"],\n  [\"\u5e86\", null],\n  [\"\u9996\", \"\u0430\u043b\u0493\u0430\u0448\u049b\u044b\"],\n  [\"\u4e2a\", null],\n  [\"\u4f4e\", \"\u0442\u04e9\u043c\u0435\u043d\"],\n  [\"\u5bc6\", \"\u0442\u044b\u0493\u044b\u0437\u0434\u044b\u049b\u0442\u044b\"],\n  [\"\u5ea6\", null],\n  [\"<0xE6>\", null],\n  [\"<0xBB>\", null],\n  [\"<0xA8>\", null],\n  [\"\u6c34\", \"\u0441\u0443\"],\n  [\"\u5546\", \"\u0441\u0430\u0443\u0434\u0430\"],\n  [\"\u5708\", \"\u0430\u0439\u043c\u0430\u0493\u044b\"],\n  [\"\u2014\u2014\", null],\n  [\"\u793c\", \"\u041b\u0438\u0446\u0437\u044f\"],\n  [\"\u5609\", null],\n  [\"<0xE6>\", null],\n  [\"<0xBB>\", null],\n  [\"<0xA8>\", null],\n  [\"\u6c34\", \"\u0441\u0443\"],\n  [\"\u5546\", \"\u0441\u0430\u0443\u0434\u0430\"],\n  [\"\u5708\", \"\u0430\u0439\u043c\u0430\u0493\u044b\"],\n  [\"\uff0c\", null],\n  [\"\u88ab\", null],\n  [\"\u4e5d\", \"\u0442\u043e\u0493\u044b\u0437\"],\n  [\"\u4e2a\", null],\n  [\"\u516c\", \"\u0441\u0430\u044f\u0431\u0430\u049b\u043f\u0435\u043d\"],\n  [\"\u56ed\", null],\n  [\"\u73af\", \"\u049b\u043e\u0440\u0448\u0430\u043b\u0493\u0430\u043d\"],\n  [\"<0x7B>\", null],\n  [\"<0xBB>\", null],\n  [\"<0x95>\", null],\n  [\"\uff0c\", null],\n  [\"\u5546\", \"\u0441\u0430\u0443\u0434\u0430\"],\n  [\"\u4e1a\", null],\n  [\"\u8857\", \"\u043a\u04e9\u0448\u0435\u043b\u0435\u0440\u0456\u043d\u0456\u04a3\"],\n  [\"\u533a\", null],\n  [\"\u5bc6\", \"\u0442\u044b\u0493\u044b\u0437\u0434\u044b\u0493\u044b\"],\n  [\"\u5ea6\", null],\n  [\"\u4f4e\", \"\u0442\u04e9\u043c\u0435\u043d\"],\n  [\"\uff0c\", null],\n  [\"\u5f00\", \"\u0430\u0448\u044b\u049b\u0442\u044b\u049b\"],\n  [\"\u653e\", null],\n  [\"\u7a0b\", \"\u0434\u0435\u04a3\u0433\u0435\u0439\u0456\"],\n  [\"\u5ea6\", null],\n  [\"\u9ad8\", \"\u0436\u043e\u0493\u0430\u0440\u044b\"],\n  [\"\uff0c\", null],\n  [\"\u4f7f\", \"\u0430\u0434\u0430\u043c\u0434\u0430\u0440\u0434\u044b\u04a3\"],\n  [\"\u5f97\", null],\n  [\"\u4eba\", null],\n  [\"\u4eec\", null],\n  [\"\u8d2d\", \"\u0441\u0430\u0442\u044b\u043f\"],\n  [\"\u7269\", \"\u0430\u043b\u0443\"],\n  [\"\u65f6\", \"\u043a\u0435\u0437\u0456\u043d\u0434\u0435\"],\n  [\"\u66f4\", \"\u043a\u04e9\u0431\u0456\u0440\u0435\u043a\"],\n  [\"<0xE6>\", null],\n  [\"<0x84>\", null],\n  [\"<0xBF>\", null],\n  [\"\u610f\", null],\n  [\"\u505c\", \"\u0442\u043e\u049b\u0442\u0430\u043f\"],\n  [\"\u7559\", null],\n  [\"\u3001\", null],\n  [\"<0xE9>\", null],\n  [\"<0xA9>\", null],\n  [\"<0xBB>\", null],\n  [\"\u8db3\", \"\u049b\u0430\u043b\u0443\u0493\u0430\"],\n  [\"\u3001\", null],\n  [\"\u6162\", \"\u0431\u0430\u044f\u0443\"],\n  [\"\u6162\", null],\n  [\"\u4eab\", \"\u0440\u0430\u049b\u0430\u0442\u0442\u0430\u043d\u0443\u0493\u0430\"],\n  [\"\u53d7\", null],\n  [\"\u8d2d\", \"\u0441\u0430\u0442\u044b\u043f\"],\n  [\"\u7269\", \"\u0430\u043b\u0443\u0434\u044b\u04a3\"],\n  [\"\u7684\", null],\n  [\"\u4e50\", \"\u049b\u044b\u0437\u044b\u0493\u044b\u043d\"],\n  [\"<0xE8>\", null],\n  [\"<0xB6>\", null],\n  [\"<0xA3>\", null],\n  [\"\uff0c\", null],\n  [\"\u800c\", \"\u0430\u043b\"],\n  [\"\u4e0d\", \"\u0435\u043c\u0435\u0441\"],\n  [\"\u662f\", null],\n  [\"\u88ab\", null],\n  [\"\u540e\", \"\u0430\u0440\u0442\u044b\u043d\u0434\u0430\u0493\u044b\"],\n  [\"\u9762\", null],\n  [\"\u7684\", null],\n  [\"\u4eba\", \"\u0430\u0434\u0430\u043c\u0434\u0430\u0440\"],\n  [\"\u6d41\", \"\u0430\u0493\u044b\u043d\u044b\u043c\u0435\u043d\"],\n  [\"<0xE8>\", null],\n  [\"<0xA3>\", null],\n  [\"<0xB9>\", null],\n  [\"<0xE6>\", null],\n  [\"<0x8C>\", null],\n  [\"<0x9F>\", null],\n  [\"\u7740\", null],\n  [\"\u6025\", \"\u0430\u0441\u044b\u0493\u044b\u0441\"],\n  [\"\u6025\", null],\n  [\"<0xE5>\", null],\n  [\"<0xBF>\", null],\n  [\"<0x99>\", null],\n  [\"<0xE5>\", null],\n  [\"<0xBF>\", null],\n  [\"<0x99>\", null],\n  [\"\u5730\", null],\n  [\"\u5411\", \"\u0430\u043b\u0493\u0430\"],\n  [\"\u524d\", null],\n  [\"<0xE8>\", null],\n  [\"<0xB5>\", null],\n  [\"<0xB6>\", null],\n  [\"\uff0c\", null],\n  [\"\u771f\", \"\u0448\u044b\u043d\u044b\u043c\u0435\u043d\"],\n  [\"\u6b63\", null],\n  [\"\u5b9e\", \"\u0436\u04af\u0437\u0435\u0433\u0435\"],\n  [\"\u73b0\", \"\u0430\u0441\u044b\u0440\u0434\u044b\"],\n  [\"\u4e86\", null],\n  [\"\"\", null],\n  [\"\u6563\", \"\u0441\u0435\u0440\u0443\u0435\u043d\u0434\u0435\u043f\"],\n  [\"\u6b65\", null],\n  [\"\u5f0f\", null],\n  [\"\u8d2d\", \"\u0441\u0430\u0442\u044b\u043f\"],\n  [\"\u7269\", \"\u0430\u043b\u0443\"],\n  [\"\"\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u800c\u9f99\u6e56\u6631\u6e56\u58f9\u53f7\uff0c\u4e34\u8fd1\u91cd\u5e86\u9996\u4e2a\u4f4e\u5bc6\u5ea6\u6ee8\u6c34\u5546\u5708\u2014\u2014\u793c\u5609\u6ee8\u6c34\u5546\u5708\uff0c\u88ab\u4e5d\u4e2a\u516c\u56ed\u73af\u7ed5\uff0c\u5546\u4e1a\u8857\u533a\u5bc6\u5ea6\u4f4e\uff0c\u5f00\u653e\u7a0b\u5ea6\u9ad8\uff0c\u4f7f\u5f97\u4eba\u4eec\u8d2d\u7269\u65f6\u66f4\u613f\u610f\u505c\u7559\u3001\u9a7b\u8db3\u3001\u6162\u6162\u4eab\u53d7\u8d2d\u7269\u7684\u4e50\u8da3\uff0c\u800c\u4e0d\u662f\u88ab\u540e\u9762\u7684\u4eba\u6d41\u88f9\u631f\u7740\u6025\u6025\u5fd9\u5fd9\u5730\u5411\u524d\u8d76\uff0c\u771f\u6b63\u5b9e\u73b0\u4e86\u201c\u6563\u6b65\u5f0f\u8d2d\u7269\u201d\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u800c', '\u9f99', '\u6e56', '<0xE6>', '<0x98>', '<0xB1>', '\u6e56', '<0xE5>', '<0xA3>', '<0xB9>', '\u53f7', '\uff0c', '\u4e34', '\u8fd1', '\u91cd', '\u5e86', '\u9996', '\u4e2a', '\u4f4e', '\u5bc6', '\u5ea6', '<0xE6>', '<0xBB>', '<0xA8>', '\u6c34', '\u5546', '\u5708', '\u2014\u2014', '\u793c', '\u5609', '<0xE6>', '<0xBB>', '<0xA8>', '\u6c34', '\u5546', '\u5708', '\uff0c', '\u88ab', '\u4e5d', '\u4e2a', '\u516c', '\u56ed', '\u73af', '<0xE7>', '<0xBB>', '<0x95>', '\uff0c', '\u5546', '\u4e1a', '\u8857', '\u533a', '\u5bc6', '\u5ea6', '\u4f4e', '\uff0c', '\u5f00', '\u653e', '\u7a0b', '\u5ea6', '\u9ad8', '\uff0c', '\u4f7f', '\u5f97', '\u4eba', '\u4eec', '\u8d2d', '\u7269', '\u65f6', '\u66f4', '<0xE6>', '<0x84>', '<0xBF>', '\u610f', '\u505c', '\u7559', '\u3001', '<0xE9>', '<0xA9>', '<0xBB>', '\u8db3', '\u3001', '\u6162', '\u6162', '\u4eab', '\u53d7', '\u8d2d', '\u7269', '\u7684', '\u4e50', '<0xE8>', '<0xB6>', '<0xA3>', '\uff0c', '\u800c', '\u4e0d', '\u662f', '\u88ab', '\u540e', '\u9762', '\u7684', '\u4eba', '\u6d41', '<0xE8>', '<0xA3>', '<0xB9>', '<0xE6>', '<0x8C>', '<0x9F>', '\u7740', '\u6025', '\u6025', '<0xE5>', '<0xBF>', '<0x99>', '<0xE5>', '<0xBF>', '<0x99>', '\u5730', '\u5411', '\u524d', '<0xE8>', '<0xB5>', '<0xB6>', '\uff0c', '\u771f', '\u6b63', '\u5b9e', '\u73b0', '\u4e86', '\u201c', '\u6563', '\u6b65', '\u5f0f', '\u8d2d', '\u7269', '\u201d', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u800c\", null],\n  [\"\u9f99\", \"\u041b\u0443\u043d\u0445\u0443\"],\n  [\"\u6e56\", null],\n  [\"<0xE6>\", null],\n  [\"<0x98>\", null],\n  [\"<0xB1>\", null],\n  [\"\u6e56\", null],\n  [\"<0xE5>\", null],\n  [\"<0xA3>\", null],\n  [\"<0xB9>\", null],\n  [\"\u53f7\", null],\n  [\"\uff0c\", null],\n  [\"\u4e34\", \"\u0412\u0431\u043b\u0438\u0437\u0438\"],\n  [\"\u8fd1\", null],\n  [\"\u91cd\", \"\u0427\u0443\u043d\u0446\u0438\u043d\"],\n  [\"\u5e86\", null],\n  [\"\u9996\", \"\u043f\u0435\u0440\u0432\u044b\u0439\"],\n  [\"\u4e2a\", null],\n  [\"\u4f4e\", \"\u043d\u0438\u0437\u043a\u043e\u043f\u043b\u043e\u0442\u043d\u044b\u0439\"],\n  [\"\u5bc6\", null],\n  [\"\u5ea6\", null],\n  [\"<0xE6>\", null],\n  [\"<0xBB>\", null],\n  [\"<0xA8>\", null],\n  [\"\u6c34\", null],\n  [\"\u5546\", \"\u0442\u043e\u0440\u0433\u043e\u0432\u044b\u0439\"],\n  [\"\u5708\", null],\n  [\"\u2014\u2014\", null],\n  [\"\u793c\", \"\u041b\u0438\u0446\u0437\u044f\"],\n  [\"\u5609\", null],\n  [\"<0xE6>\", null],\n  [\"<0xBB>\", null],\n  [\"<0xA8>\", null],\n  [\"\u6c34\", null],\n  [\"\u5546\", \"\u0442\u043e\u0440\u0433\u043e\u0432\u044b\u0439\"],\n  [\"\u5708\", null],\n  [\"\uff0c\", null],\n  [\"\u88ab\", null],\n  [\"\u4e5d\", \"\u0434\u0435\u0432\u044f\u0442\u044c\"],\n  [\"\u4e2a\", null],\n  [\"\u516c\", \"\u043f\u0430\u0440\u043a\u043e\u0432\"],\n  [\"\u56ed\", null],\n  [\"\u73af\", \"\u043e\u043a\u0440\u0443\u0436\u0430\u044e\u0442\"],\n  [\"<0xE7>\", null],\n  [\"<0xBB>\", null],\n  [\"<0x95>\", null],\n  [\"\uff0c\", null],\n  [\"\u5546\", \"\u043a\u043e\u043c\u043c\u0435\u0440\u0447\u0435\u0441\u043a\u0438\u0439\"],\n  [\"\u4e1a\", null],\n  [\"\u8857\", \"\u0440\u0430\u0439\u043e\u043d\"],\n  [\"\u533a\", null],\n  [\"\u5bc6\", \"\u043d\u0438\u0437\u043a\u043e\u0439\"],\n  [\"\u5ea6\", null],\n  [\"\u4f4e\", null],\n  [\"\uff0c\", null],\n  [\"\u5f00\", \"\u0432\u044b\u0441\u043e\u043a\u043e\u0439\"],\n  [\"\u653e\", null],\n  [\"\u7a0b\", null],\n  [\"\u5ea6\", null],\n  [\"\u9ad8\", null],\n  [\"\uff0c\", null],\n  [\"\u4f7f\", \"\u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442\"],\n  [\"\u5f97\", null],\n  [\"\u4eba\", \"\u043b\u044e\u0434\u044f\u043c\"],\n  [\"\u4eec\", null],\n  [\"\u8d2d\", \"\u043f\u043e\u043a\u0443\u043f\u043a\u0438\"],\n  [\"\u7269\", null],\n  [\"\u65f6\", null],\n  [\"\u66f4\", \"\u0431\u043e\u043b\u0435\u0435\"],\n  [\"<0xE6>\", null],\n  [\"<0x84>\", null],\n  [\"<0xBF>\", null],\n  [\"\u610f\", null],\n  [\"\u505c\", \"\u043e\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0442\u044c\u0441\u044f\"],\n  [\"\u7559\", null],\n  [\"\u3001\", null],\n  [\"<0xE9>\", null],\n  [\"<0xA9>\", null],\n  [\"<0xBB>\", null],\n  [\"\u8db3\", null],\n  [\"\u3001\", null],\n  [\"\u6162\", \"\u043c\u0435\u0434\u043b\u0435\u043d\u043d\u043e\"],\n  [\"\u6162\", null],\n  [\"\u4eab\", \"\u043d\u0430\u0441\u043b\u0430\u0436\u0434\u0430\u0442\u044c\u0441\u044f\"],\n  [\"\u53d7\", null],\n  [\"\u8d2d\", \"\u043f\u043e\u043a\u0443\u043f\u043a\u0430\u043c\u0438\"],\n  [\"\u7269\", null],\n  [\"\u7684\", null],\n  [\"\u4e50\", \"\u0443\u0434\u043e\u0432\u043e\u043b\u044c\u0441\u0442\u0432\u0438\u0435\u043c\"],\n  [\"<0xE8>\", null],\n  [\"<0xB6>\", null],\n  [\"<0xA3>\", null],\n  [\"\uff0c\", null],\n  [\"\u800c\", null],\n  [\"\u4e0d\", \"\u043d\u0435\"],\n  [\"\u662f\", null],\n  [\"\u88ab\", null],\n  [\"\u540e\", \"\u043f\u043e\u0437\u0430\u0434\u0438\"],\n  [\"\u9762\", null],\n  [\"\u7684\", null],\n  [\"\u4eba\", \"\u0442\u043e\u043b\u043f\u043e\u0439\"],\n  [\"\u6d41\", null],\n  [\"<0xE8>\", null],\n  [\"<0xA3>\", null],\n  [\"<0xB9>\", null],\n  [\"<0xE6>\", null],\n  [\"<0x8C>\", null],\n  [\"<0x9F>\", null],\n  [\"\u7740\", null],\n  [\"\u6025\", \"\u0441\u043f\u0435\u0448\u0438\u0442\u044c\"],\n  [\"\u6025\", null],\n  [\"<0xE5>\", null],\n  [\"<0xBF>\", null],\n  [\"<0x99>\", null],\n  [\"<0xE5>\", null],\n  [\"<0xBF>\", null],\n  [\"<0x99>\", null],\n  [\"\u5730\", null],\n  [\"\u5411\", \"\u0432\u043f\u0435\u0440\u0435\u0434\"],\n  [\"\u524d\", null],\n  [\"<0xE8>\", null],\n  [\"<0xB5>\", null],\n  [\"<0xB6>\", null],\n  [\"\uff0c\", null],\n  [\"\u771f\", \"\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e\"],\n  [\"\u6b63\", null],\n  [\"\u5b9e\", \"\u0440\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u0442\"],\n  [\"\u73b0\", null],\n  [\"\u4e86\", null],\n  [\"\"\", null],\n  [\"\u6563\", \"\u043f\u0440\u043e\u0433\u0443\u043b\u043e\u0447\u043d\u044b\u0439\"],\n  [\"\u6b65\", null],\n  [\"\u5f0f\", null],\n  [\"\u8d2d\", \"\u0448\u043e\u043f\u0438\u043d\u0433\"],\n  [\"\u7269\", null],\n  [\"\"\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u6574\u4e2a\u666f\u89c2\u5e26\u501f\u52a9\u539f\u6709\u81ea\u7136\u751f\u6001\u548c\u5386\u53f2\u6587\u5316\u8d44\u6e90\u4f18\u52bf\uff0c\u5bf9\u5357\u8fd0\u6cb3\u4e24\u5cb8\u8fdb\u884c\u4e86\u91cd\u65b0\u89c4\u5212\u548c\u63d0\u5347\u6539\u9020\uff0c\u6253\u9020\u51fa\u5929\u5b50\u6d25\u6e21\u9057\u5740\u516c\u56ed\u60e0\u7075\u987f\u5e7f\u573a\u65b0\u5929\u5730\u98ce\u60c5\u8857\u897f\u7ad9\u8d70\u5eca\u6c34\u897f\u9057\u97f5\u7eff\u5eca\u4e94\u4e2a\u6cbf\u6cb3\u666f\u89c2\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u6574', '\u4e2a', '\u666f', '\u89c2', '\u5e26', '\u501f', '\u52a9', '\u539f', '\u6709', '\u81ea', '\u7136', '\u751f', '\u6001', '\u548c', '\u5386', '\u53f2', '\u6587', '\u5316', '\u8d44', '\u6e90', '\u4f18', '\u52bf', '\uff0c', '\u5bf9', '\u5357', '\u8fd0', '\u6cb3', '\u4e24', '<0xE5>', '<0xB2>', '<0xB8>', '\u8fdb', '\u884c', '\u4e86', '\u91cd', '\u65b0', '\u89c4', '\u5212', '\u548c', '\u63d0', '\u5347', '\u6539', '\u9020', '\uff0c', '\u6253', '\u9020', '\u51fa', '\u5929', '\u5b50', '\u6d25', '\u6e21', '\u9057', '\u5740', '\u516c', '\u56ed', '\u60e0', '\u7075', '<0xE9>', '<0xA1>', '<0xBF>', '\u5e7f', '\u573a', '\u65b0', '\u5929', '\u5730', '\u98ce', '\u60c5', '\u8857', '\u897f', '\u7ad9', '\u8d70', '<0xE5>', '<0xBB>', '<0x8A>', '\u6c34', '\u897f', '\u9057', '<0xE9>', '<0x9F>', '<0xB5>', '<0xE7>', '<0xBB>', '<0xBF>', '<0xE5>', '<0xBB>', '<0x8A>', '\u4e94', '\u4e2a', '<0xE6>', '<0xB2>', '<0xBF>', '\u6cb3', '\u666f', '\u89c2', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u6574\", \"entire\"],\n  [\"\u4e2a\", null],\n  [\"\u666f\", \"landscape\"],\n  [\"\u89c2\", null],\n  [\"\u5e26\", null],\n  [\"\u501f\", \"using\"],\n  [\"\u52a9\", null],\n  [\"\u539f\", \"original\"],\n  [\"\u6709\", null],\n  [\"\u81ea\", \"natural\"],\n  [\"\u7136\", null],\n  [\"\u751f\", \"ecological\"],\n  [\"\u6001\", null],\n  [\"\u548c\", null],\n  [\"\u5386\", \"historical\"],\n  [\"\u53f2\", null],\n  [\"\u6587\", \"cultural\"],\n  [\"\u5316\", null],\n  [\"\u8d44\", \"resources\"],\n  [\"\u6e90\", null],\n  [\"\u4f18\", \"advantages\"],\n  [\"\u52bf\", null],\n  [\"\uff0c\", null],\n  [\"\u5bf9\", null],\n  [\"\u5357\", \"South\"],\n  [\"\u8fd0\", \"Grand\"],\n  [\"\u6cb3\", \"Canal\"],\n  [\"\u4e24\", \"both\"],\n  [\"<0xE5>\", null],\n  [\"<0xB2>\", null],\n  [\"<0xB8>\", null],\n  [\"\u8fdb\", \"carried out\"],\n  [\"\u884c\", null],\n  [\"\u4e86\", null],\n  [\"\u91cd\", \"re-planning\"],\n  [\"\u65b0\", null],\n  [\"\u89c4\", null],\n  [\"\u5212\", null],\n  [\"\u548c\", null],\n  [\"\u63d0\", \"upgrading\"],\n  [\"\u5347\", null],\n  [\"\u6539\", \"renovation\"],\n  [\"\u9020\", null],\n  [\"\uff0c\", null],\n  [\"\u6253\", \"creating\"],\n  [\"\u9020\", null],\n  [\"\u51fa\", null],\n  [\"\u5929\", \"Tianzi\"],\n  [\"\u5b50\", null],\n  [\"\u6d25\", \"Ferry\"],\n  [\"\u6e21\", null],\n  [\"\u9057\", \"Relic\"],\n  [\"\u5740\", null],\n  [\"\u516c\", \"Park\"],\n  [\"\u56ed\", null],\n  [\"\u60e0\", \"Wellington\"],\n  [\"\u7075\", null],\n  [\"<0xE9>\", null],\n  [\"<0xA1>\", null],\n  [\"<0xBF>\", null],\n  [\"\u5e7f\", \"Square\"],\n  [\"\u573a\", null],\n  [\"\u65b0\", \"Xintiandi\"],\n  [\"\u5929\", null],\n  [\"\u5730\", null],\n  [\"\u98ce\", \"Style\"],\n  [\"\u60c5\", null],\n  [\"\u8857\", \"Street\"],\n  [\"\u897f\", \"West\"],\n  [\"\u7ad9\", \"Station\"],\n  [\"\u8d70\", \"Corridor\"],\n  [\"<0xE5>\", null],\n  [\"<0xBB>\", null],\n  [\"<0x8A>\", null],\n  [\"\u6c34\", \"Shuixi\"],\n  [\"\u897f\", null],\n  [\"\u9057\", \"Heritage\"],\n  [\"<0xE9>\", null],\n  [\"<0x9F>\", null],\n  [\"<0xB5>\", null],\n  [\"<0xE7>\", null],\n  [\"<0xBB>\", null],\n  [\"<0xBF>\", null],\n  [\"<0xE5>\", null],\n  [\"<0xBB>\", null],\n  [\"<0x8A>\", null],\n  [\"\u4e94\", \"five\"],\n  [\"\u4e2a\", null],\n  [\"<0xE6>\", null],\n  [\"<0xB2>\", null],\n  [\"<0xBF>\", null],\n  [\"\u6cb3\", \"riverside\"],\n  [\"\u666f\", \"landscapes\"],\n  [\"\u89c2\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u6574\u4e2a\u666f\u89c2\u5e26\u501f\u52a9\u539f\u6709\u81ea\u7136\u751f\u6001\u548c\u5386\u53f2\u6587\u5316\u8d44\u6e90\u4f18\u52bf\uff0c\u5bf9\u5357\u8fd0\u6cb3\u4e24\u5cb8\u8fdb\u884c\u4e86\u91cd\u65b0\u89c4\u5212\u548c\u63d0\u5347\u6539\u9020\uff0c\u6253\u9020\u51fa\u5929\u5b50\u6d25\u6e21\u9057\u5740\u516c\u56ed\u60e0\u7075\u987f\u5e7f\u573a\u65b0\u5929\u5730\u98ce\u60c5\u8857\u897f\u7ad9\u8d70\u5eca\u6c34\u897f\u9057\u97f5\u7eff\u5eca\u4e94\u4e2a\u6cbf\u6cb3\u666f\u89c2\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u6574', '\u4e2a', '\u666f', '\u89c2', '\u5e26', '\u501f', '\u52a9', '\u539f', '\u6709', '\u81ea', '\u7136', '\u751f', '\u6001', '\u548c', '\u5386', '\u53f2', '\u6587', '\u5316', '\u8d44', '\u6e90', '\u4f18', '\u52bf', '\uff0c', '\u5bf9', '\u5357', '\u8fd0', '\u6cb3', '\u4e24', '<0xE5>', '<0xB2>', '<0xB8>', '\u8fdb', '\u884c', '\u4e86', '\u91cd', '\u65b0', '\u89c4', '\u5212', '\u548c', '\u63d0', '\u5347', '\u6539', '\u9020', '\uff0c', '\u6253', '\u9020', '\u51fa', '\u5929', '\u5b50', '\u6d25', '\u6e21', '\u9057', '\u5740', '\u516c', '\u56ed', '\u60e0', '\u7075', '<0xE9>', '<0xA1>', '<0xBF>', '\u5e7f', '\u573a', '\u65b0', '\u5929', '\u5730', '\u98ce', '\u60c5', '\u8857', '\u897f', '\u7ad9', '\u8d70', '<0xE5>', '<0xBB>', '<0x8A>', '\u6c34', '\u897f', '\u9057', '<0xE9>', '<0x9F>', '<0xB5>', '<0xE7>', '<0xBB>', '<0xBF>', '<0xE5>', '<0xBB>', '<0x8A>', '\u4e94', '\u4e2a', '<0xE6>', '<0xB2>', '<0xBF>', '\u6cb3', '\u666f', '\u89c2', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u6574\", \"\u0442\u043e\u043b\u044b\u049b\"],\n  [\"\u4e2a\", null],\n  [\"\u666f\", \"\u043b\u0430\u043d\u0434\u0448\u0430\u0444\u0442\"],\n  [\"\u89c2\", null],\n  [\"\u5e26\", null],\n  [\"\u501f\", \"\u043f\u0430\u0439\u0434\u0430\u043b\u0430\u043d\u0443\"],\n  [\"\u52a9\", null],\n  [\"\u539f\", \"\u0431\u0430\u0441\u0442\u0430\u043f\u049b\u044b\"],\n  [\"\u6709\", null],\n  [\"\u81ea\", \"\u0442\u0430\u0431\u0438\u0493\u0438\"],\n  [\"\u7136\", null],\n  [\"\u751f\", \"\u044d\u043a\u043e\u043b\u043e\u0433\u0438\u044f\u043b\u044b\u049b\"],\n  [\"\u6001\", null],\n  [\"\u548c\", null],\n  [\"\u5386\", \"\u0442\u0430\u0440\u0438\u0445\u0438\"],\n  [\"\u53f2\", null],\n  [\"\u6587\", \"\u043c\u04d9\u0434\u0435\u043d\u0438\"],\n  [\"\u5316\", null],\n  [\"\u8d44\", \"\u0440\u0435\u0441\u0443\u0440\u0441\u0442\u0430\u0440\"],\n  [\"\u6e90\", null],\n  [\"\u4f18\", \"\u0430\u0440\u0442\u044b\u049b\u0448\u044b\u043b\u044b\u049b\u0442\u0430\u0440\u044b\"],\n  [\"\u52bf\", null],\n  [\"\uff0c\", null],\n  [\"\u5bf9\", null],\n  [\"\u5357\", \"\u043e\u04a3\u0442\u04af\u0441\u0442\u0456\u043a\"],\n  [\"\u8fd0\", \"\u0430\u0440\u043d\u0430\"],\n  [\"\u6cb3\", null],\n  [\"\u4e24\", \"\u0435\u043a\u0456\"],\n  [\"<0xE5>\", null],\n  [\"<0xB2>\", null],\n  [\"<0xB8>\", null],\n  [\"\u8fdb\", \"\u0436\u04af\u0440\u0433\u0456\u0437\u0443\"],\n  [\"\u884c\", null],\n  [\"\u4e86\", null],\n  [\"\u91cd\", \"\u049b\u0430\u0439\u0442\u0430\"],\n  [\"\u65b0\", null],\n  [\"\u89c4\", \"\u0436\u043e\u0441\u043f\u0430\u0440\u043b\u0430\u0443\"],\n  [\"\u5212\", null],\n  [\"\u548c\", null],\n  [\"\u63d0\", \"\u0436\u0430\u049b\u0441\u0430\u0440\u0442\u0443\"],\n  [\"\u5347\", null],\n  [\"\u6539\", \"\u049b\u0430\u0439\u0442\u0430 \u049b\u04b1\u0440\u0443\"],\n  [\"\u9020\", null],\n  [\"\uff0c\", null],\n  [\"\u6253\", \"\u0436\u0430\u0441\u0430\u0443\"],\n  [\"\u9020\", null],\n  [\"\u51fa\", null],\n  [\"\u5929\", \"\u0422\u044f\u043d\u044c\u0446\u0437\u044b\"],\n  [\"\u5b50\", null],\n  [\"\u6d25\", null],\n  [\"\u6e21\", null],\n  [\"\u9057\", \"\u049b\u0430\u043b\u0434\u044b\u049b\u0442\u0430\u0440\"],\n  [\"\u5740\", null],\n  [\"\u516c\", \"\u0441\u0430\u044f\u0431\u0430\u0493\u044b\"],\n  [\"\u56ed\", null],\n  [\"\u60e0\", \"\u0423\u044d\u043b\u043b\u0438\u043d\u0433\u0442\u043e\u043d\"],\n  [\"\u7075\", null],\n  [\"<0xE9>\", null],\n  [\"<0xA1>\", null],\n  [\"<0xBF>\", null],\n  [\"\u5e7f\", \"\u0430\u043b\u0430\u04a3\u044b\"],\n  [\"\u573a\", null],\n  [\"\u65b0\", \"\u0421\u0438\u043d\u044c\u0442\u044f\u043d\u044c\u0434\u0438\"],\n  [\"\u5929\", null],\n  [\"\u5730\", null],\n  [\"\u98ce\", \"\u0441\u0442\u0438\u043b\u0456\"],\n  [\"\u60c5\", null],\n  [\"\u8857\", \"\u043a\u04e9\u0448\u0435\u0441\u0456\"],\n  [\"\u897f\", \"\u0431\u0430\u0442\u044b\u0441\"],\n  [\"\u7ad9\", \"\u0441\u0442\u0430\u043d\u0446\u0438\u044f\"],\n  [\"\u8d70\", \"\u0434\u04d9\u043b\u0456\u0437\"],\n  [\"<0xE5>\", null],\n  [\"<0xBB>\", null],\n  [\"<0x8A>\", null],\n  [\"\u6c34\", \"\u0428\u0443\u0439\u0441\u0438\"],\n  [\"\u897f\", null],\n  [\"\u9057\", \"\u043c\u04b1\u0440\u0430\"],\n  [\"<0xE9>\", null],\n  [\"<0x9F>\", null],\n  [\"<0xB5>\", null],\n  [\"<0xE7>\", null],\n  [\"<0xBB>\", null],\n  [\"<0xBF>\", null],\n  [\"<0xE5>\", null],\n  [\"<0xBB>\", null],\n  [\"<0x8A>\", null],\n  [\"\u4e94\", \"\u0431\u0435\u0441\"],\n  [\"\u4e2a\", null],\n  [\"<0xE6>\", null],\n  [\"<0xB2>\", null],\n  [\"<0xBF>\", null],\n  [\"\u6cb3\", \"\u04e9\u0437\u0435\u043d\"],\n  [\"\u666f\", \"\u043b\u0430\u043d\u0434\u0448\u0430\u0444\u0442\u044b\"],\n  [\"\u89c2\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u6574\u4e2a\u666f\u89c2\u5e26\u501f\u52a9\u539f\u6709\u81ea\u7136\u751f\u6001\u548c\u5386\u53f2\u6587\u5316\u8d44\u6e90\u4f18\u52bf\uff0c\u5bf9\u5357\u8fd0\u6cb3\u4e24\u5cb8\u8fdb\u884c\u4e86\u91cd\u65b0\u89c4\u5212\u548c\u63d0\u5347\u6539\u9020\uff0c\u6253\u9020\u51fa\u5929\u5b50\u6d25\u6e21\u9057\u5740\u516c\u56ed\u60e0\u7075\u987f\u5e7f\u573a\u65b0\u5929\u5730\u98ce\u60c5\u8857\u897f\u7ad9\u8d70\u5eca\u6c34\u897f\u9057\u97f5\u7eff\u5eca\u4e94\u4e2a\u6cbf\u6cb3\u666f\u89c2\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u6574', '\u4e2a', '\u666f', '\u89c2', '\u5e26', '\u501f', '\u52a9', '\u539f', '\u6709', '\u81ea', '\u7136', '\u751f', '\u6001', '\u548c', '\u5386', '\u53f2', '\u6587', '\u5316', '\u8d44', '\u6e90', '\u4f18', '\u52bf', '\uff0c', '\u5bf9', '\u5357', '\u8fd0', '\u6cb3', '\u4e24', '<0xE5>', '<0xB2>', '<0xB8>', '\u8fdb', '\u884c', '\u4e86', '\u91cd', '\u65b0', '\u89c4', '\u5212', '\u548c', '\u63d0', '\u5347', '\u6539', '\u9020', '\uff0c', '\u6253', '\u9020', '\u51fa', '\u5929', '\u5b50', '\u6d25', '\u6e21', '\u9057', '\u5740', '\u516c', '\u56ed', '\u60e0', '\u7075', '<0xE9>', '<0xA1>', '<0xBF>', '\u5e7f', '\u573a', '\u65b0', '\u5929', '\u5730', '\u98ce', '\u60c5', '\u8857', '\u897f', '\u7ad9', '\u8d70', '<0xE5>', '<0xBB>', '<0x8A>', '\u6c34', '\u897f', '\u9057', '<0xE9>', '<0x9F>', '<0xB5>', '<0xE7>', '<0xBB>', '<0xBF>', '<0xE5>', '<0xBB>', '<0x8A>', '\u4e94', '\u4e2a', '<0xE6>', '<0xB2>', '<0xBF>', '\u6cb3', '\u666f', '\u89c2', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u6574\", \"\u0412\u0435\u0441\u044c\"],\n  [\"\u4e2a\", null],\n  [\"\u666f\", null],\n  [\"\u89c2\", null],\n  [\"\u5e26\", \"\u043b\u0430\u043d\u0434\u0448\u0430\u0444\u0442\u043d\u044b\u0439 \u043f\u043e\u044f\u0441\"],\n  [\"\u501f\", null],\n  [\"\u52a9\", null],\n  [\"\u539f\", null],\n  [\"\u6709\", null],\n  [\"\u81ea\", null],\n  [\"\u7136\", null],\n  [\"\u751f\", null],\n  [\"\u6001\", null],\n  [\"\u548c\", null],\n  [\"\u5386\", null],\n  [\"\u53f2\", null],\n  [\"\u6587\", null],\n  [\"\u5316\", null],\n  [\"\u8d44\", null],\n  [\"\u6e90\", \"\u0440\u0435\u0441\u0443\u0440\u0441\u044b\"],\n  [\"\u4f18\", null],\n  [\"\u52bf\", null],\n  [\"\uff0c\", null],\n  [\"\u5bf9\", null],\n  [\"\u5357\", \"\u042e\u0436\u043d\u044b\u0439\"],\n  [\"\u8fd0\", null],\n  [\"\u6cb3\", \"\u043a\u0430\u043d\u0430\u043b\"],\n  [\"\u4e24\", null],\n  [\"<0xE5>\", null],\n  [\"<0xB2>\", null],\n  [\"<0xB8>\", null],\n  [\"\u8fdb\", null],\n  [\"\u884c\", null],\n  [\"\u4e86\", null],\n  [\"\u91cd\", null],\n  [\"\u65b0\", null],\n  [\"\u89c4\", null],\n  [\"\u5212\", \"\u043f\u0435\u0440\u0435\u043f\u043b\u0430\u043d\u0438\u0440\u043e\u0432\u043a\u0430\"],\n  [\"\u548c\", null],\n  [\"\u63d0\", null],\n  [\"\u5347\", null],\n  [\"\u6539\", null],\n  [\"\u9020\", \"\u0440\u0435\u043a\u043e\u043d\u0441\u0442\u0440\u0443\u043a\u0446\u0438\u044f\"],\n  [\"\uff0c\", null],\n  [\"\u6253\", null],\n  [\"\u9020\", null],\n  [\"\u51fa\", null],\n  [\"\u5929\", null],\n  [\"\u5b50\", null],\n  [\"\u6d25\", null],\n  [\"\u6e21\", \"\u0422\u044f\u043d\u044c\u0446\u0437\u0438\u043d\u044c\u0441\u043a\u0430\u044f \u043f\u0435\u0440\u0435\u043f\u0440\u0430\u0432\u0430\"],\n  [\"\u9057\", null],\n  [\"\u5740\", null],\n  [\"\u516c\", null],\n  [\"\u56ed\", \"\u043f\u0430\u0440\u043a\"],\n  [\"\u60e0\", null],\n  [\"\u7075\", null],\n  [\"<0xE9>\", null],\n  [\"<0xA1>\", null],\n  [\"<0xBF>\", null],\n  [\"\u5e7f\", null],\n  [\"\u573a\", \"\u043f\u043b\u043e\u0449\u0430\u0434\u044c\"],\n  [\"\u65b0\", null],\n  [\"\u5929\", null],\n  [\"\u5730\", \"\u0421\u0438\u043d\u044c\u0442\u044f\u043d\u044c\u0434\u0438\"],\n  [\"\u98ce\", null],\n  [\"\u60c5\", null],\n  [\"\u8857\", \"\u0443\u043b\u0438\u0446\u0430\"],\n  [\"\u897f\", \"\u0417\u0430\u043f\u0430\u0434\u043d\u044b\u0439\"],\n  [\"\u7ad9\", \"\u0432\u043e\u043a\u0437\u0430\u043b\"],\n  [\"\u8d70\", null],\n  [\"<0xE5>\", null],\n  [\"<0xBB>\", null],\n  [\"<0x8A>\", null],\n  [\"\u6c34\", \"\u0412\u043e\u0434\u043d\u044b\u0439\"],\n  [\"\u897f\", \"\u0437\u0430\u043f\u0430\u0434\u043d\u044b\u0439\"],\n  [\"\u9057\", null],\n  [\"<0xE9>\", null],\n  [\"<0x9F>\", null],\n  [\"<0xB5>\", null],\n  [\"<0xE7>\", null],\n  [\"<0xBB>\", null],\n  [\"<0xBF>\", null],\n  [\"<0xE5>\", null],\n  [\"<0xBB>\", null],\n  [\"<0x8A>\", null],\n  [\"\u4e94\", \"\u043f\u044f\u0442\u044c\"],\n  [\"\u4e2a\", null],\n  [\"<0xE6>\", null],\n  [\"<0xB2>\", null],\n  [\"<0xBF>\", null],\n  [\"\u6cb3\", \"\u043f\u0440\u0438\u0431\u0440\u0435\u0436\u043d\u044b\u0445\"],\n  [\"\u666f\", null],\n  [\"\u89c2\", \"\u043b\u0430\u043d\u0434\u0448\u0430\u0444\u0442\u043e\u0432\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u5c45\u6c11\u4eec\u5f3a\u8c03\uff0c\u5728\u6c61\u67d3\u5f80\u5f80\u8f83\u8f7b\u7684\u6e29\u6696\u6708\u4efd\u91cc\u5230\u6237\u5916\u6d3b\u52a8\u5f88\u91cd\u8981\uff0c\u8981\u628a\u90a3\u4e9b\u84dd\u5929\u65e5\u5f53\u4f5c\u5728\u5386\u53f2\u60a0\u4e45\u7684\u80e1\u540c\u91cc\u5f9c\u5f89\u6216\u662f\u63a2\u7d22\u5907\u53d7\u6b22\u8fce\u7684\u65e5\u575b\u516c\u56ed\u7684\u673a\u4f1a\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u5c45', '\u6c11', '\u4eec', '\u5f3a', '\u8c03', '\uff0c', '\u5728', '<0xE6>', '<0xB1>', '<0xA1>', '\u67d3', '\u5f80', '\u5f80', '\u8f83', '\u8f7b', '\u7684', '\u6e29', '<0xE6>', '<0x9A>', '<0x96>', '\u6708', '\u4efd', '\u91cc', '\u5230', '\u6237', '\u5916', '\u6d3b', '\u52a8', '\u5f88', '\u91cd', '\u8981', '\uff0c', '\u8981', '\u628a', '\u90a3', '\u4e9b', '\u84dd', '\u5929', '\u65e5', '\u5f53', '\u4f5c', '\u5728', '\u5386', '\u53f2', '<0xE6>', '<0x82>', '<0xA0>', '\u4e45', '\u7684', '\u80e1', '\u540c', '\u91cc', '<0xE5>', '<0xBE>', '<0x9C>', '<0xE5>', '<0xBE>', '<0x89>', '\u6216', '\u662f', '\u63a2', '\u7d22', '\u5907', '\u53d7', '\u6b22', '\u8fce', '\u7684', '\u65e5', '<0xE5>', '<0x9D>', '<0x9B>', '\u516c', '\u56ed', '\u7684', '\u673a', '\u4f1a', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"\u5c45\", \"Residents\"],\n[\"\u6c11\", null],\n[\"\u4eec\", null],\n[\"\u5f3a\", \"emphasize\"],\n[\"\u8c03\", null],\n[\"\uff0c\", null],\n[\"\u5728\", null],\n[\"<0xE6>\", null],\n[\"<0xB1>\", null],\n[\"<0xA1>\", null],\n[\"\u67d3\", null],\n[\"\u5f80\", \"often\"],\n[\"\u5f80\", null],\n[\"\u8f83\", \"relatively\"],\n[\"\u8f7b\", \"light\"],\n[\"\u7684\", null],\n[\"\u6e29\", \"warm\"],\n[\"<0xE6>\", null],\n[\"<0x9A>\", null],\n[\"<0x96>\", null],\n[\"\u6708\", \"months\"],\n[\"\u4efd\", null],\n[\"\u91cc\", null],\n[\"\u5230\", null],\n[\"\u6237\", \"outdoor\"],\n[\"\u5916\", null],\n[\"\u6d3b\", \"activities\"],\n[\"\u52a8\", null],\n[\"\u5f88\", \"very\"],\n[\"\u91cd\", \"important\"],\n[\"\u8981\", null],\n[\"\uff0c\", null],\n[\"\u8981\", \"should\"],\n[\"\u628a\", null],\n[\"\u90a3\", \"those\"],\n[\"\u4e9b\", null],\n[\"\u84dd\", \"blue\"],\n[\"\u5929\", \"sky\"],\n[\"\u65e5\", \"days\"],\n[\"\u5f53\", \"as\"],\n[\"\u4f5c\", null],\n[\"\u5728\", null],\n[\"\u5386\", \"historic\"],\n[\"\u53f2\", null],\n[\"<0xE6>\", null],\n[\"<0x82>\", null],\n[\"<0xA0>\", null],\n[\"\u4e45\", null],\n[\"\u7684\", null],\n[\"\u80e1\", \"hutongs\"],\n[\"\u540c\", null],\n[\"\u91cc\", null],\n[\"<0xE5>\", null],\n[\"<0xBE>\", null],\n[\"<0x9C>\", null],\n[\"<0xE5>\", null],\n[\"<0xBE>\", null],\n[\"<0x89>\", null],\n[\"\u6216\", \"or\"],\n[\"\u662f\", null],\n[\"\u63a2\", \"explore\"],\n[\"\u7d22\", null],\n[\"\u5907\", \"popular\"],\n[\"\u53d7\", null],\n[\"\u6b22\", null],\n[\"\u8fce\", null],\n[\"\u7684\", null],\n[\"\u65e5\", \"Temple\"],\n[\"<0xE5>\", null],\n[\"<0x9D>\", null],\n[\"<0x9B>\", null],\n[\"\u516c\", \"Park\"],\n[\"\u56ed\", null],\n[\"\u7684\", null],\n[\"\u673a\", \"opportunity\"],\n[\"\u4f1a\", null],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u5c45\u6c11\u4eec\u5f3a\u8c03\uff0c\u5728\u6c61\u67d3\u5f80\u5f80\u8f83\u8f7b\u7684\u6e29\u6696\u6708\u4efd\u91cc\u5230\u6237\u5916\u6d3b\u52a8\u5f88\u91cd\u8981\uff0c\u8981\u628a\u90a3\u4e9b\u84dd\u5929\u65e5\u5f53\u4f5c\u5728\u5386\u53f2\u60a0\u4e45\u7684\u80e1\u540c\u91cc\u5f9c\u5f89\u6216\u662f\u63a2\u7d22\u5907\u53d7\u6b22\u8fce\u7684\u65e5\u575b\u516c\u56ed\u7684\u673a\u4f1a\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u5c45', '\u6c11', '\u4eec', '\u5f3a', '\u8c03', '\uff0c', '\u5728', '<0xE6>', '<0xB1>', '<0xA1>', '\u67d3', '\u5f80', '\u5f80', '\u8f83', '\u8f7b', '\u7684', '\u6e29', '<0xE6>', '<0x9A>', '<0x96>', '\u6708', '\u4efd', '\u91cc', '\u5230', '\u6237', '\u5916', '\u6d3b', '\u52a8', '\u5f88', '\u91cd', '\u8981', '\uff0c', '\u8981', '\u628a', '\u90a3', '\u4e9b', '\u84dd', '\u5929', '\u65e5', '\u5f53', '\u4f5c', '\u5728', '\u5386', '\u53f2', '<0xE6>', '<0x82>', '<0xA0>', '\u4e45', '\u7684', '\u80e1', '\u540c', '\u91cc', '<0xE5>', '<0xBE>', '<0x9C>', '<0xE5>', '<0xBE>', '<0x89>', '\u6216', '\u662f', '\u63a2', '\u7d22', '\u5907', '\u53d7', '\u6b22', '\u8fce', '\u7684', '\u65e5', '<0xE5>', '<0x9D>', '<0x9B>', '\u516c', '\u56ed', '\u7684', '\u673a', '\u4f1a', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u5c45\", \"\u0442\u04b1\u0440\u0493\u044b\u043d\u0434\u0430\u0440\"],\n  [\"\u6c11\", null],\n  [\"\u4eec\", null],\n  [\"\u5f3a\", \"\u0431\u0430\u0441\u0430\"],\n  [\"\u8c03\", null],\n  [\"\uff0c\", null],\n  [\"\u5728\", null],\n  [\"<0xE6>\", null],\n  [\"<0xB1>\", null],\n  [\"<0xA1>\", null],\n  [\"\u67d3\", null],\n  [\"\u5f80\", \"\u0436\u0438\u0456\"],\n  [\"\u5f80\", null],\n  [\"\u8f83\", \"\u0441\u0430\u043b\u044b\u0441\u0442\u044b\u0440\u043c\u0430\u043b\u044b\"],\n  [\"\u8f7b\", null],\n  [\"\u7684\", null],\n  [\"\u6e29\", \"\u0436\u044b\u043b\u044b\"],\n  [\"<0xE6>\", null],\n  [\"<0x9A>\", null],\n  [\"<0x96>\", null],\n  [\"\u6708\", \"\u0430\u0439\u043b\u0430\u0440\u0434\u0430\"],\n  [\"\u4efd\", null],\n  [\"\u91cc\", null],\n  [\"\u5230\", null],\n  [\"\u6237\", \"\u0441\u044b\u0440\u0442\u049b\u044b\"],\n  [\"\u5916\", null],\n  [\"\u6d3b\", \"\u0431\u0435\u043b\u0441\u0435\u043d\u0434\u0456\u043b\u0456\u043a\"],\n  [\"\u52a8\", null],\n  [\"\u5f88\", null],\n  [\"\u91cd\", \"\u043c\u0430\u04a3\u044b\u0437\u0434\u044b\"],\n  [\"\u8981\", null],\n  [\"\uff0c\", null],\n  [\"\u8981\", null],\n  [\"\u628a\", null],\n  [\"\u90a3\", \"\u0441\u043e\u043b\"],\n  [\"\u4e9b\", null],\n  [\"\u84dd\", \"\u043a\u04e9\u043a\"],\n  [\"\u5929\", null],\n  [\"\u65e5\", \"\u043a\u04af\u043d\u0434\u0435\u0440\u0434\u0456\"],\n  [\"\u5f53\", null],\n  [\"\u4f5c\", null],\n  [\"\u5728\", null],\n  [\"\u5386\", \"\u0442\u0430\u0440\u0438\u0445\u0438\"],\n  [\"\u53f2\", null],\n  [\"<0xE6>\", null],\n  [\"<0x82>\", null],\n  [\"<0xA0>\", null],\n  [\"\u4e45\", null],\n  [\"\u7684\", null],\n  [\"\u80e1\", \"\u0445\u0443\u0442\u043e\u04a3\u0434\u0430\u0440\u0434\u0430\"],\n  [\"\u540c\", null],\n  [\"\u91cc\", null],\n  [\"<0xE5>\", null],\n  [\"<0xBE>\", null],\n  [\"<0x9C>\", null],\n  [\"<0xE5>\", null],\n  [\"<0xBE>\", null],\n  [\"<0x89>\", null],\n  [\"\u6216\", \"\u043d\u0435\u043c\u0435\u0441\u0435\"],\n  [\"\u662f\", null],\n  [\"\u63a2\", \"\u0437\u0435\u0440\u0442\u0442\u0435\u0443\"],\n  [\"\u7d22\", null],\n  [\"\u5907\", \"\u0442\u0430\u043d\u044b\u043c\u0430\u043b\"],\n  [\"\u53d7\", null],\n  [\"\u6b22\", null],\n  [\"\u8fce\", null],\n  [\"\u7684\", null],\n  [\"\u65e5\", \"\u0420\u0438\u0442\u0430\u043d\"],\n  [\"<0xE5>\", null],\n  [\"<0x9D>\", null],\n  [\"<0x9B>\", null],\n  [\"\u516c\", \"\u0441\u0430\u044f\u0431\u0430\u0493\u044b\u043d\"],\n  [\"\u56ed\", null],\n  [\"\u7684\", null],\n  [\"\u673a\", \"\u043c\u04af\u043c\u043a\u0456\u043d\u0434\u0456\u043a\"],\n  [\"\u4f1a\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u5c45\u6c11\u4eec\u5f3a\u8c03\uff0c\u5728\u6c61\u67d3\u5f80\u5f80\u8f83\u8f7b\u7684\u6e29\u6696\u6708\u4efd\u91cc\u5230\u6237\u5916\u6d3b\u52a8\u5f88\u91cd\u8981\uff0c\u8981\u628a\u90a3\u4e9b\u84dd\u5929\u65e5\u5f53\u4f5c\u5728\u5386\u53f2\u60a0\u4e45\u7684\u80e1\u540c\u91cc\u5f9c\u5f89\u6216\u662f\u63a2\u7d22\u5907\u53d7\u6b22\u8fce\u7684\u65e5\u575b\u516c\u56ed\u7684\u673a\u4f1a\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u5c45', '\u6c11', '\u4eec', '\u5f3a', '\u8c03', '\uff0c', '\u5728', '<0xE6>', '<0xB1>', '<0xA1>', '\u67d3', '\u5f80', '\u5f80', '\u8f83', '\u8f7b', '\u7684', '\u6e29', '<0xE6>', '<0x9A>', '<0x96>', '\u6708', '\u4efd', '\u91cc', '\u5230', '\u6237', '\u5916', '\u6d3b', '\u52a8', '\u5f88', '\u91cd', '\u8981', '\uff0c', '\u8981', '\u628a', '\u90a3', '\u4e9b', '\u84dd', '\u5929', '\u65e5', '\u5f53', '\u4f5c', '\u5728', '\u5386', '\u53f2', '<0xE6>', '<0x82>', '<0xA0>', '\u4e45', '\u7684', '\u80e1', '\u540c', '\u91cc', '<0xE5>', '<0xBE>', '<0x9C>', '<0xE5>', '<0xBE>', '<0x89>', '\u6216', '\u662f', '\u63a2', '\u7d22', '\u5907', '\u53d7', '\u6b22', '\u8fce', '\u7684', '\u65e5', '<0xE5>', '<0x9D>', '<0x9B>', '\u516c', '\u56ed', '\u7684', '\u673a', '\u4f1a', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u5c45\", \"\u0416\u0438\u0442\u0435\u043b\u0438\"],\n  [\"\u6c11\", null],\n  [\"\u4eec\", null],\n  [\"\u5f3a\", \"\u043f\u043e\u0434\u0447\u0435\u0440\u043a\u0438\u0432\u0430\u044e\u0442\"],\n  [\"\u8c03\", null],\n  [\"\uff0c\", null],\n  [\"\u5728\", null],\n  [\"<0xE6>\", null],\n  [\"<0xB1>\", null],\n  [\"<0xA1>\", null],\n  [\"\u67d3\", null],\n  [\"\u5f80\", \"\u0447\u0430\u0441\u0442\u043e\"],\n  [\"\u5f80\", null],\n  [\"\u8f83\", \"\u043c\u0435\u043d\u0435\u0435\"],\n  [\"\u8f7b\", null],\n  [\"\u7684\", null],\n  [\"\u6e29\", \"\u0442\u0451\u043f\u043b\u044b\u0435\"],\n  [\"<0xE6>\", null],\n  [\"<0x9A>\", null],\n  [\"<0x96>\", null],\n  [\"\u6708\", \"\u043c\u0435\u0441\u044f\u0446\u044b\"],\n  [\"\u4efd\", null],\n  [\"\u91cc\", null],\n  [\"\u5230\", null],\n  [\"\u6237\", \"\u043d\u0430\"],\n  [\"\u5916\", \"\u0443\u043b\u0438\u0446\u0435\"],\n  [\"\u6d3b\", \"\u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c\"],\n  [\"\u52a8\", null],\n  [\"\u5f88\", \"\u043e\u0447\u0435\u043d\u044c\"],\n  [\"\u91cd\", \"\u0432\u0430\u0436\u043d\u0430\"],\n  [\"\u8981\", null],\n  [\"\uff0c\", null],\n  [\"\u8981\", \"\u043d\u0443\u0436\u043d\u043e\"],\n  [\"\u628a\", null],\n  [\"\u90a3\", \"\u044d\u0442\u0438\"],\n  [\"\u4e9b\", null],\n  [\"\u84dd\", \"\u044f\u0441\u043d\u044b\u0435\"],\n  [\"\u5929\", null],\n  [\"\u65e5\", \"\u0434\u043d\u0438\"],\n  [\"\u5f53\", \"\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\"],\n  [\"\u4f5c\", null],\n  [\"\u5728\", \"\u0434\u043b\u044f\"],\n  [\"\u5386\", \"\u0438\u0441\u0442\u043e\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u0445\"],\n  [\"\u53f2\", null],\n  [\"<0xE6>\", null],\n  [\"<0x82>\", null],\n  [\"<0xA0>\", null],\n  [\"\u4e45\", null],\n  [\"\u7684\", null],\n  [\"\u80e1\", \"\u0445\u0443\u0442\u0443\u043d\u0430\u0445\"],\n  [\"\u540c\", null],\n  [\"\u91cc\", null],\n  [\"<0xE5>\", null],\n  [\"<0xBE>\", null],\n  [\"<0x9C>\", null],\n  [\"<0xE5>\", null],\n  [\"<0xBE>\", null],\n  [\"<0x89>\", null],\n  [\"\u6216\", \"\u0438\u043b\u0438\"],\n  [\"\u662f\", null],\n  [\"\u63a2\", \"\u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u044f\"],\n  [\"\u7d22\", null],\n  [\"\u5907\", \"\u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u043e\u0433\u043e\"],\n  [\"\u53d7\", null],\n  [\"\u6b22\", null],\n  [\"\u8fce\", null],\n  [\"\u7684\", null],\n  [\"\u65e5\", \"\u043f\u0430\u0440\u043a\u0430\"],\n  [\"<0xE5>\", null],\n  [\"<0x9D>\", null],\n  [\"<0x9B>\", null],\n  [\"\u516c\", null],\n  [\"\u56ed\", null],\n  [\"\u7684\", null],\n  [\"\u673a\", \"\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c\"],\n  [\"\u4f1a\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u62c9\u624e\u9c81\u65af (Lazarus) \uff0c\u4ee5\u53ca\u5979\u7684\u3001\u5728\u5979\u53bb\u4e16\u540e\u4e3e\u884c\u6d3b\u52a8\u5c06\u5979\u7684\u8bd7\u53e5\u94ed\u523b\u5728\u57fa\u5ea7\u4e0a\u7684\u670b\u53cb\u4eec\uff0c\u5f88\u6e05\u695a\u4ed6\u4eec\u6b63\u5728\u53c2\u4e0e\u7684\u884c\u4e3a\u5177\u6709\u9ad8\u5ea6\u653f\u6cbb\u6027\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u62c9', '<0xE6>', '<0x89>', '<0x8E>', '\u9c81', '\u65af', '\u2581(', 'L', 'azar', 'us', ')', '\u2581', '\uff0c', '\u4ee5', '\u53ca', '\u5979', '\u7684', '\u3001', '\u5728', '\u5979', '\u53bb', '\u4e16', '\u540e', '\u4e3e', '\u884c', '\u6d3b', '\u52a8', '\u5c06', '\u5979', '\u7684', '<0xE8>', '<0xAF>', '<0x97>', '\u53e5', '<0xE9>', '<0x93>', '<0xAD>', '\u523b', '\u5728', '\u57fa', '\u5ea7', '\u4e0a', '\u7684', '<0xE6>', '<0x9C>', '<0x8B>', '\u53cb', '\u4eec', '\uff0c', '\u5f88', '\u6e05', '<0xE6>', '<0xA5>', '<0x9A>', '\u4ed6', '\u4eec', '\u6b63', '\u5728', '\u53c2', '\u4e0e', '\u7684', '\u884c', '\u4e3a', '\u5177', '\u6709', '\u9ad8', '\u5ea6', '\u653f', '\u6cbb', '\u6027', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u62c9\", \"Lazarus\"],\n  [\"<0xE6>\", null],\n  [\"<0x89>\", null],\n  [\"<0x8E>\", null],\n  [\"\u9c81\", null],\n  [\"\u65af\", null],\n  [\"\u2581(\", null],\n  [\"L\", null],\n  [\"azar\", null],\n  [\"us\", null],\n  [\")\", null],\n  [\"\u2581\", null],\n  [\"\uff0c\", null],\n  [\"\u4ee5\", \"and\"],\n  [\"\u53ca\", null],\n  [\"\u5979\", \"her\"],\n  [\"\u7684\", null],\n  [\"\u3001\", null],\n  [\"\u5728\", null],\n  [\"\u5979\", \"her\"],\n  [\"\u53bb\", \"death\"],\n  [\"\u4e16\", null],\n  [\"\u540e\", \"after\"],\n  [\"\u4e3e\", \"held\"],\n  [\"\u884c\", null],\n  [\"\u6d3b\", \"event\"],\n  [\"\u52a8\", null],\n  [\"\u5c06\", null],\n  [\"\u5979\", \"her\"],\n  [\"\u7684\", null],\n  [\"<0xE8>\", null],\n  [\"<0xAF>\", null],\n  [\"<0x97>\", null],\n  [\"\u53e5\", \"verses\"],\n  [\"<0xE9>\", null],\n  [\"<0x93>\", null],\n  [\"<0xAD>\", null],\n  [\"\u523b\", \"inscribed\"],\n  [\"\u5728\", \"on\"],\n  [\"\u57fa\", \"base\"],\n  [\"\u5ea7\", null],\n  [\"\u4e0a\", null],\n  [\"\u7684\", null],\n  [\"<0xE6>\", null],\n  [\"<0x9C>\", null],\n  [\"<0x8B>\", null],\n  [\"\u53cb\", \"friends\"],\n  [\"\u4eec\", null],\n  [\"\uff0c\", null],\n  [\"\u5f88\", \"very\"],\n  [\"\u6e05\", \"clearly\"],\n  [\"<0xE6>\", null],\n  [\"<0xA5>\", null],\n  [\"<0x9A>\", null],\n  [\"\u4ed6\", \"they\"],\n  [\"\u4eec\", null],\n  [\"\u6b63\", \"were\"],\n  [\"\u5728\", null],\n  [\"\u53c2\", \"participating\"],\n  [\"\u4e0e\", null],\n  [\"\u7684\", null],\n  [\"\u884c\", \"action\"],\n  [\"\u4e3a\", null],\n  [\"\u5177\", \"had\"],\n  [\"\u6709\", null],\n  [\"\u9ad8\", \"highly\"],\n  [\"\u5ea6\", null],\n  [\"\u653f\", \"political\"],\n  [\"\u6cbb\", null],\n  [\"\u6027\", \"nature\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u62c9\u624e\u9c81\u65af (Lazarus) \uff0c\u4ee5\u53ca\u5979\u7684\u3001\u5728\u5979\u53bb\u4e16\u540e\u4e3e\u884c\u6d3b\u52a8\u5c06\u5979\u7684\u8bd7\u53e5\u94ed\u523b\u5728\u57fa\u5ea7\u4e0a\u7684\u670b\u53cb\u4eec\uff0c\u5f88\u6e05\u695a\u4ed6\u4eec\u6b63\u5728\u53c2\u4e0e\u7684\u884c\u4e3a\u5177\u6709\u9ad8\u5ea6\u653f\u6cbb\u6027\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u62c9', '<0xE6>', '<0x89>', '<0x8E>', '\u9c81', '\u65af', '\u2581(', 'L', 'azar', 'us', ')', '\u2581', '\uff0c', '\u4ee5', '\u53ca', '\u5979', '\u7684', '\u3001', '\u5728', '\u5979', '\u53bb', '\u4e16', '\u540e', '\u4e3e', '\u884c', '\u6d3b', '\u52a8', '\u5c06', '\u5979', '\u7684', '<0xE8>', '<0xAF>', '<0x97>', '\u53e5', '<0xE9>', '<0x93>', '<0xAD>', '\u523b', '\u5728', '\u57fa', '\u5ea7', '\u4e0a', '\u7684', '<0xE6>', '<0x9C>', '<0x8B>', '\u53cb', '\u4eec', '\uff0c', '\u5f88', '\u6e05', '<0xE6>', '<0xA5>', '<0x9A>', '\u4ed6', '\u4eec', '\u6b63', '\u5728', '\u53c2', '\u4e0e', '\u7684', '\u884c', '\u4e3a', '\u5177', '\u6709', '\u9ad8', '\u5ea6', '\u653f', '\u6cbb', '\u6027', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"\u62c9\", \"\u041b\u0430\u0437\u0430\u0440\u0443\u0441\"],\n[\"<0xE6>\", null],\n[\"<0x89>\", null],\n[\"<0x8E>\", null],\n[\"\u9c81\", null],\n[\"\u65af\", null],\n[\"\u2581(\", null],\n[\"L\", null],\n[\"azar\", null],\n[\"us\", null],\n[\")\", null],\n[\"\u2581\", null],\n[\"\uff0c\", null],\n[\"\u4ee5\", null],\n[\"\u53ca\", null],\n[\"\u5979\", \"\u043e\u043d\u044b\u04a3\"],\n[\"\u7684\", null],\n[\"\u3001\", null],\n[\"\u5728\", null],\n[\"\u5979\", \"\u043e\u043b\"],\n[\"\u53bb\", \"\u049b\u0430\u0439\u0442\u044b\u0441\"],\n[\"\u4e16\", null],\n[\"\u540e\", \"\u0431\u043e\u043b\u0493\u0430\u043d\u043d\u0430\u043d \u043a\u0435\u0439\u0456\u043d\"],\n[\"\u4e3e\", \"\u04e9\u0442\u043a\u0456\u0437\u0456\u043b\u0433\u0435\u043d\"],\n[\"\u884c\", null],\n[\"\u6d3b\", \"\u0456\u0441-\u0448\u0430\u0440\u0430\"],\n[\"\u52a8\", null],\n[\"\u5c06\", null],\n[\"\u5979\", \"\u043e\u043d\u044b\u04a3\"],\n[\"\u7684\", null],\n[\"<0xE8>\", null],\n[\"<0xAF>\", null],\n[\"<0x97>\", null],\n[\"\u53e5\", \"\u04e9\u043b\u0435\u04a3\u0434\u0435\u0440\u0456\"],\n[\"<0xE9>\", null],\n[\"<0x93>\", null],\n[\"<0xAD>\", null],\n[\"\u523b\", \"\u043e\u0439\u044b\u043b\u0493\u0430\u043d\"],\n[\"\u5728\", null],\n[\"\u57fa\", \"\u0442\u04b1\u0493\u044b\u0440\"],\n[\"\u5ea7\", null],\n[\"\u4e0a\", null],\n[\"\u7684\", null],\n[\"<0xE6>\", null],\n[\"<0x9C>\", null],\n[\"<0x8B>\", null],\n[\"\u53cb\", \"\u0434\u043e\u0441\u0442\u0430\u0440\u044b\"],\n[\"\u4eec\", null],\n[\"\uff0c\", null],\n[\"\u5f88\", null],\n[\"\u6e05\", \"\u0436\u0430\u049b\u0441\u044b\"],\n[\"<0xE6>\", null],\n[\"<0xA5>\", null],\n[\"<0x9A>\", null],\n[\"\u4ed6\", \"\u043e\u043b\u0430\u0440\"],\n[\"\u4eec\", null],\n[\"\u6b63\", null],\n[\"\u5728\", null],\n[\"\u53c2\", \"\u049b\u0430\u0442\u044b\u0441\u044b\u043f\"],\n[\"\u4e0e\", null],\n[\"\u7684\", null],\n[\"\u884c\", \"\u04d9\u0440\u0435\u043a\u0435\u0442\"],\n[\"\u4e3a\", null],\n[\"\u5177\", null],\n[\"\u6709\", null],\n[\"\u9ad8\", \"\u0436\u043e\u0493\u0430\u0440\u044b\"],\n[\"\u5ea6\", null],\n[\"\u653f\", \"\u0441\u0430\u044f\u0441\u0438\"],\n[\"\u6cbb\", null],\n[\"\u6027\", \"\u0441\u0438\u043f\u0430\u0442\u049b\u0430 \u0438\u0435\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u62c9\u624e\u9c81\u65af (Lazarus) \uff0c\u4ee5\u53ca\u5979\u7684\u3001\u5728\u5979\u53bb\u4e16\u540e\u4e3e\u884c\u6d3b\u52a8\u5c06\u5979\u7684\u8bd7\u53e5\u94ed\u523b\u5728\u57fa\u5ea7\u4e0a\u7684\u670b\u53cb\u4eec\uff0c\u5f88\u6e05\u695a\u4ed6\u4eec\u6b63\u5728\u53c2\u4e0e\u7684\u884c\u4e3a\u5177\u6709\u9ad8\u5ea6\u653f\u6cbb\u6027\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u62c9', '<0xE6>', '<0x89>', '<0x8E>', '\u9c81', '\u65af', '\u2581(', 'L', 'azar', 'us', ')', '\u2581', '\uff0c', '\u4ee5', '\u53ca', '\u5979', '\u7684', '\u3001', '\u5728', '\u5979', '\u53bb', '\u4e16', '\u540e', '\u4e3e', '\u884c', '\u6d3b', '\u52a8', '\u5c06', '\u5979', '\u7684', '<0xE8>', '<0xAF>', '<0x97>', '\u53e5', '<0xE9>', '<0x93>', '<0xAD>', '\u523b', '\u5728', '\u57fa', '\u5ea7', '\u4e0a', '\u7684', '<0xE6>', '<0x9C>', '<0x8B>', '\u53cb', '\u4eec', '\uff0c', '\u5f88', '\u6e05', '<0xE6>', '<0xA5>', '<0x9A>', '\u4ed6', '\u4eec', '\u6b63', '\u5728', '\u53c2', '\u4e0e', '\u7684', '\u884c', '\u4e3a', '\u5177', '\u6709', '\u9ad8', '\u5ea6', '\u653f', '\u6cbb', '\u6027', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"\u62c9\", \"\u041b\u0430\u0437\u0430\u0440\u0443\u0441\"],\n[\"<0xE6>\", null],\n[\"<0x89>\", null],\n[\"<0x8E>\", null],\n[\"\u9c81\", null],\n[\"\u65af\", null],\n[\"\u2581(\", null],\n[\"L\", null],\n[\"azar\", null],\n[\"us\", null],\n[\")\", null],\n[\"\u2581\", null],\n[\"\uff0c\", null],\n[\"\u4ee5\", \"\u0430 \u0442\u0430\u043a\u0436\u0435\"],\n[\"\u53ca\", null],\n[\"\u5979\", \"\u0435\u0451\"],\n[\"\u7684\", null],\n[\"\u3001\", null],\n[\"\u5728\", null],\n[\"\u5979\", \"\u0435\u0451\"],\n[\"\u53bb\", \"\u0441\u043c\u0435\u0440\u0442\u0438\"],\n[\"\u4e16\", null],\n[\"\u540e\", \"\u043f\u043e\u0441\u043b\u0435\"],\n[\"\u4e3e\", \"\u043f\u0440\u043e\u0432\u0435\u043b\u0438\"],\n[\"\u884c\", null],\n[\"\u6d3b\", \"\u043c\u0435\u0440\u043e\u043f\u0440\u0438\u044f\u0442\u0438\u0435\"],\n[\"\u52a8\", null],\n[\"\u5c06\", null],\n[\"\u5979\", \"\u0435\u0451\"],\n[\"\u7684\", null],\n[\"<0xE8>\", null],\n[\"<0xAF>\", null],\n[\"<0x97>\", null],\n[\"\u53e5\", \"\u0441\u0442\u0438\u0445\u0438\"],\n[\"<0xE9>\", null],\n[\"<0x93>\", null],\n[\"<0xAD>\", null],\n[\"\u523b\", \"\u0432\u044b\u0433\u0440\u0430\u0432\u0438\u0440\u043e\u0432\u0430\u043b\u0438\"],\n[\"\u5728\", \"\u043d\u0430\"],\n[\"\u57fa\", \"\u043f\u043e\u0441\u0442\u0430\u043c\u0435\u043d\u0442\u0435\"],\n[\"\u5ea7\", null],\n[\"\u4e0a\", null],\n[\"\u7684\", null],\n[\"<0xE6>\", null],\n[\"<0x9C>\", null],\n[\"<0x8B>\", null],\n[\"\u53cb\", \"\u0434\u0440\u0443\u0437\u044c\u044f\"],\n[\"\u4eec\", null],\n[\"\uff0c\", null],\n[\"\u5f88\", \"\u0445\u043e\u0440\u043e\u0448\u043e\"],\n[\"\u6e05\", \"\u043f\u043e\u043d\u0438\u043c\u0430\u043b\u0438\"],\n[\"<0xE6>\", null],\n[\"<0xA5>\", null],\n[\"<0x9A>\", null],\n[\"\u4ed6\", \"\u043e\u043d\u0438\"],\n[\"\u4eec\", null],\n[\"\u6b63\", null],\n[\"\u5728\", null],\n[\"\u53c2\", \"\u0443\u0447\u0430\u0441\u0442\u0432\u043e\u0432\u0430\u043b\u0438\"],\n[\"\u4e0e\", null],\n[\"\u7684\", null],\n[\"\u884c\", \"\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438\"],\n[\"\u4e3a\", null],\n[\"\u5177\", \"\u0438\u043c\u0435\u044e\u0449\u0435\u043c\"],\n[\"\u6709\", null],\n[\"\u9ad8\", \"\u0432\u044b\u0441\u043e\u043a\u0443\u044e\"],\n[\"\u5ea6\", null],\n[\"\u653f\", \"\u043f\u043e\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0443\u044e\"],\n[\"\u6cbb\", null],\n[\"\u6027\", \"\u0437\u043d\u0430\u0447\u0438\u043c\u043e\u0441\u0442\u044c\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u6839\u636e\u5927\u4f17\u7684\u6295\u7968\uff0cFacebook CEO\u9a6c\u514b\u00b7\u624e\u514b\u4f2f\u683c\uff08Mark Zuckerberg\uff09\u81ea\u5236\u5bb6\u7528\u4eba\u5de5\u667a\u80fd\u52a9\u7406Jarvis\u5c06\u4f7f\u7528\u7f8e\u56fd\u5f71\u661f\u6469\u6839\u00b7\u5f17\u91cc\u66fc\uff08Morgan Freeman\uff09\u7684\u58f0\u97f3\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u6839', '\u636e', '\u5927', '\u4f17', '\u7684', '\u6295', '\u7968', '\uff0c', 'Face', 'book', '\u2581CEO', '\u9a6c', '\u514b', '\u00b7', '<0xE6>', '<0x89>', '<0x8E>', '\u514b', '<0xE4>', '<0xBC>', '<0xAF>', '\u683c', '\uff08', 'Mark', '\u2581Z', 'ucker', 'berg', '\uff09', '\u81ea', '\u5236', '\u5bb6', '\u7528', '\u4eba', '\u5de5', '\u667a', '\u80fd', '\u52a9', '\u7406', 'J', 'ar', 'vis', '\u5c06', '\u4f7f', '\u7528', '\u7f8e', '\u56fd', '\u5f71', '\u661f', '\u6469', '\u6839', '\u00b7', '<0xE5>', '<0xBC>', '<0x97>', '\u91cc', '<0xE6>', '<0x9B>', '<0xBC>', '\uff08', 'M', 'organ', '\u2581Fre', 'eman', '\uff09', '\u7684', '\u58f0', '\u97f3', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"\u6839\", \"According\"],\n[\"\u636e\", null],\n[\"\u5927\", \"public\"],\n[\"\u4f17\", null],\n[\"\u7684\", null],\n[\"\u6295\", \"vote\"],\n[\"\u7968\", null],\n[\"\uff0c\", null],\n[\"Face\", \"Facebook\"],\n[\"book\", null],\n[\"\u2581CEO\", \"CEO\"],\n[\"\u9a6c\", \"Mark\"],\n[\"\u514b\", null],\n[\"\u00b7\", null],\n[\"<0xE6>\", null],\n[\"<0x89>\", null],\n[\"<0x8E>\", null],\n[\"\u514b\", null],\n[\"<0xE4>\", null],\n[\"<0xBC>\", null],\n[\"<0xAF>\", null],\n[\"\u683c\", null],\n[\"\uff08\", null],\n[\"Mark\", \"Mark\"],\n[\"\u2581Z\", \"Zuckerberg\"],\n[\"ucker\", null],\n[\"berg\", null],\n[\"\uff09\", null],\n[\"\u81ea\", \"self\"],\n[\"\u5236\", null],\n[\"\u5bb6\", \"home\"],\n[\"\u7528\", null],\n[\"\u4eba\", \"artificial\"],\n[\"\u5de5\", null],\n[\"\u667a\", \"intelligence\"],\n[\"\u80fd\", null],\n[\"\u52a9\", \"assistant\"],\n[\"\u7406\", null],\n[\"J\", \"Jarvis\"],\n[\"ar\", null],\n[\"vis\", null],\n[\"\u5c06\", \"will\"],\n[\"\u4f7f\", \"use\"],\n[\"\u7528\", null],\n[\"\u7f8e\", \"American\"],\n[\"\u56fd\", null],\n[\"\u5f71\", \"movie\"],\n[\"\u661f\", \"star\"],\n[\"\u6469\", \"Morgan\"],\n[\"\u6839\", null],\n[\"\u00b7\", null],\n[\"<0xE5>\", null],\n[\"<0xBC>\", null],\n[\"<0x97>\", null],\n[\"\u91cc\", null],\n[\"<0xE6>\", null],\n[\"<0x9B>\", null],\n[\"<0xBC>\", null],\n[\"\uff08\", null],\n[\"M\", \"Morgan\"],\n[\"organ\", null],\n[\"\u2581Fre\", \"Freeman\"],\n[\"eman\", null],\n[\"\uff09\", null],\n[\"\u7684\", null],\n[\"\u58f0\", \"voice\"],\n[\"\u97f3\", null],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u6839\u636e\u5927\u4f17\u7684\u6295\u7968\uff0cFacebook CEO\u9a6c\u514b\u00b7\u624e\u514b\u4f2f\u683c\uff08Mark Zuckerberg\uff09\u81ea\u5236\u5bb6\u7528\u4eba\u5de5\u667a\u80fd\u52a9\u7406Jarvis\u5c06\u4f7f\u7528\u7f8e\u56fd\u5f71\u661f\u6469\u6839\u00b7\u5f17\u91cc\u66fc\uff08Morgan Freeman\uff09\u7684\u58f0\u97f3\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u6839', '\u636e', '\u5927', '\u4f17', '\u7684', '\u6295', '\u7968', '\uff0c', 'Face', 'book', '\u2581CEO', '\u9a6c', '\u514b', '\u00b7', '<0xE6>', '<0x89>', '<0x8E>', '\u514b', '<0xE4>', '<0xBC>', '<0xAF>', '\u683c', '\uff08', 'Mark', '\u2581Z', 'ucker', 'berg', '\uff09', '\u81ea', '\u5236', '\u5bb6', '\u7528', '\u4eba', '\u5de5', '\u667a', '\u80fd', '\u52a9', '\u7406', 'J', 'ar', 'vis', '\u5c06', '\u4f7f', '\u7528', '\u7f8e', '\u56fd', '\u5f71', '\u661f', '\u6469', '\u6839', '\u00b7', '<0xE5>', '<0xBC>', '<0x97>', '\u91cc', '<0xE6>', '<0x9B>', '<0xBC>', '\uff08', 'M', 'organ', '\u2581Fre', 'eman', '\uff09', '\u7684', '\u58f0', '\u97f3', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"\u6839\", \"\u0411\u04b1\u049b\u0430\u0440\u0430\u043d\u044b\u04a3\"],\n[\"\u636e\", null],\n[\"\u5927\", null],\n[\"\u4f17\", null],\n[\"\u7684\", null],\n[\"\u6295\", \"\u0434\u0430\u0443\u044b\u0441\"],\n[\"\u7968\", null],\n[\"\uff0c\", null],\n[\"Face\", \"Facebook\"],\n[\"book\", null],\n[\"\u2581CEO\", \"\u0431\u0430\u0441 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u044b\"],\n[\"\u9a6c\", \"\u041c\u0430\u0440\u043a\"],\n[\"\u514b\", null],\n[\"\u00b7\", null],\n[\"<0xE6>\", null],\n[\"<0x89>\", null],\n[\"<0x8E>\", null],\n[\"\u514b\", null],\n[\"<0xE4>\", null],\n[\"<0xBC>\", null],\n[\"<0xAF>\", null],\n[\"\u683c\", null],\n[\"\uff08\", null],\n[\"Mark\", \"\u041c\u0430\u0440\u043a\"],\n[\"\u2581Z\", \"\u0426\u0443\u043a\u0435\u0440\u0431\u0435\u0440\u0433\"],\n[\"ucker\", null],\n[\"berg\", null],\n[\"\uff09\", null],\n[\"\u81ea\", \"\u04e9\u0437\u0456\"],\n[\"\u5236\", \"\u0436\u0430\u0441\u0430\u0493\u0430\u043d\"],\n[\"\u5bb6\", \"\u04af\u0439\u0434\u0435\u0433\u0456\"],\n[\"\u7528\", null],\n[\"\u4eba\", \"\u0436\u0430\u0441\u0430\u043d\u0434\u044b\"],\n[\"\u5de5\", null],\n[\"\u667a\", \"\u0438\u043d\u0442\u0435\u043b\u043b\u0435\u043a\u0442\"],\n[\"\u80fd\", null],\n[\"\u52a9\", \"\u043a\u04e9\u043c\u0435\u043a\u0448\u0456\u0441\u0456\"],\n[\"\u7406\", null],\n[\"J\", \"\u0414\u0436\u0430\u0440\u0432\u0438\u0441\"],\n[\"ar\", null],\n[\"vis\", null],\n[\"\u5c06\", null],\n[\"\u4f7f\", \"\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043b\u044b\u049b\"],\n[\"\u7528\", null],\n[\"\u7f8e\", null],\n[\"\u56fd\", null],\n[\"\u5f71\", \"\u0430\u043a\u0442\u0435\u0440\"],\n[\"\u661f\", null],\n[\"\u6469\", \"\u041c\u043e\u0440\u0433\u0430\u043d\"],\n[\"\u6839\", null],\n[\"\u00b7\", null],\n[\"<0xE5>\", null],\n[\"<0xBC>\", null],\n[\"<0x97>\", null],\n[\"\u91cc\", null],\n[\"<0xE6>\", null],\n[\"<0x9B>\", null],\n[\"<0xBC>\", null],\n[\"\uff08\", null],\n[\"M\", \"\u041c\u043e\u0440\u0433\u0430\u043d\"],\n[\"organ\", null],\n[\"\u2581Fre\", \"\u0424\u0440\u0438\u043c\u0435\u043d\"],\n[\"eman\", null],\n[\"\uff09\", null],\n[\"\u7684\", null],\n[\"\u58f0\", \"\u0434\u0430\u0443\u044b\u0441\u044b\u043d\"],\n[\"\u97f3\", null],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u6839\u636e\u5927\u4f17\u7684\u6295\u7968\uff0cFacebook CEO\u9a6c\u514b\u00b7\u624e\u514b\u4f2f\u683c\uff08Mark Zuckerberg\uff09\u81ea\u5236\u5bb6\u7528\u4eba\u5de5\u667a\u80fd\u52a9\u7406Jarvis\u5c06\u4f7f\u7528\u7f8e\u56fd\u5f71\u661f\u6469\u6839\u00b7\u5f17\u91cc\u66fc\uff08Morgan Freeman\uff09\u7684\u58f0\u97f3\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u6839', '\u636e', '\u5927', '\u4f17', '\u7684', '\u6295', '\u7968', '\uff0c', 'Face', 'book', '\u2581CEO', '\u9a6c', '\u514b', '\u00b7', '<0xE6>', '<0x89>', '<0x8E>', '\u514b', '<0xE4>', '<0xBC>', '<0xAF>', '\u683c', '\uff08', 'Mark', '\u2581Z', 'ucker', 'berg', '\uff09', '\u81ea', '\u5236', '\u5bb6', '\u7528', '\u4eba', '\u5de5', '\u667a', '\u80fd', '\u52a9', '\u7406', 'J', 'ar', 'vis', '\u5c06', '\u4f7f', '\u7528', '\u7f8e', '\u56fd', '\u5f71', '\u661f', '\u6469', '\u6839', '\u00b7', '<0xE5>', '<0xBC>', '<0x97>', '\u91cc', '<0xE6>', '<0x9B>', '<0xBC>', '\uff08', 'M', 'organ', '\u2581Fre', 'eman', '\uff09', '\u7684', '\u58f0', '\u97f3', '\u3002']"}], "[\n    [\"\u2581\", null],\n    [\"\u6839\", \"\u0441\u043e\u0433\u043b\u0430\u0441\u043d\u043e\"],\n    [\"\u636e\", null],\n    [\"\u5927\", \"\u043c\u0430\u0441\u0441\u043e\u0432\u043e\u043c\u0443\"],\n    [\"\u4f17\", null],\n    [\"\u7684\", null],\n    [\"\u6295\", \"\u0433\u043e\u043b\u043e\u0441\u043e\u0432\u0430\u043d\u0438\u044e\"],\n    [\"\u7968\", null],\n    [\"\uff0c\", null],\n    [\"Face\", \"Facebook\"],\n    [\"book\", null],\n    [\"\u2581CEO\", \"\u0433\u0435\u043d\u0435\u0440\u0430\u043b\u044c\u043d\u044b\u0439 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\"],\n    [\"\u9a6c\", \"\u041c\u0430\u0440\u043a\"],\n    [\"\u514b\", null],\n    [\"\u00b7\", null],\n    [\"<0xE6>\", null],\n    [\"<0x89>\", null],\n    [\"<0x8E>\", null],\n    [\"\u514b\", null],\n    [\"<0xE4>\", null],\n    [\"<0xBC>\", null],\n    [\"<0xAF>\", null],\n    [\"\u683c\", null],\n    [\"\uff08\", null],\n    [\"Mark\", \"\u041c\u0430\u0440\u043a\"],\n    [\"\u2581Z\", \"\u0426\u0443\u043a\u0435\u0440\u0431\u0435\u0440\u0433\"],\n    [\"ucker\", null],\n    [\"berg\", null],\n    [\"\uff09\", null],\n    [\"\u81ea\", \"\u0441\u0430\u043c\u043e\u0441\u0442\u043e\u044f\u0442\u0435\u043b\u044c\u043d\u043e\"],\n    [\"\u5236\", null],\n    [\"\u5bb6\", \"\u0434\u043e\u043c\u0430\u0448\u043d\u0435\u0433\u043e\"],\n    [\"\u7528\", null],\n    [\"\u4eba\", \"\u0438\u0441\u043a\u0443\u0441\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e\"],\n    [\"\u5de5\", null],\n    [\"\u667a\", \"\u0438\u043d\u0442\u0435\u043b\u043b\u0435\u043a\u0442\u0430\"],\n    [\"\u80fd\", null],\n    [\"\u52a9\", \"\u0430\u0441\u0441\u0438\u0441\u0442\u0435\u043d\u0442\u0430\"],\n    [\"\u7406\", null],\n    [\"J\", \"\u0414\u0436\u0430\u0440\u0432\u0438\u0441\"],\n    [\"ar\", null],\n    [\"vis\", null],\n    [\"\u5c06\", \"\u0431\u0443\u0434\u0435\u0442\"],\n    [\"\u4f7f\", \"\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\"],\n    [\"\u7528\", null],\n    [\"\u7f8e\", \"\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0441\u043a\u043e\u0433\u043e\"],\n    [\"\u56fd\", null],\n    [\"\u5f71\", \"\u0430\u043a\u0442\u0435\u0440\u0430\"],\n    [\"\u661f\", null],\n    [\"\u6469\", \"\u041c\u043e\u0440\u0433\u0430\u043d\u0430\"],\n    [\"\u6839\", null],\n    [\"\u00b7\", null],\n    [\"<0xE5>\", null],\n    [\"<0xBC>\", null],\n    [\"<0x97>\", null],\n    [\"\u91cc\", null],\n    [\"<0xE6>\", null],\n    [\"<0x9B>\", null],\n    [\"<0xBC>\", null],\n    [\"\uff08\", null],\n    [\"M\", \"\u041c\u043e\u0440\u0433\u0430\u043d\"],\n    [\"organ\", null],\n    [\"\u2581Fre\", \"\u0424\u0440\u0438\u043c\u0435\u043d\"],\n    [\"eman\", null],\n    [\"\uff09\", null],\n    [\"\u7684\", null],\n    [\"\u58f0\", \"\u0433\u043e\u043b\u043e\u0441\"],\n    [\"\u97f3\", null],\n    [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u4f5c\u4e3a\u66fe\u57282010\u5e74\u5e7f\u5dde\u4e9a\u519b\u4f1a\u65af\u8bfa\u514b\u9879\u76ee\u4e2d\u51fb\u8d25\u4e01\u4fca\u6656\uff0c\u83b7\u5f97\u8be5\u9879\u76ee\u91d1\u724c\u7684\u4e16\u754c\u7ea7\u9009\u624b\uff0c\u5085\u5bb6\u4fca\u5e94\u8be5\u662f\u9999\u6e2f\u73b0\u5f79\u8fd0\u52a8\u5458\u4e2d\u77e5\u540d\u5ea6\u6700\u9ad8\u7684\u65af\u8bfa\u514b\u7403\u624b\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u4f5c', '\u4e3a', '\u66fe', '\u5728', '2', '0', '1', '0', '\u5e74', '\u5e7f', '\u5dde', '\u4e9a', '\u519b', '\u4f1a', '\u65af', '<0xE8>', '<0xAF>', '<0xBA>', '\u514b', '\u9879', '\u76ee', '\u4e2d', '\u51fb', '\u8d25', '\u4e01', '<0xE4>', '<0xBF>', '<0x8A>', '<0xE6>', '<0x99>', '<0x96>', '\uff0c', '\u83b7', '\u5f97', '\u8be5', '\u9879', '\u76ee', '\u91d1', '\u724c', '\u7684', '\u4e16', '\u754c', '\u7ea7', '\u9009', '\u624b', '\uff0c', '<0xE5>', '<0x82>', '<0x85>', '\u5bb6', '<0xE4>', '<0xBF>', '<0x8A>', '\u5e94', '\u8be5', '\u662f', '\u9999', '\u6e2f', '\u73b0', '<0xE5>', '<0xBD>', '<0xB9>', '\u8fd0', '\u52a8', '\u5458', '\u4e2d', '\u77e5', '\u540d', '\u5ea6', '\u6700', '\u9ad8', '\u7684', '\u65af', '<0xE8>', '<0xAF>', '<0xBA>', '\u514b', '\u7403', '\u624b', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u4f5c\", \"As\"],\n  [\"\u4e3a\", null],\n  [\"\u66fe\", null],\n  [\"\u5728\", null],\n  [\"2\", null],\n  [\"0\", null],\n  [\"1\", null],\n  [\"0\", null],\n  [\"\u5e74\", null],\n  [\"\u5e7f\", \"Guangzhou\"],\n  [\"\u5dde\", null],\n  [\"\u4e9a\", \"Asian\"],\n  [\"\u519b\", null],\n  [\"\u4f1a\", null],\n  [\"\u65af\", \"snooker\"],\n  [\"<0xE8>\", null],\n  [\"<0xAF>\", null],\n  [\"<0xBA>\", null],\n  [\"\u514b\", null],\n  [\"\u9879\", \"event\"],\n  [\"\u76ee\", null],\n  [\"\u4e2d\", null],\n  [\"\u51fb\", \"defeated\"],\n  [\"\u8d25\", null],\n  [\"\u4e01\", \"Ding Junhui\"],\n  [\"<0xE4>\", null],\n  [\"<0xBF>\", null],\n  [\"<0x8A>\", null],\n  [\"<0xE6>\", null],\n  [\"<0x99>\", null],\n  [\"<0x96>\", null],\n  [\"\uff0c\", null],\n  [\"\u83b7\", \"won\"],\n  [\"\u5f97\", null],\n  [\"\u8be5\", null],\n  [\"\u9879\", \"event\"],\n  [\"\u76ee\", null],\n  [\"\u91d1\", \"gold\"],\n  [\"\u724c\", null],\n  [\"\u7684\", null],\n  [\"\u4e16\", \"world-class\"],\n  [\"\u754c\", null],\n  [\"\u7ea7\", null],\n  [\"\u9009\", \"player\"],\n  [\"\u624b\", null],\n  [\"\uff0c\", null],\n  [\"<0xE5>\", null],\n  [\"<0x82>\", null],\n  [\"<0x85>\", null],\n  [\"\u5bb6\", \"Marco Fu\"],\n  [\"<0xE4>\", null],\n  [\"<0xBF>\", null],\n  [\"<0x8A>\", null],\n  [\"\u5e94\", \"should\"],\n  [\"\u8be5\", null],\n  [\"\u662f\", \"be\"],\n  [\"\u9999\", \"Hong Kong\"],\n  [\"\u6e2f\", null],\n  [\"\u73b0\", \"current\"],\n  [\"<0xE5>\", null],\n  [\"<0xBD>\", null],\n  [\"<0xB9>\", null],\n  [\"\u8fd0\", \"athlete\"],\n  [\"\u52a8\", null],\n  [\"\u5458\", null],\n  [\"\u4e2d\", null],\n  [\"\u77e5\", \"best-known\"],\n  [\"\u540d\", null],\n  [\"\u5ea6\", null],\n  [\"\u6700\", null],\n  [\"\u9ad8\", null],\n  [\"\u7684\", null],\n  [\"\u65af\", \"snooker\"],\n  [\"<0xE8>\", null],\n  [\"<0xAF>\", null],\n  [\"<0xBA>\", null],\n  [\"\u514b\", null],\n  [\"\u7403\", \"player\"],\n  [\"\u624b\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u4f5c\u4e3a\u66fe\u57282010\u5e74\u5e7f\u5dde\u4e9a\u519b\u4f1a\u65af\u8bfa\u514b\u9879\u76ee\u4e2d\u51fb\u8d25\u4e01\u4fca\u6656\uff0c\u83b7\u5f97\u8be5\u9879\u76ee\u91d1\u724c\u7684\u4e16\u754c\u7ea7\u9009\u624b\uff0c\u5085\u5bb6\u4fca\u5e94\u8be5\u662f\u9999\u6e2f\u73b0\u5f79\u8fd0\u52a8\u5458\u4e2d\u77e5\u540d\u5ea6\u6700\u9ad8\u7684\u65af\u8bfa\u514b\u7403\u624b\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u4f5c', '\u4e3a', '\u66fe', '\u5728', '2', '0', '1', '0', '\u5e74', '\u5e7f', '\u5dde', '\u4e9a', '\u519b', '\u4f1a', '\u65af', '<0xE8>', '<0xAF>', '<0xBA>', '\u514b', '\u9879', '\u76ee', '\u4e2d', '\u51fb', '\u8d25', '\u4e01', '<0xE4>', '<0xBF>', '<0x8A>', '<0xE6>', '<0x99>', '<0x96>', '\uff0c', '\u83b7', '\u5f97', '\u8be5', '\u9879', '\u76ee', '\u91d1', '\u724c', '\u7684', '\u4e16', '\u754c', '\u7ea7', '\u9009', '\u624b', '\uff0c', '<0xE5>', '<0x82>', '<0x85>', '\u5bb6', '<0xE4>', '<0xBF>', '<0x8A>', '\u5e94', '\u8be5', '\u662f', '\u9999', '\u6e2f', '\u73b0', '<0xE5>', '<0xBD>', '<0xB9>', '\u8fd0', '\u52a8', '\u5458', '\u4e2d', '\u77e5', '\u540d', '\u5ea6', '\u6700', '\u9ad8', '\u7684', '\u65af', '<0xE8>', '<0xAF>', '<0xBA>', '\u514b', '\u7403', '\u624b', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u4f5c\", \"\u0440\u0435\u0442\u0456\u043d\u0434\u0435\"],\n  [\"\u4e3a\", null],\n  [\"\u66fe\", \"\u0431\u04b1\u0440\u044b\u043d\"],\n  [\"\u5728\", null],\n  [\"2\", null],\n  [\"0\", null],\n  [\"1\", null],\n  [\"0\", null],\n  [\"\u5e74\", \"\u0436\u044b\u043b\u0493\u044b\"],\n  [\"\u5e7f\", \"\u0413\u0443\u0430\u043d\u0447\u0436\u043e\u0443\"],\n  [\"\u5dde\", null],\n  [\"\u4e9a\", \"\u0410\u0437\u0438\u044f\"],\n  [\"\u519b\", null],\n  [\"\u4f1a\", \"\u043e\u0439\u044b\u043d\u0434\u0430\u0440\u044b\"],\n  [\"\u65af\", \"\u0441\u043d\u0443\u043a\u0435\u0440\"],\n  [\"<0xE8>\", null],\n  [\"<0xAF>\", null],\n  [\"<0xBA>\", null],\n  [\"\u514b\", null],\n  [\"\u9879\", \"\u0436\u0430\u0440\u044b\u0441\u044b\u043d\u0434\u0430\"],\n  [\"\u76ee\", null],\n  [\"\u4e2d\", null],\n  [\"\u51fb\", \"\u0436\u0435\u04a3\u0456\u043f\"],\n  [\"\u8d25\", null],\n  [\"\u4e01\", \"\u0414\u0438\u043d\"],\n  [\"<0xE4>\", null],\n  [\"<0xBF>\", null],\n  [\"<0x8A>\", null],\n  [\"<0xE6>\", null],\n  [\"<0x99>\", null],\n  [\"<0x96>\", null],\n  [\"\uff0c\", null],\n  [\"\u83b7\", \"\u0430\u043b\u0493\u0430\u043d\"],\n  [\"\u5f97\", null],\n  [\"\u8be5\", \"\u043e\u0441\u044b\"],\n  [\"\u9879\", \"\u0436\u0430\u0440\u044b\u0441\u0442\u0430\"],\n  [\"\u76ee\", null],\n  [\"\u91d1\", \"\u0430\u043b\u0442\u044b\u043d\"],\n  [\"\u724c\", \"\u043c\u0435\u0434\u0430\u043b\u044c\"],\n  [\"\u7684\", null],\n  [\"\u4e16\", \"\u04d9\u043b\u0435\u043c\u0434\u0456\u043a\"],\n  [\"\u754c\", null],\n  [\"\u7ea7\", \"\u0434\u0435\u04a3\u0433\u0435\u0439\u0434\u0435\u0433\u0456\"],\n  [\"\u9009\", \"\u043e\u0439\u044b\u043d\u0448\u044b\"],\n  [\"\u624b\", null],\n  [\"\uff0c\", null],\n  [\"<0xE5>\", null],\n  [\"<0x82>\", null],\n  [\"<0x85>\", null],\n  [\"\u5bb6\", \"\u0424\u0443\"],\n  [\"<0xE4>\", null],\n  [\"<0xBF>\", null],\n  [\"<0x8A>\", null],\n  [\"\u5e94\", null],\n  [\"\u8be5\", \"\u043c\u04af\u043c\u043a\u0456\u043d\"],\n  [\"\u662f\", null],\n  [\"\u9999\", \"\u0413\u043e\u043d\u043a\u043e\u043d\u0433\u0442\u044b\u04a3\"],\n  [\"\u6e2f\", null],\n  [\"\u73b0\", \"\u049b\u0430\u0437\u0456\u0440\u0433\u0456\"],\n  [\"<0xE5>\", null],\n  [\"<0xBD>\", null],\n  [\"<0xB9>\", null],\n  [\"\u8fd0\", \"\u0441\u043f\u043e\u0440\u0442\u0448\u044b\u043b\u0430\u0440\u044b\"],\n  [\"\u52a8\", null],\n  [\"\u5458\", null],\n  [\"\u4e2d\", \"\u0430\u0440\u0430\u0441\u044b\u043d\u0434\u0430\"],\n  [\"\u77e5\", \"\u0435\u04a3\"],\n  [\"\u540d\", \"\u0442\u0430\u043d\u044b\u043c\u0430\u043b\"],\n  [\"\u5ea6\", null],\n  [\"\u6700\", null],\n  [\"\u9ad8\", null],\n  [\"\u7684\", null],\n  [\"\u65af\", \"\u0441\u043d\u0443\u043a\u0435\u0440\"],\n  [\"<0xE8>\", null],\n  [\"<0xAF>\", null],\n  [\"<0xBA>\", null],\n  [\"\u514b\", null],\n  [\"\u7403\", \"\u043e\u0439\u044b\u043d\u0448\u044b\u0441\u044b\"],\n  [\"\u624b\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u4f5c\u4e3a\u66fe\u57282010\u5e74\u5e7f\u5dde\u4e9a\u519b\u4f1a\u65af\u8bfa\u514b\u9879\u76ee\u4e2d\u51fb\u8d25\u4e01\u4fca\u6656\uff0c\u83b7\u5f97\u8be5\u9879\u76ee\u91d1\u724c\u7684\u4e16\u754c\u7ea7\u9009\u624b\uff0c\u5085\u5bb6\u4fca\u5e94\u8be5\u662f\u9999\u6e2f\u73b0\u5f79\u8fd0\u52a8\u5458\u4e2d\u77e5\u540d\u5ea6\u6700\u9ad8\u7684\u65af\u8bfa\u514b\u7403\u624b\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u4f5c', '\u4e3a', '\u66fe', '\u5728', '2', '0', '1', '0', '\u5e74', '\u5e7f', '\u5dde', '\u4e9a', '\u519b', '\u4f1a', '\u65af', '<0xE8>', '<0xAF>', '<0xBA>', '\u514b', '\u9879', '\u76ee', '\u4e2d', '\u51fb', '\u8d25', '\u4e01', '<0xE4>', '<0xBF>', '<0x8A>', '<0xE6>', '<0x99>', '<0x96>', '\uff0c', '\u83b7', '\u5f97', '\u8be5', '\u9879', '\u76ee', '\u91d1', '\u724c', '\u7684', '\u4e16', '\u754c', '\u7ea7', '\u9009', '\u624b', '\uff0c', '<0xE5>', '<0x82>', '<0x85>', '\u5bb6', '<0xE4>', '<0xBF>', '<0x8A>', '\u5e94', '\u8be5', '\u662f', '\u9999', '\u6e2f', '\u73b0', '<0xE5>', '<0xBD>', '<0xB9>', '\u8fd0', '\u52a8', '\u5458', '\u4e2d', '\u77e5', '\u540d', '\u5ea6', '\u6700', '\u9ad8', '\u7684', '\u65af', '<0xE8>', '<0xAF>', '<0xBA>', '\u514b', '\u7403', '\u624b', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u4f5c\", \"\u041a\u0430\u043a\"],\n  [\"\u4e3a\", null],\n  [\"\u66fe\", null],\n  [\"\u5728\", null],\n  [\"2\", null],\n  [\"0\", null],\n  [\"1\", null],\n  [\"0\", null],\n  [\"\u5e74\", \"\u0433\u043e\u0434\u0443\"],\n  [\"\u5e7f\", \"\u0413\u0443\u0430\u043d\u0447\u0436\u043e\u0443\"],\n  [\"\u5dde\", null],\n  [\"\u4e9a\", \"\u0410\u0437\u0438\u0430\u0442\u0441\u043a\u0438\u0445\"],\n  [\"\u519b\", null],\n  [\"\u4f1a\", \"\u0438\u0433\u0440\"],\n  [\"\u65af\", \"\u0441\u043d\u0443\u043a\u0435\u0440\"],\n  [\"<0xE8>\", null],\n  [\"<0xAF>\", null],\n  [\"<0xBA>\", null],\n  [\"\u514b\", null],\n  [\"\u9879\", null],\n  [\"\u76ee\", null],\n  [\"\u4e2d\", null],\n  [\"\u51fb\", \"\u043f\u043e\u0431\u0435\u0434\u0438\u0432\u0448\u0438\u0439\"],\n  [\"\u8d25\", null],\n  [\"\u4e01\", \"\u0414\u0438\u043d\"],\n  [\"<0xE4>\", null],\n  [\"<0xBF>\", null],\n  [\"<0x8A>\", null],\n  [\"<0xE6>\", null],\n  [\"<0x99>\", null],\n  [\"<0x96>\", null],\n  [\"\uff0c\", null],\n  [\"\u83b7\", \"\u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0439\"],\n  [\"\u5f97\", null],\n  [\"\u8be5\", null],\n  [\"\u9879\", null],\n  [\"\u76ee\", null],\n  [\"\u91d1\", \"\u0437\u043e\u043b\u043e\u0442\u0443\u044e\"],\n  [\"\u724c\", \"\u043c\u0435\u0434\u0430\u043b\u044c\"],\n  [\"\u7684\", null],\n  [\"\u4e16\", \"\u043c\u0438\u0440\u043e\u0432\u043e\u0433\u043e\"],\n  [\"\u754c\", null],\n  [\"\u7ea7\", \"\u043a\u043b\u0430\u0441\u0441\u0430\"],\n  [\"\u9009\", \"\u0438\u0433\u0440\u043e\u043a\"],\n  [\"\u624b\", null],\n  [\"\uff0c\", null],\n  [\"<0xE5>\", null],\n  [\"<0x82>\", null],\n  [\"<0x85>\", null],\n  [\"\u5bb6\", null],\n  [\"<0xE4>\", null],\n  [\"<0xBF>\", null],\n  [\"<0x8A>\", null],\n  [\"\u5e94\", \"\u0434\u043e\u043b\u0436\u0435\u043d\"],\n  [\"\u8be5\", null],\n  [\"\u662f\", \"\u0431\u044b\u0442\u044c\"],\n  [\"\u9999\", \"\u0413\u043e\u043d\u043a\u043e\u043d\u0433\u0430\"],\n  [\"\u6e2f\", null],\n  [\"\u73b0\", \"\u0434\u0435\u0439\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445\"],\n  [\"<0xE5>\", null],\n  [\"<0xBD>\", null],\n  [\"<0xB9>\", null],\n  [\"\u8fd0\", \"\u0441\u043f\u043e\u0440\u0442\u0441\u043c\u0435\u043d\u043e\u0432\"],\n  [\"\u52a8\", null],\n  [\"\u5458\", null],\n  [\"\u4e2d\", \"\u0441\u0440\u0435\u0434\u0438\"],\n  [\"\u77e5\", \"\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u043c\"],\n  [\"\u540d\", null],\n  [\"\u5ea6\", null],\n  [\"\u6700\", \"\u0441\u0430\u043c\u044b\u043c\"],\n  [\"\u9ad8\", null],\n  [\"\u7684\", null],\n  [\"\u65af\", \"\u0441\u043d\u0443\u043a\u0435\u0440\u0430\"],\n  [\"<0xE8>\", null],\n  [\"<0xAF>\", null],\n  [\"<0xBA>\", null],\n  [\"\u514b\", null],\n  [\"\u7403\", null],\n  [\"\u624b\", \"\u0438\u0433\u0440\u043e\u043a\u043e\u043c\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u4e3a\u6b64\uff0c\u5979\u8058\u8bf7\u4e86\u6765\u81ea Duck\uff06Shed \u5efa\u7b51\u548c\u8bbe\u8ba1\u5de5\u4f5c\u5ba4\uff08\u4f4d\u4e8e\u8bfa\u4e01\u5c71\uff09\u7684\u57c3\u5c14\u51e1\u00b7\u963f\u624e\u8fea\u6765\u5229\u7528\u8fd9\u4e2a 12.5 \u82f1\u5c3a\u9ad8\u7684\u5929\u82b1\u677f\uff0c\u7cbe\u5fc3\u6253\u9020\u4e00\u4e2a\u5bb6\u3002\u8fd9\u4e2a\u5bb6\u4e0d\u4ec5\u662f\u5f00\u53d1\u9879\u76ee\uff0c\u8fd8\u662f\u4e54\u5b89\u59ae\u5c45\u4f4f\u7684\u5730\u65b9\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u4e3a', '\u6b64', '\uff0c', '\u5979', '<0xE8>', '<0x81>', '<0x98>', '\u8bf7', '\u4e86', '\u6765', '\u81ea', '\u2581D', 'uck', '<0xEF>', '<0xBC>', '<0x86>', 'S', 'hed', '\u2581', '\u5efa', '<0xE7>', '<0xAD>', '<0x91>', '\u548c', '\u8bbe', '\u8ba1', '\u5de5', '\u4f5c', '\u5ba4', '\uff08', '\u4f4d', '\u4e8e', '<0xE8>', '<0xAF>', '<0xBA>', '\u4e01', '\u5c71', '\uff09', '\u7684', '<0xE5>', '<0x9F>', '<0x83>', '\u5c14', '<0xE5>', '<0x87>', '<0xA1>', '\u00b7', '\u963f', '<0xE6>', '<0x89>', '<0x8E>', '<0xE8>', '<0xBF>', '<0xAA>', '\u6765', '\u5229', '\u7528', '\u8fd9', '\u4e2a', '\u2581', '1', '2', '.', '5', '\u2581', '\u82f1', '\u5c3a', '\u9ad8', '\u7684', '\u5929', '\u82b1', '\u677f', '\uff0c', '\u7cbe', '\u5fc3', '\u6253', '\u9020', '\u4e00\u4e2a', '\u5bb6', '\u3002', '\u8fd9', '\u4e2a', '\u5bb6', '\u4e0d', '\u4ec5', '\u662f', '\u5f00', '\u53d1', '\u9879', '\u76ee', '\uff0c', '\u8fd8', '\u662f', '<0xE4>', '<0xB9>', '<0x94>', '\u5b89', '<0xE5>', '<0xA6>', '<0xAE>', '\u5c45', '\u4f4f', '\u7684', '\u5730', '\u65b9', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u4e3a\", \"For\"],\n  [\"\u6b64\", null],\n  [\"\uff0c\", null],\n  [\"\u5979\", \"she\"],\n  [\"<0xE8>\", null],\n  [\"<0x81>\", null],\n  [\"<0x98>\", null],\n  [\"\u8bf7\", \"hired\"],\n  [\"\u4e86\", null],\n  [\"\u6765\", \"from\"],\n  [\"\u81ea\", null],\n  [\"\u2581D\", \"Duck\"],\n  [\"uck\", null],\n  [\"<0xEF>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x86>\", null],\n  [\"S\", \"Shed\"],\n  [\"hed\", null],\n  [\"\u2581\", null],\n  [\"\u5efa\", \"architecture\"],\n  [\"<0xE7>\", null],\n  [\"<0xAD>\", null],\n  [\"<0x91>\", null],\n  [\"\u548c\", \"and\"],\n  [\"\u8bbe\", \"design\"],\n  [\"\u8ba1\", null],\n  [\"\u5de5\", \"studio\"],\n  [\"\u4f5c\", null],\n  [\"\u5ba4\", null],\n  [\"\uff08\", null],\n  [\"\u4f4d\", \"located\"],\n  [\"\u4e8e\", null],\n  [\"<0xE8>\", null],\n  [\"<0xAF>\", null],\n  [\"<0xBA>\", null],\n  [\"\u4e01\", \"Notting\"],\n  [\"\u5c71\", \"Hill\"],\n  [\"\uff09\", null],\n  [\"\u7684\", null],\n  [\"<0xE5>\", null],\n  [\"<0x9F>\", null],\n  [\"<0x83>\", null],\n  [\"\u5c14\", \"Elfan\"],\n  [\"<0xE5>\", null],\n  [\"<0x87>\", null],\n  [\"<0xA1>\", null],\n  [\"\u00b7\", null],\n  [\"\u963f\", \"Azadi\"],\n  [\"<0xE6>\", null],\n  [\"<0x89>\", null],\n  [\"<0x8E>\", null],\n  [\"<0xE8>\", null],\n  [\"<0xBF>\", null],\n  [\"<0xAA>\", null],\n  [\"\u6765\", \"to\"],\n  [\"\u5229\", \"utilize\"],\n  [\"\u7528\", null],\n  [\"\u8fd9\", \"this\"],\n  [\"\u4e2a\", null],\n  [\"\u2581\", null],\n  [\"1\", \"12.5\"],\n  [\"2\", null],\n  [\".\", null],\n  [\"5\", null],\n  [\"\u2581\", null],\n  [\"\u82f1\", \"foot\"],\n  [\"\u5c3a\", null],\n  [\"\u9ad8\", \"high\"],\n  [\"\u7684\", null],\n  [\"\u5929\", \"ceiling\"],\n  [\"\u82b1\", null],\n  [\"\u677f\", null],\n  [\"\uff0c\", null],\n  [\"\u7cbe\", \"meticulously\"],\n  [\"\u5fc3\", null],\n  [\"\u6253\", \"create\"],\n  [\"\u9020\", null],\n  [\"\u4e00\u4e2a\", \"a\"],\n  [\"\u5bb6\", \"home\"],\n  [\"\u3002\", null],\n  [\"\u8fd9\", \"This\"],\n  [\"\u4e2a\", null],\n  [\"\u5bb6\", \"home\"],\n  [\"\u4e0d\", \"not\"],\n  [\"\u4ec5\", \"only\"],\n  [\"\u662f\", \"is\"],\n  [\"\u5f00\", \"development\"],\n  [\"\u53d1\", null],\n  [\"\u9879\", \"project\"],\n  [\"\u76ee\", null],\n  [\"\uff0c\", null],\n  [\"\u8fd8\", \"but also\"],\n  [\"\u662f\", \"is\"],\n  [\"<0xE4>\", null],\n  [\"<0xB9>\", null],\n  [\"<0x94>\", null],\n  [\"\u5b89\", \"Joanne's\"],\n  [\"<0xE5>\", null],\n  [\"<0xA6>\", null],\n  [\"<0xAE>\", null],\n  [\"\u5c45\", \"living\"],\n  [\"\u4f4f\", null],\n  [\"\u7684\", null],\n  [\"\u5730\", \"place\"],\n  [\"\u65b9\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u4e3a\u6b64\uff0c\u5979\u8058\u8bf7\u4e86\u6765\u81ea Duck\uff06Shed \u5efa\u7b51\u548c\u8bbe\u8ba1\u5de5\u4f5c\u5ba4\uff08\u4f4d\u4e8e\u8bfa\u4e01\u5c71\uff09\u7684\u57c3\u5c14\u51e1\u00b7\u963f\u624e\u8fea\u6765\u5229\u7528\u8fd9\u4e2a 12.5 \u82f1\u5c3a\u9ad8\u7684\u5929\u82b1\u677f\uff0c\u7cbe\u5fc3\u6253\u9020\u4e00\u4e2a\u5bb6\u3002\u8fd9\u4e2a\u5bb6\u4e0d\u4ec5\u662f\u5f00\u53d1\u9879\u76ee\uff0c\u8fd8\u662f\u4e54\u5b89\u59ae\u5c45\u4f4f\u7684\u5730\u65b9\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u4e3a', '\u6b64', '\uff0c', '\u5979', '<0xE8>', '<0x81>', '<0x98>', '\u8bf7', '\u4e86', '\u6765', '\u81ea', '\u2581D', 'uck', '<0xEF>', '<0xBC>', '<0x86>', 'S', 'hed', '\u2581', '\u5efa', '<0xE7>', '<0xAD>', '<0x91>', '\u548c', '\u8bbe', '\u8ba1', '\u5de5', '\u4f5c', '\u5ba4', '\uff08', '\u4f4d', '\u4e8e', '<0xE8>', '<0xAF>', '<0xBA>', '\u4e01', '\u5c71', '\uff09', '\u7684', '<0xE5>', '<0x9F>', '<0x83>', '\u5c14', '<0xE5>', '<0x87>', '<0xA1>', '\u00b7', '\u963f', '<0xE6>', '<0x89>', '<0x8E>', '<0xE8>', '<0xBF>', '<0xAA>', '\u6765', '\u5229', '\u7528', '\u8fd9', '\u4e2a', '\u2581', '1', '2', '.', '5', '\u2581', '\u82f1', '\u5c3a', '\u9ad8', '\u7684', '\u5929', '\u82b1', '\u677f', '\uff0c', '\u7cbe', '\u5fc3', '\u6253', '\u9020', '\u4e00\u4e2a', '\u5bb6', '\u3002', '\u8fd9', '\u4e2a', '\u5bb6', '\u4e0d', '\u4ec5', '\u662f', '\u5f00', '\u53d1', '\u9879', '\u76ee', '\uff0c', '\u8fd8', '\u662f', '<0xE4>', '<0xB9>', '<0x94>', '\u5b89', '<0xE5>', '<0xA6>', '<0xAE>', '\u5c45', '\u4f4f', '\u7684', '\u5730', '\u65b9', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"\u4e3a\", \"\u041e\u0441\u044b\"],\n[\"\u6b64\", null],\n[\"\uff0c\", null],\n[\"\u5979\", \"\u043e\u043b\"],\n[\"<0xE8>\", null],\n[\"<0x81>\", null],\n[\"<0x98>\", null],\n[\"\u8bf7\", null],\n[\"\u4e86\", null],\n[\"\u6765\", null],\n[\"\u81ea\", null],\n[\"\u2581D\", null],\n[\"uck\", null],\n[\"<0xEF>\", null],\n[\"<0xBC>\", null],\n[\"<0x86>\", null],\n[\"S\", null],\n[\"hed\", null],\n[\"\u2581\", null],\n[\"\u5efa\", \"\u049b\u04b1\u0440\u044b\u043b\u044b\u0441\"],\n[\"<0xE7>\", null],\n[\"<0xAD>\", null],\n[\"<0x91>\", null],\n[\"\u548c\", \"\u0436\u04d9\u043d\u0435\"],\n[\"\u8bbe\", \"\u0434\u0438\u0437\u0430\u0439\u043d\"],\n[\"\u8ba1\", null],\n[\"\u5de5\", \"\u0441\u0442\u0443\u0434\u0438\u044f\u0441\u044b\"],\n[\"\u4f5c\", null],\n[\"\u5ba4\", null],\n[\"\uff08\", null],\n[\"\u4f4d\", \"\u043e\u0440\u043d\u0430\u043b\u0430\u0441\u049b\u0430\u043d\"],\n[\"\u4e8e\", null],\n[\"<0xE8>\", null],\n[\"<0xAF>\", null],\n[\"<0xBA>\", null],\n[\"\u4e01\", \"\u041d\u043e\u0442\u0442\u0438\u043d\u0433-\u0425\u0438\u043b\u043b\"],\n[\"\u5c71\", null],\n[\"\uff09\", null],\n[\"\u7684\", null],\n[\"<0xE5>\", null],\n[\"<0x9F>\", null],\n[\"<0x83>\", null],\n[\"\u5c14\", null],\n[\"<0xE5>\", null],\n[\"<0x87>\", null],\n[\"<0xA1>\", null],\n[\"\u00b7\", null],\n[\"\u963f\", \"\u042d\u043b\u044c\u0444\u0430\u043d\"],\n[\"<0xE6>\", null],\n[\"<0x89>\", null],\n[\"<0x8E>\", null],\n[\"<0xE8>\", null],\n[\"<0xBF>\", null],\n[\"<0xAA>\", null],\n[\"\u6765\", null],\n[\"\u5229\", \"\u043f\u0430\u0439\u0434\u0430\u043b\u0430\u043d\u0443\"],\n[\"\u7528\", null],\n[\"\u8fd9\", \"\u0431\u04b1\u043b\"],\n[\"\u4e2a\", null],\n[\"\u2581\", null],\n[\"1\", null],\n[\"2\", null],\n[\".\", null],\n[\"5\", null],\n[\"\u2581\", null],\n[\"\u82f1\", \"\u0444\u0443\u0442\"],\n[\"\u5c3a\", null],\n[\"\u9ad8\", \"\u0431\u0438\u0456\u043a\u0442\u0456\u0433\u0456\"],\n[\"\u7684\", null],\n[\"\u5929\", \"\u0442\u04e9\u0431\u0435\"],\n[\"\u82b1\", null],\n[\"\u677f\", null],\n[\"\uff0c\", null],\n[\"\u7cbe\", \"\u043c\u04b1\u049b\u0438\u044f\u0442\"],\n[\"\u5fc3\", null],\n[\"\u6253\", \"\u0436\u0430\u0441\u0430\u0443\"],\n[\"\u9020\", null],\n[\"\u4e00\u4e2a\", \"\u04af\u0439\"],\n[\"\u5bb6\", null],\n[\"\u3002\", null],\n[\"\u8fd9\", \"\u0411\u04b1\u043b\"],\n[\"\u4e2a\", null],\n[\"\u5bb6\", \"\u04af\u0439\"],\n[\"\u4e0d\", null],\n[\"\u4ec5\", \"\u0442\u0435\u043a\"],\n[\"\u662f\", null],\n[\"\u5f00\", \"\u0436\u043e\u0431\u0430\"],\n[\"\u53d1\", null],\n[\"\u9879\", null],\n[\"\u76ee\", null],\n[\"\uff0c\", null],\n[\"\u8fd8\", \"\u0441\u043e\u043d\u044b\u043c\u0435\u043d\"],\n[\"\u662f\", \"\u049b\u0430\u0442\u0430\u0440\"],\n[\"<0xE4>\", null],\n[\"<0xB9>\", null],\n[\"<0x94>\", null],\n[\"\u5b89\", \"\u0414\u0436\u043e\u0430\u043d\u043d\u0430\"],\n[\"<0xE5>\", null],\n[\"<0xA6>\", null],\n[\"<0xAE>\", null],\n[\"\u5c45\", \"\u0442\u04b1\u0440\u0430\u0442\u044b\u043d\"],\n[\"\u4f4f\", null],\n[\"\u7684\", null],\n[\"\u5730\", \"\u0436\u0435\u0440\"],\n[\"\u65b9\", null],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u4e3a\u6b64\uff0c\u5979\u8058\u8bf7\u4e86\u6765\u81ea Duck\uff06Shed \u5efa\u7b51\u548c\u8bbe\u8ba1\u5de5\u4f5c\u5ba4\uff08\u4f4d\u4e8e\u8bfa\u4e01\u5c71\uff09\u7684\u57c3\u5c14\u51e1\u00b7\u963f\u624e\u8fea\u6765\u5229\u7528\u8fd9\u4e2a 12.5 \u82f1\u5c3a\u9ad8\u7684\u5929\u82b1\u677f\uff0c\u7cbe\u5fc3\u6253\u9020\u4e00\u4e2a\u5bb6\u3002\u8fd9\u4e2a\u5bb6\u4e0d\u4ec5\u662f\u5f00\u53d1\u9879\u76ee\uff0c\u8fd8\u662f\u4e54\u5b89\u59ae\u5c45\u4f4f\u7684\u5730\u65b9\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u4e3a', '\u6b64', '\uff0c', '\u5979', '<0xE8>', '<0x81>', '<0x98>', '\u8bf7', '\u4e86', '\u6765', '\u81ea', '\u2581D', 'uck', '<0xEF>', '<0xBC>', '<0x86>', 'S', 'hed', '\u2581', '\u5efa', '<0xE7>', '<0xAD>', '<0x91>', '\u548c', '\u8bbe', '\u8ba1', '\u5de5', '\u4f5c', '\u5ba4', '\uff08', '\u4f4d', '\u4e8e', '<0xE8>', '<0xAF>', '<0xBA>', '\u4e01', '\u5c71', '\uff09', '\u7684', '<0xE5>', '<0x9F>', '<0x83>', '\u5c14', '<0xE5>', '<0x87>', '<0xA1>', '\u00b7', '\u963f', '<0xE6>', '<0x89>', '<0x8E>', '<0xE8>', '<0xBF>', '<0xAA>', '\u6765', '\u5229', '\u7528', '\u8fd9', '\u4e2a', '\u2581', '1', '2', '.', '5', '\u2581', '\u82f1', '\u5c3a', '\u9ad8', '\u7684', '\u5929', '\u82b1', '\u677f', '\uff0c', '\u7cbe', '\u5fc3', '\u6253', '\u9020', '\u4e00\u4e2a', '\u5bb6', '\u3002', '\u8fd9', '\u4e2a', '\u5bb6', '\u4e0d', '\u4ec5', '\u662f', '\u5f00', '\u53d1', '\u9879', '\u76ee', '\uff0c', '\u8fd8', '\u662f', '<0xE4>', '<0xB9>', '<0x94>', '\u5b89', '<0xE5>', '<0xA6>', '<0xAE>', '\u5c45', '\u4f4f', '\u7684', '\u5730', '\u65b9', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"\u4e3a\", \"\u0414\u043b\u044f\"],\n[\"\u6b64\", null],\n[\"\uff0c\", null],\n[\"\u5979\", \"\u043e\u043d\u0430\"],\n[\"<0xE8>\", null],\n[\"<0x81>\", null],\n[\"<0x98>\", null],\n[\"\u8bf7\", null],\n[\"\u4e86\", null],\n[\"\u6765\", null],\n[\"\u81ea\", null],\n[\"\u2581D\", null],\n[\"uck\", null],\n[\"<0xEF>\", null],\n[\"<0xBC>\", null],\n[\"<0x86>\", null],\n[\"S\", null],\n[\"hed\", null],\n[\"\u2581\", null],\n[\"\u5efa\", \"\u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u043d\u043e\u0439\"],\n[\"<0xE7>\", null],\n[\"<0xAD>\", null],\n[\"<0x91>\", null],\n[\"\u548c\", \"\u0438\"],\n[\"\u8bbe\", \"\u0434\u0438\u0437\u0430\u0439\u043d\u0435\u0440\u0441\u043a\u043e\u0439\"],\n[\"\u8ba1\", null],\n[\"\u5de5\", null],\n[\"\u4f5c\", null],\n[\"\u5ba4\", \"\u0441\u0442\u0443\u0434\u0438\u0438\"],\n[\"\uff08\", null],\n[\"\u4f4d\", \"\u0440\u0430\u0441\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u043d\u043e\u0439\"],\n[\"\u4e8e\", null],\n[\"<0xE8>\", null],\n[\"<0xAF>\", null],\n[\"<0xBA>\", null],\n[\"\u4e01\", \"\u041d\u043e\u0442\u0442\u0438\u043d\u0433-\u0425\u0438\u043b\u043b\u0435\"],\n[\"\u5c71\", null],\n[\"\uff09\", null],\n[\"\u7684\", null],\n[\"<0xE5>\", null],\n[\"<0x9F>\", null],\n[\"<0x83>\", null],\n[\"\u5c14\", null],\n[\"<0xE5>\", null],\n[\"<0x87>\", null],\n[\"<0xA1>\", null],\n[\"\u00b7\", null],\n[\"\u963f\", \"\u042d\u043b\u044c\u0432\u0430\u043d\u0430\"],\n[\"<0xE6>\", null],\n[\"<0x89>\", null],\n[\"<0x8E>\", null],\n[\"<0xE8>\", null],\n[\"<0xBF>\", null],\n[\"<0xAA>\", null],\n[\"\u6765\", null],\n[\"\u5229\", \"\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\"],\n[\"\u7528\", null],\n[\"\u8fd9\", \"\u044d\u0442\u043e\u0442\"],\n[\"\u4e2a\", null],\n[\"\u2581\", null],\n[\"1\", null],\n[\"2\", null],\n[\".\", null],\n[\"5\", null],\n[\"\u2581\", null],\n[\"\u82f1\", \"\u0444\u0443\u0442\u043e\u0432\u044b\u0439\"],\n[\"\u5c3a\", null],\n[\"\u9ad8\", \"\u0432\u044b\u0441\u043e\u043a\u0438\u0439\"],\n[\"\u7684\", null],\n[\"\u5929\", \"\u043f\u043e\u0442\u043e\u043b\u043e\u043a\"],\n[\"\u82b1\", null],\n[\"\u677f\", null],\n[\"\uff0c\", null],\n[\"\u7cbe\", \"\u0442\u0449\u0430\u0442\u0435\u043b\u044c\u043d\u043e\"],\n[\"\u5fc3\", null],\n[\"\u6253\", \"\u0441\u043e\u0437\u0434\u0430\u0442\u044c\"],\n[\"\u9020\", null],\n[\"\u4e00\u4e2a\", \"\u0434\u043e\u043c\"],\n[\"\u5bb6\", null],\n[\"\u3002\", null],\n[\"\u8fd9\", \"\u042d\u0442\u043e\u0442\"],\n[\"\u4e2a\", null],\n[\"\u5bb6\", \"\u0434\u043e\u043c\"],\n[\"\u4e0d\", \"\u043d\u0435\"],\n[\"\u4ec5\", \"\u0442\u043e\u043b\u044c\u043a\u043e\"],\n[\"\u662f\", null],\n[\"\u5f00\", \"\u043f\u0440\u043e\u0435\u043a\u0442\u043e\u043c\"],\n[\"\u53d1\", null],\n[\"\u9879\", null],\n[\"\u76ee\", null],\n[\"\uff0c\", null],\n[\"\u8fd8\", \"\u043d\u043e\"],\n[\"\u662f\", \"\u0438\"],\n[\"<0xE4>\", null],\n[\"<0xB9>\", null],\n[\"<0x94>\", null],\n[\"\u5b89\", \"\u043c\u0435\u0441\u0442\u043e\u043c\"],\n[\"<0xE5>\", null],\n[\"<0xA6>\", null],\n[\"<0xAE>\", null],\n[\"\u5c45\", \"\u043f\u0440\u043e\u0436\u0438\u0432\u0430\u043d\u0438\u044f\"],\n[\"\u4f4f\", null],\n[\"\u7684\", null],\n[\"\u5730\", \"\u0414\u0436\u043e\u0430\u043d\u043d\u044b\"],\n[\"\u65b9\", null],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u8fd9\u5341\u90e8\u97f3\u4e50IP\u4f5c\u54c1\uff0c\u65b9\u6587\u5c71\u4f1a\u81ea\u5df1\u6267\u5bfc\u4e00\u5230\u4e24\u90e8\uff0c\u5f53\u5468\u8463\u88ab\u95ee\u5230\u662f\u5426\u4f1a\u5e2e\u5fd9\u6267\u5bfc\u65f6\uff0c\u5468\u6770\u4f26\u5766\u8a00\u5f53\u5bfc\u6f14\u538b\u529b\u592a\u5927\uff0c\u5e0c\u671b\u627e\u4e00\u4e9b\u5e74\u8f7b\u7684\u5bfc\u6f14\uff0c\u8ba9\u4ed6\u4eec\u6709\u53d1\u6325\u7684\u7a7a\u95f4\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u8fd9', '\u5341', '\u90e8', '\u97f3', '\u4e50', 'IP', '\u4f5c', '\u54c1', '\uff0c', '\u65b9', '\u6587', '\u5c71', '\u4f1a', '\u81ea', '\u5df1', '\u6267', '\u5bfc', '\u4e00', '\u5230', '\u4e24', '\u90e8', '\uff0c', '\u5f53', '\u5468', '<0xE8>', '<0x91>', '<0xA3>', '\u88ab', '\u95ee', '\u5230', '\u662f', '\u5426', '\u4f1a', '\u5e2e', '<0xE5>', '<0xBF>', '<0x99>', '\u6267', '\u5bfc', '\u65f6', '\uff0c', '\u5468', '<0xE6>', '<0x9D>', '<0xB0>', '<0xE4>', '<0xBC>', '<0xA6>', '<0xE5>', '<0x9D>', '<0xA6>', '\u8a00', '\u5f53', '\u5bfc', '\u6f14', '\u538b', '\u529b', '\u592a', '\u5927', '\uff0c', '\u5e0c', '\u671b', '\u627e', '\u4e00', '\u4e9b', '\u5e74', '\u8f7b', '\u7684', '\u5bfc', '\u6f14', '\uff0c', '\u8ba9', '\u4ed6', '\u4eec', '\u6709', '\u53d1', '<0xE6>', '<0x8C>', '<0xA5>', '\u7684', '\u7a7a', '\u95f4', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u8fd9\", \"These\"],\n  [\"\u5341\", null],\n  [\"\u90e8\", \"ten\"],\n  [\"\u97f3\", null],\n  [\"\u4e50\", \"music\"],\n  [\"IP\", \"IP\"],\n  [\"\u4f5c\", null],\n  [\"\u54c1\", \"works\"],\n  [\"\uff0c\", null],\n  [\"\u65b9\", null],\n  [\"\u6587\", null],\n  [\"\u5c71\", \"Fang Wenshan\"],\n  [\"\u4f1a\", \"will\"],\n  [\"\u81ea\", null],\n  [\"\u5df1\", \"himself\"],\n  [\"\u6267\", null],\n  [\"\u5bfc\", \"direct\"],\n  [\"\u4e00\", \"one\"],\n  [\"\u5230\", \"to\"],\n  [\"\u4e24\", \"two\"],\n  [\"\u90e8\", null],\n  [\"\uff0c\", null],\n  [\"\u5f53\", \"When\"],\n  [\"\u5468\", null],\n  [\"<0xE8>\", null],\n  [\"<0x91>\", null],\n  [\"<0xA3>\", \"Jay\"],\n  [\"\u88ab\", \"was\"],\n  [\"\u95ee\", \"asked\"],\n  [\"\u5230\", null],\n  [\"\u662f\", \"whether\"],\n  [\"\u5426\", null],\n  [\"\u4f1a\", \"would\"],\n  [\"\u5e2e\", null],\n  [\"<0xE5>\", null],\n  [\"<0xBF>\", null],\n  [\"<0x99>\", \"help\"],\n  [\"\u6267\", null],\n  [\"\u5bfc\", \"direct\"],\n  [\"\u65f6\", null],\n  [\"\uff0c\", null],\n  [\"\u5468\", null],\n  [\"<0xE6>\", null],\n  [\"<0x9D>\", null],\n  [\"<0xB0>\", null],\n  [\"<0xE4>\", null],\n  [\"<0xBC>\", null],\n  [\"<0xA6>\", null],\n  [\"<0xE5>\", null],\n  [\"<0x9D>\", null],\n  [\"<0xA6>\", \"Jay Chou\"],\n  [\"\u8a00\", \"said\"],\n  [\"\u5f53\", \"being\"],\n  [\"\u5bfc\", null],\n  [\"\u6f14\", \"director\"],\n  [\"\u538b\", null],\n  [\"\u529b\", \"pressure\"],\n  [\"\u592a\", \"too\"],\n  [\"\u5927\", \"big\"],\n  [\"\uff0c\", null],\n  [\"\u5e0c\", null],\n  [\"\u671b\", \"hopes\"],\n  [\"\u627e\", \"to find\"],\n  [\"\u4e00\", \"some\"],\n  [\"\u4e9b\", null],\n  [\"\u5e74\", null],\n  [\"\u8f7b\", \"young\"],\n  [\"\u7684\", null],\n  [\"\u5bfc\", null],\n  [\"\u6f14\", \"directors\"],\n  [\"\uff0c\", null],\n  [\"\u8ba9\", \"to let\"],\n  [\"\u4ed6\", \"them\"],\n  [\"\u4eec\", null],\n  [\"\u6709\", \"have\"],\n  [\"\u53d1\", null],\n  [\"<0xE6>\", null],\n  [\"<0x8C>\", null],\n  [\"<0xA5>\", \"space\"],\n  [\"\u7684\", null],\n  [\"\u7a7a\", null],\n  [\"\u95f4\", \"to develop\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u8fd9\u5341\u90e8\u97f3\u4e50IP\u4f5c\u54c1\uff0c\u65b9\u6587\u5c71\u4f1a\u81ea\u5df1\u6267\u5bfc\u4e00\u5230\u4e24\u90e8\uff0c\u5f53\u5468\u8463\u88ab\u95ee\u5230\u662f\u5426\u4f1a\u5e2e\u5fd9\u6267\u5bfc\u65f6\uff0c\u5468\u6770\u4f26\u5766\u8a00\u5f53\u5bfc\u6f14\u538b\u529b\u592a\u5927\uff0c\u5e0c\u671b\u627e\u4e00\u4e9b\u5e74\u8f7b\u7684\u5bfc\u6f14\uff0c\u8ba9\u4ed6\u4eec\u6709\u53d1\u6325\u7684\u7a7a\u95f4\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u8fd9', '\u5341', '\u90e8', '\u97f3', '\u4e50', 'IP', '\u4f5c', '\u54c1', '\uff0c', '\u65b9', '\u6587', '\u5c71', '\u4f1a', '\u81ea', '\u5df1', '\u6267', '\u5bfc', '\u4e00', '\u5230', '\u4e24', '\u90e8', '\uff0c', '\u5f53', '\u5468', '<0xE8>', '<0x91>', '<0xA3>', '\u88ab', '\u95ee', '\u5230', '\u662f', '\u5426', '\u4f1a', '\u5e2e', '<0xE5>', '<0xBF>', '<0x99>', '\u6267', '\u5bfc', '\u65f6', '\uff0c', '\u5468', '<0xE6>', '<0x9D>', '<0xB0>', '<0xE4>', '<0xBC>', '<0xA6>', '<0xE5>', '<0x9D>', '<0xA6>', '\u8a00', '\u5f53', '\u5bfc', '\u6f14', '\u538b', '\u529b', '\u592a', '\u5927', '\uff0c', '\u5e0c', '\u671b', '\u627e', '\u4e00', '\u4e9b', '\u5e74', '\u8f7b', '\u7684', '\u5bfc', '\u6f14', '\uff0c', '\u8ba9', '\u4ed6', '\u4eec', '\u6709', '\u53d1', '<0xE6>', '<0x8C>', '<0xA5>', '\u7684', '\u7a7a', '\u95f4', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"\u8fd9\", \"\u0431\u04b1\u043b\"],\n[\"\u5341\", \"\u043e\u043d\"],\n[\"\u90e8\", null],\n[\"\u97f3\", \"\u043c\u0443\u0437\u044b\u043a\u0430\u043b\u044b\u049b\"],\n[\"\u4e50\", null],\n[\"IP\", null],\n[\"\u4f5c\", \"\u0442\u0443\u044b\u043d\u0434\u044b\"],\n[\"\u54c1\", null],\n[\"\uff0c\", null],\n[\"\u65b9\", \"\u0424\u0430\u043d\"],\n[\"\u6587\", null],\n[\"\u5c71\", null],\n[\"\u4f1a\", \"\u04e9\u0437\u0456\"],\n[\"\u81ea\", null],\n[\"\u5df1\", null],\n[\"\u6267\", \"\u0440\u0435\u0436\u0438\u0441\u0441\u0435\u0440\u043b\u0456\u043a\"],\n[\"\u5bfc\", null],\n[\"\u4e00\", \"\u0431\u0456\u0440\"],\n[\"\u5230\", null],\n[\"\u4e24\", \"\u0435\u043a\u0456\"],\n[\"\u90e8\", null],\n[\"\uff0c\", null],\n[\"\u5f53\", \"\u043a\u0435\u0437\u0434\u0435\"],\n[\"\u5468\", \"\u0427\u0436\u043e\u0443\"],\n[\"<0xE8>\", null],\n[\"<0x91>\", null],\n[\"<0xA3>\", null],\n[\"\u88ab\", null],\n[\"\u95ee\", \"\u0441\u04b1\u0440\u0430\u043b\u0493\u0430\u043d\"],\n[\"\u5230\", null],\n[\"\u662f\", null],\n[\"\u5426\", null],\n[\"\u4f1a\", null],\n[\"\u5e2e\", \"\u043a\u04e9\u043c\u0435\u043a\u0442\u0435\u0441\u0443\"],\n[\"<0xE5>\", null],\n[\"<0xBF>\", null],\n[\"<0x99>\", null],\n[\"\u6267\", \"\u0440\u0435\u0436\u0438\u0441\u0441\u0435\u0440\u043b\u0456\u043a\"],\n[\"\u5bfc\", null],\n[\"\u65f6\", null],\n[\"\uff0c\", null],\n[\"\u5468\", \"\u0427\u0436\u043e\u0443\"],\n[\"<0xE6>\", null],\n[\"<0x9D>\", null],\n[\"<0xB0>\", null],\n[\"<0xE4>\", null],\n[\"<0xBC>\", null],\n[\"<0xA6>\", null],\n[\"<0xE5>\", null],\n[\"<0x9D>\", null],\n[\"<0xA6>\", null],\n[\"\u8a00\", \"\u0430\u0439\u0442\u0442\u044b\"],\n[\"\u5f53\", null],\n[\"\u5bfc\", \"\u0440\u0435\u0436\u0438\u0441\u0441\u0435\u0440\"],\n[\"\u6f14\", null],\n[\"\u538b\", \"\u049b\u044b\u0441\u044b\u043c\"],\n[\"\u529b\", null],\n[\"\u592a\", \"\u0442\u044b\u043c\"],\n[\"\u5927\", \"\u04af\u043b\u043a\u0435\u043d\"],\n[\"\uff0c\", null],\n[\"\u5e0c\", \"\u04af\u043c\u0456\u0442\u0442\u0435\u043d\u0435\u0434\u0456\"],\n[\"\u671b\", null],\n[\"\u627e\", \"\u0442\u0430\u0431\u0443\"],\n[\"\u4e00\", null],\n[\"\u4e9b\", null],\n[\"\u5e74\", \"\u0436\u0430\u0441\"],\n[\"\u8f7b\", null],\n[\"\u7684\", null],\n[\"\u5bfc\", \"\u0440\u0435\u0436\u0438\u0441\u0441\u0435\u0440\u043b\u0435\u0440\"],\n[\"\u6f14\", null],\n[\"\uff0c\", null],\n[\"\u8ba9\", \"\u043c\u04af\u043c\u043a\u0456\u043d\u0434\u0456\u043a\"],\n[\"\u4ed6\", \"\u043e\u043b\u0430\u0440\u0493\u0430\"],\n[\"\u4eec\", null],\n[\"\u6709\", null],\n[\"\u53d1\", \"\u043a\u04e9\u0440\u0441\u0435\u0442\u0443\"],\n[\"<0xE6>\", null],\n[\"<0x8C>\", null],\n[\"<0xA5>\", null],\n[\"\u7684\", null],\n[\"\u7a7a\", \"\u043a\u0435\u04a3\u0456\u0441\u0442\u0456\u043a\"],\n[\"\u95f4\", null],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u8fd9\u5341\u90e8\u97f3\u4e50IP\u4f5c\u54c1\uff0c\u65b9\u6587\u5c71\u4f1a\u81ea\u5df1\u6267\u5bfc\u4e00\u5230\u4e24\u90e8\uff0c\u5f53\u5468\u8463\u88ab\u95ee\u5230\u662f\u5426\u4f1a\u5e2e\u5fd9\u6267\u5bfc\u65f6\uff0c\u5468\u6770\u4f26\u5766\u8a00\u5f53\u5bfc\u6f14\u538b\u529b\u592a\u5927\uff0c\u5e0c\u671b\u627e\u4e00\u4e9b\u5e74\u8f7b\u7684\u5bfc\u6f14\uff0c\u8ba9\u4ed6\u4eec\u6709\u53d1\u6325\u7684\u7a7a\u95f4\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u8fd9', '\u5341', '\u90e8', '\u97f3', '\u4e50', 'IP', '\u4f5c', '\u54c1', '\uff0c', '\u65b9', '\u6587', '\u5c71', '\u4f1a', '\u81ea', '\u5df1', '\u6267', '\u5bfc', '\u4e00', '\u5230', '\u4e24', '\u90e8', '\uff0c', '\u5f53', '\u5468', '<0xE8>', '<0x91>', '<0xA3>', '\u88ab', '\u95ee', '\u5230', '\u662f', '\u5426', '\u4f1a', '\u5e2e', '<0xE5>', '<0xBF>', '<0x99>', '\u6267', '\u5bfc', '\u65f6', '\uff0c', '\u5468', '<0xE6>', '<0x9D>', '<0xB0>', '<0xE4>', '<0xBC>', '<0xA6>', '<0xE5>', '<0x9D>', '<0xA6>', '\u8a00', '\u5f53', '\u5bfc', '\u6f14', '\u538b', '\u529b', '\u592a', '\u5927', '\uff0c', '\u5e0c', '\u671b', '\u627e', '\u4e00', '\u4e9b', '\u5e74', '\u8f7b', '\u7684', '\u5bfc', '\u6f14', '\uff0c', '\u8ba9', '\u4ed6', '\u4eec', '\u6709', '\u53d1', '<0xE6>', '<0x8C>', '<0xA5>', '\u7684', '\u7a7a', '\u95f4', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u8fd9\", \"\u042d\u0442\u0438\"],\n  [\"\u5341\", null],\n  [\"\u90e8\", \"\u0434\u0435\u0441\u044f\u0442\u044c\"],\n  [\"\u97f3\", null],\n  [\"\u4e50\", \"\u043c\u0443\u0437\u044b\u043a\u0430\u043b\u044c\u043d\u044b\u0445\"],\n  [\"IP\", \"IP\"],\n  [\"\u4f5c\", null],\n  [\"\u54c1\", \"\u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u043d\u0438\u0439\"],\n  [\"\uff0c\", null],\n  [\"\u65b9\", null],\n  [\"\u6587\", null],\n  [\"\u5c71\", \"\u0424\u0430\u043d \u0412\u044d\u043d\u044c\u0448\u0430\u043d\u044c\"],\n  [\"\u4f1a\", null],\n  [\"\u81ea\", null],\n  [\"\u5df1\", \"\u0441\u0430\u043c\"],\n  [\"\u6267\", null],\n  [\"\u5bfc\", \"\u0441\u0440\u0435\u0436\u0438\u0441\u0441\u0438\u0440\u0443\u0435\u0442\"],\n  [\"\u4e00\", null],\n  [\"\u5230\", null],\n  [\"\u4e24\", \"\u043e\u0434\u0438\u043d-\u0434\u0432\u0430\"],\n  [\"\u90e8\", null],\n  [\"\uff0c\", null],\n  [\"\u5f53\", \"\u041a\u043e\u0433\u0434\u0430\"],\n  [\"\u5468\", null],\n  [\"<0xE8>\", null],\n  [\"<0x91>\", null],\n  [\"<0xA3>\", null],\n  [\"\u88ab\", null],\n  [\"\u95ee\", \"\u0441\u043f\u0440\u043e\u0441\u0438\u043b\u0438\"],\n  [\"\u5230\", null],\n  [\"\u662f\", null],\n  [\"\u5426\", null],\n  [\"\u4f1a\", null],\n  [\"\u5e2e\", null],\n  [\"<0xE5>\", null],\n  [\"<0xBF>\", null],\n  [\"<0x99>\", null],\n  [\"\u6267\", null],\n  [\"\u5bfc\", \"\u0440\u0435\u0436\u0438\u0441\u0441\u0438\u0440\u043e\u0432\u0430\u0442\u044c\"],\n  [\"\u65f6\", null],\n  [\"\uff0c\", null],\n  [\"\u5468\", null],\n  [\"<0xE6>\", null],\n  [\"<0x9D>\", null],\n  [\"<0xB0>\", null],\n  [\"<0xE4>\", null],\n  [\"<0xBC>\", null],\n  [\"<0xA6>\", null],\n  [\"<0xE5>\", null],\n  [\"<0x9D>\", null],\n  [\"<0xA6>\", null],\n  [\"\u8a00\", \"\u0414\u0436\u0435\u0439 \u0427\u043e\u0443\"],\n  [\"\u5f53\", null],\n  [\"\u5bfc\", null],\n  [\"\u6f14\", \"\u0440\u0435\u0436\u0438\u0441\u0441\u0435\u0440\"],\n  [\"\u538b\", null],\n  [\"\u529b\", \"\u0434\u0430\u0432\u043b\u0435\u043d\u0438\u0435\"],\n  [\"\u592a\", null],\n  [\"\u5927\", \"\u0441\u043b\u0438\u0448\u043a\u043e\u043c \u0432\u0435\u043b\u0438\u043a\u043e\"],\n  [\"\uff0c\", null],\n  [\"\u5e0c\", null],\n  [\"\u671b\", \"\u043d\u0430\u0434\u0435\u0435\u0442\u0441\u044f\"],\n  [\"\u627e\", \"\u043d\u0430\u0439\u0442\u0438\"],\n  [\"\u4e00\", null],\n  [\"\u4e9b\", \"\u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e\"],\n  [\"\u5e74\", null],\n  [\"\u8f7b\", \"\u043c\u043e\u043b\u043e\u0434\u044b\u0445\"],\n  [\"\u7684\", null],\n  [\"\u5bfc\", null],\n  [\"\u6f14\", \"\u0440\u0435\u0436\u0438\u0441\u0441\u0435\u0440\u043e\u0432\"],\n  [\"\uff0c\", null],\n  [\"\u8ba9\", \"\u0447\u0442\u043e\u0431\u044b\"],\n  [\"\u4ed6\", null],\n  [\"\u4eec\", \"\u043e\u043d\u0438\"],\n  [\"\u6709\", \"\u0438\u043c\u0435\u043b\u0438\"],\n  [\"\u53d1\", null],\n  [\"<0xE6>\", null],\n  [\"<0x8C>\", null],\n  [\"<0xA5>\", null],\n  [\"\u7684\", null],\n  [\"\u7a7a\", \"\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u043e\"],\n  [\"\u95f4\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n2016\u5e74\uff0ceMarketer\u8ddf\u8e2a\u4e86180\u5bb6\u6700\u5927\u7684\u7535\u5546\u4e4b\u540e\u4f30\u8ba1\uff0c\u7535\u5546\u603b\u9500\u552e\u989d\u7ea6\u4e3a2000\u4ebf\u7f8e\u5143\uff0c\u800c\u5176\u4e2d\u6700\u5927\u768425\u5bb6\u5360\u5230\u4e861590\u591a\u4ebf\u7f8e\u5143\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '2', '0', '1', '6', '\u5e74', '\uff0c', 'e', 'Mark', 'eter', '\u8ddf', '<0xE8>', '<0xB8>', '<0xAA>', '\u4e86', '1', '8', '0', '\u5bb6', '\u6700', '\u5927', '\u7684', '\u7535', '\u5546', '\u4e4b', '\u540e', '<0xE4>', '<0xBC>', '<0xB0>', '\u8ba1', '\uff0c', '\u7535', '\u5546', '\u603b', '\u9500', '\u552e', '\u989d', '\u7ea6', '\u4e3a', '2', '0', '0', '0', '<0xE4>', '<0xBA>', '<0xBF>', '\u7f8e', '\u5143', '\uff0c', '\u800c', '\u5176', '\u4e2d', '\u6700', '\u5927', '\u7684', '2', '5', '\u5bb6', '\u5360', '\u5230', '\u4e86', '1', '5', '9', '0', '\u591a', '<0xE4>', '<0xBA>', '<0xBF>', '\u7f8e', '\u5143', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"2\", \"2\"],\n  [\"0\", \"0\"],\n  [\"1\", \"1\"],\n  [\"6\", \"6\"],\n  [\"\u5e74\", \"In\"],\n  [\"\uff0c\", null],\n  [\"e\", \"e\"],\n  [\"Mark\", \"Mark\"],\n  [\"eter\", null],\n  [\"\u8ddf\", \"tracked\"],\n  [\"<0xE8>\", null],\n  [\"<0xB8>\", null],\n  [\"<0xAA>\", null],\n  [\"\u4e86\", null],\n  [\"1\", \"1\"],\n  [\"8\", \"8\"],\n  [\"0\", \"0\"],\n  [\"\u5bb6\", null],\n  [\"\u6700\", \"largest\"],\n  [\"\u5927\", null],\n  [\"\u7684\", null],\n  [\"\u7535\", \"e-commerce\"],\n  [\"\u5546\", null],\n  [\"\u4e4b\", null],\n  [\"\u540e\", \"after\"],\n  [\"<0xE4>\", null],\n  [\"<0xBC>\", null],\n  [\"<0xB0>\", null],\n  [\"\u8ba1\", \"estimated\"],\n  [\"\uff0c\", null],\n  [\"\u7535\", \"e-commerce\"],\n  [\"\u5546\", null],\n  [\"\u603b\", \"total\"],\n  [\"\u9500\", \"sales\"],\n  [\"\u552e\", null],\n  [\"\u989d\", null],\n  [\"\u7ea6\", \"approximately\"],\n  [\"\u4e3a\", null],\n  [\"2\", \"2\"],\n  [\"0\", \"0\"],\n  [\"0\", \"0\"],\n  [\"0\", \"0\"],\n  [\"<0xE4>\", null],\n  [\"<0xBA>\", null],\n  [\"<0xBF>\", null],\n  [\"\u7f8e\", \"dollars\"],\n  [\"\u5143\", null],\n  [\"\uff0c\", null],\n  [\"\u800c\", \"while\"],\n  [\"\u5176\", \"the\"],\n  [\"\u4e2d\", null],\n  [\"\u6700\", \"largest\"],\n  [\"\u5927\", null],\n  [\"\u7684\", null],\n  [\"2\", \"2\"],\n  [\"5\", \"5\"],\n  [\"\u5bb6\", null],\n  [\"\u5360\", \"accounted for\"],\n  [\"\u5230\", null],\n  [\"\u4e86\", null],\n  [\"1\", \"1\"],\n  [\"5\", \"5\"],\n  [\"9\", \"9\"],\n  [\"0\", \"0\"],\n  [\"\u591a\", \"over\"],\n  [\"<0xE4>\", null],\n  [\"<0xBA>\", null],\n  [\"<0xBF>\", null],\n  [\"\u7f8e\", \"dollars\"],\n  [\"\u5143\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n2016\u5e74\uff0ceMarketer\u8ddf\u8e2a\u4e86180\u5bb6\u6700\u5927\u7684\u7535\u5546\u4e4b\u540e\u4f30\u8ba1\uff0c\u7535\u5546\u603b\u9500\u552e\u989d\u7ea6\u4e3a2000\u4ebf\u7f8e\u5143\uff0c\u800c\u5176\u4e2d\u6700\u5927\u768425\u5bb6\u5360\u5230\u4e861590\u591a\u4ebf\u7f8e\u5143\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '2', '0', '1', '6', '\u5e74', '\uff0c', 'e', 'Mark', 'eter', '\u8ddf', '<0xE8>', '<0xB8>', '<0xAA>', '\u4e86', '1', '8', '0', '\u5bb6', '\u6700', '\u5927', '\u7684', '\u7535', '\u5546', '\u4e4b', '\u540e', '<0xE4>', '<0xBC>', '<0xB0>', '\u8ba1', '\uff0c', '\u7535', '\u5546', '\u603b', '\u9500', '\u552e', '\u989d', '\u7ea6', '\u4e3a', '2', '0', '0', '0', '<0xE4>', '<0xBA>', '<0xBF>', '\u7f8e', '\u5143', '\uff0c', '\u800c', '\u5176', '\u4e2d', '\u6700', '\u5927', '\u7684', '2', '5', '\u5bb6', '\u5360', '\u5230', '\u4e86', '1', '5', '9', '0', '\u591a', '<0xE4>', '<0xBA>', '<0xBF>', '\u7f8e', '\u5143', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"2\", \"2\"],\n  [\"0\", \"0\"],\n  [\"1\", \"1\"],\n  [\"6\", \"6\"],\n  [\"\u5e74\", \"\u0436\u044b\u043b\"],\n  [\"\uff0c\", null],\n  [\"e\", \"e\"],\n  [\"Mark\", \"Mark\"],\n  [\"eter\", null],\n  [\"\u8ddf\", null],\n  [\"<0xE8>\", null],\n  [\"<0xB8>\", null],\n  [\"<0xAA>\", null],\n  [\"\u4e86\", null],\n  [\"1\", \"1\"],\n  [\"8\", \"8\"],\n  [\"0\", \"0\"],\n  [\"\u5bb6\", null],\n  [\"\u6700\", null],\n  [\"\u5927\", \"\u0456\u0440\u0456\"],\n  [\"\u7684\", null],\n  [\"\u7535\", \"\u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u0434\u044b\"],\n  [\"\u5546\", null],\n  [\"\u4e4b\", null],\n  [\"\u540e\", \"\u043a\u0435\u0439\u0456\u043d\"],\n  [\"<0xE4>\", null],\n  [\"<0xBC>\", null],\n  [\"<0xB0>\", null],\n  [\"\u8ba1\", \"\u0435\u0441\u0435\u043f\u0442\u0435\u0434\u0456\"],\n  [\"\uff0c\", null],\n  [\"\u7535\", \"\u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u0434\u044b\"],\n  [\"\u5546\", null],\n  [\"\u603b\", \"\u0436\u0430\u043b\u043f\u044b\"],\n  [\"\u9500\", \"\u0441\u0430\u0442\u0443\"],\n  [\"\u552e\", null],\n  [\"\u989d\", null],\n  [\"\u7ea6\", \"\u0448\u0430\u043c\u0430\u043c\u0435\u043d\"],\n  [\"\u4e3a\", null],\n  [\"2\", \"2\"],\n  [\"0\", \"0\"],\n  [\"0\", \"0\"],\n  [\"0\", \"0\"],\n  [\"<0xE4>\", null],\n  [\"<0xBA>\", null],\n  [\"<0xBF>\", null],\n  [\"\u7f8e\", \"\u0410\u049a\u0428\"],\n  [\"\u5143\", \"\u0434\u043e\u043b\u043b\u0430\u0440\u044b\"],\n  [\"\uff0c\", null],\n  [\"\u800c\", null],\n  [\"\u5176\", null],\n  [\"\u4e2d\", null],\n  [\"\u6700\", null],\n  [\"\u5927\", \"\u0456\u0440\u0456\"],\n  [\"\u7684\", null],\n  [\"2\", \"2\"],\n  [\"5\", \"5\"],\n  [\"\u5bb6\", null],\n  [\"\u5360\", \"\u049b\u04b1\u0440\u0430\u0434\u044b\"],\n  [\"\u5230\", null],\n  [\"\u4e86\", null],\n  [\"1\", \"1\"],\n  [\"5\", \"5\"],\n  [\"9\", \"9\"],\n  [\"0\", \"0\"],\n  [\"\u591a\", null],\n  [\"<0xE4>\", null],\n  [\"<0xBA>\", null],\n  [\"<0xBF>\", null],\n  [\"\u7f8e\", \"\u0410\u049a\u0428\"],\n  [\"\u5143\", \"\u0434\u043e\u043b\u043b\u0430\u0440\u044b\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n2016\u5e74\uff0ceMarketer\u8ddf\u8e2a\u4e86180\u5bb6\u6700\u5927\u7684\u7535\u5546\u4e4b\u540e\u4f30\u8ba1\uff0c\u7535\u5546\u603b\u9500\u552e\u989d\u7ea6\u4e3a2000\u4ebf\u7f8e\u5143\uff0c\u800c\u5176\u4e2d\u6700\u5927\u768425\u5bb6\u5360\u5230\u4e861590\u591a\u4ebf\u7f8e\u5143\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '2', '0', '1', '6', '\u5e74', '\uff0c', 'e', 'Mark', 'eter', '\u8ddf', '<0xE8>', '<0xB8>', '<0xAA>', '\u4e86', '1', '8', '0', '\u5bb6', '\u6700', '\u5927', '\u7684', '\u7535', '\u5546', '\u4e4b', '\u540e', '<0xE4>', '<0xBC>', '<0xB0>', '\u8ba1', '\uff0c', '\u7535', '\u5546', '\u603b', '\u9500', '\u552e', '\u989d', '\u7ea6', '\u4e3a', '2', '0', '0', '0', '<0xE4>', '<0xBA>', '<0xBF>', '\u7f8e', '\u5143', '\uff0c', '\u800c', '\u5176', '\u4e2d', '\u6700', '\u5927', '\u7684', '2', '5', '\u5bb6', '\u5360', '\u5230', '\u4e86', '1', '5', '9', '0', '\u591a', '<0xE4>', '<0xBA>', '<0xBF>', '\u7f8e', '\u5143', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"2\", \"2\"],\n  [\"0\", \"0\"],\n  [\"1\", \"1\"],\n  [\"6\", \"6\"],\n  [\"\u5e74\", \"\u0433\u043e\u0434\"],\n  [\"\uff0c\", null],\n  [\"e\", \"\u044d\"],\n  [\"Mark\", \"\u041c\u0430\u0440\u043a\"],\n  [\"eter\", null],\n  [\"\u8ddf\", null],\n  [\"<0xE8>\", null],\n  [\"<0xB8>\", null],\n  [\"<0xAA>\", null],\n  [\"\u4e86\", null],\n  [\"1\", \"1\"],\n  [\"8\", \"8\"],\n  [\"0\", \"0\"],\n  [\"\u5bb6\", null],\n  [\"\u6700\", \"\u0441\u0430\u043c\u044b\u0445\"],\n  [\"\u5927\", \"\u043a\u0440\u0443\u043f\u043d\u044b\u0445\"],\n  [\"\u7684\", null],\n  [\"\u7535\", \"\u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u043d\u043e\u0439\"],\n  [\"\u5546\", \"\u043a\u043e\u043c\u043c\u0435\u0440\u0446\u0438\u0438\"],\n  [\"\u4e4b\", null],\n  [\"\u540e\", \"\u043f\u043e\u0441\u043b\u0435\"],\n  [\"<0xE4>\", null],\n  [\"<0xBC>\", null],\n  [\"<0xB0>\", null],\n  [\"\u8ba1\", \"\u043e\u0446\u0435\u043d\u0438\u043b\"],\n  [\"\uff0c\", null],\n  [\"\u7535\", \"\u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u043d\u043e\u0439\"],\n  [\"\u5546\", \"\u043a\u043e\u043c\u043c\u0435\u0440\u0446\u0438\u0438\"],\n  [\"\u603b\", \"\u043e\u0431\u0449\u0438\u0439\"],\n  [\"\u9500\", \"\u043e\u0431\u044a\u0435\u043c\"],\n  [\"\u552e\", \"\u043f\u0440\u043e\u0434\u0430\u0436\"],\n  [\"\u989d\", null],\n  [\"\u7ea6\", \"\u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e\"],\n  [\"\u4e3a\", null],\n  [\"2\", \"2\"],\n  [\"0\", \"0\"],\n  [\"0\", \"0\"],\n  [\"0\", \"0\"],\n  [\"<0xE4>\", null],\n  [\"<0xBA>\", null],\n  [\"<0xBF>\", null],\n  [\"\u7f8e\", \"\u0434\u043e\u043b\u043b\u0430\u0440\u043e\u0432\"],\n  [\"\u5143\", null],\n  [\"\uff0c\", null],\n  [\"\u800c\", \"\u0430\"],\n  [\"\u5176\", null],\n  [\"\u4e2d\", null],\n  [\"\u6700\", \"\u0441\u0430\u043c\u044b\u0445\"],\n  [\"\u5927\", \"\u043a\u0440\u0443\u043f\u043d\u044b\u0445\"],\n  [\"\u7684\", null],\n  [\"2\", \"2\"],\n  [\"5\", \"5\"],\n  [\"\u5bb6\", null],\n  [\"\u5360\", \"\u0441\u043e\u0441\u0442\u0430\u0432\u0438\u043b\u0438\"],\n  [\"\u5230\", null],\n  [\"\u4e86\", null],\n  [\"1\", \"1\"],\n  [\"5\", \"5\"],\n  [\"9\", \"9\"],\n  [\"0\", \"0\"],\n  [\"\u591a\", \"\u0431\u043e\u043b\u0435\u0435\"],\n  [\"<0xE4>\", null],\n  [\"<0xBA>\", null],\n  [\"<0xBF>\", null],\n  [\"\u7f8e\", \"\u0434\u043e\u043b\u043b\u0430\u0440\u043e\u0432\"],\n  [\"\u5143\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u4ece\u800c\u6709\u4eba\u56e0\u4e3a\u6536\u5165\u8fc7\u4f4e\u652f\u4ed8\u4e0d\u8d77\u623f\u8d37\uff0c\u4e8e\u662f\u94f6\u884c\u5c31\u5f00\u59cb\u6536\u56de\u623f\u4ea7\uff0c\u5f53\u94f6\u884c\u4ece\u62d6\u6b20\u8d37\u6b3e\u7684\u8d2d\u623f\u8005\u624b\u4e2d\u6536\u56de\u623f\u4ea7\u4e4b\u540e\uff0c\u4ed6\u4eec\u4f1a\u4ee5\u6781\u4f4e\u7684\u4ef7\u683c\u552e\u51fa\uff0c\u4ece\u800c\u5bfc\u81f4\u4e86\u8be5\u5730\u533a\u7684\u623f\u4ef7\u4e00\u6b65\u6b65\u4e0b\u6ed1\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u53c8\u5f15\u53d1\u4e86\u623f\u4ef7\u8fdb\u4e00\u6b65\u7684\u4e0b\u6ed1\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u6076\u6027\u5faa\u73af\uff0c\u5f53\u623f\u4ef7\u4e0b\u8dcc\u5230\u4e00\u5b9a\u7684\u7a0b\u5ea6\u4e4b\u540e\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u8d2d\u623f\u8005\u53d1\u73b0\u81ea\u5df1\u7684\u623f\u5b50\u4ef7\u503c\u751a\u81f3\u5df2\u7ecf\u6bd4\u81ea\u5df1\u7684\u8d37\u6b3e\u8fd8\u4f4e\u4e86\uff0c\u4e8e\u662f\u5c31\u6709\u4e86\u66f4\u591a\u7684\u4eba\u505c\u6b62\u7ee7\u7eed\u8fd8\u623f\u8d37\uff0c\u53c8\u9020\u6210\u4e86\u66f4\u591a\u7684\u574f\u8d26\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u4ece', '\u800c', '\u6709', '\u4eba', '\u56e0', '\u4e3a', '\u6536', '\u5165', '\u8fc7', '\u4f4e', '\u652f', '\u4ed8', '\u4e0d', '\u8d77', '\u623f', '<0xE8>', '<0xB4>', '<0xB7>', '\uff0c', '\u4e8e', '\u662f', '\u94f6', '\u884c', '\u5c31', '\u5f00', '\u59cb', '\u6536', '\u56de', '\u623f', '\u4ea7', '\uff0c', '\u5f53', '\u94f6', '\u884c', '\u4ece', '\u62d6', '<0xE6>', '<0xAC>', '<0xA0>', '<0xE8>', '<0xB4>', '<0xB7>', '\u6b3e', '\u7684', '\u8d2d', '\u623f', '\u8005', '\u624b', '\u4e2d', '\u6536', '\u56de', '\u623f', '\u4ea7', '\u4e4b', '\u540e', '\uff0c', '\u4ed6', '\u4eec', '\u4f1a', '\u4ee5', '\u6781', '\u4f4e', '\u7684', '\u4ef7', '\u683c', '\u552e', '\u51fa', '\uff0c', '\u4ece', '\u800c', '\u5bfc', '\u81f4', '\u4e86', '\u8be5', '\u5730', '\u533a', '\u7684', '\u623f', '\u4ef7', '\u4e00', '\u6b65', '\u6b65', '\u4e0b', '\u6ed1', '\uff0c', '\u8fd9', '\u4e2a', '\u8fc7', '\u7a0b', '\u53c8', '\u5f15', '\u53d1', '\u4e86', '\u623f', '\u4ef7', '\u8fdb', '\u4e00', '\u6b65', '\u7684', '\u4e0b', '\u6ed1', '\uff0c', '\u5f62', '\u6210', '\u4e86', '\u4e00\u4e2a', '<0xE6>', '<0x81>', '<0xB6>', '\u6027', '\u5faa', '\u73af', '\uff0c', '\u5f53', '\u623f', '\u4ef7', '\u4e0b', '<0xE8>', '<0xB7>', '<0x8C>', '\u5230', '\u4e00', '\u5b9a', '\u7684', '\u7a0b', '\u5ea6', '\u4e4b', '\u540e', '\uff0c', '\u8d8a', '\u6765', '\u8d8a', '\u591a', '\u7684', '\u8d2d', '\u623f', '\u8005', '\u53d1', '\u73b0', '\u81ea', '\u5df1', '\u7684', '\u623f', '\u5b50', '\u4ef7', '\u503c', '<0xE7>', '<0x94>', '<0x9A>', '\u81f3', '\u5df2', '\u7ecf', '\u6bd4', '\u81ea', '\u5df1', '\u7684', '<0xE8>', '<0xB4>', '<0xB7>', '\u6b3e', '\u8fd8', '\u4f4e', '\u4e86', '\uff0c', '\u4e8e', '\u662f', '\u5c31', '\u6709', '\u4e86', '\u66f4', '\u591a', '\u7684', '\u4eba', '\u505c', '\u6b62', '\u7ee7', '\u7eed', '\u8fd8', '\u623f', '<0xE8>', '<0xB4>', '<0xB7>', '\uff0c', '\u53c8', '\u9020', '\u6210', '\u4e86', '\u66f4', '\u591a', '\u7684', '<0xE5>', '<0x9D>', '<0x8F>', '\u8d26', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u4ece\", null],\n  [\"\u800c\", null],\n  [\"\u6709\", null],\n  [\"\u4eba\", \"people\"],\n  [\"\u56e0\", null],\n  [\"\u4e3a\", null],\n  [\"\u6536\", \"income\"],\n  [\"\u5165\", null],\n  [\"\u8fc7\", null],\n  [\"\u4f4e\", \"low\"],\n  [\"\u652f\", \"pay\"],\n  [\"\u4ed8\", null],\n  [\"\u4e0d\", null],\n  [\"\u8d77\", null],\n  [\"\u623f\", \"mortgage\"],\n  [\"<0xE8>\", null],\n  [\"<0xB4>\", null],\n  [\"<0xB7>\", null],\n  [\"\uff0c\", null],\n  [\"\u4e8e\", null],\n  [\"\u662f\", null],\n  [\"\u94f6\", \"bank\"],\n  [\"\u884c\", null],\n  [\"\u5c31\", null],\n  [\"\u5f00\", \"start\"],\n  [\"\u59cb\", null],\n  [\"\u6536\", \"repossess\"],\n  [\"\u56de\", null],\n  [\"\u623f\", \"property\"],\n  [\"\u4ea7\", null],\n  [\"\uff0c\", null],\n  [\"\u5f53\", null],\n  [\"\u94f6\", \"bank\"],\n  [\"\u884c\", null],\n  [\"\u4ece\", null],\n  [\"\u62d6\", \"defaulted\"],\n  [\"<0xE6>\", null],\n  [\"<0xAC>\", null],\n  [\"<0xA0>\", null],\n  [\"<0xE8>\", null],\n  [\"<0xB4>\", null],\n  [\"<0xB7>\", null],\n  [\"\u6b3e\", null],\n  [\"\u7684\", null],\n  [\"\u8d2d\", \"homebuyers\"],\n  [\"\u623f\", null],\n  [\"\u8005\", null],\n  [\"\u624b\", null],\n  [\"\u4e2d\", null],\n  [\"\u6536\", \"repossess\"],\n  [\"\u56de\", null],\n  [\"\u623f\", \"property\"],\n  [\"\u4ea7\", null],\n  [\"\u4e4b\", null],\n  [\"\u540e\", null],\n  [\"\uff0c\", null],\n  [\"\u4ed6\", \"they\"],\n  [\"\u4eec\", null],\n  [\"\u4f1a\", null],\n  [\"\u4ee5\", null],\n  [\"\u6781\", \"extremely\"],\n  [\"\u4f4e\", \"low\"],\n  [\"\u7684\", null],\n  [\"\u4ef7\", \"price\"],\n  [\"\u683c\", null],\n  [\"\u552e\", \"sell\"],\n  [\"\u51fa\", null],\n  [\"\uff0c\", null],\n  [\"\u4ece\", null],\n  [\"\u800c\", null],\n  [\"\u5bfc\", \"lead\"],\n  [\"\u81f4\", null],\n  [\"\u4e86\", null],\n  [\"\u8be5\", \"that\"],\n  [\"\u5730\", \"area\"],\n  [\"\u533a\", null],\n  [\"\u7684\", null],\n  [\"\u623f\", \"housing\"],\n  [\"\u4ef7\", \"prices\"],\n  [\"\u4e00\", null],\n  [\"\u6b65\", \"step\"],\n  [\"\u6b65\", null],\n  [\"\u4e0b\", \"decline\"],\n  [\"\u6ed1\", null],\n  [\"\uff0c\", null],\n  [\"\u8fd9\", \"this\"],\n  [\"\u4e2a\", null],\n  [\"\u8fc7\", \"process\"],\n  [\"\u7a0b\", null],\n  [\"\u53c8\", null],\n  [\"\u5f15\", \"trigger\"],\n  [\"\u53d1\", null],\n  [\"\u4e86\", null],\n  [\"\u623f\", \"housing\"],\n  [\"\u4ef7\", \"prices\"],\n  [\"\u8fdb\", \"further\"],\n  [\"\u4e00\", null],\n  [\"\u6b65\", null],\n  [\"\u7684\", null],\n  [\"\u4e0b\", \"decline\"],\n  [\"\u6ed1\", null],\n  [\"\uff0c\", null],\n  [\"\u5f62\", \"form\"],\n  [\"\u6210\", null],\n  [\"\u4e86\", null],\n  [\"\u4e00\u4e2a\", \"a\"],\n  [\"<0xE6>\", null],\n  [\"<0x81>\", null],\n  [\"<0xB6>\", null],\n  [\"\u6027\", \"vicious\"],\n  [\"\u5faa\", \"cycle\"],\n  [\"\u73af\", null],\n  [\"\uff0c\", null],\n  [\"\u5f53\", \"when\"],\n  [\"\u623f\", \"housing\"],\n  [\"\u4ef7\", \"prices\"],\n  [\"\u4e0b\", \"fall\"],\n  [\"<0xE8>\", null],\n  [\"<0xB7>\", null],\n  [\"<0x8C>\", null],\n  [\"\u5230\", null],\n  [\"\u4e00\", \"certain\"],\n  [\"\u5b9a\", null],\n  [\"\u7684\", null],\n  [\"\u7a0b\", \"level\"],\n  [\"\u5ea6\", null],\n  [\"\u4e4b\", null],\n  [\"\u540e\", null],\n  [\"\uff0c\", null],\n  [\"\u8d8a\", \"more\"],\n  [\"\u6765\", null],\n  [\"\u8d8a\", null],\n  [\"\u591a\", null],\n  [\"\u7684\", null],\n  [\"\u8d2d\", \"homebuyers\"],\n  [\"\u623f\", null],\n  [\"\u8005\", null],\n  [\"\u53d1\", \"discover\"],\n  [\"\u73b0\", null],\n  [\"\u81ea\", \"their\"],\n  [\"\u5df1\", null],\n  [\"\u7684\", null],\n  [\"\u623f\", \"house\"],\n  [\"\u5b50\", null],\n  [\"\u4ef7\", \"value\"],\n  [\"\u503c\", null],\n  [\"<0xE7>\", null],\n  [\"<0x94>\", null],\n  [\"<0x9A>\", null],\n  [\"\u81f3\", \"even\"],\n  [\"\u5df2\", null],\n  [\"\u7ecf\", null],\n  [\"\u6bd4\", \"than\"],\n  [\"\u81ea\", \"their\"],\n  [\"\u5df1\", null],\n  [\"\u7684\", null],\n  [\"<0xE8>\", null],\n  [\"<0xB4>\", null],\n  [\"<0xB7>\", null],\n  [\"\u6b3e\", \"loan\"],\n  [\"\u8fd8\", null],\n  [\"\u4f4e\", \"lower\"],\n  [\"\u4e86\", null],\n  [\"\uff0c\", null],\n  [\"\u4e8e\", null],\n  [\"\u662f\", null],\n  [\"\u5c31\", null],\n  [\"\u6709\", null],\n  [\"\u4e86\", null],\n  [\"\u66f4\", \"more\"],\n  [\"\u591a\", null],\n  [\"\u7684\", null],\n  [\"\u4eba\", \"people\"],\n  [\"\u505c\", \"stop\"],\n  [\"\u6b62\", null],\n  [\"\u7ee7\", \"continue\"],\n  [\"\u7eed\", null],\n  [\"\u8fd8\", \"repaying\"],\n  [\"\u623f\", \"mortgage\"],\n  [\"<0xE8>\", null],\n  [\"<0xB4>\", null],\n  [\"<0xB7>\", null],\n  [\"\uff0c\", null],\n  [\"\u53c8\", null],\n  [\"\u9020\", \"create\"],\n  [\"\u6210\", null],\n  [\"\u4e86\", null],\n  [\"\u66f4\", \"more\"],\n  [\"\u591a\", null],\n  [\"\u7684\", null],\n  [\"<0xE5>\", null],\n  [\"<0x9D>\", null],\n  [\"<0x8F>\", null],\n  [\"\u8d26\", \"bad debts\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u4ece\u800c\u6709\u4eba\u56e0\u4e3a\u6536\u5165\u8fc7\u4f4e\u652f\u4ed8\u4e0d\u8d77\u623f\u8d37\uff0c\u4e8e\u662f\u94f6\u884c\u5c31\u5f00\u59cb\u6536\u56de\u623f\u4ea7\uff0c\u5f53\u94f6\u884c\u4ece\u62d6\u6b20\u8d37\u6b3e\u7684\u8d2d\u623f\u8005\u624b\u4e2d\u6536\u56de\u623f\u4ea7\u4e4b\u540e\uff0c\u4ed6\u4eec\u4f1a\u4ee5\u6781\u4f4e\u7684\u4ef7\u683c\u552e\u51fa\uff0c\u4ece\u800c\u5bfc\u81f4\u4e86\u8be5\u5730\u533a\u7684\u623f\u4ef7\u4e00\u6b65\u6b65\u4e0b\u6ed1\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u53c8\u5f15\u53d1\u4e86\u623f\u4ef7\u8fdb\u4e00\u6b65\u7684\u4e0b\u6ed1\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u6076\u6027\u5faa\u73af\uff0c\u5f53\u623f\u4ef7\u4e0b\u8dcc\u5230\u4e00\u5b9a\u7684\u7a0b\u5ea6\u4e4b\u540e\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u8d2d\u623f\u8005\u53d1\u73b0\u81ea\u5df1\u7684\u623f\u5b50\u4ef7\u503c\u751a\u81f3\u5df2\u7ecf\u6bd4\u81ea\u5df1\u7684\u8d37\u6b3e\u8fd8\u4f4e\u4e86\uff0c\u4e8e\u662f\u5c31\u6709\u4e86\u66f4\u591a\u7684\u4eba\u505c\u6b62\u7ee7\u7eed\u8fd8\u623f\u8d37\uff0c\u53c8\u9020\u6210\u4e86\u66f4\u591a\u7684\u574f\u8d26\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u4ece', '\u800c', '\u6709', '\u4eba', '\u56e0', '\u4e3a', '\u6536', '\u5165', '\u8fc7', '\u4f4e', '\u652f', '\u4ed8', '\u4e0d', '\u8d77', '\u623f', '<0xE8>', '<0xB4>', '<0xB7>', '\uff0c', '\u4e8e', '\u662f', '\u94f6', '\u884c', '\u5c31', '\u5f00', '\u59cb', '\u6536', '\u56de', '\u623f', '\u4ea7', '\uff0c', '\u5f53', '\u94f6', '\u884c', '\u4ece', '\u62d6', '<0xE6>', '<0xAC>', '<0xA0>', '<0xE8>', '<0xB4>', '<0xB7>', '\u6b3e', '\u7684', '\u8d2d', '\u623f', '\u8005', '\u624b', '\u4e2d', '\u6536', '\u56de', '\u623f', '\u4ea7', '\u4e4b', '\u540e', '\uff0c', '\u4ed6', '\u4eec', '\u4f1a', '\u4ee5', '\u6781', '\u4f4e', '\u7684', '\u4ef7', '\u683c', '\u552e', '\u51fa', '\uff0c', '\u4ece', '\u800c', '\u5bfc', '\u81f4', '\u4e86', '\u8be5', '\u5730', '\u533a', '\u7684', '\u623f', '\u4ef7', '\u4e00', '\u6b65', '\u6b65', '\u4e0b', '\u6ed1', '\uff0c', '\u8fd9', '\u4e2a', '\u8fc7', '\u7a0b', '\u53c8', '\u5f15', '\u53d1', '\u4e86', '\u623f', '\u4ef7', '\u8fdb', '\u4e00', '\u6b65', '\u7684', '\u4e0b', '\u6ed1', '\uff0c', '\u5f62', '\u6210', '\u4e86', '\u4e00\u4e2a', '<0xE6>', '<0x81>', '<0xB6>', '\u6027', '\u5faa', '\u73af', '\uff0c', '\u5f53', '\u623f', '\u4ef7', '\u4e0b', '<0xE8>', '<0xB7>', '<0x8C>', '\u5230', '\u4e00', '\u5b9a', '\u7684', '\u7a0b', '\u5ea6', '\u4e4b', '\u540e', '\uff0c', '\u8d8a', '\u6765', '\u8d8a', '\u591a', '\u7684', '\u8d2d', '\u623f', '\u8005', '\u53d1', '\u73b0', '\u81ea', '\u5df1', '\u7684', '\u623f', '\u5b50', '\u4ef7', '\u503c', '<0xE7>', '<0x94>', '<0x9A>', '\u81f3', '\u5df2', '\u7ecf', '\u6bd4', '\u81ea', '\u5df1', '\u7684', '<0xE8>', '<0xB4>', '<0xB7>', '\u6b3e', '\u8fd8', '\u4f4e', '\u4e86', '\uff0c', '\u4e8e', '\u662f', '\u5c31', '\u6709', '\u4e86', '\u66f4', '\u591a', '\u7684', '\u4eba', '\u505c', '\u6b62', '\u7ee7', '\u7eed', '\u8fd8', '\u623f', '<0xE8>', '<0xB4>', '<0xB7>', '\uff0c', '\u53c8', '\u9020', '\u6210', '\u4e86', '\u66f4', '\u591a', '\u7684', '<0xE5>', '<0x9D>', '<0x8F>', '\u8d26', '\u3002']"}], "[[\"\u2581\", null],\n[\"\u4ece\", \"\u0441\u043e\u043d\u0434\u044b\u049b\u0442\u0430\u043d\"],\n[\"\u800c\", null],\n[\"\u6709\", \"\u0431\u0430\u0440\"],\n[\"\u4eba\", \"\u0430\u0434\u0430\u043c\u0434\u0430\u0440\"],\n[\"\u56e0\", null],\n[\"\u4e3a\", null],\n[\"\u6536\", \"\u0442\u0430\u0431\u044b\u0441\"],\n[\"\u5165\", null],\n[\"\u8fc7\", null],\n[\"\u4f4e\", \"\u0442\u04e9\u043c\u0435\u043d\"],\n[\"\u652f\", \"\u0442\u04e9\u043b\u0435\u0443\"],\n[\"\u4ed8\", null],\n[\"\u4e0d\", null],\n[\"\u8d77\", null],\n[\"\u623f\", \"\u0438\u043f\u043e\u0442\u0435\u043a\u0430\"],\n[\"<0xE8>\", null],\n[\"<0xB4>\", null],\n[\"<0xB7>\", null],\n[\"\uff0c\", null],\n[\"\u4e8e\", null],\n[\"\u662f\", null],\n[\"\u94f6\", \"\u0431\u0430\u043d\u043a\"],\n[\"\u884c\", null],\n[\"\u5c31\", null],\n[\"\u5f00\", \"\u0431\u0430\u0441\u0442\u0430\u0443\"],\n[\"\u59cb\", null],\n[\"\u6536\", \"\u049b\u0430\u0439\u0442\u0430\u0440\u044b\u043f\"],\n[\"\u56de\", null],\n[\"\u623f\", \"\u0436\u044b\u043b\u0436\u044b\u043c\u0430\u0439\u0442\u044b\u043d\"],\n[\"\u4ea7\", \"\u043c\u04af\u043b\u0456\u043a\"],\n[\"\uff0c\", null],\n[\"\u5f53\", \"\u049b\u0430\u0448\u0430\u043d\"],\n[\"\u94f6\", \"\u0431\u0430\u043d\u043a\"],\n[\"\u884c\", null],\n[\"\u4ece\", null],\n[\"\u62d6\", \"\u043d\u0435\u0441\u0438\u0435\"],\n[\"<0xE6>\", null],\n[\"<0xAC>\", null],\n[\"<0xA0>\", null],\n[\"<0xE8>\", null],\n[\"<0xB4>\", null],\n[\"<0xB7>\", null],\n[\"\u6b3e\", null],\n[\"\u7684\", null],\n[\"\u8d2d\", \"\u04af\u0439\"],\n[\"\u623f\", null],\n[\"\u8005\", \"\u0441\u0430\u0442\u044b\u043f \u0430\u043b\u0443\u0448\u044b\u043b\u0430\u0440\u0434\u0430\u043d\"],\n[\"\u624b\", null],\n[\"\u4e2d\", null],\n[\"\u6536\", \"\u049b\u0430\u0439\u0442\u0430\u0440\u044b\u043f\"],\n[\"\u56de\", null],\n[\"\u623f\", \"\u0436\u044b\u043b\u0436\u044b\u043c\u0430\u0439\u0442\u044b\u043d\"],\n[\"\u4ea7\", \"\u043c\u04af\u043b\u0456\u043a\"],\n[\"\u4e4b\", null],\n[\"\u540e\", null],\n[\"\uff0c\", null],\n[\"\u4ed6\", \"\u043e\u043b\u0430\u0440\"],\n[\"\u4eec\", null],\n[\"\u4f1a\", null],\n[\"\u4ee5\", null],\n[\"\u6781\", \"\u04e9\u0442\u0435\"],\n[\"\u4f4e\", \"\u0442\u04e9\u043c\u0435\u043d\"],\n[\"\u7684\", null],\n[\"\u4ef7\", \"\u0431\u0430\u0493\u0430\u043c\u0435\u043d\"],\n[\"\u683c\", null],\n[\"\u552e\", \"\u0441\u0430\u0442\u0430\u0434\u044b\"],\n[\"\u51fa\", null],\n[\"\uff0c\", null],\n[\"\u4ece\", null],\n[\"\u800c\", null],\n[\"\u5bfc\", \"\u04d9\u043a\u0435\u043b\u0435\u0434\u0456\"],\n[\"\u81f4\", null],\n[\"\u4e86\", null],\n[\"\u8be5\", \"\u0441\u043e\u043b\"],\n[\"\u5730\", \"\u0430\u0439\u043c\u0430\u049b\u0442\u0430\u0493\u044b\"],\n[\"\u533a\", null],\n[\"\u7684\", null],\n[\"\u623f\", \"\u04af\u0439\"],\n[\"\u4ef7\", \"\u0431\u0430\u0493\u0430\u0441\u044b\u043d\u044b\u04a3\"],\n[\"\u4e00\", null],\n[\"\u6b65\", \"\u049b\u0430\u0434\u0430\u043c\"],\n[\"\u6b65\", null],\n[\"\u4e0b\", \"\u0442\u04e9\u043c\u0435\u043d\u0434\u0435\u0443\u0456\u043d\u0435\"],\n[\"\u6ed1\", null],\n[\"\uff0c\", null],\n[\"\u8fd9\", \"\u0431\u04b1\u043b\"],\n[\"\u4e2a\", null],\n[\"\u8fc7\", \"\u04af\u0434\u0435\u0440\u0456\u0441\"],\n[\"\u7a0b\", null],\n[\"\u53c8\", null],\n[\"\u5f15\", \"\u0442\u0443\u0434\u044b\u0440\u0430\u0434\u044b\"],\n[\"\u53d1\", null],\n[\"\u4e86\", null],\n[\"\u623f\", \"\u04af\u0439\"],\n[\"\u4ef7\", \"\u0431\u0430\u0493\u0430\u0441\u044b\u043d\u044b\u04a3\"],\n[\"\u8fdb\", null],\n[\"\u4e00\", \"\u043e\u0434\u0430\u043d \u04d9\u0440\u0456\"],\n[\"\u6b65\", null],\n[\"\u7684\", null],\n[\"\u4e0b\", \"\u0442\u04e9\u043c\u0435\u043d\u0434\u0435\u0443\u0456\u043d\"],\n[\"\u6ed1\", null],\n[\"\uff0c\", null],\n[\"\u5f62\", \"\u049b\u0430\u043b\u044b\u043f\u0442\u0430\u0441\u0442\u044b\u0440\u0430\u0434\u044b\"],\n[\"\u6210\", null],\n[\"\u4e86\", null],\n[\"\u4e00\u4e2a\", \"\u0431\u0456\u0440\"],\n[\"<0xE6>\", null],\n[\"<0x81>\", null],\n[\"<0xB6>\", null],\n[\"\u6027\", \"\u0442\u0435\u0440\u0456\u0441\"],\n[\"\u5faa\", \"\u0430\u0439\u043d\u0430\u043b\u044b\u043c\"],\n[\"\u73af\", null],\n[\"\uff0c\", null],\n[\"\u5f53\", \"\u049b\u0430\u0448\u0430\u043d\"],\n[\"\u623f\", \"\u04af\u0439\"],\n[\"\u4ef7\", \"\u0431\u0430\u0493\u0430\u0441\u044b\"],\n[\"\u4e0b\", \"\u0442\u04e9\u043c\u0435\u043d\u0434\u0435\u043f\"],\n[\"<0xE8>\", null],\n[\"<0xB7>\", null],\n[\"<0x8C>\", null],\n[\"\u5230\", null],\n[\"\u4e00\", \"\u0431\u0435\u043b\u0433\u0456\u043b\u0456\"],\n[\"\u5b9a\", null],\n[\"\u7684\", null],\n[\"\u7a0b\", \"\u0434\u04d9\u0440\u0435\u0436\u0435\u0433\u0435\"],\n[\"\u5ea6\", null],\n[\"\u4e4b\", null],\n[\"\u540e\", \"\u0436\u0435\u0442\u043a\u0435\u043d\u0434\u0435\"],\n[\"\uff0c\", null],\n[\"\u8d8a\", \"\u043a\u04e9\u0431\u0456\u0440\u0435\u043a\"],\n[\"\u6765\", null],\n[\"\u8d8a\", null],\n[\"\u591a\", null],\n[\"\u7684\", null],\n[\"\u8d2d\", \"\u04af\u0439\"],\n[\"\u623f\", null],\n[\"\u8005\", \"\u0441\u0430\u0442\u044b\u043f \u0430\u043b\u0443\u0448\u044b\u043b\u0430\u0440\"],\n[\"\u53d1\", \"\u0431\u0430\u0439\u049b\u0430\u0439\u0434\u044b\"],\n[\"\u73b0\", null],\n[\"\u81ea\", \"\u04e9\u0437\u0434\u0435\u0440\u0456\u043d\u0456\u04a3\"],\n[\"\u5df1\", null],\n[\"\u7684\", null],\n[\"\u623f\", \"\u04af\u0439\u043b\u0435\u0440\u0456\u043d\u0456\u04a3\"],\n[\"\u5b50\", null],\n[\"\u4ef7\", \"\u049b\u04b1\u043d\u044b\"],\n[\"\u503c\", null],\n[\"<0xE7>\", null],\n[\"<0x94>\", null],\n[\"<0x9A>\", null],\n[\"\u81f3\", \"\u0442\u0456\u043f\u0442\u0456\"],\n[\"\u5df2\", null],\n[\"\u7ecf\", null],\n[\"\u6bd4\", null],\n[\"\u81ea\", \"\u04e9\u0437\u0434\u0435\u0440\u0456\u043d\u0456\u04a3\"],\n[\"\u5df1\", null],\n[\"\u7684\", null],\n[\"<0xE8>\", null],\n[\"<0xB4>\", null],\n[\"<0xB7>\", null],\n[\"\u6b3e\", \"\u043d\u0435\u0441\u0438\u0435\u0441\u0456\u043d\u0435\u043d\"],\n[\"\u8fd8\", \"\u0434\u0435\"],\n[\"\u4f4e\", \"\u0442\u04e9\u043c\u0435\u043d\"],\n[\"\u4e86\", null],\n[\"\uff0c\", null],\n[\"\u4e8e\", null],\n[\"\u662f\", null],\n[\"\u5c31\", null],\n[\"\u6709\", null],\n[\"\u4e86\", null],\n[\"\u66f4\", \"\u043a\u04e9\u0431\u0456\u0440\u0435\u043a\"],\n[\"\u591a\", null],\n[\"\u7684\", null],\n[\"\u4eba\", \"\u0430\u0434\u0430\u043c\u0434\u0430\u0440\"],\n[\"\u505c\", \"\u0442\u043e\u049b\u0442\u0430\u0442\u0430\u0434\u044b\"],\n[\"\u6b62\", null],\n[\"\u7ee7\", \"\u0438\u043f\u043e\u0442\u0435\u043a\u0430\u043d\u044b\"],\n[\"\u7eed\", null],\n[\"\u8fd8\", \"\u0442\u04e9\u043b\u0435\u0443\u0434\u0456\"],\n[\"\u623f\", null],\n[\"<0xE8>\", null],\n[\"<0xB4>\", null],\n[\"<0xB7>\", null],\n[\"\uff0c\", null],\n[\"\u53c8\", null],\n[\"\u9020\", \"\u0442\u0443\u0434\u044b\u0440\u0430\u0434\u044b\"],\n[\"\u6210\", null],\n[\"\u4e86\", null],\n[\"\u66f4\", \"\u043a\u04e9\u0431\u0456\u0440\u0435\u043a\"],\n[\"\u591a\", null],\n[\"\u7684\", null],\n[\"<0xE5>\", null],\n[\"<0x9D>\", null],\n[\"<0x8F>\", null],\n[\"\u8d26\", \"\u043d\u0435\u0441\u0438\u0435\"]]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u4ece\u800c\u6709\u4eba\u56e0\u4e3a\u6536\u5165\u8fc7\u4f4e\u652f\u4ed8\u4e0d\u8d77\u623f\u8d37\uff0c\u4e8e\u662f\u94f6\u884c\u5c31\u5f00\u59cb\u6536\u56de\u623f\u4ea7\uff0c\u5f53\u94f6\u884c\u4ece\u62d6\u6b20\u8d37\u6b3e\u7684\u8d2d\u623f\u8005\u624b\u4e2d\u6536\u56de\u623f\u4ea7\u4e4b\u540e\uff0c\u4ed6\u4eec\u4f1a\u4ee5\u6781\u4f4e\u7684\u4ef7\u683c\u552e\u51fa\uff0c\u4ece\u800c\u5bfc\u81f4\u4e86\u8be5\u5730\u533a\u7684\u623f\u4ef7\u4e00\u6b65\u6b65\u4e0b\u6ed1\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u53c8\u5f15\u53d1\u4e86\u623f\u4ef7\u8fdb\u4e00\u6b65\u7684\u4e0b\u6ed1\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u6076\u6027\u5faa\u73af\uff0c\u5f53\u623f\u4ef7\u4e0b\u8dcc\u5230\u4e00\u5b9a\u7684\u7a0b\u5ea6\u4e4b\u540e\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u8d2d\u623f\u8005\u53d1\u73b0\u81ea\u5df1\u7684\u623f\u5b50\u4ef7\u503c\u751a\u81f3\u5df2\u7ecf\u6bd4\u81ea\u5df1\u7684\u8d37\u6b3e\u8fd8\u4f4e\u4e86\uff0c\u4e8e\u662f\u5c31\u6709\u4e86\u66f4\u591a\u7684\u4eba\u505c\u6b62\u7ee7\u7eed\u8fd8\u623f\u8d37\uff0c\u53c8\u9020\u6210\u4e86\u66f4\u591a\u7684\u574f\u8d26\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u4ece', '\u800c', '\u6709', '\u4eba', '\u56e0', '\u4e3a', '\u6536', '\u5165', '\u8fc7', '\u4f4e', '\u652f', '\u4ed8', '\u4e0d', '\u8d77', '\u623f', '<0xE8>', '<0xB4>', '<0xB7>', '\uff0c', '\u4e8e', '\u662f', '\u94f6', '\u884c', '\u5c31', '\u5f00', '\u59cb', '\u6536', '\u56de', '\u623f', '\u4ea7', '\uff0c', '\u5f53', '\u94f6', '\u884c', '\u4ece', '\u62d6', '<0xE6>', '<0xAC>', '<0xA0>', '<0xE8>', '<0xB4>', '<0xB7>', '\u6b3e', '\u7684', '\u8d2d', '\u623f', '\u8005', '\u624b', '\u4e2d', '\u6536', '\u56de', '\u623f', '\u4ea7', '\u4e4b', '\u540e', '\uff0c', '\u4ed6', '\u4eec', '\u4f1a', '\u4ee5', '\u6781', '\u4f4e', '\u7684', '\u4ef7', '\u683c', '\u552e', '\u51fa', '\uff0c', '\u4ece', '\u800c', '\u5bfc', '\u81f4', '\u4e86', '\u8be5', '\u5730', '\u533a', '\u7684', '\u623f', '\u4ef7', '\u4e00', '\u6b65', '\u6b65', '\u4e0b', '\u6ed1', '\uff0c', '\u8fd9', '\u4e2a', '\u8fc7', '\u7a0b', '\u53c8', '\u5f15', '\u53d1', '\u4e86', '\u623f', '\u4ef7', '\u8fdb', '\u4e00', '\u6b65', '\u7684', '\u4e0b', '\u6ed1', '\uff0c', '\u5f62', '\u6210', '\u4e86', '\u4e00\u4e2a', '<0xE6>', '<0x81>', '<0xB6>', '\u6027', '\u5faa', '\u73af', '\uff0c', '\u5f53', '\u623f', '\u4ef7', '\u4e0b', '<0xE8>', '<0xB7>', '<0x8C>', '\u5230', '\u4e00', '\u5b9a', '\u7684', '\u7a0b', '\u5ea6', '\u4e4b', '\u540e', '\uff0c', '\u8d8a', '\u6765', '\u8d8a', '\u591a', '\u7684', '\u8d2d', '\u623f', '\u8005', '\u53d1', '\u73b0', '\u81ea', '\u5df1', '\u7684', '\u623f', '\u5b50', '\u4ef7', '\u503c', '<0xE7>', '<0x94>', '<0x9A>', '\u81f3', '\u5df2', '\u7ecf', '\u6bd4', '\u81ea', '\u5df1', '\u7684', '<0xE8>', '<0xB4>', '<0xB7>', '\u6b3e', '\u8fd8', '\u4f4e', '\u4e86', '\uff0c', '\u4e8e', '\u662f', '\u5c31', '\u6709', '\u4e86', '\u66f4', '\u591a', '\u7684', '\u4eba', '\u505c', '\u6b62', '\u7ee7', '\u7eed', '\u8fd8', '\u623f', '<0xE8>', '<0xB4>', '<0xB7>', '\uff0c', '\u53c8', '\u9020', '\u6210', '\u4e86', '\u66f4', '\u591a', '\u7684', '<0xE5>', '<0x9D>', '<0x8F>', '\u8d26', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"\u4ece\", \"\u0412\"],\n[\"\u800c\", null],\n[\"\u6709\", null],\n[\"\u4eba\", \"\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0435\"],\n[\"\u56e0\", null],\n[\"\u4e3a\", null],\n[\"\u6536\", \"\u0434\u043e\u0445\u043e\u0434\u044b\"],\n[\"\u5165\", null],\n[\"\u8fc7\", null],\n[\"\u4f4e\", \"\u043d\u0438\u0437\u043a\u0438\u0435\"],\n[\"\u652f\", \"\u043e\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u0442\u044c\"],\n[\"\u4ed8\", null],\n[\"\u4e0d\", null],\n[\"\u8d77\", null],\n[\"\u623f\", \"\u0438\u043f\u043e\u0442\u0435\u043a\u0443\"],\n[\"<0xE8>\", null],\n[\"<0xB4>\", null],\n[\"<0xB7>\", null],\n[\"\uff0c\", null],\n[\"\u4e8e\", null],\n[\"\u662f\", null],\n[\"\u94f6\", \"\u0431\u0430\u043d\u043a\u0438\"],\n[\"\u884c\", null],\n[\"\u5c31\", null],\n[\"\u5f00\", \"\u043d\u0430\u0447\u0438\u043d\u0430\u044e\u0442\"],\n[\"\u59cb\", null],\n[\"\u6536\", \"\u0438\u0437\u044b\u043c\u0430\u0442\u044c\"],\n[\"\u56de\", null],\n[\"\u623f\", \"\u043d\u0435\u0434\u0432\u0438\u0436\u0438\u043c\u043e\u0441\u0442\u044c\"],\n[\"\u4ea7\", null],\n[\"\uff0c\", null],\n[\"\u5f53\", \"\u041a\u043e\u0433\u0434\u0430\"],\n[\"\u94f6\", \"\u0431\u0430\u043d\u043a\u0438\"],\n[\"\u884c\", null],\n[\"\u4ece\", null],\n[\"\u62d6\", \"\u043d\u0435\u043f\u043b\u0430\u0442\u0435\u043b\u044c\u0449\u0438\u043a\u043e\u0432\"],\n[\"<0xE6>\", null],\n[\"<0xAC>\", null],\n[\"<0xA0>\", null],\n[\"<0xE8>\", null],\n[\"<0xB4>\", null],\n[\"<0xB7>\", null],\n[\"\u6b3e\", null],\n[\"\u7684\", null],\n[\"\u8d2d\", \"\u043f\u043e\u043a\u0443\u043f\u0430\u0442\u0435\u043b\u0435\u0439\"],\n[\"\u623f\", null],\n[\"\u8005\", null],\n[\"\u624b\", null],\n[\"\u4e2d\", null],\n[\"\u6536\", \"\u0438\u0437\u044b\u043c\u0430\u044e\u0442\"],\n[\"\u56de\", null],\n[\"\u623f\", \"\u043d\u0435\u0434\u0432\u0438\u0436\u0438\u043c\u043e\u0441\u0442\u044c\"],\n[\"\u4ea7\", null],\n[\"\u4e4b\", null],\n[\"\u540e\", null],\n[\"\uff0c\", null],\n[\"\u4ed6\", \"\u043e\u043d\u0438\"],\n[\"\u4eec\", null],\n[\"\u4f1a\", null],\n[\"\u4ee5\", null],\n[\"\u6781\", \"\u043a\u0440\u0430\u0439\u043d\u0435\"],\n[\"\u4f4e\", \"\u043d\u0438\u0437\u043a\u043e\u0439\"],\n[\"\u7684\", null],\n[\"\u4ef7\", \"\u0446\u0435\u043d\u0435\"],\n[\"\u683c\", null],\n[\"\u552e\", \"\u043f\u0440\u043e\u0434\u0430\u044e\u0442\"],\n[\"\u51fa\", null],\n[\"\uff0c\", null],\n[\"\u4ece\", \"\u0447\u0442\u043e\"],\n[\"\u800c\", null],\n[\"\u5bfc\", \"\u043f\u0440\u0438\u0432\u043e\u0434\u0438\u0442\"],\n[\"\u81f4\", null],\n[\"\u4e86\", null],\n[\"\u8be5\", \"\u043a\"],\n[\"\u5730\", \"\u0441\u043d\u0438\u0436\u0435\u043d\u0438\u044e\"],\n[\"\u533a\", null],\n[\"\u7684\", null],\n[\"\u623f\", \"\u0446\u0435\u043d\"],\n[\"\u4ef7\", null],\n[\"\u4e00\", null],\n[\"\u6b65\", \"\u0448\u0430\u0433\"],\n[\"\u6b65\", null],\n[\"\u4e0b\", \"\u0437\u0430\"],\n[\"\u6ed1\", \"\u0448\u0430\u0433\u043e\u043c\"],\n[\"\uff0c\", null],\n[\"\u8fd9\", \"\u042d\u0442\u043e\u0442\"],\n[\"\u4e2a\", null],\n[\"\u8fc7\", \"\u043f\u0440\u043e\u0446\u0435\u0441\u0441\"],\n[\"\u7a0b\", null],\n[\"\u53c8\", null],\n[\"\u5f15\", \"\u0432\u044b\u0437\u044b\u0432\u0430\u0435\u0442\"],\n[\"\u53d1\", null],\n[\"\u4e86\", null],\n[\"\u623f\", \"\u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0435\"],\n[\"\u4ef7\", null],\n[\"\u8fdb\", \"\u0441\u043d\u0438\u0436\u0435\u043d\u0438\u0435\"],\n[\"\u4e00\", null],\n[\"\u6b65\", null],\n[\"\u7684\", null],\n[\"\u4e0b\", \"\u0446\u0435\u043d\"],\n[\"\u6ed1\", null],\n[\"\uff0c\", null],\n[\"\u5f62\", \"\u043e\u0431\u0440\u0430\u0437\u0443\u044f\"],\n[\"\u6210\", null],\n[\"\u4e86\", null],\n[\"\u4e00\u4e2a\", \"\u043f\u043e\u0440\u043e\u0447\u043d\u044b\u0439\"],\n[\"<0xE6>\", null],\n[\"<0x81>\", null],\n[\"<0xB6>\", null],\n[\"\u6027\", null],\n[\"\u5faa\", \"\u043a\u0440\u0443\u0433\"],\n[\"\u73af\", null],\n[\"\uff0c\", null],\n[\"\u5f53\", \"\u041a\u043e\u0433\u0434\u0430\"],\n[\"\u623f\", \"\u0446\u0435\u043d\u044b\"],\n[\"\u4ef7\", null],\n[\"\u4e0b\", \"\u043f\u0430\u0434\u0430\u044e\u0442\"],\n[\"<0xE8>\", null],\n[\"<0xB7>\", null],\n[\"<0x8C>\", null],\n[\"\u5230\", \"\u0434\u043e\"],\n[\"\u4e00\", \"\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0433\u043e\"],\n[\"\u5b9a\", null],\n[\"\u7684\", null],\n[\"\u7a0b\", \"\u0443\u0440\u043e\u0432\u043d\u044f\"],\n[\"\u5ea6\", null],\n[\"\u4e4b\", null],\n[\"\u540e\", null],\n[\"\uff0c\", null],\n[\"\u8d8a\", \"\u0432\u0441\u0435\"],\n[\"\u6765\", null],\n[\"\u8d8a\", \"\u0431\u043e\u043b\u044c\u0448\u0435\"],\n[\"\u591a\", null],\n[\"\u7684\", null],\n[\"\u8d2d\", \"\u043f\u043e\u043a\u0443\u043f\u0430\u0442\u0435\u043b\u0435\u0439\"],\n[\"\u623f\", null],\n[\"\u8005\", null],\n[\"\u53d1\", \"\u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0438\u0432\u0430\u044e\u0442\"],\n[\"\u73b0\", null],\n[\"\u81ea\", \"\u0447\u0442\u043e\"],\n[\"\u5df1\", null],\n[\"\u7684\", null],\n[\"\u623f\", \"\u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u044c\"],\n[\"\u5b50\", null],\n[\"\u4ef7\", null],\n[\"\u503c\", null],\n[\"<0xE7>\", null],\n[\"<0x94>\", null],\n[\"<0x9A>\", null],\n[\"\u81f3\", \"\u0434\u0430\u0436\u0435\"],\n[\"\u5df2\", null],\n[\"\u7ecf\", null],\n[\"\u6bd4\", \"\u043d\u0438\u0436\u0435\"],\n[\"\u81ea\", \"\u0438\u0445\"],\n[\"\u5df1\", null],\n[\"\u7684\", null],\n[\"<0xE8>\", null],\n[\"<0xB4>\", null],\n[\"<0xB7>\", null],\n[\"\u6b3e\", \"\u0438\u043f\u043e\u0442\u0435\u043a\u0438\"],\n[\"\u8fd8\", null],\n[\"\u4f4e\", null],\n[\"\u4e86\", null],\n[\"\uff0c\", null],\n[\"\u4e8e\", \"\u0432\"],\n[\"\u662f\", null],\n[\"\u5c31\", null],\n[\"\u6709\", \"\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0435\"],\n[\"\u4e86\", null],\n[\"\u66f4\", \"\u0435\u0449\u0435\"],\n[\"\u591a\", \"\u0431\u043e\u043b\u044c\u0448\u0435\"],\n[\"\u7684\", null],\n[\"\u4eba\", \"\u043b\u044e\u0434\u0435\u0439\"],\n[\"\u505c\", \"\u043f\u0440\u0435\u043a\u0440\u0430\u0449\u0430\u044e\u0442\"],\n[\"\u6b62\", null],\n[\"\u7ee7\", \"\u0432\u044b\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u0442\u044c\"],\n[\"\u7eed\", null],\n[\"\u8fd8\", null],\n[\"\u623f\", \"\u0438\u043f\u043e\u0442\u0435\u043a\u0443\"],\n[\"<0xE8>\", null],\n[\"<0xB4>\", null],\n[\"<0xB7>\", null],\n[\"\uff0c\", null],\n[\"\u53c8\", \"\u0447\u0442\u043e\"],\n[\"\u9020\", \"\u043f\u0440\u0438\u0432\u043e\u0434\u0438\u0442\"],\n[\"\u6210\", null],\n[\"\u4e86\", null],\n[\"\u66f4\", \"\u043a\"],\n[\"\u591a\", \"\u0435\u0449\u0435\"],\n[\"\u7684\", null],\n[\"<0xE5>\", null],\n[\"<0x9D>\", null],\n[\"<0x8F>\", null],\n[\"\u8d26\", \"\u0431\u043e\u043b\u044c\u0448\u0435\u043c\u0443 \u0447\u0438\u0441\u043b\u0443 \u043f\u0440\u043e\u0441\u0440\u043e\u0447\u0435\u043d\u043d\u044b\u0445 \u043a\u0440\u0435\u0434\u0438\u0442\u043e\u0432\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u5409\u5c14\u4f2f\u6258\u00b7\u7f57\u5fb7\u91cc\u683c\u65af\u00b7\u6b27\u534e\u62c9\uff08\u8fbe\u7c73\u5b89\u00b7\u963f\u5c14\u5361\u624e\u9970\uff09\u662f\u5361\u5229\u96c6\u56e2\u7684\u9886\u5bfc\u4eba\uff0c\u7c73\u683c\u5c14\u00b7\u7f57\u5fb7\u91cc\u683c\u65af\u00b7\u6b27\u534e\u62c9\uff08\u5f17\u6717\u897f\u65af\u79d1\u00b7\u4e39\u5c3c\u65af\u9970\uff09\u662f\u5361\u5229\u96c6\u56e2\u7684\u667a\u56ca\uff0cPacho Herrera\u5e15\u4e54\u00b7\u8d6b\u96f7\u62c9\uff08\u963f\u5c14\u4f2f\u6258\u00b7\u963f\u66fc\u9970\uff09\u8d1f\u8d23\u8fd0\u8425\u58a8\u897f\u54e5\u7684\u7f51\u7edc\uff0c\u5f7b\u666e\u00b7\u5723\u514b\u9c81\u65af\u00b7\u4f26\u6566\u8bfa\uff08\u4f69\u4f69\u00b7\u62c9\u5e15\u4f50\u7279\u9970\uff09\u8d1f\u8d23\u7ebd\u7ea6\u7684\u8fd0\u8425\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u5409', '\u5c14', '<0xE4>', '<0xBC>', '<0xAF>', '\u6258', '\u00b7', '\u7f57', '\u5fb7', '\u91cc', '\u683c', '\u65af', '\u00b7', '<0xE6>', '<0xAC>', '<0xA7>', '\u534e', '\u62c9', '\uff08', '\u8fbe', '\u7c73', '\u5b89', '\u00b7', '\u963f', '\u5c14', '\u5361', '<0xE6>', '<0x89>', '<0x8E>', '\u9970', '\uff09', '\u662f', '\u5361', '\u5229', '\u96c6', '\u56e2', '\u7684', '\u9886', '\u5bfc', '\u4eba', '\uff0c', '\u7c73', '\u683c', '\u5c14', '\u00b7', '\u7f57', '\u5fb7', '\u91cc', '\u683c', '\u65af', '\u00b7', '<0xE6>', '<0xAC>', '<0xA7>', '\u534e', '\u62c9', '\uff08', '<0xE5>', '<0xBC>', '<0x97>', '<0xE6>', '<0x9C>', '<0x97>', '\u897f', '\u65af', '\u79d1', '\u00b7', '\u4e39', '\u5c3c', '\u65af', '\u9970', '\uff09', '\u662f', '\u5361', '\u5229', '\u96c6', '\u56e2', '\u7684', '\u667a', '<0xE5>', '<0x9B>', '<0x8A>', '\uff0c', 'P', 'ach', 'o', '\u2581Herr', 'era', '<0xE5>', '<0xB8>', '<0x95>', '<0xE4>', '<0xB9>', '<0x94>', '\u00b7', '<0xE8>', '<0xB5>', '<0xAB>', '\u96f7', '\u62c9', '\uff08', '\u963f', '\u5c14', '<0xE4>', '<0xBC>', '<0xAF>', '\u6258', '\u00b7', '\u963f', '<0xE6>', '<0x9B>', '<0xBC>', '\u9970', '\uff09', '\u8d1f', '\u8d23', '\u8fd0', '\u8425', '<0xE5>', '<0xA2>', '<0xA8>', '\u897f', '\u54e5', '\u7684', '\u7f51', '\u7edc', '\uff0c', '<0xE5>', '<0xBD>', '<0xBB>', '\u666e', '\u00b7', '\u5723', '\u514b', '\u9c81', '\u65af', '\u00b7', '<0xE4>', '<0xBC>', '<0xA6>', '<0xE6>', '<0x95>', '<0xA6>', '<0xE8>', '<0xAF>', '<0xBA>', '\uff08', '<0xE4>', '<0xBD>', '<0xA9>', '<0xE4>', '<0xBD>', '<0xA9>', '\u00b7', '\u62c9', '<0xE5>', '<0xB8>', '<0x95>', '\u4f50', '\u7279', '\u9970', '\uff09', '\u8d1f', '\u8d23', '<0xE7>', '<0xBA>', '<0xBD>', '\u7ea6', '\u7684', '\u8fd0', '\u8425', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"\u5409\", \"Gilberto\"],\n[\"\u5c14\", null],\n[\"<0xE4>\", null],\n[\"<0xBC>\", null],\n[\"<0xAF>\", null],\n[\"\u6258\", null],\n[\"\u00b7\", null],\n[\"\u7f57\", \"Rodriguez\"],\n[\"\u5fb7\", null],\n[\"\u91cc\", null],\n[\"\u683c\", null],\n[\"\u65af\", null],\n[\"\u00b7\", null],\n[\"<0xE6>\", null],\n[\"<0xAC>\", null],\n[\"<0xA7>\", null],\n[\"\u534e\", \"Orejuela\"],\n[\"\u62c9\", null],\n[\"\uff08\", null],\n[\"\u8fbe\", \"Damian\"],\n[\"\u7c73\", null],\n[\"\u5b89\", null],\n[\"\u00b7\", null],\n[\"\u963f\", \"Alcazar\"],\n[\"\u5c14\", null],\n[\"\u5361\", null],\n[\"<0xE6>\", null],\n[\"<0x89>\", null],\n[\"<0x8E>\", null],\n[\"\u9970\", null],\n[\"\uff09\", null],\n[\"\u662f\", null],\n[\"\u5361\", \"Cali\"],\n[\"\u5229\", null],\n[\"\u96c6\", \"cartel\"],\n[\"\u56e2\", null],\n[\"\u7684\", null],\n[\"\u9886\", \"leader\"],\n[\"\u5bfc\", null],\n[\"\u4eba\", null],\n[\"\uff0c\", null],\n[\"\u7c73\", \"Miguel\"],\n[\"\u683c\", null],\n[\"\u5c14\", null],\n[\"\u00b7\", null],\n[\"\u7f57\", \"Rodriguez\"],\n[\"\u5fb7\", null],\n[\"\u91cc\", null],\n[\"\u683c\", null],\n[\"\u65af\", null],\n[\"\u00b7\", null],\n[\"<0xE6>\", null],\n[\"<0xAC>\", null],\n[\"<0xA7>\", null],\n[\"\u534e\", \"Orejuela\"],\n[\"\u62c9\", null],\n[\"\uff08\", null],\n[\"<0xE5>\", null],\n[\"<0xBC>\", null],\n[\"<0x97>\", null],\n[\"<0xE6>\", null],\n[\"<0x9C>\", null],\n[\"<0x97>\", null],\n[\"\u897f\", \"Francisco\"],\n[\"\u65af\", null],\n[\"\u79d1\", null],\n[\"\u00b7\", null],\n[\"\u4e39\", \"Denis\"],\n[\"\u5c3c\", null],\n[\"\u65af\", null],\n[\"\u9970\", null],\n[\"\uff09\", null],\n[\"\u662f\", null],\n[\"\u5361\", \"Cali\"],\n[\"\u5229\", null],\n[\"\u96c6\", \"cartel\"],\n[\"\u56e2\", null],\n[\"\u7684\", null],\n[\"\u667a\", \"strategist\"],\n[\"<0xE5>\", null],\n[\"<0x9B>\", null],\n[\"<0x8A>\", null],\n[\"\uff0c\", null],\n[\"P\", \"Pacho\"],\n[\"ach\", null],\n[\"o\", null],\n[\"\u2581Herr\", \"Herrera\"],\n[\"era\", null],\n[\"<0xE5>\", null],\n[\"<0xB8>\", null],\n[\"<0x95>\", null],\n[\"<0xE4>\", null],\n[\"<0xB9>\", null],\n[\"<0x94>\", null],\n[\"\u00b7\", null],\n[\"<0xE8>\", null],\n[\"<0xB5>\", null],\n[\"<0xAB>\", null],\n[\"\u96f7\", null],\n[\"\u62c9\", null],\n[\"\uff08\", null],\n[\"\u963f\", \"Alberto\"],\n[\"\u5c14\", null],\n[\"<0xE4>\", null],\n[\"<0xBC>\", null],\n[\"<0xAF>\", null],\n[\"\u6258\", null],\n[\"\u00b7\", null],\n[\"\u963f\", \"Ammann\"],\n[\"<0x6>\", null],\n[\"<0x9B>\", null],\n[\"<0xBC>\", null],\n[\"\u9970\", null],\n[\"\uff09\", null],\n[\"\u8d1f\", \"responsible\"],\n[\"\u8d23\", null],\n[\"\u8fd0\", \"operating\"],\n[\"\u8425\", null],\n[\"<0xE5>\", null],\n[\"<0xA2>\", null],\n[\"<0xA8>\", null],\n[\"\u897f\", \"Mexico\"],\n[\"\u54e5\", null],\n[\"\u7684\", null],\n[\"\u7f51\", \"network\"],\n[\"\u7edc\", null],\n[\"\uff0c\", null],\n[\"<0xE5>\", null],\n[\"<0xBD>\", null],\n[\"<0xBB>\", null],\n[\"\u666e\", \"Chepe\"],\n[\"\u00b7\", null],\n[\"\u5723\", \"Santa\"],\n[\"\u514b\", null],\n[\"\u9c81\", null],\n[\"\u65af\", null],\n[\"\u00b7\", null],\n[\"<0xE4>\", null],\n[\"<0xBC>\", null],\n[\"<0xA6>\", null],\n[\"<0xE6>\", null],\n[\"<0x95>\", null],\n[\"<0xA6>\", null],\n[\"<0xE8>\", null],\n[\"<0xAF>\", null],\n[\"<0xBA>\", null],\n[\"\uff08\", null],\n[\"<0xE4>\", null],\n[\"<0xBD>\", null],\n[\"<0xA9>\", null],\n[\"<0xE4>\", null],\n[\"<0xBD>\", null],\n[\"<0xA9>\", null],\n[\"\u00b7\", null],\n[\"\u62c9\", \"Rapaport\"],\n[\"<0xE5>\", null],\n[\"<0xB8>\", null],\n[\"<0x95>\", null],\n[\"\u4f50\", null],\n[\"\u7279\", null],\n[\"\u9970\", null],\n[\"\uff09\", null],\n[\"\u8d1f\", \"responsible\"],\n[\"\u8d23\", null],\n[\"<0xE7>\", null],\n[\"<0xBA>\", null],\n[\"<0xBD>\", null],\n[\"\u7ea6\", \"New York\"],\n[\"\u7684\", null],\n[\"\u8fd0\", \"operations\"],\n[\"\u8425\", null],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u5409\u5c14\u4f2f\u6258\u00b7\u7f57\u5fb7\u91cc\u683c\u65af\u00b7\u6b27\u534e\u62c9\uff08\u8fbe\u7c73\u5b89\u00b7\u963f\u5c14\u5361\u624e\u9970\uff09\u662f\u5361\u5229\u96c6\u56e2\u7684\u9886\u5bfc\u4eba\uff0c\u7c73\u683c\u5c14\u00b7\u7f57\u5fb7\u91cc\u683c\u65af\u00b7\u6b27\u534e\u62c9\uff08\u5f17\u6717\u897f\u65af\u79d1\u00b7\u4e39\u5c3c\u65af\u9970\uff09\u662f\u5361\u5229\u96c6\u56e2\u7684\u667a\u56ca\uff0cPacho Herrera\u5e15\u4e54\u00b7\u8d6b\u96f7\u62c9\uff08\u963f\u5c14\u4f2f\u6258\u00b7\u963f\u66fc\u9970\uff09\u8d1f\u8d23\u8fd0\u8425\u58a8\u897f\u54e5\u7684\u7f51\u7edc\uff0c\u5f7b\u666e\u00b7\u5723\u514b\u9c81\u65af\u00b7\u4f26\u6566\u8bfa\uff08\u4f69\u4f69\u00b7\u62c9\u5e15\u4f50\u7279\u9970\uff09\u8d1f\u8d23\u7ebd\u7ea6\u7684\u8fd0\u8425\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u5409', '\u5c14', '<0xE4>', '<0xBC>', '<0xAF>', '\u6258', '\u00b7', '\u7f57', '\u5fb7', '\u91cc', '\u683c', '\u65af', '\u00b7', '<0xE6>', '<0xAC>', '<0xA7>', '\u534e', '\u62c9', '\uff08', '\u8fbe', '\u7c73', '\u5b89', '\u00b7', '\u963f', '\u5c14', '\u5361', '<0xE6>', '<0x89>', '<0x8E>', '\u9970', '\uff09', '\u662f', '\u5361', '\u5229', '\u96c6', '\u56e2', '\u7684', '\u9886', '\u5bfc', '\u4eba', '\uff0c', '\u7c73', '\u683c', '\u5c14', '\u00b7', '\u7f57', '\u5fb7', '\u91cc', '\u683c', '\u65af', '\u00b7', '<0xE6>', '<0xAC>', '<0xA7>', '\u534e', '\u62c9', '\uff08', '<0xE5>', '<0xBC>', '<0x97>', '<0xE6>', '<0x9C>', '<0x97>', '\u897f', '\u65af', '\u79d1', '\u00b7', '\u4e39', '\u5c3c', '\u65af', '\u9970', '\uff09', '\u662f', '\u5361', '\u5229', '\u96c6', '\u56e2', '\u7684', '\u667a', '<0xE5>', '<0x9B>', '<0x8A>', '\uff0c', 'P', 'ach', 'o', '\u2581Herr', 'era', '<0xE5>', '<0xB8>', '<0x95>', '<0xE4>', '<0xB9>', '<0x94>', '\u00b7', '<0xE8>', '<0xB5>', '<0xAB>', '\u96f7', '\u62c9', '\uff08', '\u963f', '\u5c14', '<0xE4>', '<0xBC>', '<0xAF>', '\u6258', '\u00b7', '\u963f', '<0xE6>', '<0x9B>', '<0xBC>', '\u9970', '\uff09', '\u8d1f', '\u8d23', '\u8fd0', '\u8425', '<0xE5>', '<0xA2>', '<0xA8>', '\u897f', '\u54e5', '\u7684', '\u7f51', '\u7edc', '\uff0c', '<0xE5>', '<0xBD>', '<0xBB>', '\u666e', '\u00b7', '\u5723', '\u514b', '\u9c81', '\u65af', '\u00b7', '<0xE4>', '<0xBC>', '<0xA6>', '<0xE6>', '<0x95>', '<0xA6>', '<0xE8>', '<0xAF>', '<0xBA>', '\uff08', '<0xE4>', '<0xBD>', '<0xA9>', '<0xE4>', '<0xBD>', '<0xA9>', '\u00b7', '\u62c9', '<0xE5>', '<0xB8>', '<0x95>', '\u4f50', '\u7279', '\u9970', '\uff09', '\u8d1f', '\u8d23', '<0xE7>', '<0xBA>', '<0xBD>', '\u7ea6', '\u7684', '\u8fd0', '\u8425', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"\u5409\", \"\u0413\u0438\u043b\u0431\u0435\u0440\u0442\u043e\"],\n  [\"\u5c14\", null],\n  [\"<0xE4>\", null],\n  [\"<0xBC>\", null],\n  [\"<0xAF>\", null],\n  [\"\u6258\", null],\n  [\"\u00b7\", null],\n  [\"\u7f57\", \"\u0420\u043e\u0434\u0440\u0438\u0433\u0435\u0441\"],\n  [\"\u5fb7\", null],\n  [\"\u91cc\", null],\n  [\"\u683c\", null],\n  [\"\u65af\", null],\n  [\"\u00b7\", null],\n  [\"<0xE6>\", null],\n  [\"<0xAC>\", null],\n  [\"<0xA7>\", null],\n  [\"\u534e\", \"\u041e\u0432\u0430\u0440\u043e\"],\n  [\"\u62c9\", null],\n  [\"\uff08\", null],\n  [\"\u8fbe\", \"\u0414\u044d\u043c\u0438\u0435\u043d\"],\n  [\"\u7c73\", null],\n  [\"\u5b89\", null],\n  [\"\u00b7\", null],\n  [\"\u963f\", \"\u0410\u043b\u043a\u0430\u0441\u0430\u0440\"],\n  [\"\u5c14\", null],\n  [\"\u5361\", null],\n  [\"<0xE6>\", null],\n  [\"<0x89>\", null],\n  [\"<0x8E>\", null],\n  [\"\u9970\", null],\n  [\"\uff09\", null],\n  [\"\u662f\", null],\n  [\"\u5361\", \"\u041a\u0430\u043b\u0438\"],\n  [\"\u5229\", null],\n  [\"\u96c6\", \"\u043a\u0430\u0440\u0442\u0435\u043b\u0456\u043d\u0456\u04a3\"],\n  [\"\u56e2\", null],\n  [\"\u7684\", null],\n  [\"\u9886\", \"\u043a\u04e9\u0448\u0431\u0430\u0441\u0448\u044b\u0441\u044b\"],\n  [\"\u5bfc\", null],\n  [\"\u4eba\", null],\n  [\"\uff0c\", null],\n  [\"\u7c73\", \"\u041c\u0438\u0433\u0435\u043b\u044c\"],\n  [\"\u683c\", null],\n  [\"\u5c14\", null],\n  [\"\u00b7\", null],\n  [\"\u7f57\", \"\u0420\u043e\u0434\u0440\u0438\u0433\u0435\u0441\"],\n  [\"\u5fb7\", null],\n  [\"\u91cc\", null],\n  [\"\u683c\", null],\n  [\"\u65af\", null],\n  [\"\u00b7\", null],\n  [\"<0xE6>\", null],\n  [\"<0xAC>\", null],\n  [\"<0xA7>\", null],\n  [\"\u534e\", \"\u041e\u0432\u0430\u0440\u043e\"],\n  [\"\u62c9\", null],\n  [\"\uff08\", null],\n  [\"<0xE5>\", null],\n  [\"<0xBC>\", null],\n  [\"<0x97>\", null],\n  [\"<0xE6>\", null],\n  [\"<0x9C>\", null],\n  [\"<0x97>\", null],\n  [\"\u897f\", \"\u0424\u0440\u0430\u043d\u0441\u0438\u0441\u043a\u043e\"],\n  [\"\u65af\", null],\n  [\"\u79d1\", null],\n  [\"\u00b7\", null],\n  [\"\u4e39\", \"\u0414\u0435\u043d\u0438\u0441\"],\n  [\"\u5c3c\", null],\n  [\"\u65af\", null],\n  [\"\u9970\", null],\n  [\"\uff09\", null],\n  [\"\u662f\", null],\n  [\"\u5361\", \"\u041a\u0430\u043b\u0438\"],\n  [\"\u5229\", null],\n  [\"\u96c6\", \"\u043a\u0430\u0440\u0442\u0435\u043b\u0456\u043d\u0456\u04a3\"],\n  [\"\u56e2\", null],\n  [\"\u7684\", null],\n  [\"\u667a\", \"\u0430\u049b\u044b\u043b\u0448\u044b\u0441\u044b\"],\n  [\"<0xE5>\", null],\n  [\"<0x9B>\", null],\n  [\"<0x8A>\", null],\n  [\"\uff0c\", null],\n  [\"P\", \"\u041f\u0430\u0447\u043e\"],\n  [\"ach\", null],\n  [\"o\", null],\n  [\"\u2581Herr\", \"\u042d\u0440\u0440\u0435\u0440\u0430\"],\n  [\"era\", null],\n  [\"<0xE5>\", null],\n  [\"<0xB8>\", null],\n  [\"<0x95>\", null],\n  [\"<0xE4>\", null],\n  [\"<0xB9>\", null],\n  [\"<0x94>\", null],\n  [\"\u00b7\", null],\n  [\"<0xE8>\", null],\n  [\"<0xB5>\", null],\n  [\"<0xAB>\", null],\n  [\"\u96f7\", null],\n  [\"\u62c9\", null],\n  [\"\uff08\", null],\n  [\"\u963f\", \"\u0410\u043b\u044c\u0431\u0435\u0440\u0442\u043e\"],\n  [\"\u5c14\", null],\n  [\"<0xE4>\", null],\n  [\"<0xBC>\", null],\n  [\"<0xAF>\", null],\n  [\"\u6258\", null],\n  [\"\u00b7\", null],\n  [\"\u963f\", \"\u0410\u043c\u043c\u0430\u043d\u043d\"],\n  [\"<0xE6>\", null],\n  [\"<0x9B>\", null],\n  [\"<0xBC>\", null],\n  [\"\u9970\", null],\n  [\"\uff09\", null],\n  [\"\u8d1f\", \"\u041c\u0435\u043a\u0441\u0438\u043a\u0430\u0434\u0430\u0493\u044b\"],\n  [\"\u8d23\", null],\n  [\"\u8fd0\", \"\u0436\u0435\u043b\u0456\u043d\u0456\"],\n  [\"\u8425\", null],\n  [\"<0xE5>\", null],\n  [\"<0xA2>\", null],\n  [\"<0xA8>\", null],\n  [\"\u897f\", \"\u0431\u0430\u0441\u049b\u0430\u0440\u0430\u0434\u044b\"],\n  [\"\u54e5\", null],\n  [\"\u7684\", null],\n  [\"\u7f51\", null],\n  [\"\u7edc\", null],\n  [\"\uff0c\", null],\n  [\"<0xE5>\", null],\n  [\"<0xBD>\", null],\n  [\"<0xBB>\", null],\n  [\"\u666e\", \"\u0427\u0435\u043f\u043e\"],\n  [\"\u00b7\", null],\n  [\"\u5723\", \"\u0421\u0430\u043d\u0442\u0430-\u041a\u0440\u0443\u0441\"],\n  [\"\u514b\", null],\n  [\"\u9c81\", null],\n  [\"\u65af\", null],\n  [\"\u00b7\", null],\n  [\"<0xE4>\", null],\n  [\"<0xBC>\", null],\n  [\"<0xA6>\", null],\n  [\"<0xE6>\", null],\n  [\"<0x95>\", null],\n  [\"<0xA6>\", null],\n  [\"<0xE8>\", null],\n  [\"<0xAF>\", null],\n  [\"<0xBA>\", null],\n  [\"\uff08\", null],\n  [\"<0xE4>\", null],\n  [\"<0xBD>\", null],\n  [\"<0xA9>\", null],\n  [\"<0xE4>\", null],\n  [\"<0xBD>\", null],\n  [\"<0xA9>\", null],\n  [\"\u00b7\", null],\n  [\"\u62c9\", \"\u041b\u0430\u043f\u0430\u0441\u043e\u0442\u0435\"],\n  [\"<0xE5>\", null],\n  [\"<0xB8>\", null],\n  [\"<0x95>\", null],\n  [\"\u4f50\", null],\n  [\"\u7279\", null],\n  [\"\u9970\", null],\n  [\"\uff09\", null],\n  [\"\u8d1f\", \"\u041d\u044c\u044e-\u0419\u043e\u0440\u043a\u0442\u0435\u0433\u0456\"],\n  [\"\u8d23\", null],\n  [\"<0xE7>\", null],\n  [\"<0xBA>\", null],\n  [\"<0xBD>\", null],\n  [\"\u7ea6\", \"\u043e\u043f\u0435\u0440\u0430\u0446\u0438\u044f\u043b\u0430\u0440\u0434\u044b\"],\n  [\"\u7684\", null],\n  [\"\u8fd0\", \"\u0431\u0430\u0441\u049b\u0430\u0440\u0430\u0434\u044b\"],\n  [\"\u8425\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u5409\u5c14\u4f2f\u6258\u00b7\u7f57\u5fb7\u91cc\u683c\u65af\u00b7\u6b27\u534e\u62c9\uff08\u8fbe\u7c73\u5b89\u00b7\u963f\u5c14\u5361\u624e\u9970\uff09\u662f\u5361\u5229\u96c6\u56e2\u7684\u9886\u5bfc\u4eba\uff0c\u7c73\u683c\u5c14\u00b7\u7f57\u5fb7\u91cc\u683c\u65af\u00b7\u6b27\u534e\u62c9\uff08\u5f17\u6717\u897f\u65af\u79d1\u00b7\u4e39\u5c3c\u65af\u9970\uff09\u662f\u5361\u5229\u96c6\u56e2\u7684\u667a\u56ca\uff0cPacho Herrera\u5e15\u4e54\u00b7\u8d6b\u96f7\u62c9\uff08\u963f\u5c14\u4f2f\u6258\u00b7\u963f\u66fc\u9970\uff09\u8d1f\u8d23\u8fd0\u8425\u58a8\u897f\u54e5\u7684\u7f51\u7edc\uff0c\u5f7b\u666e\u00b7\u5723\u514b\u9c81\u65af\u00b7\u4f26\u6566\u8bfa\uff08\u4f69\u4f69\u00b7\u62c9\u5e15\u4f50\u7279\u9970\uff09\u8d1f\u8d23\u7ebd\u7ea6\u7684\u8fd0\u8425\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u5409', '\u5c14', '<0xE4>', '<0xBC>', '<0xAF>', '\u6258', '\u00b7', '\u7f57', '\u5fb7', '\u91cc', '\u683c', '\u65af', '\u00b7', '<0xE6>', '<0xAC>', '<0xA7>', '\u534e', '\u62c9', '\uff08', '\u8fbe', '\u7c73', '\u5b89', '\u00b7', '\u963f', '\u5c14', '\u5361', '<0xE6>', '<0x89>', '<0x8E>', '\u9970', '\uff09', '\u662f', '\u5361', '\u5229', '\u96c6', '\u56e2', '\u7684', '\u9886', '\u5bfc', '\u4eba', '\uff0c', '\u7c73', '\u683c', '\u5c14', '\u00b7', '\u7f57', '\u5fb7', '\u91cc', '\u683c', '\u65af', '\u00b7', '<0xE6>', '<0xAC>', '<0xA7>', '\u534e', '\u62c9', '\uff08', '<0xE5>', '<0xBC>', '<0x97>', '<0xE6>', '<0x9C>', '<0x97>', '\u897f', '\u65af', '\u79d1', '\u00b7', '\u4e39', '\u5c3c', '\u65af', '\u9970', '\uff09', '\u662f', '\u5361', '\u5229', '\u96c6', '\u56e2', '\u7684', '\u667a', '<0xE5>', '<0x9B>', '<0x8A>', '\uff0c', 'P', 'ach', 'o', '\u2581Herr', 'era', '<0xE5>', '<0xB8>', '<0x95>', '<0xE4>', '<0xB9>', '<0x94>', '\u00b7', '<0xE8>', '<0xB5>', '<0xAB>', '\u96f7', '\u62c9', '\uff08', '\u963f', '\u5c14', '<0xE4>', '<0xBC>', '<0xAF>', '\u6258', '\u00b7', '\u963f', '<0xE6>', '<0x9B>', '<0xBC>', '\u9970', '\uff09', '\u8d1f', '\u8d23', '\u8fd0', '\u8425', '<0xE5>', '<0xA2>', '<0xA8>', '\u897f', '\u54e5', '\u7684', '\u7f51', '\u7edc', '\uff0c', '<0xE5>', '<0xBD>', '<0xBB>', '\u666e', '\u00b7', '\u5723', '\u514b', '\u9c81', '\u65af', '\u00b7', '<0xE4>', '<0xBC>', '<0xA6>', '<0xE6>', '<0x95>', '<0xA6>', '<0xE8>', '<0xAF>', '<0xBA>', '\uff08', '<0xE4>', '<0xBD>', '<0xA9>', '<0xE4>', '<0xBD>', '<0xA9>', '\u00b7', '\u62c9', '<0xE5>', '<0xB8>', '<0x95>', '\u4f50', '\u7279', '\u9970', '\uff09', '\u8d1f', '\u8d23', '<0xE7>', '<0xBA>', '<0xBD>', '\u7ea6', '\u7684', '\u8fd0', '\u8425', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"\u5409\", \"\u0425\u0438\u043b\u044c\u0431\u0435\u0440\u0442\u043e\"],\n[\"\u5c14\", null],\n[\"<0xE4>\", null],\n[\"<0xBC>\", null],\n[\"<0xAF>\", null],\n[\"\u6258\", null],\n[\"\u00b7\", null],\n[\"\u7f57\", \"\u0420\u043e\u0434\u0440\u0438\u0433\u0435\u0441\"],\n[\"\u5fb7\", null],\n[\"\u91cc\", null],\n[\"\u683c\", null],\n[\"\u65af\", null],\n[\"\u00b7\", null],\n[\"<0xE6>\", null],\n[\"<0xAC>\", null],\n[\"<0xA7>\", null],\n[\"\u534e\", \"\u041e\u0432\u0430\u043b\u044c\"],\n[\"\u62c9\", null],\n[\"\uff08\", null],\n[\"\u8fbe\", \"\u0414\u044d\u043c\u0438\u0430\u043d\"],\n[\"\u7c73\", null],\n[\"\u5b89\", null],\n[\"\u00b7\", null],\n[\"\u963f\", \"\u0410\u043b\u044c\u043a\u0430\u0441\u0430\u0440\"],\n[\"\u5c14\", null],\n[\"\u5361\", null],\n[\"<0xE6>\", null],\n[\"<0x89>\", null],\n[\"<0x8E>\", null],\n[\"\u9970\", null],\n[\"\uff09\", null],\n[\"\u662f\", null],\n[\"\u5361\", \"\u043a\u0430\u0440\u0442\u0435\u043b\u044c\"],\n[\"\u5229\", null],\n[\"\u96c6\", null],\n[\"\u56e2\", null],\n[\"\u7684\", null],\n[\"\u9886\", \"\u043b\u0438\u0434\u0435\u0440\"],\n[\"\u5bfc\", null],\n[\"\u4eba\", null],\n[\"\uff0c\", null],\n[\"\u7c73\", \"\u041c\u0438\u0433\u0435\u043b\u044c\"],\n[\"\u683c\", null],\n[\"\u5c14\", null],\n[\"\u00b7\", null],\n[\"\u7f57\", \"\u0420\u043e\u0434\u0440\u0438\u0433\u0435\u0441\"],\n[\"\u5fb7\", null],\n[\"\u91cc\", null],\n[\"\u683c\", null],\n[\"\u65af\", null],\n[\"\u00b7\", null],\n[\"<0xE6>\", null],\n[\"<0xAC>\", null],\n[\"<0xA7>\", null],\n[\"\u534e\", \"\u041e\u0432\u0430\u043b\u044c\"],\n[\"\u62c9\", null],\n[\"\uff08\", null],\n[\"<0xE5>\", null],\n[\"<0xBC>\", null],\n[\"<0x97>\", null],\n[\"<0xE6>\", null],\n[\"<0x9C>\", null],\n[\"<0x97>\", null],\n[\"\u897f\", \"\u0424\u0440\u0430\u043d\u0441\u0438\u0441\u043a\u043e\"],\n[\"\u65af\", null],\n[\"\u79d1\", null],\n[\"\u00b7\", null],\n[\"\u4e39\", \"\u0414\u0435\u043d\u0438\u0441\"],\n[\"\u5c3c\", null],\n[\"\u65af\", null],\n[\"\u9970\", null],\n[\"\uff09\", null],\n[\"\u662f\", null],\n[\"\u5361\", \"\u043a\u0430\u0440\u0442\u0435\u043b\u044c\"],\n[\"\u5229\", null],\n[\"\u96c6\", null],\n[\"\u56e2\", null],\n[\"\u7684\", null],\n[\"\u667a\", \"\u043c\u043e\u0437\u0433\"],\n[\"<0xE5>\", null],\n[\"<0x9B>\", null],\n[\"<0x8A>\", null],\n[\"\uff0c\", null],\n[\"P\", null],\n[\"ach\", null],\n[\"o\", null],\n[\"\u2581Herr\", null],\n[\"era\", null],\n[\"<0xE5>\", null],\n[\"<0xB8>\", null],\n[\"<0x95>\", null],\n[\"<0xE4>\", null],\n[\"<0xB9>\", null],\n[\"<0x94>\", null],\n[\"\u00b7\", null],\n[\"<0xE8>\", null],\n[\"<0xB5>\", null],\n[\"<0xAF>\", null],\n[\"\u96f7\", \"\u042d\u0440\u0440\u0435\u0440\u0430\"],\n[\"\u62c9\", null],\n[\"\uff08\", null],\n[\"\u963f\", \"\u0410\u043b\u044c\u0431\u0435\u0440\u0442\u043e\"],\n[\"\u5c14\", null],\n[\"<0xE4>\", null],\n[\"<0xBC>\", null],\n[\"<0xAF>\", null],\n[\"\u6258\", null],\n[\"\u00b7\", null],\n[\"\u963f\", \"\u0410\u043c\u043c\u0430\u043d\"],\n[\"<0xE6>\", null],\n[\"<0x9B>\", null],\n[\"<0xBC>\", null],\n[\"\u9970\", null],\n[\"\uff09\", null],\n[\"\u8d1f\", \"\u043e\u0442\u0432\u0435\u0447\u0430\u0435\u0442\"],\n[\"\u8d23\", null],\n[\"\u8fd0\", \"\u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438\"],\n[\"\u8425\", null],\n[\"<0xE5>\", null],\n[\"<0xA2>\", null],\n[\"<0xA8>\", null],\n[\"\u897f\", \"\u041c\u0435\u043a\u0441\u0438\u043a\u0430\"],\n[\"\u54e5\", null],\n[\"\u7684\", null],\n[\"\u7f51\", \"\u0441\u0435\u0442\u044c\"],\n[\"\u7edc\", null],\n[\"\uff0c\", null],\n[\"<0xE5>\", null],\n[\"<0xBD>\", null],\n[\"<0xBB>\", null],\n[\"\u666e\", \"\u0427\u0435\u043f\u043e\"],\n[\"\u00b7\", null],\n[\"\u5723\", \"\u0421\u0430\u043d\u0442\u0430\"],\n[\"\u514b\", null],\n[\"\u9c81\", null],\n[\"\u65af\", null],\n[\"\u00b7\", null],\n[\"<0xE4>\", null],\n[\"<0xBC>\", null],\n[\"<0xA6>\", null],\n[\"<0xE6>\", null],\n[\"<0x95>\", null],\n[\"<0xA6>\", null],\n[\"<0xE8>\", null],\n[\"<0xAF>\", null],\n[\"<0xBA>\", null],\n[\"\uff08\", null],\n[\"<0xE4>\", null],\n[\"<0xBD>\", null],\n[\"<0xA9>\", null],\n[\"<0xE4>\", null],\n[\"<0xBD>\", null],\n[\"<0xA9>\", null],\n[\"\u00b7\", null],\n[\"\u62c9\", \"\u041b\u0430\u043f\u0430\u0441\u043e\u0442\u0435\"],\n[\"<0xE5>\", null],\n[\"<0xB8>\", null],\n[\"<0x95>\", null],\n[\"\u4f50\", null],\n[\"\u7279\", null],\n[\"\u9970\", null],\n[\"\uff09\", null],\n[\"\u8d1f\", \"\u043e\u0442\u0432\u0435\u0447\u0430\u0435\u0442\"],\n[\"\u8d23\", null],\n[\"<0xE7>\", null],\n[\"<0xBA>\", null],\n[\"<0xBD>\", null],\n[\"\u7ea6\", \"\u041d\u044c\u044e-\u0419\u043e\u0440\u043a\"],\n[\"\u7684\", null],\n[\"\u8fd0\", \"\u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438\"],\n[\"\u8425\", null],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n24 \u5c81\u7684\u5e15\u5c3c\u827e\u5f53\u5730\u4eba\u963f\u6ce2\u65af\u00b7\u5c24\u642c\u5230\u4e86\u7701\u4f1a\u67e5\u4e9a\u666e\u62c9\u53bb\u627e\u5de5\u4f5c\uff0c\u4ed6\u89e3\u91ca\u9053\u201c\u90a3\u4e9b\u79fb\u6c11\u5f00\u59cb\u5728\u4f20\u7edf\u5e02\u573a\u4ee5\u6bd4\u5f53\u5730\u5df4\u5e03\u4e9a\u4eba\u66f4\u4f4e\u7684\u4ef7\u683c\u51fa\u552e\u9e21\u8089\u548c\u852c\u83dc\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '2', '4', '\u2581', '<0xE5>', '<0xB2>', '<0x81>', '\u7684', '<0xE5>', '<0xB8>', '<0x95>', '\u5c3c', '<0xE8>', '<0x89>', '<0xBE>', '\u5f53', '\u5730', '\u4eba', '\u963f', '\u6ce2', '\u65af', '\u00b7', '<0xE5>', '<0xB0>', '<0xA4>', '<0xE6>', '<0x90>', '<0xAC>', '\u5230', '\u4e86', '\u7701', '\u4f1a', '\u67e5', '\u4e9a', '\u666e', '\u62c9', '\u53bb', '\u627e', '\u5de5', '\u4f5c', '\uff0c', '\u4ed6', '\u89e3', '\u91ca', '\u9053', '\u201c', '\u90a3', '\u4e9b', '\u79fb', '\u6c11', '\u5f00', '\u59cb', '\u5728', '\u4f20', '\u7edf', '\u5e02', '\u573a', '\u4ee5', '\u6bd4', '\u5f53', '\u5730', '\u5df4', '\u5e03', '\u4e9a', '\u4eba', '\u66f4', '\u4f4e', '\u7684', '\u4ef7', '\u683c', '\u51fa', '\u552e', '<0xE9>', '<0xB8>', '<0xA1>', '\u8089', '\u548c', '<0xE8>', '<0x94>', '<0xAC>', '\u83dc', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"2\", null],\n  [\"4\", null],\n  [\"\u2581\", null],\n  [\"<0xE5>\", null],\n  [\"<0xB2>\", null],\n  [\"<0x81>\", null],\n  [\"\u7684\", null],\n  [\"<0xE5>\", null],\n  [\"<0xB8>\", null],\n  [\"<0x95>\", null],\n  [\"\u5c3c\", \"Pani\"],\n  [\"<0xE8>\", null],\n  [\"<0x89>\", null],\n  [\"<0xBF>\", null],\n  [\"\u5f53\", null],\n  [\"\u5730\", null],\n  [\"\u4eba\", null],\n  [\"\u963f\", \"Apos\"],\n  [\"\u6ce2\", null],\n  [\"\u65af\", null],\n  [\"\u00b7\", null],\n  [\"<0xE5>\", null],\n  [\"<0xB0>\", null],\n  [\"<0xA4>\", null],\n  [\"<0xE6>\", null],\n  [\"<0x90>\", null],\n  [\"<0xAC>\", null],\n  [\"\u5230\", \"moved\"],\n  [\"\u4e86\", null],\n  [\"\u7701\", \"provincial\"],\n  [\"\u4f1a\", null],\n  [\"\u67e5\", \"Jayapura\"],\n  [\"\u4e9a\", null],\n  [\"\u666e\", null],\n  [\"\u62c9\", null],\n  [\"\u53bb\", \"to\"],\n  [\"\u627e\", \"find\"],\n  [\"\u5de5\", \"work\"],\n  [\"\u4f5c\", null],\n  [\"\uff0c\", null],\n  [\"\u4ed6\", \"he\"],\n  [\"\u89e3\", \"explained\"],\n  [\"\u91ca\", null],\n  [\"\u9053\", null],\n  [\"\"\", null],\n  [\"\u90a3\", \"Those\"],\n  [\"\u4e9b\", null],\n  [\"\u79fb\", \"immigrants\"],\n  [\"\u6c11\", null],\n  [\"\u5f00\", \"started\"],\n  [\"\u59cb\", null],\n  [\"\u5728\", \"in\"],\n  [\"\u4f20\", \"traditional\"],\n  [\"\u7edf\", null],\n  [\"\u5e02\", \"markets\"],\n  [\"\u573a\", null],\n  [\"\u4ee5\", \"at\"],\n  [\"\u6bd4\", \"lower\"],\n  [\"\u5f53\", null],\n  [\"\u5730\", null],\n  [\"\u5df4\", \"Papuans\"],\n  [\"\u5e03\", null],\n  [\"\u4e9a\", null],\n  [\"\u4eba\", null],\n  [\"\u66f4\", null],\n  [\"\u4f4e\", null],\n  [\"\u7684\", null],\n  [\"\u4ef7\", \"prices\"],\n  [\"\u683c\", null],\n  [\"\u51fa\", \"selling\"],\n  [\"\u552e\", null],\n  [\"<0xE9>\", null],\n  [\"<0xB8>\", null],\n  [\"<0xA1>\", null],\n  [\"\u8089\", \"chicken\"],\n  [\"\u548c\", \"and\"],\n  [\"<0xE8>\", null],\n  [\"<0x94>\", null],\n  [\"<0xAC>\", null],\n  [\"\u83dc\", \"vegetables\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n24 \u5c81\u7684\u5e15\u5c3c\u827e\u5f53\u5730\u4eba\u963f\u6ce2\u65af\u00b7\u5c24\u642c\u5230\u4e86\u7701\u4f1a\u67e5\u4e9a\u666e\u62c9\u53bb\u627e\u5de5\u4f5c\uff0c\u4ed6\u89e3\u91ca\u9053\u201c\u90a3\u4e9b\u79fb\u6c11\u5f00\u59cb\u5728\u4f20\u7edf\u5e02\u573a\u4ee5\u6bd4\u5f53\u5730\u5df4\u5e03\u4e9a\u4eba\u66f4\u4f4e\u7684\u4ef7\u683c\u51fa\u552e\u9e21\u8089\u548c\u852c\u83dc\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '2', '4', '\u2581', '<0xE5>', '<0xB2>', '<0x81>', '\u7684', '<0xE5>', '<0xB8>', '<0x95>', '\u5c3c', '<0xE8>', '<0x89>', '<0xBE>', '\u5f53', '\u5730', '\u4eba', '\u963f', '\u6ce2', '\u65af', '\u00b7', '<0xE5>', '<0xB0>', '<0xA4>', '<0xE6>', '<0x90>', '<0xAC>', '\u5230', '\u4e86', '\u7701', '\u4f1a', '\u67e5', '\u4e9a', '\u666e', '\u62c9', '\u53bb', '\u627e', '\u5de5', '\u4f5c', '\uff0c', '\u4ed6', '\u89e3', '\u91ca', '\u9053', '\u201c', '\u90a3', '\u4e9b', '\u79fb', '\u6c11', '\u5f00', '\u59cb', '\u5728', '\u4f20', '\u7edf', '\u5e02', '\u573a', '\u4ee5', '\u6bd4', '\u5f53', '\u5730', '\u5df4', '\u5e03', '\u4e9a', '\u4eba', '\u66f4', '\u4f4e', '\u7684', '\u4ef7', '\u683c', '\u51fa', '\u552e', '<0xE9>', '<0xB8>', '<0xA1>', '\u8089', '\u548c', '<0xE8>', '<0x94>', '<0xAC>', '\u83dc', '\u3002']"}], "[\n  [\"\u2581\", null],\n  [\"2\", null],\n  [\"4\", null],\n  [\"\u2581\", null],\n  [\"<0xE5>\", null],\n  [\"<0xB2>\", null],\n  [\"<0x81>\", null],\n  [\"\u7684\", null],\n  [\"<0xE5>\", null],\n  [\"<0xB8>\", null],\n  [\"<0x95>\", null],\n  [\"\u5c3c\", null],\n  [\"<0xE8>\", null],\n  [\"<0x89>\", null],\n  [\"<0xBF>\", null],\n  [\"\u5f53\", null],\n  [\"\u5730\", null],\n  [\"\u4eba\", \"\u0430\u0434\u0430\u043c\"],\n  [\"\u963f\", null],\n  [\"\u6ce2\", null],\n  [\"\u65af\", null],\n  [\"\u00b7\", null],\n  [\"<0xE5>\", null],\n  [\"<0xB0>\", null],\n  [\"<0xA4>\", null],\n  [\"<0xE6>\", null],\n  [\"<0x90>\", null],\n  [\"<0xAC>\", null],\n  [\"\u5230\", null],\n  [\"\u4e86\", null],\n  [\"\u7701\", \"\u043e\u0431\u043b\u044b\u0441\"],\n  [\"\u4f1a\", null],\n  [\"\u67e5\", null],\n  [\"\u4e9a\", null],\n  [\"\u666e\", null],\n  [\"\u62c9\", null],\n  [\"\u53bb\", null],\n  [\"\u627e\", \"\u0456\u0437\u0434\u0435\u0443\"],\n  [\"\u5de5\", \"\u0436\u04b1\u043c\u044b\u0441\"],\n  [\"\u4f5c\", null],\n  [\"\uff0c\", null],\n  [\"\u4ed6\", \"\u043e\u043b\"],\n  [\"\u89e3\", \"\u0442\u04af\u0441\u0456\u043d\u0434\u0456\u0440\u0443\"],\n  [\"\u91ca\", null],\n  [\"\u9053\", null],\n  [\"\"\", null],\n  [\"\u90a3\", \"\u0441\u043e\u043b\"],\n  [\"\u4e9b\", null],\n  [\"\u79fb\", \"\u043a\u04e9\u0448\u0456\u043f-\u049b\u043e\u043d\u0443\u0448\u044b\u043b\u0430\u0440\"],\n  [\"\u6c11\", null],\n  [\"\u5f00\", \"\u0431\u0430\u0441\u0442\u0430\u0443\"],\n  [\"\u59cb\", null],\n  [\"\u5728\", null],\n  [\"\u4f20\", \"\u0434\u04d9\u0441\u0442\u04af\u0440\u043b\u0456\"],\n  [\"\u7edf\", null],\n  [\"\u5e02\", \"\u0431\u0430\u0437\u0430\u0440\"],\n  [\"\u573a\", null],\n  [\"\u4ee5\", null],\n  [\"\u6bd4\", null],\n  [\"\u5f53\", null],\n  [\"\u5730\", null],\n  [\"\u5df4\", null],\n  [\"\u5e03\", null],\n  [\"\u4e9a\", null],\n  [\"\u4eba\", \"\u0430\u0434\u0430\u043c\u0434\u0430\u0440\"],\n  [\"\u66f4\", null],\n  [\"\u4f4e\", \"\u0442\u04e9\u043c\u0435\u043d\"],\n  [\"\u7684\", null],\n  [\"\u4ef7\", \"\u0431\u0430\u0493\u0430\"],\n  [\"\u683c\", null],\n  [\"\u51fa\", \"\u0441\u0430\u0442\u0443\"],\n  [\"\u552e\", null],\n  [\"<0xE9>\", null],\n  [\"<0xB8>\", null],\n  [\"<0xA1>\", null],\n  [\"\u8089\", \"\u0435\u0442\"],\n  [\"\u548c\", \"\u0436\u04d9\u043d\u0435\"],\n  [\"<0xE8>\", null],\n  [\"<0x94>\", null],\n  [\"<0xAC>\", null],\n  [\"\u83dc\", \"\u043a\u04e9\u043a\u04e9\u043d\u0456\u0441\u0442\u0435\u0440\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n24 \u5c81\u7684\u5e15\u5c3c\u827e\u5f53\u5730\u4eba\u963f\u6ce2\u65af\u00b7\u5c24\u642c\u5230\u4e86\u7701\u4f1a\u67e5\u4e9a\u666e\u62c9\u53bb\u627e\u5de5\u4f5c\uff0c\u4ed6\u89e3\u91ca\u9053\u201c\u90a3\u4e9b\u79fb\u6c11\u5f00\u59cb\u5728\u4f20\u7edf\u5e02\u573a\u4ee5\u6bd4\u5f53\u5730\u5df4\u5e03\u4e9a\u4eba\u66f4\u4f4e\u7684\u4ef7\u683c\u51fa\u552e\u9e21\u8089\u548c\u852c\u83dc\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '2', '4', '\u2581', '<0xE5>', '<0xB2>', '<0x81>', '\u7684', '<0xE5>', '<0xB8>', '<0x95>', '\u5c3c', '<0xE8>', '<0x89>', '<0xBE>', '\u5f53', '\u5730', '\u4eba', '\u963f', '\u6ce2', '\u65af', '\u00b7', '<0xE5>', '<0xB0>', '<0xA4>', '<0xE6>', '<0x90>', '<0xAC>', '\u5230', '\u4e86', '\u7701', '\u4f1a', '\u67e5', '\u4e9a', '\u666e', '\u62c9', '\u53bb', '\u627e', '\u5de5', '\u4f5c', '\uff0c', '\u4ed6', '\u89e3', '\u91ca', '\u9053', '\u201c', '\u90a3', '\u4e9b', '\u79fb', '\u6c11', '\u5f00', '\u59cb', '\u5728', '\u4f20', '\u7edf', '\u5e02', '\u573a', '\u4ee5', '\u6bd4', '\u5f53', '\u5730', '\u5df4', '\u5e03', '\u4e9a', '\u4eba', '\u66f4', '\u4f4e', '\u7684', '\u4ef7', '\u683c', '\u51fa', '\u552e', '<0xE9>', '<0xB8>', '<0xA1>', '\u8089', '\u548c', '<0xE8>', '<0x94>', '<0xAC>', '\u83dc', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"2\", \"2\"],\n[\"4\", \"4\"],\n[\"\u2581\", null],\n[\"<0xE5><0xB2><0x81>\", \"\u043b\u0435\u0442\"],\n[\"\u7684\", null],\n[\"<0xE5><0xB8><0x95>\", \"\u041f\u0430\u043f\u0443\u0430\"],\n[\"\u5c3c\", null],\n[\"<0xE8><0x89><0xBE>\", null],\n[\"\u5f53\", \"\u043c\u0435\u0441\u0442\u043d\u044b\u0439\"],\n[\"\u5730\", null],\n[\"\u4eba\", null],\n[\"\u963f\", \"\u0410\u0431\u043e\u0441\"],\n[\"\u6ce2\", null],\n[\"\u65af\", null],\n[\"\u00b7\", null],\n[\"<0xE5><0xB0><0xA4>\", \"\u042e\"],\n[\"<0xE6><0x90><0xAC>\", \"\u043f\u0435\u0440\u0435\u0435\u0445\u0430\u043b\"],\n[\"\u5230\", null],\n[\"\u4e86\", null],\n[\"\u7701\", \"\u043f\u0440\u043e\u0432\u0438\u043d\u0446\u0438\u0430\u043b\u044c\u043d\u0443\u044e\"],\n[\"\u4f1a\", null],\n[\"\u67e5\", \"\u0414\u0436\u0430\u044f\u043f\u0443\u0440\u0443\"],\n[\"\u4e9a\", null],\n[\"\u666e\", null],\n[\"\u62c9\", null],\n[\"\u53bb\", null],\n[\"\u627e\", \"\u0438\u0441\u043a\u0430\u0442\u044c\"],\n[\"\u5de5\", \"\u0440\u0430\u0431\u043e\u0442\u0443\"],\n[\"\u4f5c\", null],\n[\"\uff0c\", null],\n[\"\u4ed6\", \"\u043e\u043d\"],\n[\"\u89e3\", \"\u043e\u0431\u044a\u044f\u0441\u043d\u0438\u043b\"],\n[\"\u91ca\", null],\n[\"\u9053\", null],\n[\"\"\", null],\n[\"\u90a3\", \"\u0442\u0435\"],\n[\"\u4e9b\", null],\n[\"\u79fb\", \"\u0438\u043c\u043c\u0438\u0433\u0440\u0430\u043d\u0442\u044b\"],\n[\"\u6c11\", null],\n[\"\u5f00\", \"\u043d\u0430\u0447\u0430\u043b\u0438\"],\n[\"\u59cb\", null],\n[\"\u5728\", null],\n[\"\u4f20\", \"\u0442\u0440\u0430\u0434\u0438\u0446\u0438\u043e\u043d\u043d\u044b\u0445\"],\n[\"\u7edf\", null],\n[\"\u5e02\", \"\u0440\u044b\u043d\u043a\u0430\u0445\"],\n[\"\u573a\", null],\n[\"\u4ee5\", null],\n[\"\u6bd4\", \"\u043f\u043e\"],\n[\"\u5f53\", \"\u043c\u0435\u0441\u0442\u043d\u044b\u0445\"],\n[\"\u5730\", null],\n[\"\u5df4\", \"\u043f\u0430\u043f\u0443\u0430\u0441\u043e\u0432\"],\n[\"\u5e03\", null],\n[\"\u4e9a\", null],\n[\"\u4eba\", null],\n[\"\u66f4\", \"\u0431\u043e\u043b\u0435\u0435\"],\n[\"\u4f4e\", \"\u043d\u0438\u0437\u043a\u0438\u043c\"],\n[\"\u7684\", null],\n[\"\u4ef7\", \"\u0446\u0435\u043d\u0430\u043c\"],\n[\"\u683c\", null],\n[\"\u51fa\", \"\u043f\u0440\u043e\u0434\u0430\u0432\u0430\u0442\u044c\"],\n[\"\u552e\", null],\n[\"<0xE9><0xB8><0xA1>\", \"\u043a\u0443\u0440\u0438\u0446\u0443\"],\n[\"\u8089\", null],\n[\"\u548c\", \"\u0438\"],\n[\"<0xE8><0x94><0xAC>\", \"\u043e\u0432\u043e\u0449\u0438\"],\n[\"\u83dc\", null],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u4e2d\u56fd\u5171\u4ea7\u515a\u4e0e\u4e16\u754c\u653f\u515a\u9ad8\u5c42\u5bf9\u8bdd\u4f1a\u4ee5\u6784\u5efa\u4eba\u7c7b\u547d\u8fd0\u5171\u540c\u4f53\u3001\u5171\u540c\u5efa\u8bbe\u7f8e\u597d\u4e16\u754c\u653f\u515a\u7684\u8d23\u4efb\u4e3a\u4e3b\u9898\uff0c\u6765\u81ea\u4e00\u767e\u4e8c\u591a\u4e2a\u56fd\u5bb6\u7684\u8fd1\u4e09\u767e\u4e2a\u653f\u515a\u548c\u653f\u6cbb\u7ec4\u7ec7\u7684\u9886\u5bfc\u4eba\u5171\u516d\u767e\u591a\u540d\u4e2d\u5916\u65b9\u4ee3\u8868\u4e0e\u4f1a\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u4e2d', '\u56fd', '\u5171', '\u4ea7', '\u515a', '\u4e0e', '\u4e16', '\u754c', '\u653f', '\u515a', '\u9ad8', '\u5c42', '\u5bf9', '\u8bdd', '\u4f1a', '\u4ee5', '\u6784', '\u5efa', '\u4eba', '\u7c7b', '\u547d', '\u8fd0', '\u5171', '\u540c', '\u4f53', '\u3001', '\u5171', '\u540c', '\u5efa', '\u8bbe', '\u7f8e', '\u597d', '\u4e16', '\u754c', '\u653f', '\u515a', '\u7684', '\u8d23', '\u4efb', '\u4e3a', '\u4e3b', '\u9898', '\uff0c', '\u6765', '\u81ea', '\u4e00', '\u767e', '\u4e8c', '\u591a', '\u4e2a', '\u56fd', '\u5bb6', '\u7684', '\u8fd1', '\u4e09', '\u767e', '\u4e2a', '\u653f', '\u515a', '\u548c', '\u653f', '\u6cbb', '\u7ec4', '\u7ec7', '\u7684', '\u9886', '\u5bfc', '\u4eba', '\u5171', '\u516d', '\u767e', '\u591a', '\u540d', '\u4e2d', '\u5916', '\u65b9', '\u4ee3', '\u8868', '\u4e0e', '\u4f1a', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"\u4e2d\", \"Chinese\"],\n[\"\u56fd\", null],\n[\"\u5171\", \"Communist\"],\n[\"\u4ea7\", null],\n[\"\u515a\", null],\n[\"\u4e0e\", null],\n[\"\u4e16\", \"world\"],\n[\"\u754c\", null],\n[\"\u653f\", \"political\"],\n[\"\u515a\", null],\n[\"\u9ad8\", \"high\"],\n[\"\u5c42\", null],\n[\"\u5bf9\", null],\n[\"\u8bdd\", \"dialogue\"],\n[\"\u4f1a\", null],\n[\"\u4ee5\", null],\n[\"\u6784\", \"building\"],\n[\"\u5efa\", null],\n[\"\u4eba\", \"human\"],\n[\"\u7c7b\", null],\n[\"\u547d\", \"destiny\"],\n[\"\u8fd0\", null],\n[\"\u5171\", \"community\"],\n[\"\u540c\", null],\n[\"\u4f53\", null],\n[\"\u3001\", null],\n[\"\u5171\", \"jointly\"],\n[\"\u540c\", null],\n[\"\u5efa\", \"constructing\"],\n[\"\u8bbe\", null],\n[\"\u7f8e\", \"beautiful\"],\n[\"\u597d\", null],\n[\"\u4e16\", \"world\"],\n[\"\u754c\", null],\n[\"\u653f\", \"political\"],\n[\"\u515a\", null],\n[\"\u7684\", null],\n[\"\u8d23\", \"responsibility\"],\n[\"\u4efb\", null],\n[\"\u4e3a\", null],\n[\"\u4e3b\", \"theme\"],\n[\"\u9898\", null],\n[\"\uff0c\", null],\n[\"\u6765\", \"from\"],\n[\"\u81ea\", null],\n[\"\u4e00\", \"one\"],\n[\"\u767e\", \"hundred\"],\n[\"\u4e8c\", \"two\"],\n[\"\u591a\", null],\n[\"\u4e2a\", null],\n[\"\u56fd\", \"countries\"],\n[\"\u5bb6\", null],\n[\"\u7684\", null],\n[\"\u8fd1\", \"nearly\"],\n[\"\u4e09\", \"three\"],\n[\"\u767e\", \"hundred\"],\n[\"\u4e2a\", null],\n[\"\u653f\", \"political\"],\n[\"\u515a\", null],\n[\"\u548c\", null],\n[\"\u653f\", \"political\"],\n[\"\u6cbb\", null],\n[\"\u7ec4\", \"organizations\"],\n[\"\u7ec7\", null],\n[\"\u7684\", null],\n[\"\u9886\", \"leaders\"],\n[\"\u5bfc\", null],\n[\"\u4eba\", null],\n[\"\u5171\", \"total\"],\n[\"\u516d\", \"six\"],\n[\"\u767e\", \"hundred\"],\n[\"\u591a\", null],\n[\"\u540d\", null],\n[\"\u4e2d\", \"Chinese\"],\n[\"\u5916\", \"foreign\"],\n[\"\u65b9\", null],\n[\"\u4ee3\", \"representatives\"],\n[\"\u8868\", null],\n[\"\u4e0e\", null],\n[\"\u4f1a\", \"attended\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u4e2d\u56fd\u5171\u4ea7\u515a\u4e0e\u4e16\u754c\u653f\u515a\u9ad8\u5c42\u5bf9\u8bdd\u4f1a\u4ee5\u6784\u5efa\u4eba\u7c7b\u547d\u8fd0\u5171\u540c\u4f53\u3001\u5171\u540c\u5efa\u8bbe\u7f8e\u597d\u4e16\u754c\u653f\u515a\u7684\u8d23\u4efb\u4e3a\u4e3b\u9898\uff0c\u6765\u81ea\u4e00\u767e\u4e8c\u591a\u4e2a\u56fd\u5bb6\u7684\u8fd1\u4e09\u767e\u4e2a\u653f\u515a\u548c\u653f\u6cbb\u7ec4\u7ec7\u7684\u9886\u5bfc\u4eba\u5171\u516d\u767e\u591a\u540d\u4e2d\u5916\u65b9\u4ee3\u8868\u4e0e\u4f1a\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u4e2d', '\u56fd', '\u5171', '\u4ea7', '\u515a', '\u4e0e', '\u4e16', '\u754c', '\u653f', '\u515a', '\u9ad8', '\u5c42', '\u5bf9', '\u8bdd', '\u4f1a', '\u4ee5', '\u6784', '\u5efa', '\u4eba', '\u7c7b', '\u547d', '\u8fd0', '\u5171', '\u540c', '\u4f53', '\u3001', '\u5171', '\u540c', '\u5efa', '\u8bbe', '\u7f8e', '\u597d', '\u4e16', '\u754c', '\u653f', '\u515a', '\u7684', '\u8d23', '\u4efb', '\u4e3a', '\u4e3b', '\u9898', '\uff0c', '\u6765', '\u81ea', '\u4e00', '\u767e', '\u4e8c', '\u591a', '\u4e2a', '\u56fd', '\u5bb6', '\u7684', '\u8fd1', '\u4e09', '\u767e', '\u4e2a', '\u653f', '\u515a', '\u548c', '\u653f', '\u6cbb', '\u7ec4', '\u7ec7', '\u7684', '\u9886', '\u5bfc', '\u4eba', '\u5171', '\u516d', '\u767e', '\u591a', '\u540d', '\u4e2d', '\u5916', '\u65b9', '\u4ee3', '\u8868', '\u4e0e', '\u4f1a', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"\u4e2d\", \"\u049a\u044b\u0442\u0430\u0439\"],\n[\"\u56fd\", null],\n[\"\u5171\", \"\u043a\u043e\u043c\u043c\u0443\u043d\u0438\u0441\u0442\u0456\u043a\"],\n[\"\u4ea7\", null],\n[\"\u515a\", null],\n[\"\u4e0e\", null],\n[\"\u4e16\", \"\u04d9\u043b\u0435\u043c\u0434\u0456\u043a\"],\n[\"\u754c\", null],\n[\"\u653f\", \"\u0441\u0430\u044f\u0441\u0438\"],\n[\"\u515a\", null],\n[\"\u9ad8\", \"\u0436\u043e\u0493\u0430\u0440\u044b\"],\n[\"\u5c42\", null],\n[\"\u5bf9\", null],\n[\"\u8bdd\", \"\u0434\u0438\u0430\u043b\u043e\u0433\"],\n[\"\u4f1a\", null],\n[\"\u4ee5\", null],\n[\"\u6784\", \"\u049b\u04b1\u0440\u0443\"],\n[\"\u5efa\", null],\n[\"\u4eba\", \"\u0430\u0434\u0430\u043c\u0437\u0430\u0442\"],\n[\"\u7c7b\", null],\n[\"\u547d\", \"\u0442\u0430\u0493\u0434\u044b\u0440\"],\n[\"\u8fd0\", null],\n[\"\u5171\", \"\u043e\u0440\u0442\u0430\u049b\"],\n[\"\u540c\", null],\n[\"\u4f53\", null],\n[\"\u3001\", null],\n[\"\u5171\", \"\u0431\u0456\u0440\u043b\u0435\u0441\u0456\u043f\"],\n[\"\u540c\", null],\n[\"\u5efa\", \"\u049b\u04b1\u0440\u0443\"],\n[\"\u8bbe\", null],\n[\"\u7f8e\", \"\u0436\u0430\u049b\u0441\u044b\"],\n[\"\u597d\", null],\n[\"\u4e16\", \"\u04d9\u043b\u0435\u043c\"],\n[\"\u754c\", null],\n[\"\u653f\", \"\u0441\u0430\u044f\u0441\u0438\"],\n[\"\u515a\", null],\n[\"\u7684\", null],\n[\"\u8d23\", \"\u0436\u0430\u0443\u0430\u043f\u043a\u0435\u0440\u0448\u0456\u043b\u0456\u043a\"],\n[\"\u4efb\", null],\n[\"\u4e3a\", null],\n[\"\u4e3b\", \"\u0442\u0430\u049b\u044b\u0440\u044b\u043f\"],\n[\"\u9898\", null],\n[\"\uff0c\", null],\n[\"\u6765\", null],\n[\"\u81ea\", null],\n[\"\u4e00\", \"\u0436\u04af\u0437\"],\n[\"\u767e\", null],\n[\"\u4e8c\", \"\u0436\u0438\u044b\u0440\u043c\u0430\"],\n[\"\u591a\", null],\n[\"\u4e2a\", null],\n[\"\u56fd\", \"\u0435\u043b\"],\n[\"\u5bb6\", null],\n[\"\u7684\", null],\n[\"\u8fd1\", \"\u0448\u0430\u043c\u0430\u043c\u0435\u043d\"],\n[\"\u4e09\", \"\u04af\u0448\"],\n[\"\u767e\", \"\u0436\u04af\u0437\"],\n[\"\u4e2a\", null],\n[\"\u653f\", \"\u0441\u0430\u044f\u0441\u0438\"],\n[\"\u515a\", \"\u043f\u0430\u0440\u0442\u0438\u044f\"],\n[\"\u548c\", null],\n[\"\u653f\", \"\u0441\u0430\u044f\u0441\u0438\"],\n[\"\u6cbb\", null],\n[\"\u7ec4\", \"\u04b1\u0439\u044b\u043c\"],\n[\"\u7ec7\", null],\n[\"\u7684\", null],\n[\"\u9886\", \"\u043a\u04e9\u0448\u0431\u0430\u0441\u0448\u044b\"],\n[\"\u5bfc\", null],\n[\"\u4eba\", null],\n[\"\u5171\", \"\u0431\u0430\u0440\u043b\u044b\u0493\u044b\"],\n[\"\u516d\", \"\u0430\u043b\u0442\u044b\"],\n[\"\u767e\", \"\u0436\u04af\u0437\"],\n[\"\u591a\", null],\n[\"\u540d\", null],\n[\"\u4e2d\", \"\u049b\u044b\u0442\u0430\u0439\u043b\u044b\u049b\"],\n[\"\u5916\", \"\u0448\u0435\u0442\u0435\u043b\u0434\u0456\u043a\"],\n[\"\u65b9\", null],\n[\"\u4ee3\", \"\u04e9\u043a\u0456\u043b\"],\n[\"\u8868\", null],\n[\"\u4e0e\", null],\n[\"\u4f1a\", \"\u049b\u0430\u0442\u044b\u0441\u0442\u044b\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u4e2d\u56fd\u5171\u4ea7\u515a\u4e0e\u4e16\u754c\u653f\u515a\u9ad8\u5c42\u5bf9\u8bdd\u4f1a\u4ee5\u6784\u5efa\u4eba\u7c7b\u547d\u8fd0\u5171\u540c\u4f53\u3001\u5171\u540c\u5efa\u8bbe\u7f8e\u597d\u4e16\u754c\u653f\u515a\u7684\u8d23\u4efb\u4e3a\u4e3b\u9898\uff0c\u6765\u81ea\u4e00\u767e\u4e8c\u591a\u4e2a\u56fd\u5bb6\u7684\u8fd1\u4e09\u767e\u4e2a\u653f\u515a\u548c\u653f\u6cbb\u7ec4\u7ec7\u7684\u9886\u5bfc\u4eba\u5171\u516d\u767e\u591a\u540d\u4e2d\u5916\u65b9\u4ee3\u8868\u4e0e\u4f1a\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u4e2d', '\u56fd', '\u5171', '\u4ea7', '\u515a', '\u4e0e', '\u4e16', '\u754c', '\u653f', '\u515a', '\u9ad8', '\u5c42', '\u5bf9', '\u8bdd', '\u4f1a', '\u4ee5', '\u6784', '\u5efa', '\u4eba', '\u7c7b', '\u547d', '\u8fd0', '\u5171', '\u540c', '\u4f53', '\u3001', '\u5171', '\u540c', '\u5efa', '\u8bbe', '\u7f8e', '\u597d', '\u4e16', '\u754c', '\u653f', '\u515a', '\u7684', '\u8d23', '\u4efb', '\u4e3a', '\u4e3b', '\u9898', '\uff0c', '\u6765', '\u81ea', '\u4e00', '\u767e', '\u4e8c', '\u591a', '\u4e2a', '\u56fd', '\u5bb6', '\u7684', '\u8fd1', '\u4e09', '\u767e', '\u4e2a', '\u653f', '\u515a', '\u548c', '\u653f', '\u6cbb', '\u7ec4', '\u7ec7', '\u7684', '\u9886', '\u5bfc', '\u4eba', '\u5171', '\u516d', '\u767e', '\u591a', '\u540d', '\u4e2d', '\u5916', '\u65b9', '\u4ee3', '\u8868', '\u4e0e', '\u4f1a', '\u3002']"}], "[\n[\"\u2581\", null],\n[\"\u4e2d\", \"\u041a\u0438\u0442\u0430\u0439\"],\n[\"\u56fd\", null],\n[\"\u5171\", \"\u041a\u043e\u043c\u043c\u0443\u043d\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f\"],\n[\"\u4ea7\", null],\n[\"\u515a\", null],\n[\"\u4e0e\", null],\n[\"\u4e16\", \"\u043c\u0438\u0440\u043e\u0432\u043e\u0439\"],\n[\"\u754c\", null],\n[\"\u653f\", \"\u043f\u043e\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439\"],\n[\"\u515a\", null],\n[\"\u9ad8\", \"\u0432\u044b\u0441\u043e\u043a\u043e\u043f\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0439\"],\n[\"\u5c42\", null],\n[\"\u5bf9\", null],\n[\"\u8bdd\", \"\u0434\u0438\u0430\u043b\u043e\u0433\"],\n[\"\u4f1a\", null],\n[\"\u4ee5\", null],\n[\"\u6784\", \"\u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435\"],\n[\"\u5efa\", null],\n[\"\u4eba\", \"\u0447\u0435\u043b\u043e\u0432\u0435\u0447\u0435\u0441\u0442\u0432\u043e\"],\n[\"\u7c7b\", null],\n[\"\u547d\", \"\u0441\u0443\u0434\u044c\u0431\u0430\"],\n[\"\u8fd0\", null],\n[\"\u5171\", \"\u043e\u0431\u0449\u0438\u0439\"],\n[\"\u540c\", null],\n[\"\u4f53\", null],\n[\"\u3001\", null],\n[\"\u5171\", \"\u0441\u043e\u0432\u043c\u0435\u0441\u0442\u043d\u044b\u0439\"],\n[\"\u540c\", null],\n[\"\u5efa\", \"\u0441\u0442\u0440\u043e\u0438\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u043e\"],\n[\"\u8bbe\", null],\n[\"\u7f8e\", \"\u043f\u0440\u0435\u043a\u0440\u0430\u0441\u043d\u044b\u0439\"],\n[\"\u597d\", null],\n[\"\u4e16\", \"\u043c\u0438\u0440\"],\n[\"\u754c\", null],\n[\"\u653f\", \"\u043f\u043e\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439\"],\n[\"\u515a\", null],\n[\"\u7684\", null],\n[\"\u8d23\", \"\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0441\u0442\u044c\"],\n[\"\u4efb\", null],\n[\"\u4e3a\", null],\n[\"\u4e3b\", \"\u0442\u0435\u043c\u0430\"],\n[\"\u9898\", null],\n[\"\uff0c\", null],\n[\"\u6765\", null],\n[\"\u81ea\", null],\n[\"\u4e00\", \"\u0441\u0442\u043e\"],\n[\"\u767e\", null],\n[\"\u4e8c\", \"\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c\"],\n[\"\u591a\", null],\n[\"\u4e2a\", null],\n[\"\u56fd\", \"\u0441\u0442\u0440\u0430\u043d\u0430\"],\n[\"\u5bb6\", null],\n[\"\u7684\", null],\n[\"\u8fd1\", \"\u043e\u043a\u043e\u043b\u043e\"],\n[\"\u4e09\", \"\u0442\u0440\u0438\u0441\u0442\u0430\"],\n[\"\u767e\", null],\n[\"\u4e2a\", null],\n[\"\u653f\", \"\u043f\u043e\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f\"],\n[\"\u515a\", null],\n[\"\u548c\", null],\n[\"\u653f\", \"\u043f\u043e\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f\"],\n[\"\u6cbb\", null],\n[\"\u7ec4\", \"\u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u044f\"],\n[\"\u7ec7\", null],\n[\"\u7684\", null],\n[\"\u9886\", \"\u043b\u0438\u0434\u0435\u0440\"],\n[\"\u5bfc\", null],\n[\"\u4eba\", null],\n[\"\u5171\", \"\u0432\u0441\u0435\u0433\u043e\"],\n[\"\u516d\", \"\u0448\u0435\u0441\u0442\u044c\u0441\u043e\u0442\"],\n[\"\u767e\", null],\n[\"\u591a\", null],\n[\"\u540d\", null],\n[\"\u4e2d\", \"\u043a\u0438\u0442\u0430\u0439\u0441\u043a\u0438\u0439\"],\n[\"\u5916\", \"\u0438\u043d\u043e\u0441\u0442\u0440\u0430\u043d\u043d\u044b\u0439\"],\n[\"\u65b9\", null],\n[\"\u4ee3\", \"\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u0435\u043b\u044c\"],\n[\"\u8868\", null],\n[\"\u4e0e\", null],\n[\"\u4f1a\", \"\u0443\u0447\u0430\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u04e8\u0437\u0456\u043d\u0456\u04a3 \u049a\u043e\u043b\u043c\u0430-\u049b\u043e\u043b \u0430\u049b\u0448\u0430 \u049b\u0430\u0440\u0493\u044b\u0441\u044b\u0430\u0442\u0442\u044b\u0436\u0430\u04a3\u0430 \u043a\u0456\u0442\u0430\u0431\u044b\u043d\u0434\u0430\u041a\u0435\u043d\u043d\u0435\u0442 \u0420\u043e\u0433\u043e\u0444\u0444 \u0441\u0430\u043b\u044b\u049b\u0442\u044b \u0442\u04e9\u043b\u0435\u0443\u0434\u0435\u043d \u0436\u0430\u043b\u0442\u0430\u0440\u0443 \u0436\u04d9\u043d\u0435 \u049b\u044b\u043b\u043c\u044b\u0441\u0442\u044b\u049b \u04d9\u0440\u0435\u043a\u0435\u0442\u043f\u0435\u043d \u043a\u04af\u0440\u0435\u0441\u0443 \u043c\u0430\u049b\u0441\u0430\u0442\u044b\u043d\u0434\u0430 \u0436\u043e\u0493\u0430\u0440\u044b \u0430\u0442\u0430\u0443\u043b\u044b \u0431\u0430\u043d\u043a\u043d\u043e\u0442\u0430\u043b\u0430\u0440\u0434\u044b \u0436\u043e\u044e\u0434\u044b \u049b\u043e\u043b\u0434\u0430\u0439\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '<0xD3>', '<0xA8>', '\u0437\u0456', '\u043d\u0456', '\u04a3', '\u2581', '\u049a', '\u043e', '\u043b', '\u043c\u0430', '-', '\u049b', '\u043e', '\u043b', '\u2581\u0430', '\u049b', '\u0448\u0430', '\u2581', '\u049b', '\u0430\u0440', '\u0493', '\u044b', '\u0441\u044b', '\u0430\u0442', '\u0442\u044b', '\u0436\u0430', '\u04a3', '\u0430', '\u2581\u043a', '\u0456', '\u0442\u0430', '\u0431\u044b', '\u043d', '\u0434\u0430', '\u041a', '\u0435\u043d', '\u043d\u0435', '\u0442', '\u2581\u0420\u043e', '\u0433\u043e', '\u0444', '\u0444', '\u2581\u0441\u0430', '\u043b\u044b', '\u049b', '\u0442\u044b', '\u2581\u0442', '\u04e9', '\u043b\u0435', '\u0443', '\u0434\u0435\u043d', '\u2581', '\u0436\u0430', '\u043b\u0442\u0430', '\u0440\u0443', '\u2581\u0436', '\u04d9', '\u043d\u0435', '\u2581', '\u049b', '\u044b', '\u043b', '\u043c\u044b', '\u0441\u0442\u044b', '\u049b', '\u2581', '\u04d9', '\u0440\u0435', '\u043a', '\u0435\u0442', '\u043f\u0435\u043d', '\u2581\u043a', '\u04af', '\u0440\u0435', '\u0441\u0443', '\u2581\u043c\u0430', '\u049b', '\u0441\u0430', '\u0442\u044b', '\u043d', '\u0434\u0430', '\u2581\u0436', '\u043e', '\u0493', '\u0430\u0440', '\u044b', '\u2581\u0430', '\u0442\u0430', '\u0443', '\u043b\u044b', '\u2581\u0431', '\u0430\u043d', '\u043a', '\u043d\u043e', '\u0442\u0430', '\u043b\u0430', '\u0440', '\u0434\u044b', '\u2581\u0436', '\u043e', '\u044e', '\u0434\u044b', '\u2581', '\u049b', '\u043e', '\u043b', '\u0434\u0430', '\u0439', '\u0434\u044b', '.']"}], "[\n    [\"\u2581\", null],\n    [\"<0xD3>\", null],\n    [\"<0xA8>\", null],\n    [\"\u0437\u0456\", \"In\"],\n    [\"\u043d\u0456\", null],\n    [\"\u04a3\", null],\n    [\"\u2581\", null],\n    [\"\u049a\", \"Cash\"],\n    [\"\u043e\", null],\n    [\"\u043b\", null],\n    [\"\u043c\u0430\", null],\n    [\"-\", null],\n    [\"\u049b\", null],\n    [\"\u043e\", null],\n    [\"\u043b\", null],\n    [\"\u2581\u0430\", null],\n    [\"\u049b\", null],\n    [\"\u0448\u0430\", null],\n    [\"\u2581\", null],\n    [\"\u049b\", \"Curse\"],\n    [\"\u0430\u0440\", null],\n    [\"\u0493\", null],\n    [\"\u044b\", null],\n    [\"\u0441\u044b\", null],\n    [\"\u0430\u0442\", null],\n    [\"\u0442\u044b\", null],\n    [\"\u0436\u0430\", \"new\"],\n    [\"\u04a3\", null],\n    [\"\u0430\", null],\n    [\"\u2581\u043a\", \"book\"],\n    [\"\u0456\", null],\n    [\"\u0442\u0430\", null],\n    [\"\u0431\u044b\", null],\n    [\"\u043d\", null],\n    [\"\u0434\u0430\", null],\n    [\"\u041a\", \"Kenneth\"],\n    [\"\u0435\u043d\", null],\n    [\"\u043d\u0435\", null],\n    [\"\u0442\", null],\n    [\"\u2581\u0420\u043e\", \"Rogoff\"],\n    [\"\u0433\u043e\", null],\n    [\"\u0444\", null],\n    [\"\u0444\", null],\n    [\"\u2581\u0441\u0430\", \"tax\"],\n    [\"\u043b\u044b\", null],\n    [\"\u049b\", null],\n    [\"\u0442\u044b\", null],\n    [\"\u2581\u0442\", \"evasion\"],\n    [\"\u04e9\", null],\n    [\"\u043b\u0435\", null],\n    [\"\u0443\", null],\n    [\"\u0434\u0435\u043d\", null],\n    [\"\u2581\", null],\n    [\"\u0436\u0430\", \"and\"],\n    [\"\u043b\u0442\u0430\", null],\n    [\"\u0440\u0443\", null],\n    [\"\u2581\u0436\", \"criminal\"],\n    [\"\u04d9\", null],\n    [\"\u043d\u0435\", null],\n    [\"\u2581\", null],\n    [\"\u049b\", \"activities\"],\n    [\"\u044b\", null],\n    [\"\u043b\", null],\n    [\"\u043c\u044b\", null],\n    [\"\u0441\u0442\u044b\", null],\n    [\"\u049b\", null],\n    [\"\u2581\", null],\n    [\"\u04d9\", \"to\"],\n    [\"\u0440\u0435\", null],\n    [\"\u043a\", null],\n    [\"\u0435\u0442\", null],\n    [\"\u043f\u0435\u043d\", null],\n    [\"\u2581\u043a\", \"combat\"],\n    [\"\u04af\", null],\n    [\"\u0440\u0435\", null],\n    [\"\u0441\u0443\", null],\n    [\"\u2581\u043c\u0430\", \"in\"],\n    [\"\u049b\", null],\n    [\"\u0441\u0430\", null],\n    [\"\u0442\u044b\", null],\n    [\"\u043d\", null],\n    [\"\u0434\u0430\", null],\n    [\"\u2581\u0436\", \"order\"],\n    [\"\u043e\", null],\n    [\"\u0493\", null],\n    [\"\u0430\u0440\", null],\n    [\"\u044b\", null],\n    [\"\u2581\u0430\", \"high\"],\n    [\"\u0442\u0430\", null],\n    [\"\u0443\", null],\n    [\"\u043b\u044b\", null],\n    [\"\u2581\u0431\", \"denomination\"],\n    [\"\u0430\u043d\", null],\n    [\"\u043a\", null],\n    [\"\u043d\u043e\", null],\n    [\"\u0442\u0430\", null],\n    [\"\u043b\u0430\", null],\n    [\"\u0440\", null],\n    [\"\u0434\u044b\", null],\n    [\"\u2581\u0436\", \"abolish\"],\n    [\"\u043e\", null],\n    [\"\u044e\", null],\n    [\"\u0434\u044b\", null],\n    [\"\u2581\", null],\n    [\"\u049b\", \"supports\"],\n    [\"\u043e\", null],\n    [\"\u043b\", null],\n    [\"\u0434\u0430\", null],\n    [\"\u0439\", null],\n    [\"\u0434\u044b\", null],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u04e8\u0437\u0456\u043d\u0456\u04a3 \u049a\u043e\u043b\u043c\u0430-\u049b\u043e\u043b \u0430\u049b\u0448\u0430 \u049b\u0430\u0440\u0493\u044b\u0441\u044b\u0430\u0442\u0442\u044b\u0436\u0430\u04a3\u0430 \u043a\u0456\u0442\u0430\u0431\u044b\u043d\u0434\u0430\u041a\u0435\u043d\u043d\u0435\u0442 \u0420\u043e\u0433\u043e\u0444\u0444 \u0441\u0430\u043b\u044b\u049b\u0442\u044b \u0442\u04e9\u043b\u0435\u0443\u0434\u0435\u043d \u0436\u0430\u043b\u0442\u0430\u0440\u0443 \u0436\u04d9\u043d\u0435 \u049b\u044b\u043b\u043c\u044b\u0441\u0442\u044b\u049b \u04d9\u0440\u0435\u043a\u0435\u0442\u043f\u0435\u043d \u043a\u04af\u0440\u0435\u0441\u0443 \u043c\u0430\u049b\u0441\u0430\u0442\u044b\u043d\u0434\u0430 \u0436\u043e\u0493\u0430\u0440\u044b \u0430\u0442\u0430\u0443\u043b\u044b \u0431\u0430\u043d\u043a\u043d\u043e\u0442\u0430\u043b\u0430\u0440\u0434\u044b \u0436\u043e\u044e\u0434\u044b \u049b\u043e\u043b\u0434\u0430\u0439\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '<0xD3>', '<0xA8>', '\u0437\u0456', '\u043d\u0456', '\u04a3', '\u2581', '\u049a', '\u043e', '\u043b', '\u043c\u0430', '-', '\u049b', '\u043e', '\u043b', '\u2581\u0430', '\u049b', '\u0448\u0430', '\u2581', '\u049b', '\u0430\u0440', '\u0493', '\u044b', '\u0441\u044b', '\u0430\u0442', '\u0442\u044b', '\u0436\u0430', '\u04a3', '\u0430', '\u2581\u043a', '\u0456', '\u0442\u0430', '\u0431\u044b', '\u043d', '\u0434\u0430', '\u041a', '\u0435\u043d', '\u043d\u0435', '\u0442', '\u2581\u0420\u043e', '\u0433\u043e', '\u0444', '\u0444', '\u2581\u0441\u0430', '\u043b\u044b', '\u049b', '\u0442\u044b', '\u2581\u0442', '\u04e9', '\u043b\u0435', '\u0443', '\u0434\u0435\u043d', '\u2581', '\u0436\u0430', '\u043b\u0442\u0430', '\u0440\u0443', '\u2581\u0436', '\u04d9', '\u043d\u0435', '\u2581', '\u049b', '\u044b', '\u043b', '\u043c\u044b', '\u0441\u0442\u044b', '\u049b', '\u2581', '\u04d9', '\u0440\u0435', '\u043a', '\u0435\u0442', '\u043f\u0435\u043d', '\u2581\u043a', '\u04af', '\u0440\u0435', '\u0441\u0443', '\u2581\u043c\u0430', '\u049b', '\u0441\u0430', '\u0442\u044b', '\u043d', '\u0434\u0430', '\u2581\u0436', '\u043e', '\u0493', '\u0430\u0440', '\u044b', '\u2581\u0430', '\u0442\u0430', '\u0443', '\u043b\u044b', '\u2581\u0431', '\u0430\u043d', '\u043a', '\u043d\u043e', '\u0442\u0430', '\u043b\u0430', '\u0440', '\u0434\u044b', '\u2581\u0436', '\u043e', '\u044e', '\u0434\u044b', '\u2581', '\u049b', '\u043e', '\u043b', '\u0434\u0430', '\u0439', '\u0434\u044b', '.']"}], "[\n  [\"\u2581\", null],\n  [\"<0xD3>\", null],\n  [\"<0xA8>\", null],\n  [\"\u0437\u0456\", null],\n  [\"\u043d\u0456\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\", null],\n  [\"\u049a\", \"\u5728\"],\n  [\"\u043e\", null],\n  [\"\u043b\", null],\n  [\"\u043c\u0430\", null],\n  [\"-\", null],\n  [\"\u049b\", null],\n  [\"\u043e\", null],\n  [\"\u043b\", null],\n  [\"\u2581\u0430\", null],\n  [\"\u049b\", null],\n  [\"\u0448\u0430\", null],\n  [\"\u2581\", null],\n  [\"\u049b\", null],\n  [\"\u0430\u0440\", null],\n  [\"\u0493\", null],\n  [\"\u044b\", null],\n  [\"\u0441\u044b\", null],\n  [\"\u0430\u0442\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u0436\u0430\", null],\n  [\"\u04a3\", null],\n  [\"\u0430\", null],\n  [\"\u2581\u043a\", null],\n  [\"\u0456\", null],\n  [\"\u0442\u0430\", null],\n  [\"\u0431\u044b\", null],\n  [\"\u043d\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u041a\", \"\u80af\u5c3c\u65af\"],\n  [\"\u0435\u043d\", null],\n  [\"\u043d\u0435\", null],\n  [\"\u0442\", null],\n  [\"\u2581\u0420\u043e\", \"\u7f57\u6208\u592b\"],\n  [\"\u0433\u043e\", null],\n  [\"\u0444\", null],\n  [\"\u0444\", null],\n  [\"\u2581\u0441\u0430\", \"\u7a0e\u6536\"],\n  [\"\u043b\u044b\", null],\n  [\"\u049b\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u2581\u0442\", \"\u652f\u4ed8\"],\n  [\"\u04e9\", null],\n  [\"\u043b\u0435\", null],\n  [\"\u0443\", null],\n  [\"\u0434\u0435\u043d\", null],\n  [\"\u2581\", null],\n  [\"\u0436\u0430\", \"\u9003\u907f\"],\n  [\"\u043b\u0442\u0430\", null],\n  [\"\u0440\u0443\", null],\n  [\"\u2581\u0436\", \"\u548c\"],\n  [\"\u04d9\", null],\n  [\"\u043d\u0435\", null],\n  [\"\u2581\", null],\n  [\"\u049b\", \"\u72af\u7f6a\"],\n  [\"\u044b\", null],\n  [\"\u043b\", null],\n  [\"\u043c\u044b\", null],\n  [\"\u0441\u0442\u044b\", null],\n  [\"\u049b\", null],\n  [\"\u2581\", null],\n  [\"\u04d9\", \"\u6d3b\u52a8\"],\n  [\"\u0440\u0435\", null],\n  [\"\u043a\", null],\n  [\"\u0435\u0442\", null],\n  [\"\u043f\u0435\u043d\", null],\n  [\"\u2581\u043a\", \"\u6597\u4e89\"],\n  [\"\u04af\", null],\n  [\"\u0440\u0435\", null],\n  [\"\u0441\u0443\", null],\n  [\"\u2581\u043c\u0430\", \"\u76ee\u7684\"],\n  [\"\u049b\", null],\n  [\"\u0441\u0430\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u043d\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u2581\u0436\", \"\u9ad8\"],\n  [\"\u043e\", null],\n  [\"\u0493\", null],\n  [\"\u0430\u0440\", null],\n  [\"\u044b\", null],\n  [\"\u2581\u0430\", \"\u9762\u989d\"],\n  [\"\u0442\u0430\", null],\n  [\"\u0443\", null],\n  [\"\u043b\u044b\", null],\n  [\"\u2581\u0431\", \"\u7eb8\u5e01\"],\n  [\"\u0430\u043d\", null],\n  [\"\u043a\", null],\n  [\"\u043d\u043e\", null],\n  [\"\u0442\u0430\", null],\n  [\"\u043b\u0430\", null],\n  [\"\u0440\", null],\n  [\"\u0434\u044b\", null],\n  [\"\u2581\u0436\", \"\u53d6\u6d88\"],\n  [\"\u043e\", null],\n  [\"\u044e\", null],\n  [\"\u0434\u044b\", null],\n  [\"\u2581\", null],\n  [\"\u049b\", \"\u652f\u6301\"],\n  [\"\u043e\", null],\n  [\"\u043b\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u0439\", null],\n  [\"\u0434\u044b\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u04e8\u0437\u0456\u043d\u0456\u04a3 \u049a\u043e\u043b\u043c\u0430-\u049b\u043e\u043b \u0430\u049b\u0448\u0430 \u049b\u0430\u0440\u0493\u044b\u0441\u044b\u0430\u0442\u0442\u044b\u0436\u0430\u04a3\u0430 \u043a\u0456\u0442\u0430\u0431\u044b\u043d\u0434\u0430\u041a\u0435\u043d\u043d\u0435\u0442 \u0420\u043e\u0433\u043e\u0444\u0444 \u0441\u0430\u043b\u044b\u049b\u0442\u044b \u0442\u04e9\u043b\u0435\u0443\u0434\u0435\u043d \u0436\u0430\u043b\u0442\u0430\u0440\u0443 \u0436\u04d9\u043d\u0435 \u049b\u044b\u043b\u043c\u044b\u0441\u0442\u044b\u049b \u04d9\u0440\u0435\u043a\u0435\u0442\u043f\u0435\u043d \u043a\u04af\u0440\u0435\u0441\u0443 \u043c\u0430\u049b\u0441\u0430\u0442\u044b\u043d\u0434\u0430 \u0436\u043e\u0493\u0430\u0440\u044b \u0430\u0442\u0430\u0443\u043b\u044b \u0431\u0430\u043d\u043a\u043d\u043e\u0442\u0430\u043b\u0430\u0440\u0434\u044b \u0436\u043e\u044e\u0434\u044b \u049b\u043e\u043b\u0434\u0430\u0439\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '<0xD3>', '<0xA8>', '\u0437\u0456', '\u043d\u0456', '\u04a3', '\u2581', '\u049a', '\u043e', '\u043b', '\u043c\u0430', '-', '\u049b', '\u043e', '\u043b', '\u2581\u0430', '\u049b', '\u0448\u0430', '\u2581', '\u049b', '\u0430\u0440', '\u0493', '\u044b', '\u0441\u044b', '\u0430\u0442', '\u0442\u044b', '\u0436\u0430', '\u04a3', '\u0430', '\u2581\u043a', '\u0456', '\u0442\u0430', '\u0431\u044b', '\u043d', '\u0434\u0430', '\u041a', '\u0435\u043d', '\u043d\u0435', '\u0442', '\u2581\u0420\u043e', '\u0433\u043e', '\u0444', '\u0444', '\u2581\u0441\u0430', '\u043b\u044b', '\u049b', '\u0442\u044b', '\u2581\u0442', '\u04e9', '\u043b\u0435', '\u0443', '\u0434\u0435\u043d', '\u2581', '\u0436\u0430', '\u043b\u0442\u0430', '\u0440\u0443', '\u2581\u0436', '\u04d9', '\u043d\u0435', '\u2581', '\u049b', '\u044b', '\u043b', '\u043c\u044b', '\u0441\u0442\u044b', '\u049b', '\u2581', '\u04d9', '\u0440\u0435', '\u043a', '\u0435\u0442', '\u043f\u0435\u043d', '\u2581\u043a', '\u04af', '\u0440\u0435', '\u0441\u0443', '\u2581\u043c\u0430', '\u049b', '\u0441\u0430', '\u0442\u044b', '\u043d', '\u0434\u0430', '\u2581\u0436', '\u043e', '\u0493', '\u0430\u0440', '\u044b', '\u2581\u0430', '\u0442\u0430', '\u0443', '\u043b\u044b', '\u2581\u0431', '\u0430\u043d', '\u043a', '\u043d\u043e', '\u0442\u0430', '\u043b\u0430', '\u0440', '\u0434\u044b', '\u2581\u0436', '\u043e', '\u044e', '\u0434\u044b', '\u2581', '\u049b', '\u043e', '\u043b', '\u0434\u0430', '\u0439', '\u0434\u044b', '.']"}], "[\n  [\"\u2581\", null],\n  [\"<0xD3>\", null],\n  [\"<0xA8>\", null],\n  [\"\u0437\u0456\", null],\n  [\"\u043d\u0456\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\", null],\n  [\"\u049a\", \"\u0412\"],\n  [\"\u043e\", null],\n  [\"\u043b\", null],\n  [\"\u043c\u0430\", null],\n  [\"-\", null],\n  [\"\u049b\", null],\n  [\"\u043e\", null],\n  [\"\u043b\", null],\n  [\"\u2581\u0430\", null],\n  [\"\u049b\", null],\n  [\"\u0448\u0430\", null],\n  [\"\u2581\", null],\n  [\"\u049b\", null],\n  [\"\u0430\u0440\", null],\n  [\"\u0493\", null],\n  [\"\u044b\", null],\n  [\"\u0441\u044b\", null],\n  [\"\u0430\u0442\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u0436\u0430\", null],\n  [\"\u04a3\", null],\n  [\"\u0430\", null],\n  [\"\u2581\u043a\", \"\u043a\u043d\u0438\u0433\u0435\"],\n  [\"\u0456\", null],\n  [\"\u0442\u0430\", null],\n  [\"\u0431\u044b\", null],\n  [\"\u043d\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u041a\", \"\u041a\u0435\u043d\u043d\u0435\u0442\"],\n  [\"\u0435\u043d\", null],\n  [\"\u043d\u0435\", null],\n  [\"\u0442\", null],\n  [\"\u2581\u0420\u043e\", \"\u0420\u043e\u0433\u043e\u0444\u0444\"],\n  [\"\u0433\u043e\", null],\n  [\"\u0444\", null],\n  [\"\u0444\", null],\n  [\"\u2581\u0441\u0430\", \"\u043d\u0430\u043b\u043e\u0433\"],\n  [\"\u043b\u044b\", null],\n  [\"\u049b\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u2581\u0442\", \"\u0443\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0435\"],\n  [\"\u04e9\", null],\n  [\"\u043b\u0435\", null],\n  [\"\u0443\", null],\n  [\"\u0434\u0435\u043d\", null],\n  [\"\u2581\", null],\n  [\"\u0436\u0430\", null],\n  [\"\u043b\u0442\u0430\", null],\n  [\"\u0440\u0443\", null],\n  [\"\u2581\u0436\", \"\u0438\"],\n  [\"\u04d9\", null],\n  [\"\u043d\u0435\", null],\n  [\"\u2581\", null],\n  [\"\u049b\", \"\u043f\u0440\u0435\u0441\u0442\u0443\u043f\u043d\u043e\u0439\"],\n  [\"\u044b\", null],\n  [\"\u043b\", null],\n  [\"\u043c\u044b\", null],\n  [\"\u0441\u0442\u044b\", null],\n  [\"\u049b\", null],\n  [\"\u2581\", null],\n  [\"\u04d9\", \"\u0434\u0435\u044f\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c\u044e\"],\n  [\"\u0440\u0435\", null],\n  [\"\u043a\", null],\n  [\"\u0435\u0442\", null],\n  [\"\u043f\u0435\u043d\", null],\n  [\"\u2581\u043a\", \"\u0431\u043e\u0440\u044c\u0431\u044b\"],\n  [\"\u04af\", null],\n  [\"\u0440\u0435\", null],\n  [\"\u0441\u0443\", null],\n  [\"\u2581\u043c\u0430\", \"\u0446\u0435\u043b\u044c\u044e\"],\n  [\"\u049b\", null],\n  [\"\u0441\u0430\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u043d\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u2581\u0436\", \"\u0432\u044b\u0441\u043e\u043a\u0438\u0445\"],\n  [\"\u043e\", null],\n  [\"\u0493\", null],\n  [\"\u0430\u0440\", null],\n  [\"\u044b\", null],\n  [\"\u2581\u0430\", \"\u043d\u043e\u043c\u0438\u043d\u0430\u043b\u043e\u0432\"],\n  [\"\u0442\u0430\", null],\n  [\"\u0443\", null],\n  [\"\u043b\u044b\", null],\n  [\"\u2581\u0431\", \"\u0431\u0430\u043d\u043a\u043d\u043e\u0442\"],\n  [\"\u0430\u043d\", null],\n  [\"\u043a\", null],\n  [\"\u043d\u043e\", null],\n  [\"\u0442\u0430\", null],\n  [\"\u043b\u0430\", null],\n  [\"\u0440\", null],\n  [\"\u0434\u044b\", null],\n  [\"\u2581\u0436\", \"\u0443\u043d\u0438\u0447\u0442\u043e\u0436\u0435\u043d\u0438\u0435\"],\n  [\"\u043e\", null],\n  [\"\u044e\", null],\n  [\"\u0434\u044b\", null],\n  [\"\u2581\", null],\n  [\"\u049b\", \"\u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0435\u0442\"],\n  [\"\u043e\", null],\n  [\"\u043b\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u0439\", null],\n  [\"\u0434\u044b\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u041e\u0434\u0430\u043d \u0431\u0430\u0441\u049b\u0430 \u0435\u0448\u049b\u0430\u043d\u0434\u0430\u0439 \u0432\u0430\u043b\u044e\u0442\u0430 \u0435\u043d \u0431\u0430\u0439\u043b\u044b\u049b\u0442\u044b\u04a3 \u049b\u0430\u0443\u0456\u043f\u0441\u0456\u0437\u0434\u0456\u0433\u0456 \u043c\u0435\u043d \u04e9\u0442\u0456\u043c\u0434\u0456\u043b\u0456\u0433\u0456\u043d \u043e\u043d\u0434\u0430\u0439 \u0434\u0435\u04a3\u0433\u0435\u0439\u0434\u0435 \u049b\u0430\u043c\u0442\u0430\u043c\u0430\u0441\u044b\u0437 \u0435\u0442\u0435 \u0430\u043b\u0493\u0430\u043d \u0435\u043c\u0435\u0441.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041e', '\u0434\u0430\u043d', '\u2581\u0431\u0430', '\u0441', '\u049b', '\u0430', '\u2581\u0435', '\u0448', '\u049b', '\u0430\u043d', '\u0434\u0430', '\u0439', '\u2581\u0432\u0430', '\u043b\u044e', '\u0442\u0430', '\u2581', '\u0435\u043d', '\u2581\u0431\u0430', '\u0439', '\u043b\u044b', '\u049b', '\u0442\u044b', '\u04a3', '\u2581', '\u049b', '\u0430', '\u0443', '\u0456', '\u043f', '\u0441\u0456', '\u0437', '\u0434\u0456', '\u0433\u0456', '\u2581\u043c', '\u0435\u043d', '\u2581', '\u04e9', '\u0442\u0456', '\u043c', '\u0434\u0456', '\u043b\u0456', '\u0433\u0456', '\u043d', '\u2581\u043e\u043d', '\u0434\u0430', '\u0439', '\u2581\u0434\u0435', '\u04a3', '\u0433', '\u0435\u0439', '\u0434\u0435', '\u2581', '\u049b', '\u0430', '\u043c', '\u0442\u0430', '\u043c\u0430', '\u0441\u044b', '\u0437', '\u2581\u0435', '\u0442\u0435', '\u2581\u0430\u043b', '\u0493', '\u0430\u043d', '\u2581\u0435', '\u043c\u0435', '\u0441', '.']"}], "[\n[\"\u2581\u041e\", \"Other\"],\n[\"\u0434\u0430\u043d\", null],\n[\"\u2581\u0431\u0430\", null],\n[\"\u0441\", null],\n[\"\u049b\", null],\n[\"\u0430\", null],\n[\"\u2581\u0435\", null],\n[\"\u0448\", null],\n[\"\u049b\", null],\n[\"\u0430\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u0439\", null],\n[\"\u2581\u0432\u0430\", \"currency\"],\n[\"\u043b\u044e\", null],\n[\"\u0442\u0430\", null],\n[\"\u2581\", null],\n[\"\u0435\u043d\", null],\n[\"\u2581\u0431\u0430\", \"wealth\"],\n[\"\u0439\", null],\n[\"\u043b\u044b\", null],\n[\"\u049b\", null],\n[\"\u0442\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"security\"],\n[\"\u0430\", null],\n[\"\u0443\", null],\n[\"\u0456\", null],\n[\"\u043f\", null],\n[\"\u0441\u0456\", null],\n[\"\u0437\", null],\n[\"\u0434\u0456\", null],\n[\"\u0433\u0456\", null],\n[\"\u2581\u043c\", \"and\"],\n[\"\u0435\u043d\", null],\n[\"\u2581\", null],\n[\"\u04e9\", \"liquidity\"],\n[\"\u0442\u0456\", null],\n[\"\u043c\", null],\n[\"\u0434\u0456\", null],\n[\"\u043b\u0456\", null],\n[\"\u0433\u0456\", null],\n[\"\u043d\", null],\n[\"\u2581\u043e\u043d\", \"such\"],\n[\"\u0434\u0430\", null],\n[\"\u0439\", null],\n[\"\u2581\u0434\u0435\", \"level\"],\n[\"\u04a3\", null],\n[\"\u0433\", null],\n[\"\u0435\u0439\", null],\n[\"\u0434\u0435\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"ensure\"],\n[\"\u0430\", null],\n[\"\u043c\", null],\n[\"\u0442\u0430\", null],\n[\"\u043c\u0430\", null],\n[\"\u0441\u044b\", null],\n[\"\u0437\", null],\n[\"\u2581\u0435\", \"could\"],\n[\"\u0442\u0435\", null],\n[\"\u2581\u0430\u043b\", null],\n[\"\u0493\", null],\n[\"\u0430\u043d\", null],\n[\"\u2581\u0435\", null],\n[\"\u043c\u0435\", null],\n[\"\u0441\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u041e\u0434\u0430\u043d \u0431\u0430\u0441\u049b\u0430 \u0435\u0448\u049b\u0430\u043d\u0434\u0430\u0439 \u0432\u0430\u043b\u044e\u0442\u0430 \u0435\u043d \u0431\u0430\u0439\u043b\u044b\u049b\u0442\u044b\u04a3 \u049b\u0430\u0443\u0456\u043f\u0441\u0456\u0437\u0434\u0456\u0433\u0456 \u043c\u0435\u043d \u04e9\u0442\u0456\u043c\u0434\u0456\u043b\u0456\u0433\u0456\u043d \u043e\u043d\u0434\u0430\u0439 \u0434\u0435\u04a3\u0433\u0435\u0439\u0434\u0435 \u049b\u0430\u043c\u0442\u0430\u043c\u0430\u0441\u044b\u0437 \u0435\u0442\u0435 \u0430\u043b\u0493\u0430\u043d \u0435\u043c\u0435\u0441.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041e', '\u0434\u0430\u043d', '\u2581\u0431\u0430', '\u0441', '\u049b', '\u0430', '\u2581\u0435', '\u0448', '\u049b', '\u0430\u043d', '\u0434\u0430', '\u0439', '\u2581\u0432\u0430', '\u043b\u044e', '\u0442\u0430', '\u2581', '\u0435\u043d', '\u2581\u0431\u0430', '\u0439', '\u043b\u044b', '\u049b', '\u0442\u044b', '\u04a3', '\u2581', '\u049b', '\u0430', '\u0443', '\u0456', '\u043f', '\u0441\u0456', '\u0437', '\u0434\u0456', '\u0433\u0456', '\u2581\u043c', '\u0435\u043d', '\u2581', '\u04e9', '\u0442\u0456', '\u043c', '\u0434\u0456', '\u043b\u0456', '\u0433\u0456', '\u043d', '\u2581\u043e\u043d', '\u0434\u0430', '\u0439', '\u2581\u0434\u0435', '\u04a3', '\u0433', '\u0435\u0439', '\u0434\u0435', '\u2581', '\u049b', '\u0430', '\u043c', '\u0442\u0430', '\u043c\u0430', '\u0441\u044b', '\u0437', '\u2581\u0435', '\u0442\u0435', '\u2581\u0430\u043b', '\u0493', '\u0430\u043d', '\u2581\u0435', '\u043c\u0435', '\u0441', '.']"}], "[\n  [\"\u2581\u041e\", \"\u9664\"],\n  [\"\u0434\u0430\u043d\", null],\n  [\"\u2581\u0431\u0430\", null],\n  [\"\u0441\", null],\n  [\"\u049b\", null],\n  [\"\u0430\", null],\n  [\"\u2581\u0435\", null],\n  [\"\u0448\", null],\n  [\"\u049b\", null],\n  [\"\u0430\u043d\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u0439\", null],\n  [\"\u2581\u0432\u0430\", \"\u8d27\u5e01\"],\n  [\"\u043b\u044e\", null],\n  [\"\u0442\u0430\", null],\n  [\"\u2581\", null],\n  [\"\u0435\u043d\", null],\n  [\"\u2581\u0431\u0430\", \"\u8d22\u5bcc\"],\n  [\"\u0439\", null],\n  [\"\u043b\u044b\", null],\n  [\"\u049b\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\", null],\n  [\"\u049b\", \"\u5b89\u5168\u6027\"],\n  [\"\u0430\", null],\n  [\"\u0443\", null],\n  [\"\u0456\", null],\n  [\"\u043f\", null],\n  [\"\u0441\u0456\", null],\n  [\"\u0437\", null],\n  [\"\u0434\u0456\", null],\n  [\"\u0433\u0456\", null],\n  [\"\u2581\u043c\", \"\u548c\"],\n  [\"\u0435\u043d\", null],\n  [\"\u2581\", null],\n  [\"\u04e9\", \"\u6d41\u52a8\u6027\"],\n  [\"\u0442\u0456\", null],\n  [\"\u043c\", null],\n  [\"\u0434\u0456\", null],\n  [\"\u043b\u0456\", null],\n  [\"\u0433\u0456\", null],\n  [\"\u043d\", null],\n  [\"\u2581\u043e\u043d\", \"\u90a3\u6837\"],\n  [\"\u0434\u0430\", null],\n  [\"\u0439\", null],\n  [\"\u2581\u0434\u0435\", \"\u7a0b\u5ea6\"],\n  [\"\u04a3\", null],\n  [\"\u0433\", null],\n  [\"\u0435\u0439\", null],\n  [\"\u0434\u0435\", null],\n  [\"\u2581\", null],\n  [\"\u049b\", \"\u4fdd\u969c\"],\n  [\"\u0430\", null],\n  [\"\u043c\", null],\n  [\"\u0442\u0430\", null],\n  [\"\u043c\u0430\", null],\n  [\"\u0441\u044b\", null],\n  [\"\u0437\", null],\n  [\"\u2581\u0435\", null],\n  [\"\u0442\u0435\", null],\n  [\"\u2581\u0430\u043b\", \"\u80fd\u591f\"],\n  [\"\u0493\", null],\n  [\"\u0430\u043d\", null],\n  [\"\u2581\u0435\", null],\n  [\"\u043c\u0435\", null],\n  [\"\u0441\", null],\n  [\".\", \"\u3002\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u041e\u0434\u0430\u043d \u0431\u0430\u0441\u049b\u0430 \u0435\u0448\u049b\u0430\u043d\u0434\u0430\u0439 \u0432\u0430\u043b\u044e\u0442\u0430 \u0435\u043d \u0431\u0430\u0439\u043b\u044b\u049b\u0442\u044b\u04a3 \u049b\u0430\u0443\u0456\u043f\u0441\u0456\u0437\u0434\u0456\u0433\u0456 \u043c\u0435\u043d \u04e9\u0442\u0456\u043c\u0434\u0456\u043b\u0456\u0433\u0456\u043d \u043e\u043d\u0434\u0430\u0439 \u0434\u0435\u04a3\u0433\u0435\u0439\u0434\u0435 \u049b\u0430\u043c\u0442\u0430\u043c\u0430\u0441\u044b\u0437 \u0435\u0442\u0435 \u0430\u043b\u0493\u0430\u043d \u0435\u043c\u0435\u0441.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041e', '\u0434\u0430\u043d', '\u2581\u0431\u0430', '\u0441', '\u049b', '\u0430', '\u2581\u0435', '\u0448', '\u049b', '\u0430\u043d', '\u0434\u0430', '\u0439', '\u2581\u0432\u0430', '\u043b\u044e', '\u0442\u0430', '\u2581', '\u0435\u043d', '\u2581\u0431\u0430', '\u0439', '\u043b\u044b', '\u049b', '\u0442\u044b', '\u04a3', '\u2581', '\u049b', '\u0430', '\u0443', '\u0456', '\u043f', '\u0441\u0456', '\u0437', '\u0434\u0456', '\u0433\u0456', '\u2581\u043c', '\u0435\u043d', '\u2581', '\u04e9', '\u0442\u0456', '\u043c', '\u0434\u0456', '\u043b\u0456', '\u0433\u0456', '\u043d', '\u2581\u043e\u043d', '\u0434\u0430', '\u0439', '\u2581\u0434\u0435', '\u04a3', '\u0433', '\u0435\u0439', '\u0434\u0435', '\u2581', '\u049b', '\u0430', '\u043c', '\u0442\u0430', '\u043c\u0430', '\u0441\u044b', '\u0437', '\u2581\u0435', '\u0442\u0435', '\u2581\u0430\u043b', '\u0493', '\u0430\u043d', '\u2581\u0435', '\u043c\u0435', '\u0441', '.']"}], "[\n  [\"\u2581\u041e\", \"\u041a\u0440\u043e\u043c\u0435\"],\n  [\"\u0434\u0430\u043d\", null],\n  [\"\u2581\u0431\u0430\", null],\n  [\"\u0441\", null],\n  [\"\u049b\", null],\n  [\"\u0430\", null],\n  [\"\u2581\u0435\", null],\n  [\"\u0448\", null],\n  [\"\u049b\", null],\n  [\"\u0430\u043d\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u0439\", null],\n  [\"\u2581\u0432\u0430\", \"\u0432\u0430\u043b\u044e\u0442\u0430\"],\n  [\"\u043b\u044e\", null],\n  [\"\u0442\u0430\", null],\n  [\"\u2581\", null],\n  [\"\u0435\u043d\", null],\n  [\"\u2581\u0431\u0430\", \"\u0431\u043e\u0433\u0430\u0442\u0441\u0442\u0432\u0430\"],\n  [\"\u0439\", null],\n  [\"\u043b\u044b\", null],\n  [\"\u049b\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\", null],\n  [\"\u049b\", \"\u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u044c\"],\n  [\"\u0430\", null],\n  [\"\u0443\", null],\n  [\"\u0456\", null],\n  [\"\u043f\", null],\n  [\"\u0441\u0456\", null],\n  [\"\u0437\", null],\n  [\"\u0434\u0456\", null],\n  [\"\u0433\u0456\", null],\n  [\"\u2581\u043c\", \"\u0438\"],\n  [\"\u0435\u043d\", null],\n  [\"\u2581\", null],\n  [\"\u04e9\", \"\u043b\u0438\u043a\u0432\u0438\u0434\u043d\u043e\u0441\u0442\u044c\"],\n  [\"\u0442\u0456\", null],\n  [\"\u043c\", null],\n  [\"\u0434\u0456\", null],\n  [\"\u043b\u0456\", null],\n  [\"\u0433\u0456\", null],\n  [\"\u043d\", null],\n  [\"\u2581\u043e\u043d\", \"\u0442\u0430\u043a\"],\n  [\"\u0434\u0430\", null],\n  [\"\u0439\", null],\n  [\"\u2581\u0434\u0435\", \"\u0443\u0440\u043e\u0432\u043d\u0435\"],\n  [\"\u04a3\", null],\n  [\"\u0433\", null],\n  [\"\u0435\u0439\", null],\n  [\"\u0434\u0435\", null],\n  [\"\u2581\", null],\n  [\"\u049b\", \"\u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0442\u044c\"],\n  [\"\u0430\", null],\n  [\"\u043c\", null],\n  [\"\u0442\u0430\", null],\n  [\"\u043c\u0430\", null],\n  [\"\u0441\u044b\", null],\n  [\"\u0437\", null],\n  [\"\u2581\u0435\", \"\u0441\u043c\u043e\u0433\u043b\u0430\"],\n  [\"\u0442\u0435\", null],\n  [\"\u2581\u0430\u043b\", null],\n  [\"\u0493\", null],\n  [\"\u0430\u043d\", null],\n  [\"\u2581\u0435\", null],\n  [\"\u043c\u0435\", null],\n  [\"\u0441\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u0422\u0435\u043a \u0430\u0437\u0430\u043c\u0430\u0442\u0442\u044b\u049b \u049b\u043e\u0493\u0430\u043c\u043d\u044b\u04a3 \u0493\u0430\u043d\u0430 \u0435\u043c\u0435\u0441, \u0431\u04af\u043a\u0456\u043b \u04e9\u0440\u043a\u0435\u043d\u0438\u0435\u0442\u0442\u0456\u04a3 \u0442\u0430\u0493\u0434\u044b\u0440\u044b \u049b\u044b\u043b \u04af\u0441\u0442\u0456\u043d\u0434\u0435 \u0442\u04b1\u0440.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0422\u0435', '\u043a', '\u2581\u0430', '\u0437\u0430', '\u043c\u0430\u0442', '\u0442\u044b', '\u049b', '\u2581', '\u049b', '\u043e', '\u0493', '\u0430', '\u043c', '\u043d\u044b', '\u04a3', '\u2581', '\u0493', '\u0430\u043d\u0430', '\u2581\u0435', '\u043c\u0435', '\u0441', ',', '\u2581\u0431', '\u04af', '\u043a\u0456', '\u043b', '\u2581', '\u04e9', '\u0440', '\u043a\u0435', '\u043d\u0438\u0435', '\u0442', '\u0442\u0456', '\u04a3', '\u2581\u0442\u0430', '\u0493', '\u0434\u044b', '\u0440\u044b', '\u2581', '\u049b', '\u044b', '\u043b', '\u2581', '\u04af', '\u0441\u0442\u0456', '\u043d', '\u0434\u0435', '\u2581\u0442', '\u04b1', '\u0440', '.']"}], "[\n  [\"\u2581\u0422\u0435\", null],\n  [\"\u043a\", \"Not\"],\n  [\"\u2581\u0430\", null],\n  [\"\u0437\u0430\", null],\n  [\"\u043c\u0430\u0442\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u049b\", null],\n  [\"\u2581\", null],\n  [\"\u049b\", null],\n  [\"\u043e\", null],\n  [\"\u0493\", null],\n  [\"\u0430\", null],\n  [\"\u043c\", null],\n  [\"\u043d\u044b\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\", null],\n  [\"\u0493\", null],\n  [\"\u0430\u043d\u0430\", \"only\"],\n  [\"\u2581\u0435\", null],\n  [\"\u043c\u0435\", null],\n  [\"\u0441\", null],\n  [\",\", null],\n  [\"\u2581\u0431\", null],\n  [\"\u04af\", null],\n  [\"\u043a\u0456\", null],\n  [\"\u043b\", null],\n  [\"\u2581\", null],\n  [\"\u04e9\", null],\n  [\"\u0440\", null],\n  [\"\u043a\u0435\", null],\n  [\"\u043d\u0438\u0435\", null],\n  [\"\u0442\", null],\n  [\"\u0442\u0456\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\u0442\u0430\", null],\n  [\"\u0493\", null],\n  [\"\u0434\u044b\", null],\n  [\"\u0440\u044b\", null],\n  [\"\u2581\", null],\n  [\"\u049b\", null],\n  [\"\u044b\", null],\n  [\"\u043b\", null],\n  [\"\u2581\", null],\n  [\"\u04af\", null],\n  [\"\u0441\u0442\u0456\", null],\n  [\"\u043d\", null],\n  [\"\u0434\u0435\", null],\n  [\"\u2581\u0442\", null],\n  [\"\u04b1\", null],\n  [\"\u0440\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u0422\u0435\u043a \u0430\u0437\u0430\u043c\u0430\u0442\u0442\u044b\u049b \u049b\u043e\u0493\u0430\u043c\u043d\u044b\u04a3 \u0493\u0430\u043d\u0430 \u0435\u043c\u0435\u0441, \u0431\u04af\u043a\u0456\u043b \u04e9\u0440\u043a\u0435\u043d\u0438\u0435\u0442\u0442\u0456\u04a3 \u0442\u0430\u0493\u0434\u044b\u0440\u044b \u049b\u044b\u043b \u04af\u0441\u0442\u0456\u043d\u0434\u0435 \u0442\u04b1\u0440.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0422\u0435', '\u043a', '\u2581\u0430', '\u0437\u0430', '\u043c\u0430\u0442', '\u0442\u044b', '\u049b', '\u2581', '\u049b', '\u043e', '\u0493', '\u0430', '\u043c', '\u043d\u044b', '\u04a3', '\u2581', '\u0493', '\u0430\u043d\u0430', '\u2581\u0435', '\u043c\u0435', '\u0441', ',', '\u2581\u0431', '\u04af', '\u043a\u0456', '\u043b', '\u2581', '\u04e9', '\u0440', '\u043a\u0435', '\u043d\u0438\u0435', '\u0442', '\u0442\u0456', '\u04a3', '\u2581\u0442\u0430', '\u0493', '\u0434\u044b', '\u0440\u044b', '\u2581', '\u049b', '\u044b', '\u043b', '\u2581', '\u04af', '\u0441\u0442\u0456', '\u043d', '\u0434\u0435', '\u2581\u0442', '\u04b1', '\u0440', '.']"}], "[\n[\"\u2581\u0422\u0435\", \"\u4e0d\u4ec5\"],\n[\"\u043a\", null],\n[\"\u2581\u0430\", null],\n[\"\u0437\u0430\", null],\n[\"\u043c\u0430\u0442\", null],\n[\"\u0442\u044b\", null],\n[\"\u049b\", null],\n[\"\u2581\", null],\n[\"\u049b\", null],\n[\"\u043e\", null],\n[\"\u0493\", null],\n[\"\u0430\", null],\n[\"\u043c\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\", null],\n[\"\u0493\", null],\n[\"\u0430\u043d\u0430\", \"\u4ec5\"],\n[\"\u2581\u0435\", null],\n[\"\u043c\u0435\", null],\n[\"\u0441\", null],\n[\",\" , null],\n[\"\u2581\u0431\", null],\n[\"\u04af\", null],\n[\"\u043a\u0456\", null],\n[\"\u043b\", null],\n[\"\u2581\", null],\n[\"\u04e9\", \"\u6587\u660e\"],\n[\"\u0440\", null],\n[\"\u043a\u0435\", null],\n[\"\u043d\u0438\u0435\", null],\n[\"\u0442\", null],\n[\"\u0442\u0456\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0442\u0430\", \"\u547d\u8fd0\"],\n[\"\u0493\", null],\n[\"\u0434\u044b\", null],\n[\"\u0440\u044b\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"\u5c8c\u5c8c\u53ef\u5371\"],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\"\u2581\", null],\n[\"\u04af\", null],\n[\"\u0441\u0442\u0456\", null],\n[\"\u043d\", null],\n[\"\u0434\u0435\", null],\n[\"\u2581\u0442\", null],\n[\"\u04b1\", null],\n[\"\u0440\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u0422\u0435\u043a \u0430\u0437\u0430\u043c\u0430\u0442\u0442\u044b\u049b \u049b\u043e\u0493\u0430\u043c\u043d\u044b\u04a3 \u0493\u0430\u043d\u0430 \u0435\u043c\u0435\u0441, \u0431\u04af\u043a\u0456\u043b \u04e9\u0440\u043a\u0435\u043d\u0438\u0435\u0442\u0442\u0456\u04a3 \u0442\u0430\u0493\u0434\u044b\u0440\u044b \u049b\u044b\u043b \u04af\u0441\u0442\u0456\u043d\u0434\u0435 \u0442\u04b1\u0440.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0422\u0435', '\u043a', '\u2581\u0430', '\u0437\u0430', '\u043c\u0430\u0442', '\u0442\u044b', '\u049b', '\u2581', '\u049b', '\u043e', '\u0493', '\u0430', '\u043c', '\u043d\u044b', '\u04a3', '\u2581', '\u0493', '\u0430\u043d\u0430', '\u2581\u0435', '\u043c\u0435', '\u0441', ',', '\u2581\u0431', '\u04af', '\u043a\u0456', '\u043b', '\u2581', '\u04e9', '\u0440', '\u043a\u0435', '\u043d\u0438\u0435', '\u0442', '\u0442\u0456', '\u04a3', '\u2581\u0442\u0430', '\u0493', '\u0434\u044b', '\u0440\u044b', '\u2581', '\u049b', '\u044b', '\u043b', '\u2581', '\u04af', '\u0441\u0442\u0456', '\u043d', '\u0434\u0435', '\u2581\u0442', '\u04b1', '\u0440', '.']"}], "[\n[\"\u2581\u0422\u0435\", \"\u0421\u0443\u0434\u044c\u0431\u0430\"],\n[\"\u043a\", null],\n[\"\u2581\u0430\", null],\n[\"\u0437\u0430\", null],\n[\"\u043c\u0430\u0442\", null],\n[\"\u0442\u044b\", null],\n[\"\u049b\", null],\n[\"\u2581\", null],\n[\"\u049b\", null],\n[\"\u043e\", null],\n[\"\u0493\", null],\n[\"\u0430\", null],\n[\"\u043c\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\", null],\n[\"\u0493\", null],\n[\"\u0430\u043d\u0430\", null],\n[\"\u2581\u0435\", null],\n[\"\u043c\u0435\", null],\n[\"\u0441\", null],\n[\",\", null],\n[\"\u2581\u0431\", \"\u0432\u0441\u0435\u0439\"],\n[\"\u04af\", null],\n[\"\u043a\u0456\", null],\n[\"\u043b\", null],\n[\"\u2581\", null],\n[\"\u04e9\", \"\u0446\u0438\u0432\u0438\u043b\u0438\u0437\u0430\u0446\u0438\u0438\"],\n[\"\u0440\", null],\n[\"\u043a\u0435\", null],\n[\"\u043d\u0438\u0435\", null],\n[\"\u0442\", null],\n[\"\u0442\u0456\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0442\u0430\", null],\n[\"\u0493\", null],\n[\"\u0434\u044b\", null],\n[\"\u0440\u044b\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"\u0432\u0438\u0441\u0438\u0442\"],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\"\u2581\", null],\n[\"\u04af\", \"\u043d\u0430\"],\n[\"\u0441\u0442\u0456\", null],\n[\"\u043d\", null],\n[\"\u0434\u0435\", null],\n[\"\u2581\u0442\", \"\u0432\u043e\u043b\u043e\u0441\u043a\u0435\"],\n[\"\u04b1\", null],\n[\"\u0440\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u0411\u0430\u0441\u049b\u0430 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u0442\u0430\u0440 \u0430\u0439\u044b\u0440\u043c\u0430\u0448\u044b\u043b\u044b\u0493\u044b \u2013 \u041c\u0430\u043a\u0440\u043e\u043d \u0415\u0443\u0440\u043e\u043f\u0430 \u043e\u0434\u0430\u0493\u044b\u043d \u0442\u04af\u0431\u0456\u0440\u0456\u043c\u0435\u043d \u04e9\u0437\u0433\u0435\u0440\u0442\u0456\u043f \u049b\u0430\u043d\u0430 \u049b\u043e\u0439\u043c\u0430\u0439, \u0431\u04af\u043a\u0456\u043b \u0415\u0443\u0440\u043e\u043f\u0430\u0434\u0430 \u044b\u043d\u0442\u044b\u043c\u0430\u049b\u0442\u0430\u0441\u0442\u044b\u049b\u0442\u044b \u0430\u0440\u0442\u0442\u044b\u0440\u0443 \u043a\u0435\u0440\u0435\u043a \u0434\u0435\u043f \u0441\u0430\u043d\u0430\u0439\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0411\u0430', '\u0441', '\u049b', '\u0430', '\u2581\u043a\u0430', '\u043d', '\u0434\u0438', '\u0434\u0430\u0442', '\u0442\u0430\u0440', '\u2581\u0430', '\u0439', '\u044b', '\u0440', '\u043c\u0430', '\u0448', '\u044b', '\u043b\u044b', '\u0493', '\u044b', '\u2581\u2013', '\u2581\u041c\u0430', '\u043a', '\u0440\u043e\u043d', '\u2581\u0415', '\u0443', '\u0440\u043e', '\u043f\u0430', '\u2581\u043e', '\u0434\u0430', '\u0493', '\u044b', '\u043d', '\u2581\u0442', '\u04af', '\u0431\u0456', '\u0440\u0456', '\u043c\u0435\u043d', '\u2581', '\u04e9', '\u0437', '\u0433', '\u0435\u0440', '\u0442\u0456', '\u043f', '\u2581', '\u049b', '\u0430\u043d\u0430', '\u2581', '\u049b', '\u043e', '\u0439', '\u043c\u0430', '\u0439', ',', '\u2581\u0431', '\u04af', '\u043a\u0456', '\u043b', '\u2581\u0415', '\u0443', '\u0440\u043e', '\u043f\u0430\u0434\u0430', '\u2581', '\u044b', '\u043d', '\u0442\u044b', '\u043c\u0430', '\u049b', '\u0442\u0430', '\u0441\u0442\u044b', '\u049b', '\u0442\u044b', '\u2581\u0430\u0440', '\u0442', '\u0442\u044b', '\u0440\u0443', '\u2581\u043a', '\u0435\u0440\u0435', '\u043a', '\u2581\u0434\u0435', '\u043f', '\u2581\u0441\u0430', '\u043d\u0430', '\u0439', '\u0434\u044b', '.']"}], "[\n[\"\u2581\u0411\u0430\", \"Unlike\"],\n[\"\u0441\", null],\n[\"\u049b\", null],\n[\"\u0430\", null],\n[\"\u2581\u043a\u0430\", null],\n[\"\u043d\", null],\n[\"\u0434\u0438\", null],\n[\"\u0434\u0430\u0442\", null],\n[\"\u0442\u0430\u0440\", null],\n[\"\u2581\u0430\", null],\n[\"\u0439\", null],\n[\"\u044b\", null],\n[\"\u0440\", null],\n[\"\u043c\u0430\", null],\n[\"\u0448\", null],\n[\"\u044b\", null],\n[\"\u043b\u044b\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u2581\u2013\", null],\n[\"\u2581\u041c\u0430\", \"Macron\"],\n[\"\u043a\", null],\n[\"\u0440\u043e\u043d\", null],\n[\"\u2581\u0415\", \"European\"],\n[\"\u0443\", null],\n[\"\u0440\u043e\", null],\n[\"\u043f\u0430\", null],\n[\"\u2581\u043e\", \"Union\"],\n[\"\u0434\u0430\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u2581\u0442\", \"radically\"],\n[\"\u04af\", null],\n[\"\u0431\u0456\", null],\n[\"\u0440\u0456\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u2581\", null],\n[\"\u04e9\", \"change\"],\n[\"\u0437\", null],\n[\"\u0433\", null],\n[\"\u0435\u0440\", null],\n[\"\u0442\u0456\", null],\n[\"\u043f\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"only\"],\n[\"\u0430\u043d\u0430\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"not\"],\n[\"\u043e\", null],\n[\"\u0439\", null],\n[\"\u043c\u0430\", null],\n[\"\u0439\", null],\n[\",\", null],\n[\"\u2581\u0431\", \"all\"],\n[\"\u04af\", null],\n[\"\u043a\u0456\", null],\n[\"\u043b\", null],\n[\"\u2581\u0415\", \"Europe\"],\n[\"\u0443\", null],\n[\"\u0440\u043e\", null],\n[\"\u043f\u0430\u0434\u0430\", null],\n[\"\u2581\", null],\n[\"\u044b\", \"cooperation\"],\n[\"\u043d\", null],\n[\"\u0442\u044b\", null],\n[\"\u043c\u0430\", null],\n[\"\u049b\", null],\n[\"\u0442\u0430\", null],\n[\"\u0441\u0442\u044b\", null],\n[\"\u049b\", null],\n[\"\u0442\u044b\", null],\n[\"\u2581\u0430\u0440\", \"increase\"],\n[\"\u0442\", null],\n[\"\u0442\u044b\", null],\n[\"\u0440\u0443\", null],\n[\"\u2581\u043a\", \"necessary\"],\n[\"\u0435\u0440\u0435\", null],\n[\"\u043a\", null],\n[\"\u2581\u0434\u0435\", \"believes\"],\n[\"\u043f\", null],\n[\"\u2581\u0441\u0430\", null],\n[\"\u043d\u0430\", null],\n[\"\u0439\", null],\n[\"\u0434\u044b\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u0411\u0430\u0441\u049b\u0430 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u0442\u0430\u0440 \u0430\u0439\u044b\u0440\u043c\u0430\u0448\u044b\u043b\u044b\u0493\u044b \u2013 \u041c\u0430\u043a\u0440\u043e\u043d \u0415\u0443\u0440\u043e\u043f\u0430 \u043e\u0434\u0430\u0493\u044b\u043d \u0442\u04af\u0431\u0456\u0440\u0456\u043c\u0435\u043d \u04e9\u0437\u0433\u0435\u0440\u0442\u0456\u043f \u049b\u0430\u043d\u0430 \u049b\u043e\u0439\u043c\u0430\u0439, \u0431\u04af\u043a\u0456\u043b \u0415\u0443\u0440\u043e\u043f\u0430\u0434\u0430 \u044b\u043d\u0442\u044b\u043c\u0430\u049b\u0442\u0430\u0441\u0442\u044b\u049b\u0442\u044b \u0430\u0440\u0442\u0442\u044b\u0440\u0443 \u043a\u0435\u0440\u0435\u043a \u0434\u0435\u043f \u0441\u0430\u043d\u0430\u0439\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0411\u0430', '\u0441', '\u049b', '\u0430', '\u2581\u043a\u0430', '\u043d', '\u0434\u0438', '\u0434\u0430\u0442', '\u0442\u0430\u0440', '\u2581\u0430', '\u0439', '\u044b', '\u0440', '\u043c\u0430', '\u0448', '\u044b', '\u043b\u044b', '\u0493', '\u044b', '\u2581\u2013', '\u2581\u041c\u0430', '\u043a', '\u0440\u043e\u043d', '\u2581\u0415', '\u0443', '\u0440\u043e', '\u043f\u0430', '\u2581\u043e', '\u0434\u0430', '\u0493', '\u044b', '\u043d', '\u2581\u0442', '\u04af', '\u0431\u0456', '\u0440\u0456', '\u043c\u0435\u043d', '\u2581', '\u04e9', '\u0437', '\u0433', '\u0435\u0440', '\u0442\u0456', '\u043f', '\u2581', '\u049b', '\u0430\u043d\u0430', '\u2581', '\u049b', '\u043e', '\u0439', '\u043c\u0430', '\u0439', ',', '\u2581\u0431', '\u04af', '\u043a\u0456', '\u043b', '\u2581\u0415', '\u0443', '\u0440\u043e', '\u043f\u0430\u0434\u0430', '\u2581', '\u044b', '\u043d', '\u0442\u044b', '\u043c\u0430', '\u049b', '\u0442\u0430', '\u0441\u0442\u044b', '\u049b', '\u0442\u044b', '\u2581\u0430\u0440', '\u0442', '\u0442\u044b', '\u0440\u0443', '\u2581\u043a', '\u0435\u0440\u0435', '\u043a', '\u2581\u0434\u0435', '\u043f', '\u2581\u0441\u0430', '\u043d\u0430', '\u0439', '\u0434\u044b', '.']"}], "[\n[\"\u2581\u0411\u0430\", \"\u4e0e\"],\n[\"\u0441\", null],\n[\"\u049b\", null],\n[\"\u0430\", null],\n[\"\u2581\u043a\u0430\", \"\u5176\u4ed6\"],\n[\"\u043d\", null],\n[\"\u0434\u0438\", null],\n[\"\u0434\u0430\u0442\", null],\n[\"\u0442\u0430\u0440\", null],\n[\"\u2581\u0430\", null],\n[\"\u0439\", null],\n[\"\u044b\", null],\n[\"\u0440\", null],\n[\"\u043c\u0430\", null],\n[\"\u0448\", null],\n[\"\u044b\", null],\n[\"\u043b\u044b\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u2581\u2013\", null],\n[\"\u2581\u041c\u0430\", \"\u9a6c\u514b\u9f99\"],\n[\"\u043a\", null],\n[\"\u0440\u043e\u043d\", null],\n[\"\u2581\u0415\", \"\u6b27\u6d32\"],\n[\"\u0443\", null],\n[\"\u0440\u043e\", null],\n[\"\u043f\u0430\", null],\n[\"\u2581\u043e\", \"\u8054\u76df\"],\n[\"\u0434\u0430\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u2581\u0442\", \"\u5f7b\u5e95\"],\n[\"\u04af\", null],\n[\"\u0431\u0456\", null],\n[\"\u0440\u0456\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u2581\", null],\n[\"\u04e9\", \"\u6539\u53d8\"],\n[\"\u0437\", null],\n[\"\u0433\", null],\n[\"\u0435\u0440\", null],\n[\"\u0442\u0456\", null],\n[\"\u043f\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"\u4e0d\u4ec5\"],\n[\"\u0430\u043d\u0430\", null],\n[\"\u2581\", null],\n[\"\u049b\", null],\n[\"\u043e\", null],\n[\"\u0439\", null],\n[\"\u043c\u0430\", null],\n[\"\u0439\", null],\n[\",\", null],\n[\"\u2581\u0431\", \"\u6574\u4e2a\"],\n[\"\u04af\", null],\n[\"\u043a\u0456\", null],\n[\"\u043b\", null],\n[\"\u2581\u0415\", \"\u6b27\u6d32\"],\n[\"\u0443\", null],\n[\"\u0440\u043e\", null],\n[\"\u043f\u0430\u0434\u0430\", null],\n[\"\u2581\", null],\n[\"\u044b\", \"\u5408\u4f5c\"],\n[\"\u043d\", null],\n[\"\u0442\u044b\", null],\n[\"\u043c\u0430\", null],\n[\"\u049b\", null],\n[\"\u0442\u0430\", null],\n[\"\u0441\u0442\u044b\", null],\n[\"\u049b\", null],\n[\"\u0442\u044b\", null],\n[\"\u2581\u0430\u0440\", \"\u52a0\u5f3a\"],\n[\"\u0442\", null],\n[\"\u0442\u044b\", null],\n[\"\u0440\u0443\", null],\n[\"\u2581\u043a\", \"\u5e94\u8be5\"],\n[\"\u0435\u0440\u0435\", null],\n[\"\u043a\", null],\n[\"\u2581\u0434\u0435\", \"\u8ba4\u4e3a\"],\n[\"\u043f\", null],\n[\"\u2581\u0441\u0430\", null],\n[\"\u043d\u0430\", null],\n[\"\u0439\", null],\n[\"\u0434\u044b\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u0411\u0430\u0441\u049b\u0430 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u0442\u0430\u0440 \u0430\u0439\u044b\u0440\u043c\u0430\u0448\u044b\u043b\u044b\u0493\u044b \u2013 \u041c\u0430\u043a\u0440\u043e\u043d \u0415\u0443\u0440\u043e\u043f\u0430 \u043e\u0434\u0430\u0493\u044b\u043d \u0442\u04af\u0431\u0456\u0440\u0456\u043c\u0435\u043d \u04e9\u0437\u0433\u0435\u0440\u0442\u0456\u043f \u049b\u0430\u043d\u0430 \u049b\u043e\u0439\u043c\u0430\u0439, \u0431\u04af\u043a\u0456\u043b \u0415\u0443\u0440\u043e\u043f\u0430\u0434\u0430 \u044b\u043d\u0442\u044b\u043c\u0430\u049b\u0442\u0430\u0441\u0442\u044b\u049b\u0442\u044b \u0430\u0440\u0442\u0442\u044b\u0440\u0443 \u043a\u0435\u0440\u0435\u043a \u0434\u0435\u043f \u0441\u0430\u043d\u0430\u0439\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0411\u0430', '\u0441', '\u049b', '\u0430', '\u2581\u043a\u0430', '\u043d', '\u0434\u0438', '\u0434\u0430\u0442', '\u0442\u0430\u0440', '\u2581\u0430', '\u0439', '\u044b', '\u0440', '\u043c\u0430', '\u0448', '\u044b', '\u043b\u044b', '\u0493', '\u044b', '\u2581\u2013', '\u2581\u041c\u0430', '\u043a', '\u0440\u043e\u043d', '\u2581\u0415', '\u0443', '\u0440\u043e', '\u043f\u0430', '\u2581\u043e', '\u0434\u0430', '\u0493', '\u044b', '\u043d', '\u2581\u0442', '\u04af', '\u0431\u0456', '\u0440\u0456', '\u043c\u0435\u043d', '\u2581', '\u04e9', '\u0437', '\u0433', '\u0435\u0440', '\u0442\u0456', '\u043f', '\u2581', '\u049b', '\u0430\u043d\u0430', '\u2581', '\u049b', '\u043e', '\u0439', '\u043c\u0430', '\u0439', ',', '\u2581\u0431', '\u04af', '\u043a\u0456', '\u043b', '\u2581\u0415', '\u0443', '\u0440\u043e', '\u043f\u0430\u0434\u0430', '\u2581', '\u044b', '\u043d', '\u0442\u044b', '\u043c\u0430', '\u049b', '\u0442\u0430', '\u0441\u0442\u044b', '\u049b', '\u0442\u044b', '\u2581\u0430\u0440', '\u0442', '\u0442\u044b', '\u0440\u0443', '\u2581\u043a', '\u0435\u0440\u0435', '\u043a', '\u2581\u0434\u0435', '\u043f', '\u2581\u0441\u0430', '\u043d\u0430', '\u0439', '\u0434\u044b', '.']"}], "[\n[\"\u2581\u0411\u0430\", \"\u0412\"],\n[\"\u0441\", null],\n[\"\u049b\", null],\n[\"\u0430\", null],\n[\"\u2581\u043a\u0430\", \"\u043e\u0442\u043b\u0438\u0447\u0438\u0435\"],\n[\"\u043d\", null],\n[\"\u0434\u0438\", null],\n[\"\u0434\u0430\u0442\", null],\n[\"\u0442\u0430\u0440\", null],\n[\"\u2581\u0430\", null],\n[\"\u0439\", null],\n[\"\u044b\", null],\n[\"\u0440\", null],\n[\"\u043c\u0430\", null],\n[\"\u0448\", null],\n[\"\u044b\", null],\n[\"\u043b\u044b\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u2581\u2013\", null],\n[\"\u2581\u041c\u0430\", \"\u041c\u0430\u043a\u0440\u043e\u043d\"],\n[\"\u043a\", null],\n[\"\u0440\u043e\u043d\", null],\n[\"\u2581\u0415\", \"\u0415\u0432\u0440\u043e\u043f\u0435\u0439\u0441\u043a\u0438\u0439\"],\n[\"\u0443\", null],\n[\"\u0440\u043e\", null],\n[\"\u043f\u0430\", null],\n[\"\u2581\u043e\", \"\u0441\u043e\u044e\u0437\"],\n[\"\u0434\u0430\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u2581\u0442\", \"\u043a\u0430\u0440\u0434\u0438\u043d\u0430\u043b\u044c\u043d\u043e\"],\n[\"\u04af\", null],\n[\"\u0431\u0456\", null],\n[\"\u0440\u0456\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u2581\", null],\n[\"\u04e9\", \"\u0438\u0437\u043c\u0435\u043d\u0438\u0442\u044c\"],\n[\"\u0437\", null],\n[\"\u0433\", null],\n[\"\u0435\u0440\", null],\n[\"\u0442\u0456\", null],\n[\"\u043f\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"\u0442\u043e\u043b\u044c\u043a\u043e\"],\n[\"\u0430\u043d\u0430\", null],\n[\"\u2581\", null],\n[\"\u049b\", null],\n[\"\u043e\", null],\n[\"\u0439\", null],\n[\"\u043c\u0430\", null],\n[\"\u0439\", null],\n[\",\", null],\n[\"\u2581\u0431\", \"\u0432\u0435\u0441\u044c\"],\n[\"\u04af\", null],\n[\"\u043a\u0456\", null],\n[\"\u043b\", null],\n[\"\u2581\u0415\", \"\u0415\u0432\u0440\u043e\u043f\u0430\"],\n[\"\u0443\", null],\n[\"\u0440\u043e\", null],\n[\"\u043f\u0430\u0434\u0430\", null],\n[\"\u2581\", null],\n[\"\u044b\", \"\u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u0447\u0435\u0441\u0442\u0432\u043e\"],\n[\"\u043d\", null],\n[\"\u0442\u044b\", null],\n[\"\u043c\u0430\", null],\n[\"\u049b\", null],\n[\"\u0442\u0430\", null],\n[\"\u0441\u0442\u044b\", null],\n[\"\u049b\", null],\n[\"\u0442\u044b\", null],\n[\"\u2581\u0430\u0440\", \"\u0443\u0441\u0438\u043b\u0438\u0442\u044c\"],\n[\"\u0442\", null],\n[\"\u0442\u044b\", null],\n[\"\u0440\u0443\", null],\n[\"\u2581\u043a\", \"\u043d\u0443\u0436\u043d\u043e\"],\n[\"\u0435\u0440\u0435\", null],\n[\"\u043a\", null],\n[\"\u2581\u0434\u0435\", \"\u0441\u0447\u0438\u0442\u0430\u0435\u0442\"],\n[\"\u043f\", null],\n[\"\u2581\u0441\u0430\", null],\n[\"\u043d\u0430\", null],\n[\"\u0439\", null],\n[\"\u0434\u044b\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u0414\u0435\u0441\u0435 \u0434\u0435, \u0441\u0430\u044f\u0441\u0438 \u0442\u04d9\u0443\u0435\u043a\u0435\u043b\u0434\u0435\u0440\u0433\u0435 \u043a\u0435\u043b\u0433\u0435\u043d\u0434\u0435 \u0438\u043d\u0432\u0435\u0441\u0442\u043e\u0440\u043b\u0430\u0440 \u04d9\u0434\u0435\u0442\u0442\u0435\u0433\u0456\u0434\u0435\u043d \u0431\u0456\u0440\u0448\u0430\u043c\u0430 \u0440\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u0434\u044b \u04d9\u0440\u0435\u043a\u0435\u0442 \u0435\u0442\u0435\u0434\u0456.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0414\u0435', '\u0441\u0435', '\u2581\u0434\u0435', ',', '\u2581\u0441\u0430', '\u044f', '\u0441\u0438', '\u2581\u0442', '\u04d9', '\u0443', '\u0435', '\u043a\u0435', '\u043b', '\u0434\u0435\u0440', '\u0433\u0435', '\u2581\u043a', '\u0435', '\u043b', '\u0433\u0435\u043d', '\u0434\u0435', '\u2581\u0438\u043d', '\u0432\u0435', '\u0441\u0442\u043e', '\u0440', '\u043b\u0430', '\u0440', '\u2581', '\u04d9', '\u0434\u0435', '\u0442', '\u0442\u0435', '\u0433', '\u0456\u0434', '\u0435\u043d', '\u2581\u0431\u0456', '\u0440', '\u0448\u0430', '\u043c\u0430', '\u2581\u0440\u0430', '\u0446\u0438\u043e\u043d\u0430', '\u043b', '\u0434\u044b', '\u2581', '\u04d9', '\u0440\u0435', '\u043a', '\u0435\u0442', '\u2581\u0435', '\u0442\u0435', '\u0434\u0456', '.']"}], "[\n[\"\u2581\u0414\u0435\", \"However\"],\n[\"\u0441\u0435\", null],\n[\"\u2581\u0434\u0435\", null],\n[\",\", null],\n[\"\u2581\u0441\u0430\", \"political\"],\n[\"\u044f\", null],\n[\"\u0441\u0438\", null],\n[\"\u2581\u0442\", \"risks\"],\n[\"\u04d9\", null],\n[\"\u0443\", null],\n[\"\u0435\", null],\n[\"\u043a\u0435\", null],\n[\"\u043b\", null],\n[\"\u0434\u0435\u0440\", null],\n[\"\u0433\u0435\", null],\n[\"\u2581\u043a\", \"when\"],\n[\"\u0435\", null],\n[\"\u043b\", null],\n[\"\u0433\u0435\u043d\", null],\n[\"\u0434\u0435\", null],\n[\"\u2581\u0438\u043d\", \"investors\"],\n[\"\u0432\u0435\", null],\n[\"\u0441\u0442\u043e\", null],\n[\"\u0440\", null],\n[\"\u043b\u0430\", null],\n[\"\u0440\", null],\n[\"\u2581\", null],\n[\"\u04d9\", \"usually\"],\n[\"\u0434\u0435\", null],\n[\"\u0442\", null],\n[\"\u0442\u0435\", null],\n[\"\u0433\", null],\n[\"\u0456\u0434\", null],\n[\"\u0435\u043d\", null],\n[\"\u2581\u0431\u0456\", \"more\"],\n[\"\u0440\", null],\n[\"\u0448\u0430\", null],\n[\"\u043c\u0430\", null],\n[\"\u2581\u0440\u0430\", \"rationally\"],\n[\"\u0446\u0438\u043e\u043d\u0430\", null],\n[\"\u043b\", null],\n[\"\u0434\u044b\", null],\n[\"\u2581\", null],\n[\"\u04d9\", \"act\"],\n[\"\u0440\u0435\", null],\n[\"\u043a\", null],\n[\"\u0435\u0442\", null],\n[\"\u2581\u0435\", null],\n[\"\u0442\u0435\", null],\n[\"\u0434\u0456\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u0414\u0435\u0441\u0435 \u0434\u0435, \u0441\u0430\u044f\u0441\u0438 \u0442\u04d9\u0443\u0435\u043a\u0435\u043b\u0434\u0435\u0440\u0433\u0435 \u043a\u0435\u043b\u0433\u0435\u043d\u0434\u0435 \u0438\u043d\u0432\u0435\u0441\u0442\u043e\u0440\u043b\u0430\u0440 \u04d9\u0434\u0435\u0442\u0442\u0435\u0433\u0456\u0434\u0435\u043d \u0431\u0456\u0440\u0448\u0430\u043c\u0430 \u0440\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u0434\u044b \u04d9\u0440\u0435\u043a\u0435\u0442 \u0435\u0442\u0435\u0434\u0456.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0414\u0435', '\u0441\u0435', '\u2581\u0434\u0435', ',', '\u2581\u0441\u0430', '\u044f', '\u0441\u0438', '\u2581\u0442', '\u04d9', '\u0443', '\u0435', '\u043a\u0435', '\u043b', '\u0434\u0435\u0440', '\u0433\u0435', '\u2581\u043a', '\u0435', '\u043b', '\u0433\u0435\u043d', '\u0434\u0435', '\u2581\u0438\u043d', '\u0432\u0435', '\u0441\u0442\u043e', '\u0440', '\u043b\u0430', '\u0440', '\u2581', '\u04d9', '\u0434\u0435', '\u0442', '\u0442\u0435', '\u0433', '\u0456\u0434', '\u0435\u043d', '\u2581\u0431\u0456', '\u0440', '\u0448\u0430', '\u043c\u0430', '\u2581\u0440\u0430', '\u0446\u0438\u043e\u043d\u0430', '\u043b', '\u0434\u044b', '\u2581', '\u04d9', '\u0440\u0435', '\u043a', '\u0435\u0442', '\u2581\u0435', '\u0442\u0435', '\u0434\u0456', '.']"}], "[\n  [\"\u2581\u0414\u0435\", \"\u7136\u800c\"],\n  [\"\u0441\u0435\", null],\n  [\"\u2581\u0434\u0435\", null],\n  [\",\", null],\n  [\"\u2581\u0441\u0430\", \"\u653f\u6cbb\"],\n  [\"\u044f\", null],\n  [\"\u0441\u0438\", null],\n  [\"\u2581\u0442\", \"\u98ce\u9669\"],\n  [\"\u04d9\", null],\n  [\"\u0443\", null],\n  [\"\u0435\", null],\n  [\"\u043a\u0435\", null],\n  [\"\u043b\", null],\n  [\"\u0434\u0435\u0440\", null],\n  [\"\u0433\u0435\", null],\n  [\"\u2581\u043a\", \"\u5f53\"],\n  [\"\u0435\", null],\n  [\"\u043b\", null],\n  [\"\u0433\u0435\u043d\", null],\n  [\"\u0434\u0435\", null],\n  [\"\u2581\u0438\u043d\", \"\u6295\u8d44\u8005\"],\n  [\"\u0432\u0435\", null],\n  [\"\u0441\u0442\u043e\", null],\n  [\"\u0440\", null],\n  [\"\u043b\u0430\", null],\n  [\"\u0440\", null],\n  [\"\u2581\", null],\n  [\"\u04d9\", \"\u901a\u5e38\"],\n  [\"\u0434\u0435\", null],\n  [\"\u0442\", null],\n  [\"\u0442\u0435\", null],\n  [\"\u0433\", null],\n  [\"\u0456\u0434\", null],\n  [\"\u0435\u043d\", null],\n  [\"\u2581\u0431\u0456\", \"\u66f4\u52a0\"],\n  [\"\u0440\", null],\n  [\"\u0448\u0430\", null],\n  [\"\u043c\u0430\", null],\n  [\"\u2581\u0440\u0430\", \"\u7406\u6027\"],\n  [\"\u0446\u0438\u043e\u043d\u0430\", null],\n  [\"\u043b\", null],\n  [\"\u0434\u044b\", null],\n  [\"\u2581\", null],\n  [\"\u04d9\", \"\u884c\u52a8\"],\n  [\"\u0440\u0435\", null],\n  [\"\u043a\", null],\n  [\"\u0435\u0442\", null],\n  [\"\u2581\u0435\", null],\n  [\"\u0442\u0435\", null],\n  [\"\u0434\u0456\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u0414\u0435\u0441\u0435 \u0434\u0435, \u0441\u0430\u044f\u0441\u0438 \u0442\u04d9\u0443\u0435\u043a\u0435\u043b\u0434\u0435\u0440\u0433\u0435 \u043a\u0435\u043b\u0433\u0435\u043d\u0434\u0435 \u0438\u043d\u0432\u0435\u0441\u0442\u043e\u0440\u043b\u0430\u0440 \u04d9\u0434\u0435\u0442\u0442\u0435\u0433\u0456\u0434\u0435\u043d \u0431\u0456\u0440\u0448\u0430\u043c\u0430 \u0440\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u0434\u044b \u04d9\u0440\u0435\u043a\u0435\u0442 \u0435\u0442\u0435\u0434\u0456.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0414\u0435', '\u0441\u0435', '\u2581\u0434\u0435', ',', '\u2581\u0441\u0430', '\u044f', '\u0441\u0438', '\u2581\u0442', '\u04d9', '\u0443', '\u0435', '\u043a\u0435', '\u043b', '\u0434\u0435\u0440', '\u0433\u0435', '\u2581\u043a', '\u0435', '\u043b', '\u0433\u0435\u043d', '\u0434\u0435', '\u2581\u0438\u043d', '\u0432\u0435', '\u0441\u0442\u043e', '\u0440', '\u043b\u0430', '\u0440', '\u2581', '\u04d9', '\u0434\u0435', '\u0442', '\u0442\u0435', '\u0433', '\u0456\u0434', '\u0435\u043d', '\u2581\u0431\u0456', '\u0440', '\u0448\u0430', '\u043c\u0430', '\u2581\u0440\u0430', '\u0446\u0438\u043e\u043d\u0430', '\u043b', '\u0434\u044b', '\u2581', '\u04d9', '\u0440\u0435', '\u043a', '\u0435\u0442', '\u2581\u0435', '\u0442\u0435', '\u0434\u0456', '.']"}], "[\n[\"\u2581\u0414\u0435\", \"\u041e\u0434\u043d\u0430\u043a\u043e\"],\n[\"\u0441\u0435\", null],\n[\"\u2581\u0434\u0435\", null],\n[\",\", null],\n[\"\u2581\u0441\u0430\", \"\u043f\u043e\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435\"],\n[\"\u044f\", null],\n[\"\u0441\u0438\", null],\n[\"\u2581\u0442\", \"\u0440\u0438\u0441\u043a\u0438\"],\n[\"\u04d9\", null],\n[\"\u0443\", null],\n[\"\u0435\", null],\n[\"\u043a\u0435\", null],\n[\"\u043b\", null],\n[\"\u0434\u0435\u0440\", null],\n[\"\u0433\u0435\", null],\n[\"\u2581\u043a\", \"\u043a\u043e\u0433\u0434\u0430\"],\n[\"\u0435\", null],\n[\"\u043b\", null],\n[\"\u0433\u0435\u043d\", null],\n[\"\u0434\u0435\", null],\n[\"\u2581\u0438\u043d\", \"\u0438\u043d\u0432\u0435\u0441\u0442\u043e\u0440\u044b\"],\n[\"\u0432\u0435\", null],\n[\"\u0441\u0442\u043e\", null],\n[\"\u0440\", null],\n[\"\u043b\u0430\", null],\n[\"\u0440\", null],\n[\"\u2581\", null],\n[\"\u04d9\", \"\u043e\u0431\u044b\u0447\u043d\u043e\"],\n[\"\u0434\u0435\", null],\n[\"\u0442\", null],\n[\"\u0442\u0435\", null],\n[\"\u0433\", null],\n[\"\u0456\u0434\", null],\n[\"\u0435\u043d\", null],\n[\"\u2581\u0431\u0456\", \"\u0431\u043e\u043b\u0435\u0435\"],\n[\"\u0440\", null],\n[\"\u0448\u0430\", null],\n[\"\u043c\u0430\", null],\n[\"\u2581\u0440\u0430\", \"\u0440\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\"],\n[\"\u0446\u0438\u043e\u043d\u0430\", null],\n[\"\u043b\", null],\n[\"\u0434\u044b\", null],\n[\"\u2581\", null],\n[\"\u04d9\", \"\u0434\u0435\u0439\u0441\u0442\u0432\u0443\u044e\u0442\"],\n[\"\u0440\u0435\", null],\n[\"\u043a\", null],\n[\"\u0435\u0442\", null],\n[\"\u2581\u0435\", null],\n[\"\u0442\u0435\", null],\n[\"\u0434\u0456\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u041d\u042c\u042e-\u0419\u041e\u0420\u041a \u2013 \u041e\u04a3\u0442\u04af\u0441\u0442\u0456\u043a \u0410\u0444\u0440\u0438\u043a\u0430 \u0420\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430\u0441\u044b\u043d\u044b\u04a3 \u04af\u043a\u0456\u043c\u0435\u0442\u0456 1997 \u0436\u044b\u043b\u044b \u0410\u0418\u0422\u0412 / \u0416\u0418\u0422\u0421-\u0442\u0456 \u0435\u043c\u0434\u0435\u0443\u0433\u0435 \u0430\u0440\u043d\u0430\u043b\u0493\u0430\u043d \u0434\u0436\u0435\u043d\u0435\u0440\u0438\u043a \u0434\u04d9\u0440\u0456-\u0434\u04d9\u0440\u043c\u0435\u043a\u0442\u0435\u0440\u0434\u0456 \u049b\u043e\u043b\u0436\u0435\u0442\u0456\u043c\u0434\u0456 \u0435\u0442\u0443 \u04af\u0448\u0456\u043d \u04e9\u0437\u0434\u0435\u0440\u0456\u043d\u0456\u04a3 \u0437\u0430\u04a3\u0434\u0430\u0440\u044b\u043d\u0430 \u04e9\u0437\u0433\u0435\u0440\u0456\u0441\u0442\u0435\u0440 \u0435\u043d\u0433\u0456\u0437\u0443\u0433\u0435 \u0442\u044b\u0440\u044b\u0441\u049b\u0430\u043d \u043a\u0435\u0437\u0434\u0435, \u0431\u04af\u043a\u0456\u043b \u04d9\u043b\u0435\u043c\u0434\u0456\u043a \u0444\u0430\u0440\u043c\u0430\u0446\u0435\u0432\u0442\u0438\u043a\u0430\u043b\u044b\u049b \u0438\u043d\u0434\u0443\u0441\u0442\u0440\u0438\u044f\u043d\u044b\u04a3 \u0437\u0430\u04a3\u0434\u044b \u043a\u04af\u0448\u0442\u0435\u0440\u0456 \u043e\u0441\u044b \u0435\u043b\u0433\u0435 \u0431\u0430\u0441 \u0441\u0430\u043b\u044b\u043f, \u043e\u043d\u044b \u0435\u043d\u0433\u0456\u0437\u0443\u0434\u0456 \u043a\u0435\u0448\u0456\u043a\u0442\u0456\u0440\u0456\u043f, \u043a\u04e9\u043f\u0442\u0435\u0433\u0435\u043d \u0430\u0434\u0430\u043c\u0434\u0430\u0440\u0434\u044b\u04a3 \u04e9\u043c\u0456\u0440\u0456\u043d \u049b\u0438\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041d', '\u042c', '\u042e', '-', '\u0419', '\u041e', '\u0420', '\u041a', '\u2581\u2013', '\u2581\u041e', '\u04a3', '\u0442', '\u04af', '\u0441\u0442\u0456', '\u043a', '\u2581\u0410', '\u0444', '\u0440\u0438', '\u043a\u0430', '\u2581\u0420\u0435', '\u0441\u043f\u0443\u0431\u043b\u0438', '\u043a\u0430', '\u0441\u044b', '\u043d\u044b', '\u04a3', '\u2581', '\u04af', '\u043a\u0456', '\u043c\u0435', '\u0442\u0456', '\u2581', '1', '9', '9', '7', '\u2581\u0436', '\u044b', '\u043b\u044b', '\u2581\u0410', '\u0418', '\u0422', '\u0412', '\u2581/', '\u2581\u0416', '\u0418', '\u0422', '\u0421', '-', '\u0442\u0456', '\u2581\u0435', '\u043c', '\u0434\u0435', '\u0443', '\u0433\u0435', '\u2581\u0430\u0440', '\u043d\u0430\u043b', '\u0493', '\u0430\u043d', '\u2581\u0434', '\u0436\u0435\u043d', '\u0435', '\u0440\u0438', '\u043a', '\u2581\u0434', '\u04d9', '\u0440\u0456', '-', '\u0434', '\u04d9', '\u0440', '\u043c\u0435', '\u043a', '\u0442\u0435\u0440', '\u0434\u0456', '\u2581', '\u049b', '\u043e', '\u043b', '\u0436\u0435', '\u0442\u0456', '\u043c', '\u0434\u0456', '\u2581\u0435', '\u0442\u0443', '\u2581', '\u04af', '\u0448', '\u0456\u043d', '\u2581', '\u04e9', '\u0437', '\u0434\u0435\u0440', '\u0456', '\u043d\u0456', '\u04a3', '\u2581\u0437\u0430', '\u04a3', '\u0434\u0430', '\u0440\u044b', '\u043d\u0430', '\u2581', '\u04e9', '\u0437', '\u0433', '\u0435\u0440', '\u0456', '\u0441\u0442\u0435\u0440', '\u2581', '\u0435\u043d', '\u0433\u0456', '\u0437\u0443', '\u0433\u0435', '\u2581\u0442', '\u044b', '\u0440\u044b', '\u0441', '\u049b', '\u0430\u043d', '\u2581\u043a', '\u0435', '\u0437', '\u0434\u0435', ',', '\u2581\u0431', '\u04af', '\u043a\u0456', '\u043b', '\u2581', '\u04d9', '\u043b\u0435\u043c', '\u0434\u0456', '\u043a', '\u2581\u0444', '\u0430\u0440', '\u043c\u0430', '\u0446\u0435\u0432', '\u0442\u0438', '\u043a\u0430', '\u043b\u044b', '\u049b', '\u2581\u0438\u043d', '\u0434\u0443', '\u0441\u0442', '\u0440\u0438\u044f', '\u043d\u044b', '\u04a3', '\u2581\u0437\u0430', '\u04a3', '\u0434\u044b', '\u2581\u043a', '\u04af', '\u0448', '\u0442\u0435', '\u0440\u0456', '\u2581\u043e\u0441', '\u044b', '\u2581\u0435', '\u043b', '\u0433\u0435', '\u2581\u0431\u0430', '\u0441', '\u2581\u0441\u0430', '\u043b\u044b', '\u043f', ',', '\u2581\u043e', '\u043d\u044b', '\u2581', '\u0435\u043d', '\u0433\u0456', '\u0437\u0443', '\u0434\u0456', '\u2581\u043a', '\u0435', '\u0448\u0456', '\u043a\u0442', '\u0456', '\u0440\u0456', '\u043f', ',', '\u2581\u043a', '\u04e9', '\u043f', '\u0442\u0435', '\u0433\u0435\u043d', '\u2581\u0430', '\u0434\u0430', '\u043c', '\u0434\u0430\u0440', '\u0434\u044b', '\u04a3', '\u2581', '\u04e9', '\u043c\u0456', '\u0440\u0456', '\u043d', '\u2581', '\u049b', '\u0438', '\u0434\u044b', '.']"}], "[\n[\"\u2581\u041d\", \"New\"],\n[\"\u042c\", null],\n[\"\u042e\", null],\n[\"-\", null],\n[\"\u0419\", null],\n[\"\u041e\", null],\n[\"\u0420\", null],\n[\"\u041a\", null],\n[\"\u2581\u2013\", null],\n[\"\u2581\u041e\", null],\n[\"\u04a3\", null],\n[\"\u0442\", null],\n[\"\u04af\", null],\n[\"\u0441\u0442\u0456\", null],\n[\"\u043a\", null],\n[\"\u2581\u0410\", \"South\"],\n[\"\u0444\", null],\n[\"\u0440\u0438\", null],\n[\"\u043a\u0430\", null],\n[\"\u2581\u0420\u0435\", \"Republic\"],\n[\"\u0441\u043f\u0443\u0431\u043b\u0438\", null],\n[\"\u043a\u0430\", null],\n[\"\u0441\u044b\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\", null],\n[\"\u04af\", \"government\"],\n[\"\u043a\u0456\", null],\n[\"\u043c\u0435\", null],\n[\"\u0442\u0456\", null],\n[\"\u2581\", null],\n[\"1\", null],\n[\"9\", null],\n[\"9\", null],\n[\"7\", null],\n[\"\u2581\u0436\", \"year\"],\n[\"\u044b\", null],\n[\"\u043b\u044b\", null],\n[\"\u2581\u0410\", \"HIV\"],\n[\"\u0418\", null],\n[\"\u0422\", null],\n[\"\u0412\", null],\n[\"\u2581/\", null],\n[\"\u2581\u0416\", \"AIDS\"],\n[\"\u0418\", null],\n[\"\u0422\", null],\n[\"\u0421\", null],\n[\"-\", null],\n[\"\u0442\u0456\", null],\n[\"\u2581\u0435\", \"treat\"],\n[\"\u043c\", null],\n[\"\u0434\u0435\", null],\n[\"\u0443\", null],\n[\"\u0433\u0435\", null],\n[\"\u2581\u0430\u0440\", \"intended\"],\n[\"\u043d\u0430\u043b\", null],\n[\"\u0493\", null],\n[\"\u0430\u043d\", null],\n[\"\u2581\u0434\", \"generic\"],\n[\"\u0436\u0435\u043d\", null],\n[\"\u0435\", null],\n[\"\u0440\u0438\", null],\n[\"\u043a\", null],\n[\"\u2581\u0434\", \"medicines\"],\n[\"\u04d9\", null],\n[\"\u0440\u0456\", null],\n[\"-\", null],\n[\"\u0434\", null],\n[\"\u04d9\", null],\n[\"\u0440\", null],\n[\"\u043c\u0435\", null],\n[\"\u043a\", null],\n[\"\u0442\u0435\u0440\", null],\n[\"\u0434\u0456\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"available\"],\n[\"\u043e\", null],\n[\"\u043b\", null],\n[\"\u0436\u0435\", null],\n[\"\u0442\u0456\", null],\n[\"\u043c\", null],\n[\"\u0434\u0456\", null],\n[\"\u2581\u0435\", null],\n[\"\u0442\u0443\", null],\n[\"\u2581\", null],\n[\"\u04af\", \"for\"],\n[\"\u0448\", null],\n[\"\u0456\u043d\", null],\n[\"\u2581\", null],\n[\"\u04e9\", \"their\"],\n[\"\u0437\", null],\n[\"\u0434\u0435\u0440\", null],\n[\"\u0456\", null],\n[\"\u043d\u0456\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0437\u0430\", \"laws\"],\n[\"\u04a3\", null],\n[\"\u0434\u0430\", null],\n[\"\u0440\u044b\", null],\n[\"\u043d\u0430\", null],\n[\"\u2581\", null],\n[\"\u04e9\", \"changes\"],\n[\"\u0437\", null],\n[\"\u0433\", null],\n[\"\u0435\u0440\", null],\n[\"\u0456\", null],\n[\"\u0441\u0442\u0435\u0440\", null],\n[\"\u2581\", null],\n[\"\u0435\u043d\", \"introduce\"],\n[\"\u0433\u0456\", null],\n[\"\u0437\u0443\", null],\n[\"\u0433\u0435\", null],\n[\"\u2581\u0442\", \"tried\"],\n[\"\u044b\", null],\n[\"\u0440\u044b\", null],\n[\"\u0441\", null],\n[\"\u049b\", null],\n[\"\u0430\u043d\", null],\n[\"\u2581\u043a\", \"when\"],\n[\"\u0435\", null],\n[\"\u0437\", null],\n[\"\u0434\u0435\", null],\n[\",\", null],\n[\"\u2581\u0431\", \"global\"],\n[\"\u04af\", null],\n[\"\u043a\u0456\", null],\n[\"\u043b\", null],\n[\"\u2581\", null],\n[\"\u04d9\", \"pharmaceutical\"],\n[\"\u043b\u0435\u043c\", null],\n[\"\u0434\u0456\", null],\n[\"\u043a\", null],\n[\"\u2581\u0444\", null],\n[\"\u0430\u0440\", null],\n[\"\u043c\u0430\", null],\n[\"\u0446\u0435\u0432\", null],\n[\"\u0442\u0438\", null],\n[\"\u043a\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u049b\", null],\n[\"\u2581\u0438\u043d\", \"industry\"],\n[\"\u0434\u0443\", null],\n[\"\u0441\u0442\", null],\n[\"\u0440\u0438\u044f\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0437\u0430\", \"legal\"],\n[\"\u04a3\", null],\n[\"\u0434\u044b\", null],\n[\"\u2581\u043a\", \"forces\"],\n[\"\u04af\", null],\n[\"\u0448\", null],\n[\"\u0442\u0435\", null],\n[\"\u0440\u0456\", null],\n[\"\u2581\u043e\u0441\", \"this\"],\n[\"\u044b\", null],\n[\"\u2581\u0435\", \"country\"],\n[\"\u043b\", null],\n[\"\u0433\u0435\", null],\n[\"\u2581\u0431\u0430\", \"attacked\"],\n[\"\u0441\", null],\n[\"\u2581\u0441\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u043f\", null],\n[\",\", null],\n[\"\u2581\u043e\", \"its\"],\n[\"\u043d\u044b\", null],\n[\"\u2581\", null],\n[\"\u0435\u043d\", \"implementation\"],\n[\"\u0433\u0456\", null],\n[\"\u0437\u0443\", null],\n[\"\u0434\u0456\", null],\n[\"\u2581\u043a\", \"delayed\"],\n[\"\u0435\", null],\n[\"\u0448\u0456\", null],\n[\"\u043a\u0442\", null],\n[\"\u0456\", null],\n[\"\u0440\u0456\", null],\n[\"\u043f\", null],\n[\",\", null],\n[\"\u2581\u043a\", \"many\"],\n[\"\u04e9\", null],\n[\"\u043f\", null],\n[\"\u0442\u0435\", null],\n[\"\u0433\u0435\u043d\", null],\n[\"\u2581\u0430\", \"people's\"],\n[\"\u0434\u0430\", null],\n[\"\u043c\", null],\n[\"\u0434\u0430\u0440\", null],\n[\"\u0434\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\", null],\n[\"\u04e9\", \"lives\"],\n[\"\u043c\u0456\", null],\n[\"\u0440\u0456\", null],\n[\"\u043d\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"took\"],\n[\"\u0438\", null],\n[\"\u0434\u044b\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u041d\u042c\u042e-\u0419\u041e\u0420\u041a \u2013 \u041e\u04a3\u0442\u04af\u0441\u0442\u0456\u043a \u0410\u0444\u0440\u0438\u043a\u0430 \u0420\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430\u0441\u044b\u043d\u044b\u04a3 \u04af\u043a\u0456\u043c\u0435\u0442\u0456 1997 \u0436\u044b\u043b\u044b \u0410\u0418\u0422\u0412 / \u0416\u0418\u0422\u0421-\u0442\u0456 \u0435\u043c\u0434\u0435\u0443\u0433\u0435 \u0430\u0440\u043d\u0430\u043b\u0493\u0430\u043d \u0434\u0436\u0435\u043d\u0435\u0440\u0438\u043a \u0434\u04d9\u0440\u0456-\u0434\u04d9\u0440\u043c\u0435\u043a\u0442\u0435\u0440\u0434\u0456 \u049b\u043e\u043b\u0436\u0435\u0442\u0456\u043c\u0434\u0456 \u0435\u0442\u0443 \u04af\u0448\u0456\u043d \u04e9\u0437\u0434\u0435\u0440\u0456\u043d\u0456\u04a3 \u0437\u0430\u04a3\u0434\u0430\u0440\u044b\u043d\u0430 \u04e9\u0437\u0433\u0435\u0440\u0456\u0441\u0442\u0435\u0440 \u0435\u043d\u0433\u0456\u0437\u0443\u0433\u0435 \u0442\u044b\u0440\u044b\u0441\u049b\u0430\u043d \u043a\u0435\u0437\u0434\u0435, \u0431\u04af\u043a\u0456\u043b \u04d9\u043b\u0435\u043c\u0434\u0456\u043a \u0444\u0430\u0440\u043c\u0430\u0446\u0435\u0432\u0442\u0438\u043a\u0430\u043b\u044b\u049b \u0438\u043d\u0434\u0443\u0441\u0442\u0440\u0438\u044f\u043d\u044b\u04a3 \u0437\u0430\u04a3\u0434\u044b \u043a\u04af\u0448\u0442\u0435\u0440\u0456 \u043e\u0441\u044b \u0435\u043b\u0433\u0435 \u0431\u0430\u0441 \u0441\u0430\u043b\u044b\u043f, \u043e\u043d\u044b \u0435\u043d\u0433\u0456\u0437\u0443\u0434\u0456 \u043a\u0435\u0448\u0456\u043a\u0442\u0456\u0440\u0456\u043f, \u043a\u04e9\u043f\u0442\u0435\u0433\u0435\u043d \u0430\u0434\u0430\u043c\u0434\u0430\u0440\u0434\u044b\u04a3 \u04e9\u043c\u0456\u0440\u0456\u043d \u049b\u0438\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041d', '\u042c', '\u042e', '-', '\u0419', '\u041e', '\u0420', '\u041a', '\u2581\u2013', '\u2581\u041e', '\u04a3', '\u0442', '\u04af', '\u0441\u0442\u0456', '\u043a', '\u2581\u0410', '\u0444', '\u0440\u0438', '\u043a\u0430', '\u2581\u0420\u0435', '\u0441\u043f\u0443\u0431\u043b\u0438', '\u043a\u0430', '\u0441\u044b', '\u043d\u044b', '\u04a3', '\u2581', '\u04af', '\u043a\u0456', '\u043c\u0435', '\u0442\u0456', '\u2581', '1', '9', '9', '7', '\u2581\u0436', '\u044b', '\u043b\u044b', '\u2581\u0410', '\u0418', '\u0422', '\u0412', '\u2581/', '\u2581\u0416', '\u0418', '\u0422', '\u0421', '-', '\u0442\u0456', '\u2581\u0435', '\u043c', '\u0434\u0435', '\u0443', '\u0433\u0435', '\u2581\u0430\u0440', '\u043d\u0430\u043b', '\u0493', '\u0430\u043d', '\u2581\u0434', '\u0436\u0435\u043d', '\u0435', '\u0440\u0438', '\u043a', '\u2581\u0434', '\u04d9', '\u0440\u0456', '-', '\u0434', '\u04d9', '\u0440', '\u043c\u0435', '\u043a', '\u0442\u0435\u0440', '\u0434\u0456', '\u2581', '\u049b', '\u043e', '\u043b', '\u0436\u0435', '\u0442\u0456', '\u043c', '\u0434\u0456', '\u2581\u0435', '\u0442\u0443', '\u2581', '\u04af', '\u0448', '\u0456\u043d', '\u2581', '\u04e9', '\u0437', '\u0434\u0435\u0440', '\u0456', '\u043d\u0456', '\u04a3', '\u2581\u0437\u0430', '\u04a3', '\u0434\u0430', '\u0440\u044b', '\u043d\u0430', '\u2581', '\u04e9', '\u0437', '\u0433', '\u0435\u0440', '\u0456', '\u0441\u0442\u0435\u0440', '\u2581', '\u0435\u043d', '\u0433\u0456', '\u0437\u0443', '\u0433\u0435', '\u2581\u0442', '\u044b', '\u0440\u044b', '\u0441', '\u049b', '\u0430\u043d', '\u2581\u043a', '\u0435', '\u0437', '\u0434\u0435', ',', '\u2581\u0431', '\u04af', '\u043a\u0456', '\u043b', '\u2581', '\u04d9', '\u043b\u0435\u043c', '\u0434\u0456', '\u043a', '\u2581\u0444', '\u0430\u0440', '\u043c\u0430', '\u0446\u0435\u0432', '\u0442\u0438', '\u043a\u0430', '\u043b\u044b', '\u049b', '\u2581\u0438\u043d', '\u0434\u0443', '\u0441\u0442', '\u0440\u0438\u044f', '\u043d\u044b', '\u04a3', '\u2581\u0437\u0430', '\u04a3', '\u0434\u044b', '\u2581\u043a', '\u04af', '\u0448', '\u0442\u0435', '\u0440\u0456', '\u2581\u043e\u0441', '\u044b', '\u2581\u0435', '\u043b', '\u0433\u0435', '\u2581\u0431\u0430', '\u0441', '\u2581\u0441\u0430', '\u043b\u044b', '\u043f', ',', '\u2581\u043e', '\u043d\u044b', '\u2581', '\u0435\u043d', '\u0433\u0456', '\u0437\u0443', '\u0434\u0456', '\u2581\u043a', '\u0435', '\u0448\u0456', '\u043a\u0442', '\u0456', '\u0440\u0456', '\u043f', ',', '\u2581\u043a', '\u04e9', '\u043f', '\u0442\u0435', '\u0433\u0435\u043d', '\u2581\u0430', '\u0434\u0430', '\u043c', '\u0434\u0430\u0440', '\u0434\u044b', '\u04a3', '\u2581', '\u04e9', '\u043c\u0456', '\u0440\u0456', '\u043d', '\u2581', '\u049b', '\u0438', '\u0434\u044b', '.']"}], "[\n[\"\u2581\u041d\", \"\u7ebd\"],\n[\"\u042c\", null],\n[\"\u042e\", null],\n[\"-\", null],\n[\"\u0419\", null],\n[\"\u041e\", null],\n[\"\u0420\", null],\n[\"\u041a\", null],\n[\"\u2581\u2013\", null],\n[\"\u2581\u041e\", \"\u5357\"],\n[\"\u04a3\", null],\n[\"\u0442\", null],\n[\"\u04af\", null],\n[\"\u0441\u0442\u0456\", null],\n[\"\u043a\", null],\n[\"\u2581\u0410\", \"\u975e\"],\n[\"\u0444\", null],\n[\"\u0440\u0438\", null],\n[\"\u043a\u0430\", null],\n[\"\u2581\u0420\u0435\", \"\u5171\u548c\u56fd\"],\n[\"\u0441\u043f\u0443\u0431\u043b\u0438\", null],\n[\"\u043a\u0430\", null],\n[\"\u0441\u044b\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\", null],\n[\"\u04af\", \"\u653f\u5e9c\"],\n[\"\u043a\u0456\", null],\n[\"\u043c\u0435\", null],\n[\"\u0442\u0456\", null],\n[\"\u2581\", null],\n[\"1\", null],\n[\"9\", null],\n[\"9\", null],\n[\"7\", null],\n[\"\u2581\u0436\", \"\u5e74\"],\n[\"\u044b\", null],\n[\"\u043b\u044b\", null],\n[\"\u2581\u0410\", \"\u827e\u6ecb\u75c5\"],\n[\"\u0418\", null],\n[\"\u0422\", null],\n[\"\u0412\", null],\n[\"\u2581/\", null],\n[\"\u2581\u0416\", null],\n[\"\u0418\", null],\n[\"\u0422\", null],\n[\"\u0421\", null],\n[\"-\", null],\n[\"\u0442\u0456\", null],\n[\"\u2581\u0435\", \"\u6cbb\u7597\"],\n[\"\u043c\", null],\n[\"\u0434\u0435\", null],\n[\"\u0443\", null],\n[\"\u0433\u0435\", null],\n[\"\u2581\u0430\u0440\", \"\u4eff\u5236\u836f\"],\n[\"\u043d\u0430\u043b\", null],\n[\"\u0493\", null],\n[\"\u0430\u043d\", null],\n[\"\u2581\u0434\", null],\n[\"\u0436\u0435\u043d\", null],\n[\"\u0435\", null],\n[\"\u0440\u0438\", null],\n[\"\u043a\", null],\n[\"\u2581\u0434\", \"\u836f\u7269\"],\n[\"\u04d9\", null],\n[\"\u0440\u0456\", null],\n[\"-\", null],\n[\"\u0434\", null],\n[\"\u04d9\", null],\n[\"\u0440\", null],\n[\"\u043c\u0435\", null],\n[\"\u043a\", null],\n[\"\u0442\u0435\u0440\", null],\n[\"\u0434\u0456\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"\u53ef\u53ca\u6027\"],\n[\"\u043e\", null],\n[\"\u043b\", null],\n[\"\u0436\u0435\", null],\n[\"\u0442\u0456\", null],\n[\"\u043c\", null],\n[\"\u0434\u0456\", null],\n[\"\u2581\u0435\", null],\n[\"\u0442\u0443\", null],\n[\"\u2581\", null],\n[\"\u04af\", \"\u4e3a\u4e86\"],\n[\"\u0448\", null],\n[\"\u0456\u043d\", null],\n[\"\u2581\", null],\n[\"\u04e9\", \"\u4fee\u6539\"],\n[\"\u0437\", null],\n[\"\u0434\u0435\u0440\", null],\n[\"\u0456\", null],\n[\"\u043d\u0456\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0437\u0430\", \"\u6cd5\u5f8b\"],\n[\"\u04a3\", null],\n[\"\u0434\u0430\", null],\n[\"\u0440\u044b\", null],\n[\"\u043d\u0430\", null],\n[\"\u2581\", null],\n[\"\u04e9\", \"\u53d8\u66f4\"],\n[\"\u0437\", null],\n[\"\u0433\", null],\n[\"\u0435\u0440\", null],\n[\"\u0456\", null],\n[\"\u0441\u0442\u0435\u0440\", null],\n[\"\u2581\", null],\n[\"\u0435\u043d\", \"\u5f15\u5165\"],\n[\"\u0433\u0456\", null],\n[\"\u0437\u0443\", null],\n[\"\u0433\u0435\", null],\n[\"\u2581\u0442\", \"\u5c1d\u8bd5\"],\n[\"\u044b\", null],\n[\"\u0440\u044b\", null],\n[\"\u0441\", null],\n[\"\u049b\", null],\n[\"\u0430\u043d\", null],\n[\"\u2581\u043a\", \"\u65f6\"],\n[\"\u0435\", null],\n[\"\u0437\", null],\n[\"\u0434\u0435\", null],\n[\",\", null],\n[\"\u2581\u0431\", \"\u5168\u7403\"],\n[\"\u04af\", null],\n[\"\u043a\u0456\", null],\n[\"\u043b\", null],\n[\"\u2581\", null],\n[\"\u04d9\", \"\u5236\u836f\"],\n[\"\u043b\u0435\u043c\", null],\n[\"\u0434\u0456\", null],\n[\"\u043a\", null],\n[\"\u2581\u0444\", null],\n[\"\u0430\u0440\", null],\n[\"\u043c\u0430\", null],\n[\"\u0446\u0435\u0432\", null],\n[\"\u0442\u0438\", null],\n[\"\u043a\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u049b\", null],\n[\"\u2581\u0438\u043d\", \"\u4ea7\u4e1a\"],\n[\"\u0434\u0443\", null],\n[\"\u0441\u0442\", null],\n[\"\u0440\u0438\u044f\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0437\u0430\", \"\u6cd5\u5f8b\"],\n[\"\u04a3\", null],\n[\"\u0434\u044b\", null],\n[\"\u2581\u043a\", \"\u529b\u91cf\"],\n[\"\u04af\", null],\n[\"\u0448\", null],\n[\"\u0442\u0435\", null],\n[\"\u0440\u0456\", null],\n[\"\u2581\u043e\u0441\", \"\u5bf9\u8be5\u56fd\"],\n[\"\u044b\", null],\n[\"\u2581\u0435\", null],\n[\"\u043b\", null],\n[\"\u0433\u0435\", null],\n[\"\u2581\u0431\u0430\", \"\u53d1\u8d77\u653b\u51fb\"],\n[\"\u0441\", null],\n[\"\u2581\u0441\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u043f\", null],\n[\",\", null],\n[\"\u2581\u043e\", \"\u63a8\u8fdf\"],\n[\"\u043d\u044b\", null],\n[\"\u2581\", null],\n[\"\u0435\u043d\", \"\u5b9e\u65bd\"],\n[\"\u0433\u0456\", null],\n[\"\u0437\u0443\", null],\n[\"\u0434\u0456\", null],\n[\"\u2581\u043a\", \"\u5ef6\u8bef\"],\n[\"\u0435\", null],\n[\"\u0448\u0456\", null],\n[\"\u043a\u0442\", null],\n[\"\u0456\", null],\n[\"\u0440\u0456\", null],\n[\"\u043f\", null],\n[\",\", null],\n[\"\u2581\u043a\", \"\u8bb8\u591a\"],\n[\"\u04e9\", null],\n[\"\u043f\", null],\n[\"\u0442\u0435\", null],\n[\"\u0433\u0435\u043d\", null],\n[\"\u2581\u0430\", \"\u4eba\"],\n[\"\u0434\u0430\", null],\n[\"\u043c\", null],\n[\"\u0434\u0430\u0440\", null],\n[\"\u0434\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\", null],\n[\"\u04e9\", \"\u751f\u547d\"],\n[\"\u043c\u0456\", null],\n[\"\u0440\u0456\", null],\n[\"\u043d\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"\u593a\u8d70\"],\n[\"\u0438\", null],\n[\"\u0434\u044b\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u041d\u042c\u042e-\u0419\u041e\u0420\u041a \u2013 \u041e\u04a3\u0442\u04af\u0441\u0442\u0456\u043a \u0410\u0444\u0440\u0438\u043a\u0430 \u0420\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430\u0441\u044b\u043d\u044b\u04a3 \u04af\u043a\u0456\u043c\u0435\u0442\u0456 1997 \u0436\u044b\u043b\u044b \u0410\u0418\u0422\u0412 / \u0416\u0418\u0422\u0421-\u0442\u0456 \u0435\u043c\u0434\u0435\u0443\u0433\u0435 \u0430\u0440\u043d\u0430\u043b\u0493\u0430\u043d \u0434\u0436\u0435\u043d\u0435\u0440\u0438\u043a \u0434\u04d9\u0440\u0456-\u0434\u04d9\u0440\u043c\u0435\u043a\u0442\u0435\u0440\u0434\u0456 \u049b\u043e\u043b\u0436\u0435\u0442\u0456\u043c\u0434\u0456 \u0435\u0442\u0443 \u04af\u0448\u0456\u043d \u04e9\u0437\u0434\u0435\u0440\u0456\u043d\u0456\u04a3 \u0437\u0430\u04a3\u0434\u0430\u0440\u044b\u043d\u0430 \u04e9\u0437\u0433\u0435\u0440\u0456\u0441\u0442\u0435\u0440 \u0435\u043d\u0433\u0456\u0437\u0443\u0433\u0435 \u0442\u044b\u0440\u044b\u0441\u049b\u0430\u043d \u043a\u0435\u0437\u0434\u0435, \u0431\u04af\u043a\u0456\u043b \u04d9\u043b\u0435\u043c\u0434\u0456\u043a \u0444\u0430\u0440\u043c\u0430\u0446\u0435\u0432\u0442\u0438\u043a\u0430\u043b\u044b\u049b \u0438\u043d\u0434\u0443\u0441\u0442\u0440\u0438\u044f\u043d\u044b\u04a3 \u0437\u0430\u04a3\u0434\u044b \u043a\u04af\u0448\u0442\u0435\u0440\u0456 \u043e\u0441\u044b \u0435\u043b\u0433\u0435 \u0431\u0430\u0441 \u0441\u0430\u043b\u044b\u043f, \u043e\u043d\u044b \u0435\u043d\u0433\u0456\u0437\u0443\u0434\u0456 \u043a\u0435\u0448\u0456\u043a\u0442\u0456\u0440\u0456\u043f, \u043a\u04e9\u043f\u0442\u0435\u0433\u0435\u043d \u0430\u0434\u0430\u043c\u0434\u0430\u0440\u0434\u044b\u04a3 \u04e9\u043c\u0456\u0440\u0456\u043d \u049b\u0438\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041d', '\u042c', '\u042e', '-', '\u0419', '\u041e', '\u0420', '\u041a', '\u2581\u2013', '\u2581\u041e', '\u04a3', '\u0442', '\u04af', '\u0441\u0442\u0456', '\u043a', '\u2581\u0410', '\u0444', '\u0440\u0438', '\u043a\u0430', '\u2581\u0420\u0435', '\u0441\u043f\u0443\u0431\u043b\u0438', '\u043a\u0430', '\u0441\u044b', '\u043d\u044b', '\u04a3', '\u2581', '\u04af', '\u043a\u0456', '\u043c\u0435', '\u0442\u0456', '\u2581', '1', '9', '9', '7', '\u2581\u0436', '\u044b', '\u043b\u044b', '\u2581\u0410', '\u0418', '\u0422', '\u0412', '\u2581/', '\u2581\u0416', '\u0418', '\u0422', '\u0421', '-', '\u0442\u0456', '\u2581\u0435', '\u043c', '\u0434\u0435', '\u0443', '\u0433\u0435', '\u2581\u0430\u0440', '\u043d\u0430\u043b', '\u0493', '\u0430\u043d', '\u2581\u0434', '\u0436\u0435\u043d', '\u0435', '\u0440\u0438', '\u043a', '\u2581\u0434', '\u04d9', '\u0440\u0456', '-', '\u0434', '\u04d9', '\u0440', '\u043c\u0435', '\u043a', '\u0442\u0435\u0440', '\u0434\u0456', '\u2581', '\u049b', '\u043e', '\u043b', '\u0436\u0435', '\u0442\u0456', '\u043c', '\u0434\u0456', '\u2581\u0435', '\u0442\u0443', '\u2581', '\u04af', '\u0448', '\u0456\u043d', '\u2581', '\u04e9', '\u0437', '\u0434\u0435\u0440', '\u0456', '\u043d\u0456', '\u04a3', '\u2581\u0437\u0430', '\u04a3', '\u0434\u0430', '\u0440\u044b', '\u043d\u0430', '\u2581', '\u04e9', '\u0437', '\u0433', '\u0435\u0440', '\u0456', '\u0441\u0442\u0435\u0440', '\u2581', '\u0435\u043d', '\u0433\u0456', '\u0437\u0443', '\u0433\u0435', '\u2581\u0442', '\u044b', '\u0440\u044b', '\u0441', '\u049b', '\u0430\u043d', '\u2581\u043a', '\u0435', '\u0437', '\u0434\u0435', ',', '\u2581\u0431', '\u04af', '\u043a\u0456', '\u043b', '\u2581', '\u04d9', '\u043b\u0435\u043c', '\u0434\u0456', '\u043a', '\u2581\u0444', '\u0430\u0440', '\u043c\u0430', '\u0446\u0435\u0432', '\u0442\u0438', '\u043a\u0430', '\u043b\u044b', '\u049b', '\u2581\u0438\u043d', '\u0434\u0443', '\u0441\u0442', '\u0440\u0438\u044f', '\u043d\u044b', '\u04a3', '\u2581\u0437\u0430', '\u04a3', '\u0434\u044b', '\u2581\u043a', '\u04af', '\u0448', '\u0442\u0435', '\u0440\u0456', '\u2581\u043e\u0441', '\u044b', '\u2581\u0435', '\u043b', '\u0433\u0435', '\u2581\u0431\u0430', '\u0441', '\u2581\u0441\u0430', '\u043b\u044b', '\u043f', ',', '\u2581\u043e', '\u043d\u044b', '\u2581', '\u0435\u043d', '\u0433\u0456', '\u0437\u0443', '\u0434\u0456', '\u2581\u043a', '\u0435', '\u0448\u0456', '\u043a\u0442', '\u0456', '\u0440\u0456', '\u043f', ',', '\u2581\u043a', '\u04e9', '\u043f', '\u0442\u0435', '\u0433\u0435\u043d', '\u2581\u0430', '\u0434\u0430', '\u043c', '\u0434\u0430\u0440', '\u0434\u044b', '\u04a3', '\u2581', '\u04e9', '\u043c\u0456', '\u0440\u0456', '\u043d', '\u2581', '\u049b', '\u0438', '\u0434\u044b', '.']"}], "[\n[\"\u2581\u041d\", \"\u041d\u044c\u044e\"],\n[\"\u042c\", null],\n[\"\u042e\", null],\n[\"-\", null],\n[\"\u0419\", null],\n[\"\u041e\", null],\n[\"\u0420\", null],\n[\"\u041a\", null],\n[\"\u2581\u2013\", null],\n[\"\u2581\u041e\", \"\u042e\u0436\u043d\u043e\"],\n[\"\u04a3\", null],\n[\"\u0442\", null],\n[\"\u04af\", null],\n[\"\u0441\u0442\u0456\", null],\n[\"\u043a\", null],\n[\"\u2581\u0410\", \"\u0410\u0444\u0440\u0438\u043a\u0430\u043d\u0441\u043a\u0430\u044f\"],\n[\"\u0444\", null],\n[\"\u0440\u0438\", null],\n[\"\u043a\u0430\", null],\n[\"\u2581\u0420\u0435\", \"\u0420\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430\"],\n[\"\u0441\u043f\u0443\u0431\u043b\u0438\", null],\n[\"\u043a\u0430\", null],\n[\"\u0441\u044b\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\", null],\n[\"\u04af\", \"\u043f\u0440\u0430\u0432\u0438\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u043e\"],\n[\"\u043a\u0456\", null],\n[\"\u043c\u0435\", null],\n[\"\u0442\u0456\", null],\n[\"\u2581\", null],\n[\"1\", null],\n[\"9\", null],\n[\"9\", null],\n[\"7\", null],\n[\"\u2581\u0436\", \"\u0433\u043e\u0434\u0443\"],\n[\"\u044b\", null],\n[\"\u043b\u044b\", null],\n[\"\u2581\u0410\", \"\u0412\u0418\u0427\"],\n[\"\u0418\", null],\n[\"\u0422\", null],\n[\"\u0412\", null],\n[\"\u2581/\", null],\n[\"\u2581\u0416\", \"\u0421\u041f\u0418\u0414\"],\n[\"\u0418\", null],\n[\"\u0422\", null],\n[\"\u0421\", null],\n[\"-\", null],\n[\"\u0442\u0456\", null],\n[\"\u2581\u0435\", \"\u043b\u0435\u0447\u0435\u043d\u0438\u044f\"],\n[\"\u043c\", null],\n[\"\u0434\u0435\", null],\n[\"\u0443\", null],\n[\"\u0433\u0435\", null],\n[\"\u2581\u0430\u0440\", \"\u043f\u0440\u0435\u0434\u043d\u0430\u0437\u043d\u0430\u0447\u0435\u043d\u043d\u044b\u0445\"],\n[\"\u043d\u0430\u043b\", null],\n[\"\u0493\", null],\n[\"\u0430\u043d\", null],\n[\"\u2581\u0434\", \"\u0434\u0436\u0435\u043d\u0435\u0440\u0438\u043a\u043e\u0432\u044b\u0445\"],\n[\"\u0436\u0435\u043d\", null],\n[\"\u0435\", null],\n[\"\u0440\u0438\", null],\n[\"\u043a\", null],\n[\"\u2581\u0434\", \"\u043b\u0435\u043a\u0430\u0440\u0441\u0442\u0432\"],\n[\"\u04d9\", null],\n[\"\u0440\u0456\", null],\n[\"-\", null],\n[\"\u0434\", null],\n[\"\u04d9\", null],\n[\"\u0440\", null],\n[\"\u043c\u0435\", null],\n[\"\u043a\", null],\n[\"\u0442\u0435\u0440\", null],\n[\"\u0434\u0456\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"\u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b\u043c\u0438\"],\n[\"\u043e\", null],\n[\"\u043b\", null],\n[\"\u0436\u0435\", null],\n[\"\u0442\u0456\", null],\n[\"\u043c\", null],\n[\"\u0434\u0456\", null],\n[\"\u2581\u0435\", \"\u0441\u0434\u0435\u043b\u0430\u0442\u044c\"],\n[\"\u0442\u0443\", null],\n[\"\u2581\", null],\n[\"\u04af\", \"\u0434\u043b\u044f\"],\n[\"\u0448\", null],\n[\"\u0456\u043d\", null],\n[\"\u2581\", null],\n[\"\u04e9\", \"\u0441\u0432\u043e\u0438\u0445\"],\n[\"\u0437\", null],\n[\"\u0434\u0435\u0440\", null],\n[\"\u0456\", null],\n[\"\u043d\u0456\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0437\u0430\", \"\u0437\u0430\u043a\u043e\u043d\u043e\u0432\"],\n[\"\u04a3\", null],\n[\"\u0434\u0430\", null],\n[\"\u0440\u044b\", null],\n[\"\u043d\u0430\", null],\n[\"\u2581\", null],\n[\"\u04e9\", \"\u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f\"],\n[\"\u0437\", null],\n[\"\u0433\", null],\n[\"\u0435\u0440\", null],\n[\"\u0456\", null],\n[\"\u0441\u0442\u0435\u0440\", null],\n[\"\u2581\", null],\n[\"\u0435\u043d\", \"\u0432\u043d\u0435\u0441\u0442\u0438\"],\n[\"\u0433\u0456\", null],\n[\"\u0437\u0443\", null],\n[\"\u0433\u0435\", null],\n[\"\u2581\u0442\", \"\u043f\u044b\u0442\u0430\u043b\u043e\u0441\u044c\"],\n[\"\u044b\", null],\n[\"\u0440\u044b\", null],\n[\"\u0441\", null],\n[\"\u049b\", null],\n[\"\u0430\u043d\", null],\n[\"\u2581\u043a\", \"\u043a\u043e\u0433\u0434\u0430\"],\n[\"\u0435\", null],\n[\"\u0437\", null],\n[\"\u0434\u0435\", null],\n[\",\", null],\n[\"\u2581\u0431\", \"\u0432\u0441\u0435\"],\n[\"\u04af\", null],\n[\"\u043a\u0456\", null],\n[\"\u043b\", null],\n[\"\u2581\", null],\n[\"\u04d9\", \"\u043c\u0438\u0440\u043e\u0432\u043e\u0439\"],\n[\"\u043b\u0435\u043c\", null],\n[\"\u0434\u0456\", null],\n[\"\u043a\", null],\n[\"\u2581\u0444\", \"\u0444\u0430\u0440\u043c\u0430\u0446\u0435\u0432\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439\"],\n[\"\u0430\u0440\", null],\n[\"\u043c\u0430\", null],\n[\"\u0446\u0435\u0432\", null],\n[\"\u0442\u0438\", null],\n[\"\u043a\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u049b\", null],\n[\"\u2581\u0438\u043d\", \"\u0438\u043d\u0434\u0443\u0441\u0442\u0440\u0438\u0438\"],\n[\"\u0434\u0443\", null],\n[\"\u0441\u0442\", null],\n[\"\u0440\u0438\u044f\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0437\u0430\", \"\u0437\u0430\u043a\u043e\u043d\u043d\u044b\u0435\"],\n[\"\u04a3\", null],\n[\"\u0434\u044b\", null],\n[\"\u2581\u043a\", \"\u0441\u0438\u043b\u044b\"],\n[\"\u04af\", null],\n[\"\u0448\", null],\n[\"\u0442\u0435\", null],\n[\"\u0440\u0456\", null],\n[\"\u2581\u043e\u0441\", \"\u044d\u0442\u0443\"],\n[\"\u044b\", null],\n[\"\u2581\u0435\", \"\u0441\u0442\u0440\u0430\u043d\u0443\"],\n[\"\u043b\", null],\n[\"\u0433\u0435\", null],\n[\"\u2581\u0431\u0430\", \"\u0430\u0442\u0430\u043a\u043e\u0432\u0430\u043b\u0438\"],\n[\"\u0441\", null],\n[\"\u2581\u0441\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u043f\", null],\n[\",\", null],\n[\"\u2581\u043e\", \"\u0435\u0435\"],\n[\"\u043d\u044b\", null],\n[\"\u2581\", null],\n[\"\u0435\u043d\", \"\u0432\u0432\u0435\u0434\u0435\u043d\u0438\u0435\"],\n[\"\u0433\u0456\", null],\n[\"\u0437\u0443\", null],\n[\"\u0434\u0456\", null],\n[\"\u2581\u043a\", \"\u043e\u0442\u0441\u0440\u043e\u0447\u0438\u0432\"],\n[\"\u0435\", null],\n[\"\u0448\u0456\", null],\n[\"\u043a\u0442\", null],\n[\"\u0456\", null],\n[\"\u0440\u0456\", null],\n[\"\u043f\", null],\n[\",\", null],\n[\"\u2581\u043a\", \"\u043c\u043d\u043e\u0433\u0438\u0445\"],\n[\"\u04e9\", null],\n[\"\u043f\", null],\n[\"\u0442\u0435\", null],\n[\"\u0433\u0435\u043d\", null],\n[\"\u2581\u0430\", \"\u043b\u044e\u0434\u0435\u0439\"],\n[\"\u0434\u0430\", null],\n[\"\u043c\", null],\n[\"\u0434\u0430\u0440\", null],\n[\"\u0434\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\", null],\n[\"\u04e9\", \"\u0436\u0438\u0437\u043d\u0438\"],\n[\"\u043c\u0456\", null],\n[\"\u0440\u0456\", null],\n[\"\u043d\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"\u0443\u043d\u0435\u0441\u043b\u0438\"],\n[\"\u0438\", null],\n[\"\u0434\u044b\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u0422\u0430\u0431\u0438\u0493\u0430\u0442\u0442\u044b \u049b\u043e\u0440\u0493\u0430\u0443 \u0430\u0433\u0435\u043d\u0442\u0442\u0456\u0433\u0456\u043d\u0456\u04a3 \u0435\u0441\u0435\u0431\u0456 \u0431\u043e\u0439\u044b\u043d\u0448\u0430, 1990-2015 \u0436\u044b\u043b\u0434\u0430\u0440 \u0430\u0440\u0430\u043b\u044b\u0493\u044b\u043d\u0434\u0430 \u0430\u0443\u0430\u0434\u0430\u0493\u044b \u049b\u0430\u0442\u0442\u044b \u0431\u04e9\u043b\u0448\u0435\u043a\u0442\u0435\u0440\u0434\u0456\u04a3 \u04af\u043b\u0435\u0441\u0456 37 \u043f\u0430\u0439\u044b\u0437\u0493\u0430 \u0430\u0437\u0430\u0439\u0493\u0430\u043d, 2010 \u0436\u044b\u043b\u044b \u043e\u0441\u044b \u0435\u0440\u0435\u0436\u0435\u043b\u0435\u0440\u0434\u0456\u04a3 \u0430\u0440\u049b\u0430\u0441\u044b\u043d\u0434\u0430 160 \u043c\u044b\u04a3 \u043c\u0435\u0437\u0433\u0456\u043b\u0441\u0456\u0437 \u04e9\u043b\u0456\u043c\u043d\u0456\u04a3 \u0430\u043b\u0434\u044b \u0430\u043b\u044b\u043d\u0493\u0430\u043d.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0422\u0430', '\u0431\u0438', '\u0493', '\u0430\u0442', '\u0442\u044b', '\u2581', '\u049b', '\u043e\u0440', '\u0493', '\u0430', '\u0443', '\u2581\u0430', '\u0433', '\u0435\u043d\u0442', '\u0442\u0456', '\u0433\u0456', '\u043d\u0456', '\u04a3', '\u2581\u0435', '\u0441\u0435', '\u0431\u0456', '\u2581\u0431\u043e', '\u0439', '\u044b', '\u043d', '\u0448\u0430', ',', '\u2581', '1', '9', '9', '0', '-', '2', '0', '1', '5', '\u2581\u0436', '\u044b', '\u043b', '\u0434\u0430\u0440', '\u2581\u0430', '\u0440\u0430', '\u043b\u044b', '\u0493', '\u044b', '\u043d', '\u0434\u0430', '\u2581\u0430', '\u0443', '\u0430', '\u0434\u0430', '\u0493', '\u044b', '\u2581', '\u049b', '\u0430\u0442', '\u0442\u044b', '\u2581\u0431', '\u04e9', '\u043b', '\u0448\u0435', '\u043a', '\u0442\u0435\u0440', '\u0434\u0456', '\u04a3', '\u2581', '\u04af', '\u043b\u0435', '\u0441\u0456', '\u2581', '3', '7', '\u2581\u043f\u0430', '\u0439', '\u044b', '\u0437', '\u0493', '\u0430', '\u2581\u0430', '\u0437\u0430', '\u0439', '\u0493', '\u0430\u043d', ',', '\u2581', '2', '0', '1', '0', '\u2581\u0436', '\u044b', '\u043b\u044b', '\u2581\u043e\u0441', '\u044b', '\u2581\u0435', '\u0440\u0435', '\u0436\u0435', '\u043b\u0435', '\u0440', '\u0434\u0456', '\u04a3', '\u2581\u0430\u0440', '\u049b', '\u0430', '\u0441\u044b', '\u043d', '\u0434\u0430', '\u2581', '1', '6', '0', '\u2581\u043c', '\u044b', '\u04a3', '\u2581\u043c\u0435', '\u0437', '\u0433\u0456', '\u043b', '\u0441\u0456', '\u0437', '\u2581', '\u04e9', '\u043b\u0456', '\u043c', '\u043d\u0456', '\u04a3', '\u2581\u0430\u043b', '\u0434\u044b', '\u2581\u0430', '\u043b\u044b', '\u043d', '\u0493', '\u0430\u043d', '.']"}], "[\n[\"\u2581\u0422\u0430\", \"According\"],\n[\"\u0431\u0438\", null],\n[\"\u0493\", null],\n[\"\u0430\u0442\", null],\n[\"\u0442\u044b\", null],\n[\"\u2581\", null],\n[\"\u049b\", null],\n[\"\u043e\u0440\", null],\n[\"\u0493\", null],\n[\"\u0430\", null],\n[\"\u0443\", null],\n[\"\u2581\u0430\", null],\n[\"\u0433\", null],\n[\"\u0435\u043d\u0442\", null],\n[\"\u0442\u0456\", null],\n[\"\u0433\u0456\", null],\n[\"\u043d\u0456\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0435\", null],\n[\"\u0441\u0435\", null],\n[\"\u0431\u0456\", null],\n[\"\u2581\u0431\u043e\", null],\n[\"\u0439\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u0448\u0430\", null],\n[\",\", null],\n[\"\u2581\", null],\n[\"1\", null],\n[\"9\", null],\n[\"9\", null],\n[\"0\", null],\n[\"-\", null],\n[\"2\", null],\n[\"0\", null],\n[\"1\", null],\n[\"5\", null],\n[\"\u2581\u0436\", \"years\"],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\"\u0434\u0430\u0440\", null],\n[\"\u2581\u0430\", null],\n[\"\u0440\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u2581\u0430\", \"air\"],\n[\"\u0443\", null],\n[\"\u0430\", null],\n[\"\u0434\u0430\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"solid\"],\n[\"\u0430\u0442\", null],\n[\"\u0442\u044b\", null],\n[\"\u2581\u0431\", \"particles\"],\n[\"\u04e9\", null],\n[\"\u043b\", null],\n[\"\u0448\u0435\", null],\n[\"\u043a\", null],\n[\"\u0442\u0435\u0440\", null],\n[\"\u0434\u0456\", null],\n[\"\u04a3\", null],\n[\"\u2581\", null],\n[\"\u04af\", \"share\"],\n[\"\u043b\u0435\", null],\n[\"\u0441\u0456\", null],\n[\"\u2581\", null],\n[\"3\", null],\n[\"7\", null],\n[\"\u2581\u043f\u0430\", \"percent\"],\n[\"\u0439\", null],\n[\"\u044b\", null],\n[\"\u0437\", null],\n[\"\u0493\", null],\n[\"\u0430\", null],\n[\"\u2581\u0430\", \"decreased\"],\n[\"\u0437\u0430\", null],\n[\"\u0439\", null],\n[\"\u0493\", null],\n[\"\u0430\u043d\", null],\n[\",\", null],\n[\"\u2581\", null],\n[\"2\", null],\n[\"0\", null],\n[\"1\", null],\n[\"0\", null],\n[\"\u2581\u0436\", \"year\"],\n[\"\u044b\", null],\n[\"\u043b\u044b\", null],\n[\"\u2581\u043e\u0441\", \"these\"],\n[\"\u044b\", null],\n[\"\u2581\u0435\", \"regulations\"],\n[\"\u0440\u0435\", null],\n[\"\u0436\u0435\", null],\n[\"\u043b\u0435\", null],\n[\"\u0440\", null],\n[\"\u0434\u0456\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0430\u0440\", \"thanks\"],\n[\"\u049b\", null],\n[\"\u0430\", null],\n[\"\u0441\u044b\", null],\n[\"\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u2581\", null],\n[\"1\", null],\n[\"6\", null],\n[\"0\", null],\n[\"\u2581\u043c\", \"thousand\"],\n[\"\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\u043c\u0435\", \"premature\"],\n[\"\u0437\", null],\n[\"\u0433\u0456\", null],\n[\"\u043b\", null],\n[\"\u0441\u0456\", null],\n[\"\u0437\", null],\n[\"\u2581\", null],\n[\"\u04e9\", \"deaths\"],\n[\"\u043b\u0456\", null],\n[\"\u043c\", null],\n[\"\u043d\u0456\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0430\u043b\", \"prevented\"],\n[\"\u0434\u044b\", null],\n[\"\u2581\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u043d\", null],\n[\"\u0493\", null],\n[\"\u0430\u043d\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u0422\u0430\u0431\u0438\u0493\u0430\u0442\u0442\u044b \u049b\u043e\u0440\u0493\u0430\u0443 \u0430\u0433\u0435\u043d\u0442\u0442\u0456\u0433\u0456\u043d\u0456\u04a3 \u0435\u0441\u0435\u0431\u0456 \u0431\u043e\u0439\u044b\u043d\u0448\u0430, 1990-2015 \u0436\u044b\u043b\u0434\u0430\u0440 \u0430\u0440\u0430\u043b\u044b\u0493\u044b\u043d\u0434\u0430 \u0430\u0443\u0430\u0434\u0430\u0493\u044b \u049b\u0430\u0442\u0442\u044b \u0431\u04e9\u043b\u0448\u0435\u043a\u0442\u0435\u0440\u0434\u0456\u04a3 \u04af\u043b\u0435\u0441\u0456 37 \u043f\u0430\u0439\u044b\u0437\u0493\u0430 \u0430\u0437\u0430\u0439\u0493\u0430\u043d, 2010 \u0436\u044b\u043b\u044b \u043e\u0441\u044b \u0435\u0440\u0435\u0436\u0435\u043b\u0435\u0440\u0434\u0456\u04a3 \u0430\u0440\u049b\u0430\u0441\u044b\u043d\u0434\u0430 160 \u043c\u044b\u04a3 \u043c\u0435\u0437\u0433\u0456\u043b\u0441\u0456\u0437 \u04e9\u043b\u0456\u043c\u043d\u0456\u04a3 \u0430\u043b\u0434\u044b \u0430\u043b\u044b\u043d\u0493\u0430\u043d.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0422\u0430', '\u0431\u0438', '\u0493', '\u0430\u0442', '\u0442\u044b', '\u2581', '\u049b', '\u043e\u0440', '\u0493', '\u0430', '\u0443', '\u2581\u0430', '\u0433', '\u0435\u043d\u0442', '\u0442\u0456', '\u0433\u0456', '\u043d\u0456', '\u04a3', '\u2581\u0435', '\u0441\u0435', '\u0431\u0456', '\u2581\u0431\u043e', '\u0439', '\u044b', '\u043d', '\u0448\u0430', ',', '\u2581', '1', '9', '9', '0', '-', '2', '0', '1', '5', '\u2581\u0436', '\u044b', '\u043b', '\u0434\u0430\u0440', '\u2581\u0430', '\u0440\u0430', '\u043b\u044b', '\u0493', '\u044b', '\u043d', '\u0434\u0430', '\u2581\u0430', '\u0443', '\u0430', '\u0434\u0430', '\u0493', '\u044b', '\u2581', '\u049b', '\u0430\u0442', '\u0442\u044b', '\u2581\u0431', '\u04e9', '\u043b', '\u0448\u0435', '\u043a', '\u0442\u0435\u0440', '\u0434\u0456', '\u04a3', '\u2581', '\u04af', '\u043b\u0435', '\u0441\u0456', '\u2581', '3', '7', '\u2581\u043f\u0430', '\u0439', '\u044b', '\u0437', '\u0493', '\u0430', '\u2581\u0430', '\u0437\u0430', '\u0439', '\u0493', '\u0430\u043d', ',', '\u2581', '2', '0', '1', '0', '\u2581\u0436', '\u044b', '\u043b\u044b', '\u2581\u043e\u0441', '\u044b', '\u2581\u0435', '\u0440\u0435', '\u0436\u0435', '\u043b\u0435', '\u0440', '\u0434\u0456', '\u04a3', '\u2581\u0430\u0440', '\u049b', '\u0430', '\u0441\u044b', '\u043d', '\u0434\u0430', '\u2581', '1', '6', '0', '\u2581\u043c', '\u044b', '\u04a3', '\u2581\u043c\u0435', '\u0437', '\u0433\u0456', '\u043b', '\u0441\u0456', '\u0437', '\u2581', '\u04e9', '\u043b\u0456', '\u043c', '\u043d\u0456', '\u04a3', '\u2581\u0430\u043b', '\u0434\u044b', '\u2581\u0430', '\u043b\u044b', '\u043d', '\u0493', '\u0430\u043d', '.']"}], "[\n[\"\u2581\u0422\u0430\", \"\u81ea\u7136\"],\n[\"\u0431\u0438\", null],\n[\"\u0493\", null],\n[\"\u0430\u0442\", null],\n[\"\u0442\u044b\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"\u4fdd\u62a4\"],\n[\"\u043e\u0440\", null],\n[\"\u0493\", null],\n[\"\u0430\", null],\n[\"\u0443\", null],\n[\"\u2581\u0430\", \"\u673a\u6784\"],\n[\"\u0433\", null],\n[\"\u0435\u043d\u0442\", null],\n[\"\u0442\u0456\", null],\n[\"\u0433\u0456\", null],\n[\"\u043d\u0456\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0435\", \"\u62a5\u544a\"],\n[\"\u0441\u0435\", null],\n[\"\u0431\u0456\", null],\n[\"\u2581\u0431\u043e\", \"\u6839\u636e\"],\n[\"\u0439\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u0448\u0430\", null],\n[\",\", null],\n[\"\u2581\", null],\n[\"1\", null],\n[\"9\", null],\n[\"9\", null],\n[\"0\", null],\n[\"-\", null],\n[\"2\", null],\n[\"0\", null],\n[\"1\", null],\n[\"5\", null],\n[\"\u2581\u0436\", \"\u5e74\"],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\"\u0434\u0430\u0440\", null],\n[\"\u2581\u0430\", \"\u671f\u95f4\"],\n[\"\u0440\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u2581\u0430\", \"\u7a7a\u6c14\"],\n[\"\u0443\", null],\n[\"\u0430\", null],\n[\"\u0434\u0430\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"\u9897\u7c92\u7269\"],\n[\"\u0430\u0442\", null],\n[\"\u0442\u044b\", null],\n[\"\u2581\u0431\", \"\u6bd4\u4f8b\"],\n[\"\u04e9\", null],\n[\"\u043b\", null],\n[\"\u0448\u0435\", null],\n[\"\u043a\", null],\n[\"\u0442\u0435\u0440\", null],\n[\"\u0434\u0456\", null],\n[\"\u04a3\", null],\n[\"\u2581\", null],\n[\"\u04af\", \"\u51cf\u5c11\"],\n[\"\u043b\u0435\", null],\n[\"\u0441\u0456\", null],\n[\"\u2581\", null],\n[\"3\", null],\n[\"7\", null],\n[\"\u2581\u043f\u0430\", \"\u767e\u5206\u6bd4\"],\n[\"\u0439\", null],\n[\"\u044b\", null],\n[\"\u0437\", null],\n[\"\u0493\", null],\n[\"\u0430\", null],\n[\"\u2581\u0430\", \"\u4e0b\u964d\"],\n[\"\u0437\u0430\", null],\n[\"\u0439\", null],\n[\"\u0493\", null],\n[\"\u0430\u043d\", null],\n[\",\", null],\n[\"\u2581\", null],\n[\"2\", null],\n[\"0\", null],\n[\"1\", null],\n[\"0\", null],\n[\"\u2581\u0436\", \"\u5e74\"],\n[\"\u044b\", null],\n[\"\u043b\u044b\", null],\n[\"\u2581\u043e\u0441\", \"\u8fd9\u4e9b\"],\n[\"\u044b\", null],\n[\"\u2581\u0435\", \"\u89c4\u5b9a\"],\n[\"\u0440\u0435\", null],\n[\"\u0436\u0435\", null],\n[\"\u043b\u0435\", null],\n[\"\u0440\", null],\n[\"\u0434\u0456\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0430\u0440\", \"\u7531\u4e8e\"],\n[\"\u049b\", null],\n[\"\u0430\", null],\n[\"\u0441\u044b\", null],\n[\"\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u2581\", null],\n[\"1\", null],\n[\"6\", null],\n[\"0\", null],\n[\"\u2581\u043c\", \"\u4e07\"],\n[\"\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\u043c\u0435\", \"\u8fc7\u65e9\"],\n[\"\u0437\", null],\n[\"\u0433\u0456\", null],\n[\"\u043b\", null],\n[\"\u0441\u0456\", null],\n[\"\u0437\", null],\n[\"\u2581\", null],\n[\"\u04e9\", \"\u6b7b\u4ea1\"],\n[\"\u043b\u0456\", null],\n[\"\u043c\", null],\n[\"\u043d\u0456\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0430\u043b\", \"\u907f\u514d\"],\n[\"\u0434\u044b\", null],\n[\"\u2581\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u043d\", null],\n[\"\u0493\", null],\n[\"\u0430\u043d\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u0422\u0430\u0431\u0438\u0493\u0430\u0442\u0442\u044b \u049b\u043e\u0440\u0493\u0430\u0443 \u0430\u0433\u0435\u043d\u0442\u0442\u0456\u0433\u0456\u043d\u0456\u04a3 \u0435\u0441\u0435\u0431\u0456 \u0431\u043e\u0439\u044b\u043d\u0448\u0430, 1990-2015 \u0436\u044b\u043b\u0434\u0430\u0440 \u0430\u0440\u0430\u043b\u044b\u0493\u044b\u043d\u0434\u0430 \u0430\u0443\u0430\u0434\u0430\u0493\u044b \u049b\u0430\u0442\u0442\u044b \u0431\u04e9\u043b\u0448\u0435\u043a\u0442\u0435\u0440\u0434\u0456\u04a3 \u04af\u043b\u0435\u0441\u0456 37 \u043f\u0430\u0439\u044b\u0437\u0493\u0430 \u0430\u0437\u0430\u0439\u0493\u0430\u043d, 2010 \u0436\u044b\u043b\u044b \u043e\u0441\u044b \u0435\u0440\u0435\u0436\u0435\u043b\u0435\u0440\u0434\u0456\u04a3 \u0430\u0440\u049b\u0430\u0441\u044b\u043d\u0434\u0430 160 \u043c\u044b\u04a3 \u043c\u0435\u0437\u0433\u0456\u043b\u0441\u0456\u0437 \u04e9\u043b\u0456\u043c\u043d\u0456\u04a3 \u0430\u043b\u0434\u044b \u0430\u043b\u044b\u043d\u0493\u0430\u043d.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0422\u0430', '\u0431\u0438', '\u0493', '\u0430\u0442', '\u0442\u044b', '\u2581', '\u049b', '\u043e\u0440', '\u0493', '\u0430', '\u0443', '\u2581\u0430', '\u0433', '\u0435\u043d\u0442', '\u0442\u0456', '\u0433\u0456', '\u043d\u0456', '\u04a3', '\u2581\u0435', '\u0441\u0435', '\u0431\u0456', '\u2581\u0431\u043e', '\u0439', '\u044b', '\u043d', '\u0448\u0430', ',', '\u2581', '1', '9', '9', '0', '-', '2', '0', '1', '5', '\u2581\u0436', '\u044b', '\u043b', '\u0434\u0430\u0440', '\u2581\u0430', '\u0440\u0430', '\u043b\u044b', '\u0493', '\u044b', '\u043d', '\u0434\u0430', '\u2581\u0430', '\u0443', '\u0430', '\u0434\u0430', '\u0493', '\u044b', '\u2581', '\u049b', '\u0430\u0442', '\u0442\u044b', '\u2581\u0431', '\u04e9', '\u043b', '\u0448\u0435', '\u043a', '\u0442\u0435\u0440', '\u0434\u0456', '\u04a3', '\u2581', '\u04af', '\u043b\u0435', '\u0441\u0456', '\u2581', '3', '7', '\u2581\u043f\u0430', '\u0439', '\u044b', '\u0437', '\u0493', '\u0430', '\u2581\u0430', '\u0437\u0430', '\u0439', '\u0493', '\u0430\u043d', ',', '\u2581', '2', '0', '1', '0', '\u2581\u0436', '\u044b', '\u043b\u044b', '\u2581\u043e\u0441', '\u044b', '\u2581\u0435', '\u0440\u0435', '\u0436\u0435', '\u043b\u0435', '\u0440', '\u0434\u0456', '\u04a3', '\u2581\u0430\u0440', '\u049b', '\u0430', '\u0441\u044b', '\u043d', '\u0434\u0430', '\u2581', '1', '6', '0', '\u2581\u043c', '\u044b', '\u04a3', '\u2581\u043c\u0435', '\u0437', '\u0433\u0456', '\u043b', '\u0441\u0456', '\u0437', '\u2581', '\u04e9', '\u043b\u0456', '\u043c', '\u043d\u0456', '\u04a3', '\u2581\u0430\u043b', '\u0434\u044b', '\u2581\u0430', '\u043b\u044b', '\u043d', '\u0493', '\u0430\u043d', '.']"}], "[\n[\"\u2581\u0422\u0430\", \"\u0421\u043e\u0433\u043b\u0430\u0441\u043d\u043e\"],\n[\"\u0431\u0438\", null],\n[\"\u0493\", null],\n[\"\u0430\u0442\", null],\n[\"\u0442\u044b\", null],\n[\"\u2581\", null],\n[\"\u049b\", null],\n[\"\u043e\u0440\", null],\n[\"\u0493\", null],\n[\"\u0430\", null],\n[\"\u0443\", null],\n[\"\u2581\u0430\", null],\n[\"\u0433\", null],\n[\"\u0435\u043d\u0442\", null],\n[\"\u0442\u0456\", null],\n[\"\u0433\u0456\", null],\n[\"\u043d\u0456\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0435\", \"\u043e\u0442\u0447\u0435\u0442\u0443\"],\n[\"\u0441\u0435\", null],\n[\"\u0431\u0456\", null],\n[\"\u2581\u0431\u043e\", \"\u0410\u0433\u0435\u043d\u0442\u0441\u0442\u0432\u0430\"],\n[\"\u0439\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u0448\u0430\", null],\n[\",\", null],\n[\"\u2581\", null],\n[\"1\", null],\n[\"9\", null],\n[\"9\", null],\n[\"0\", null],\n[\"-\", null],\n[\"2\", null],\n[\"0\", null],\n[\"1\", null],\n[\"5\", null],\n[\"\u2581\u0436\", \"\u0433\u043e\u0434\u044b\"],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\"\u0434\u0430\u0440\", null],\n[\"\u2581\u0430\", \"\u043c\u0435\u0436\u0434\u0443\"],\n[\"\u0440\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u2581\u0430\", \"\u0432\"],\n[\"\u0443\", null],\n[\"\u0430\", null],\n[\"\u0434\u0430\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"\u0442\u0432\u0435\u0440\u0434\u044b\u0445\"],\n[\"\u0430\u0442\", null],\n[\"\u0442\u044b\", null],\n[\"\u2581\u0431\", \"\u0447\u0430\u0441\u0442\u0438\u0446\"],\n[\"\u04e9\", null],\n[\"\u043b\", null],\n[\"\u0448\u0435\", null],\n[\"\u043a\", null],\n[\"\u0442\u0435\u0440\", null],\n[\"\u0434\u0456\", null],\n[\"\u04a3\", null],\n[\"\u2581\", null],\n[\"\u04af\", \"\u0434\u043e\u043b\u044f\"],\n[\"\u043b\u0435\", null],\n[\"\u0441\u0456\", null],\n[\"\u2581\", null],\n[\"3\", null],\n[\"7\", null],\n[\"\u2581\u043f\u0430\", \"\u043f\u0440\u043e\u0446\u0435\u043d\u0442\u043e\u0432\"],\n[\"\u0439\", null],\n[\"\u044b\", null],\n[\"\u0437\", null],\n[\"\u0493\", null],\n[\"\u0430\", null],\n[\"\u2581\u0430\", \"\u0443\u043c\u0435\u043d\u044c\u0448\u0438\u043b\u0430\u0441\u044c\"],\n[\"\u0437\u0430\", null],\n[\"\u0439\", null],\n[\"\u0493\", null],\n[\"\u0430\u043d\", null],\n[\",\", null],\n[\"\u2581\", null],\n[\"2\", null],\n[\"0\", null],\n[\"1\", null],\n[\"0\", null],\n[\"\u2581\u0436\", \"\u0433\u043e\u0434\u0443\"],\n[\"\u044b\", null],\n[\"\u043b\u044b\", null],\n[\"\u2581\u043e\u0441\", \"\u0431\u043b\u0430\u0433\u043e\u0434\u0430\u0440\u044f\"],\n[\"\u044b\", null],\n[\"\u2581\u0435\", \"\u044d\u0442\u0438\u043c\"],\n[\"\u0440\u0435\", null],\n[\"\u0436\u0435\", null],\n[\"\u043b\u0435\", null],\n[\"\u0440\", null],\n[\"\u0434\u0456\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0430\u0440\", \"\u043f\u0440\u0430\u0432\u0438\u043b\u0430\u043c\"],\n[\"\u049b\", null],\n[\"\u0430\", null],\n[\"\u0441\u044b\", null],\n[\"\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u2581\", null],\n[\"1\", null],\n[\"6\", null],\n[\"0\", null],\n[\"\u2581\u043c\", \"\u0442\u044b\u0441\u044f\u0447\"],\n[\"\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\u043c\u0435\", \"\u043f\u0440\u0435\u0436\u0434\u0435\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445\"],\n[\"\u0437\", null],\n[\"\u0433\u0456\", null],\n[\"\u043b\", null],\n[\"\u0441\u0456\", null],\n[\"\u0437\", null],\n[\"\u2581\", null],\n[\"\u04e9\", \"\u0441\u043c\u0435\u0440\u0442\u0435\u0439\"],\n[\"\u043b\u0456\", null],\n[\"\u043c\", null],\n[\"\u043d\u0456\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0430\u043b\", \"\u0431\u044b\u043b\u043e\"],\n[\"\u0434\u044b\", null],\n[\"\u2581\u0430\", \"\u043f\u0440\u0435\u0434\u043e\u0442\u0432\u0440\u0430\u0449\u0435\u043d\u043e\"],\n[\"\u043b\u044b\", null],\n[\"\u043d\", null],\n[\"\u0493\", null],\n[\"\u0430\u043d\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u049a\u0430\u043b\u0493\u0430\u043d\u044b \u043d\u0435\u0433\u0456\u0437\u0456\u043d\u0435\u043d \u0431\u0440\u0438\u0442\u0430\u043d\u0434\u044b\u049b \u0444\u0443\u043d\u0442 \u0436\u04d9\u043d\u0435 \u043a\u0435\u04a3\u0435\u0441\u0442\u0456\u043a \u0440\u0443\u0431\u043b\u044c \u0430\u0440\u0430\u0441\u044b\u043d\u0434\u0430 \u0431\u04e9\u043b\u0456\u043d\u0434\u0456.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u049a', '\u0430\u043b', '\u0493', '\u0430', '\u043d\u044b', '\u2581\u043d\u0435', '\u0433\u0456', '\u0437\u0456', '\u043d\u0435\u043d', '\u2581\u0431\u0440\u0438', '\u0442\u0430\u043d', '\u0434\u044b', '\u049b', '\u2581\u0444', '\u0443\u043d', '\u0442', '\u2581\u0436', '\u04d9', '\u043d\u0435', '\u2581\u043a', '\u0435', '\u04a3', '\u0435', '\u0441\u0442\u0456', '\u043a', '\u2581\u0440\u0443', '\u0431', '\u043b\u044c', '\u2581\u0430', '\u0440\u0430', '\u0441\u044b', '\u043d', '\u0434\u0430', '\u2581\u0431', '\u04e9', '\u043b\u0456', '\u043d', '\u0434\u0456', '.']"}], "[\n  [\"\u2581\", null],\n  [\"\u049a\", \"The\"],\n  [\"\u0430\u043b\", null],\n  [\"\u0493\", null],\n  [\"\u0430\", null],\n  [\"\u043d\u044b\", null],\n  [\"\u2581\u043d\u0435\", \"rest\"],\n  [\"\u0433\u0456\", null],\n  [\"\u0437\u0456\", null],\n  [\"\u043d\u0435\u043d\", null],\n  [\"\u2581\u0431\u0440\u0438\", \"British\"],\n  [\"\u0442\u0430\u043d\", null],\n  [\"\u0434\u044b\", null],\n  [\"\u049b\", null],\n  [\"\u2581\u0444\", \"pound\"],\n  [\"\u0443\u043d\", null],\n  [\"\u0442\", null],\n  [\"\u2581\u0436\", \"and\"],\n  [\"\u04d9\", null],\n  [\"\u043d\u0435\", null],\n  [\"\u2581\u043a\", \"Soviet\"],\n  [\"\u0435\", null],\n  [\"\u04a3\", null],\n  [\"\u0435\", null],\n  [\"\u0441\u0442\u0456\", null],\n  [\"\u043a\", null],\n  [\"\u2581\u0440\u0443\", \"ruble\"],\n  [\"\u0431\", null],\n  [\"\u043b\u044c\", null],\n  [\"\u2581\u0430\", \"between\"],\n  [\"\u0440\u0430\", null],\n  [\"\u0441\u044b\", null],\n  [\"\u043d\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u2581\u0431\", \"was\"],\n  [\"\u04e9\", null],\n  [\"\u043b\u0456\", null],\n  [\"\u043d\", null],\n  [\"\u0434\u0456\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u049a\u0430\u043b\u0493\u0430\u043d\u044b \u043d\u0435\u0433\u0456\u0437\u0456\u043d\u0435\u043d \u0431\u0440\u0438\u0442\u0430\u043d\u0434\u044b\u049b \u0444\u0443\u043d\u0442 \u0436\u04d9\u043d\u0435 \u043a\u0435\u04a3\u0435\u0441\u0442\u0456\u043a \u0440\u0443\u0431\u043b\u044c \u0430\u0440\u0430\u0441\u044b\u043d\u0434\u0430 \u0431\u04e9\u043b\u0456\u043d\u0434\u0456.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u049a', '\u0430\u043b', '\u0493', '\u0430', '\u043d\u044b', '\u2581\u043d\u0435', '\u0433\u0456', '\u0437\u0456', '\u043d\u0435\u043d', '\u2581\u0431\u0440\u0438', '\u0442\u0430\u043d', '\u0434\u044b', '\u049b', '\u2581\u0444', '\u0443\u043d', '\u0442', '\u2581\u0436', '\u04d9', '\u043d\u0435', '\u2581\u043a', '\u0435', '\u04a3', '\u0435', '\u0441\u0442\u0456', '\u043a', '\u2581\u0440\u0443', '\u0431', '\u043b\u044c', '\u2581\u0430', '\u0440\u0430', '\u0441\u044b', '\u043d', '\u0434\u0430', '\u2581\u0431', '\u04e9', '\u043b\u0456', '\u043d', '\u0434\u0456', '.']"}], "[\n  [\"\u2581\", null],\n  [\"\u049a\", \"\u5176\u4f59\"],\n  [\"\u0430\u043b\", null],\n  [\"\u0493\", null],\n  [\"\u0430\", null],\n  [\"\u043d\u044b\", null],\n  [\"\u2581\u043d\u0435\", \"\u4e3b\u8981\"],\n  [\"\u0433\u0456\", null],\n  [\"\u0437\u0456\", null],\n  [\"\u043d\u0435\u043d\", null],\n  [\"\u2581\u0431\u0440\u0438\", \"\u82f1\u56fd\"],\n  [\"\u0442\u0430\u043d\", null],\n  [\"\u0434\u044b\", null],\n  [\"\u049b\", null],\n  [\"\u2581\u0444\", \"\u82f1\u9551\"],\n  [\"\u0443\u043d\", null],\n  [\"\u0442\", null],\n  [\"\u2581\u0436\", \"\u548c\"],\n  [\"\u04d9\", null],\n  [\"\u043d\u0435\", null],\n  [\"\u2581\u043a\", \"\u82cf\u8054\"],\n  [\"\u0435\", null],\n  [\"\u04a3\", null],\n  [\"\u0435\", null],\n  [\"\u0441\u0442\u0456\", null],\n  [\"\u043a\", null],\n  [\"\u2581\u0440\u0443\", \"\u5362\u5e03\"],\n  [\"\u0431\", null],\n  [\"\u043b\u044c\", null],\n  [\"\u2581\u0430\", \"\u4e4b\u95f4\"],\n  [\"\u0440\u0430\", null],\n  [\"\u0441\u044b\", null],\n  [\"\u043d\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u2581\u0431\", \"\u5206\u914d\"],\n  [\"\u04e9\", null],\n  [\"\u043b\u0456\", null],\n  [\"\u043d\", null],\n  [\"\u0434\u0456\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u049a\u0430\u043b\u0493\u0430\u043d\u044b \u043d\u0435\u0433\u0456\u0437\u0456\u043d\u0435\u043d \u0431\u0440\u0438\u0442\u0430\u043d\u0434\u044b\u049b \u0444\u0443\u043d\u0442 \u0436\u04d9\u043d\u0435 \u043a\u0435\u04a3\u0435\u0441\u0442\u0456\u043a \u0440\u0443\u0431\u043b\u044c \u0430\u0440\u0430\u0441\u044b\u043d\u0434\u0430 \u0431\u04e9\u043b\u0456\u043d\u0434\u0456.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '\u049a', '\u0430\u043b', '\u0493', '\u0430', '\u043d\u044b', '\u2581\u043d\u0435', '\u0433\u0456', '\u0437\u0456', '\u043d\u0435\u043d', '\u2581\u0431\u0440\u0438', '\u0442\u0430\u043d', '\u0434\u044b', '\u049b', '\u2581\u0444', '\u0443\u043d', '\u0442', '\u2581\u0436', '\u04d9', '\u043d\u0435', '\u2581\u043a', '\u0435', '\u04a3', '\u0435', '\u0441\u0442\u0456', '\u043a', '\u2581\u0440\u0443', '\u0431', '\u043b\u044c', '\u2581\u0430', '\u0440\u0430', '\u0441\u044b', '\u043d', '\u0434\u0430', '\u2581\u0431', '\u04e9', '\u043b\u0456', '\u043d', '\u0434\u0456', '.']"}], "[\n[\"\u2581\", null],\n[\"\u049a\", \"\u041e\u0441\u0442\u0430\u043b\u044c\u043d\u043e\u0435\"],\n[\"\u0430\u043b\", null],\n[\"\u0493\", null],\n[\"\u0430\", null],\n[\"\u043d\u044b\", null],\n[\"\u2581\u043d\u0435\", \"\u0432\"],\n[\"\u0433\u0456\", null],\n[\"\u0437\u0456\", null],\n[\"\u043d\u0435\u043d\", null],\n[\"\u2581\u0431\u0440\u0438\", \"\u0431\u0440\u0438\u0442\u0430\u043d\u0441\u043a\u0438\u043c\"],\n[\"\u0442\u0430\u043d\", null],\n[\"\u0434\u044b\", null],\n[\"\u049b\", null],\n[\"\u2581\u0444\", \"\u0444\u0443\u043d\u0442\u043e\u043c\"],\n[\"\u0443\u043d\", null],\n[\"\u0442\", null],\n[\"\u2581\u0436\", \"\u0438\"],\n[\"\u04d9\", null],\n[\"\u043d\u0435\", null],\n[\"\u2581\u043a\", \"\u0441\u043e\u0432\u0435\u0442\u0441\u043a\u0438\u043c\"],\n[\"\u0435\", null],\n[\"\u04a3\", null],\n[\"\u0435\", null],\n[\"\u0441\u0442\u0456\", null],\n[\"\u043a\", null],\n[\"\u2581\u0440\u0443\", \"\u0440\u0443\u0431\u043b\u0435\u043c\"],\n[\"\u0431\", null],\n[\"\u043b\u044c\", null],\n[\"\u2581\u0430\", \"\u043c\u0435\u0436\u0434\u0443\"],\n[\"\u0440\u0430\", null],\n[\"\u0441\u044b\", null],\n[\"\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u2581\u0431\", \"\u0440\u0430\u0437\u0434\u0435\u043b\u0438\u043b\u043e\u0441\u044c\"],\n[\"\u04e9\", null],\n[\"\u043b\u0456\", null],\n[\"\u043d\", null],\n[\"\u0434\u0456\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u0411\u0456\u0440\u0430\u049b \u0414\u0421\u04b0-\u043d\u044b\u04a3 \u049b\u0430\u0439\u0442\u0430 \u0431\u0430\u0441 \u043a\u04e9\u0442\u0435\u0440\u0443\u0456\u043d\u0435 \u043c\u04af\u043c\u043a\u0456\u043d\u0434\u0456\u043a \u0431\u0435\u0440\u0435\u0442\u0456\u043d \u0431\u0430\u0441\u0442\u044b \u04af\u0448 \u04e9\u0437\u0433\u0435\u0440\u0456\u0441 \u0436\u04d9\u043d\u0435 \u04b1\u0439\u044b\u043c\u043d\u044b\u04a3 \u043a\u04e9\u043f\u0436\u0430\u049b\u0442\u044b \u0441\u0438\u043f\u0430\u0442\u044b \u0430\u049b\u044b\u0440 \u0441\u043e\u04a3\u044b\u043d\u0434\u0430 \u043c\u04af\u043b\u0434\u0435 \u0431\u0430\u0441\u049b\u0430 \u043d\u04d9\u0442\u0438\u0436\u0435\u043b\u0435\u0440\u0433\u0435 \u0430\u043b\u044b\u043f \u043a\u0435\u043b\u0443\u0456 \u043c\u04af\u043c\u043a\u0456\u043d.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0411', '\u0456', '\u0440\u0430', '\u049b', '\u2581\u0414', '\u0421', '<0xD2>', '<0xB0>', '-', '\u043d\u044b', '\u04a3', '\u2581', '\u049b', '\u0430', '\u0439', '\u0442\u0430', '\u2581\u0431\u0430', '\u0441', '\u2581\u043a', '\u04e9', '\u0442\u0435', '\u0440\u0443', '\u0456', '\u043d\u0435', '\u2581\u043c', '\u04af', '\u043c', '\u043a\u0456', '\u043d', '\u0434\u0456', '\u043a', '\u2581\u0431\u0435\u0440\u0435', '\u0442\u0456', '\u043d', '\u2581\u0431\u0430', '\u0441\u0442\u044b', '\u2581', '\u04af', '\u0448', '\u2581', '\u04e9', '\u0437', '\u0433', '\u0435\u0440', '\u0456\u0441', '\u2581\u0436', '\u04d9', '\u043d\u0435', '\u2581', '\u04b1', '\u0439', '\u044b', '\u043c', '\u043d\u044b', '\u04a3', '\u2581\u043a', '\u04e9', '\u043f', '\u0436\u0430', '\u049b', '\u0442\u044b', '\u2581\u0441\u0438', '\u043f\u0430', '\u0442\u044b', '\u2581\u0430', '\u049b', '\u044b', '\u0440', '\u2581\u0441\u043e', '\u04a3', '\u044b', '\u043d', '\u0434\u0430', '\u2581\u043c', '\u04af', '\u043b', '\u0434\u0435', '\u2581\u0431\u0430', '\u0441', '\u049b', '\u0430', '\u2581\u043d', '\u04d9', '\u0442\u0438', '\u0436\u0435', '\u043b\u0435', '\u0440', '\u0433\u0435', '\u2581\u0430', '\u043b\u044b', '\u043f', '\u2581\u043a', '\u0435', '\u043b\u0443', '\u0456', '\u2581\u043c', '\u04af', '\u043c', '\u043a\u0456', '\u043d', '.']"}], "[\n  [\"\u2581\u0411\", \"However\"],\n  [\"\u0456\", null],\n  [\"\u0440\u0430\", null],\n  [\"\u049b\", null],\n  [\"\u2581\u0414\", \"WTO\"],\n  [\"\u0421\", null],\n  [\"<0xD2>\", null],\n  [\"<0xB0>\", null],\n  [\"-\", null],\n  [\"\u043d\u044b\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\", null],\n  [\"\u049b\", \"resurgence\"],\n  [\"\u0430\", null],\n  [\"\u0439\", null],\n  [\"\u0442\u0430\", null],\n  [\"\u2581\u0431\u0430\", null],\n  [\"\u0441\", null],\n  [\"\u2581\u043a\", null],\n  [\"\u04e9\", null],\n  [\"\u0442\u0435\", null],\n  [\"\u0440\u0443\", null],\n  [\"\u0456\", null],\n  [\"\u043d\u0435\", null],\n  [\"\u2581\u043c\", \"possibility\"],\n  [\"\u04af\", null],\n  [\"\u043c\", null],\n  [\"\u043a\u0456\", null],\n  [\"\u043d\", null],\n  [\"\u0434\u0456\", null],\n  [\"\u043a\", null],\n  [\"\u2581\u0431\u0435\u0440\u0435\", \"giving\"],\n  [\"\u0442\u0456\", null],\n  [\"\u043d\", null],\n  [\"\u2581\u0431\u0430\", \"main\"],\n  [\"\u0441\u0442\u044b\", null],\n  [\"\u2581\", null],\n  [\"\u04af\", \"three\"],\n  [\"\u0448\", null],\n  [\"\u2581\", null],\n  [\"\u04e9\", \"changes\"],\n  [\"\u0437\", null],\n  [\"\u0433\", null],\n  [\"\u0435\u0440\", null],\n  [\"\u0456\u0441\", null],\n  [\"\u2581\u0436\", \"and\"],\n  [\"\u04d9\", null],\n  [\"\u043d\u0435\", null],\n  [\"\u2581\", null],\n  [\"\u04b1\", \"organization's\"],\n  [\"\u0439\", null],\n  [\"\u044b\", null],\n  [\"\u043c\", null],\n  [\"\u043d\u044b\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\u043a\", \"multilateral\"],\n  [\"\u04e9\", null],\n  [\"\u043f\", null],\n  [\"\u0436\u0430\", null],\n  [\"\u049b\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u2581\u0441\u0438\", \"nature\"],\n  [\"\u043f\u0430\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u2581\u0430\", \"ultimately\"],\n  [\"\u049b\", null],\n  [\"\u044b\", null],\n  [\"\u0440\", null],\n  [\"\u2581\u0441\u043e\", null],\n  [\"\u04a3\", null],\n  [\"\u044b\", null],\n  [\"\u043d\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u2581\u043c\", \"completely\"],\n  [\"\u04af\", null],\n  [\"\u043b\", null],\n  [\"\u0434\u0435\", null],\n  [\"\u2581\u0431\u0430\", \"different\"],\n  [\"\u0441\", null],\n  [\"\u049b\", null],\n  [\"\u0430\", null],\n  [\"\u2581\u043d\", \"results\"],\n  [\"\u04d9\", null],\n  [\"\u0442\u0438\", null],\n  [\"\u0436\u0435\", null],\n  [\"\u043b\u0435\", null],\n  [\"\u0440\", null],\n  [\"\u0433\u0435\", null],\n  [\"\u2581\u0430\", \"lead\"],\n  [\"\u043b\u044b\", null],\n  [\"\u043f\", null],\n  [\"\u2581\u043a\", null],\n  [\"\u0435\", null],\n  [\"\u043b\u0443\", null],\n  [\"\u0456\", null],\n  [\"\u2581\u043c\", \"may\"],\n  [\"\u04af\", null],\n  [\"\u043c\", null],\n  [\"\u043a\u0456\", null],\n  [\"\u043d\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u0411\u0456\u0440\u0430\u049b \u0414\u0421\u04b0-\u043d\u044b\u04a3 \u049b\u0430\u0439\u0442\u0430 \u0431\u0430\u0441 \u043a\u04e9\u0442\u0435\u0440\u0443\u0456\u043d\u0435 \u043c\u04af\u043c\u043a\u0456\u043d\u0434\u0456\u043a \u0431\u0435\u0440\u0435\u0442\u0456\u043d \u0431\u0430\u0441\u0442\u044b \u04af\u0448 \u04e9\u0437\u0433\u0435\u0440\u0456\u0441 \u0436\u04d9\u043d\u0435 \u04b1\u0439\u044b\u043c\u043d\u044b\u04a3 \u043a\u04e9\u043f\u0436\u0430\u049b\u0442\u044b \u0441\u0438\u043f\u0430\u0442\u044b \u0430\u049b\u044b\u0440 \u0441\u043e\u04a3\u044b\u043d\u0434\u0430 \u043c\u04af\u043b\u0434\u0435 \u0431\u0430\u0441\u049b\u0430 \u043d\u04d9\u0442\u0438\u0436\u0435\u043b\u0435\u0440\u0433\u0435 \u0430\u043b\u044b\u043f \u043a\u0435\u043b\u0443\u0456 \u043c\u04af\u043c\u043a\u0456\u043d.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0411', '\u0456', '\u0440\u0430', '\u049b', '\u2581\u0414', '\u0421', '<0xD2>', '<0xB0>', '-', '\u043d\u044b', '\u04a3', '\u2581', '\u049b', '\u0430', '\u0439', '\u0442\u0430', '\u2581\u0431\u0430', '\u0441', '\u2581\u043a', '\u04e9', '\u0442\u0435', '\u0440\u0443', '\u0456', '\u043d\u0435', '\u2581\u043c', '\u04af', '\u043c', '\u043a\u0456', '\u043d', '\u0434\u0456', '\u043a', '\u2581\u0431\u0435\u0440\u0435', '\u0442\u0456', '\u043d', '\u2581\u0431\u0430', '\u0441\u0442\u044b', '\u2581', '\u04af', '\u0448', '\u2581', '\u04e9', '\u0437', '\u0433', '\u0435\u0440', '\u0456\u0441', '\u2581\u0436', '\u04d9', '\u043d\u0435', '\u2581', '\u04b1', '\u0439', '\u044b', '\u043c', '\u043d\u044b', '\u04a3', '\u2581\u043a', '\u04e9', '\u043f', '\u0436\u0430', '\u049b', '\u0442\u044b', '\u2581\u0441\u0438', '\u043f\u0430', '\u0442\u044b', '\u2581\u0430', '\u049b', '\u044b', '\u0440', '\u2581\u0441\u043e', '\u04a3', '\u044b', '\u043d', '\u0434\u0430', '\u2581\u043c', '\u04af', '\u043b', '\u0434\u0435', '\u2581\u0431\u0430', '\u0441', '\u049b', '\u0430', '\u2581\u043d', '\u04d9', '\u0442\u0438', '\u0436\u0435', '\u043b\u0435', '\u0440', '\u0433\u0435', '\u2581\u0430', '\u043b\u044b', '\u043f', '\u2581\u043a', '\u0435', '\u043b\u0443', '\u0456', '\u2581\u043c', '\u04af', '\u043c', '\u043a\u0456', '\u043d', '.']"}], "[\n  [\"\u2581\u0411\", \"\u4f46\u662f\"],\n  [\"\u0456\", null],\n  [\"\u0440\u0430\", null],\n  [\"\u049b\", null],\n  [\"\u2581\u0414\", \"\u4e16\u8d38\u7ec4\u7ec7\"],\n  [\"\u0421\", null],\n  [\"<0xD2>\", null],\n  [\"<0xB0>\", null],\n  [\"-\", null],\n  [\"\u043d\u044b\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\", null],\n  [\"\u049b\", \"\u590d\u82cf\"],\n  [\"\u0430\", null],\n  [\"\u0439\", null],\n  [\"\u0442\u0430\", null],\n  [\"\u2581\u0431\u0430\", null],\n  [\"\u0441\", null],\n  [\"\u2581\u043a\", null],\n  [\"\u04e9\", null],\n  [\"\u0442\u0435\", null],\n  [\"\u0440\u0443\", null],\n  [\"\u0456\", null],\n  [\"\u043d\u0435\", null],\n  [\"\u2581\u043c\", \"\u53ef\u80fd\u6027\"],\n  [\"\u04af\", null],\n  [\"\u043c\", null],\n  [\"\u043a\u0456\", null],\n  [\"\u043d\", null],\n  [\"\u0434\u0456\", null],\n  [\"\u043a\", null],\n  [\"\u2581\u0431\u0435\u0440\u0435\", \"\u7ed9\u4e88\"],\n  [\"\u0442\u0456\", null],\n  [\"\u043d\", null],\n  [\"\u2581\u0431\u0430\", \"\u4e3b\u8981\"],\n  [\"\u0441\u0442\u044b\", null],\n  [\"\u2581\", null],\n  [\"\u04af\", \"\u4e09\"],\n  [\"\u0448\", null],\n  [\"\u2581\", null],\n  [\"\u04e9\", \"\u53d8\u5316\"],\n  [\"\u0437\", null],\n  [\"\u0433\", null],\n  [\"\u0435\u0440\", null],\n  [\"\u0456\u0441\", null],\n  [\"\u2581\u0436\", \"\u548c\"],\n  [\"\u04d9\", null],\n  [\"\u043d\u0435\", null],\n  [\"\u2581\", null],\n  [\"\u04b1\", \"\u7ec4\u7ec7\"],\n  [\"\u0439\", null],\n  [\"\u044b\", null],\n  [\"\u043c\", null],\n  [\"\u043d\u044b\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\u043a\", \"\u591a\u8fb9\"],\n  [\"\u04e9\", null],\n  [\"\u043f\", null],\n  [\"\u0436\u0430\", null],\n  [\"\u049b\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u2581\u0441\u0438\", \"\u6027\u8d28\"],\n  [\"\u043f\u0430\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u2581\u0430\", \"\u6700\u7ec8\"],\n  [\"\u049b\", null],\n  [\"\u044b\", null],\n  [\"\u0440\", null],\n  [\"\u2581\u0441\u043e\", null],\n  [\"\u04a3\", null],\n  [\"\u044b\", null],\n  [\"\u043d\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u2581\u043c\", \"\u53ef\u80fd\"],\n  [\"\u04af\", null],\n  [\"\u043b\", null],\n  [\"\u0434\u0435\", null],\n  [\"\u2581\u0431\u0430\", \"\u5bfc\u81f4\"],\n  [\"\u0441\", null],\n  [\"\u049b\", null],\n  [\"\u0430\", null],\n  [\"\u2581\u043d\", \"\u5b8c\u5168\"],\n  [\"\u04d9\", null],\n  [\"\u0442\u0438\", \"\u4e0d\u540c\"],\n  [\"\u0436\u0435\", null],\n  [\"\u043b\u0435\", null],\n  [\"\u0440\", null],\n  [\"\u0433\u0435\", null],\n  [\"\u2581\u0430\", null],\n  [\"\u043b\u044b\", null],\n  [\"\u043f\", null],\n  [\"\u2581\u043a\", null],\n  [\"\u0435\", null],\n  [\"\u043b\u0443\", null],\n  [\"\u0456\", null],\n  [\"\u2581\u043c\", null],\n  [\"\u04af\", null],\n  [\"\u043c\", null],\n  [\"\u043a\u0456\", null],\n  [\"\u043d\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u0411\u0456\u0440\u0430\u049b \u0414\u0421\u04b0-\u043d\u044b\u04a3 \u049b\u0430\u0439\u0442\u0430 \u0431\u0430\u0441 \u043a\u04e9\u0442\u0435\u0440\u0443\u0456\u043d\u0435 \u043c\u04af\u043c\u043a\u0456\u043d\u0434\u0456\u043a \u0431\u0435\u0440\u0435\u0442\u0456\u043d \u0431\u0430\u0441\u0442\u044b \u04af\u0448 \u04e9\u0437\u0433\u0435\u0440\u0456\u0441 \u0436\u04d9\u043d\u0435 \u04b1\u0439\u044b\u043c\u043d\u044b\u04a3 \u043a\u04e9\u043f\u0436\u0430\u049b\u0442\u044b \u0441\u0438\u043f\u0430\u0442\u044b \u0430\u049b\u044b\u0440 \u0441\u043e\u04a3\u044b\u043d\u0434\u0430 \u043c\u04af\u043b\u0434\u0435 \u0431\u0430\u0441\u049b\u0430 \u043d\u04d9\u0442\u0438\u0436\u0435\u043b\u0435\u0440\u0433\u0435 \u0430\u043b\u044b\u043f \u043a\u0435\u043b\u0443\u0456 \u043c\u04af\u043c\u043a\u0456\u043d.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0411', '\u0456', '\u0440\u0430', '\u049b', '\u2581\u0414', '\u0421', '<0xD2>', '<0xB0>', '-', '\u043d\u044b', '\u04a3', '\u2581', '\u049b', '\u0430', '\u0439', '\u0442\u0430', '\u2581\u0431\u0430', '\u0441', '\u2581\u043a', '\u04e9', '\u0442\u0435', '\u0440\u0443', '\u0456', '\u043d\u0435', '\u2581\u043c', '\u04af', '\u043c', '\u043a\u0456', '\u043d', '\u0434\u0456', '\u043a', '\u2581\u0431\u0435\u0440\u0435', '\u0442\u0456', '\u043d', '\u2581\u0431\u0430', '\u0441\u0442\u044b', '\u2581', '\u04af', '\u0448', '\u2581', '\u04e9', '\u0437', '\u0433', '\u0435\u0440', '\u0456\u0441', '\u2581\u0436', '\u04d9', '\u043d\u0435', '\u2581', '\u04b1', '\u0439', '\u044b', '\u043c', '\u043d\u044b', '\u04a3', '\u2581\u043a', '\u04e9', '\u043f', '\u0436\u0430', '\u049b', '\u0442\u044b', '\u2581\u0441\u0438', '\u043f\u0430', '\u0442\u044b', '\u2581\u0430', '\u049b', '\u044b', '\u0440', '\u2581\u0441\u043e', '\u04a3', '\u044b', '\u043d', '\u0434\u0430', '\u2581\u043c', '\u04af', '\u043b', '\u0434\u0435', '\u2581\u0431\u0430', '\u0441', '\u049b', '\u0430', '\u2581\u043d', '\u04d9', '\u0442\u0438', '\u0436\u0435', '\u043b\u0435', '\u0440', '\u0433\u0435', '\u2581\u0430', '\u043b\u044b', '\u043f', '\u2581\u043a', '\u0435', '\u043b\u0443', '\u0456', '\u2581\u043c', '\u04af', '\u043c', '\u043a\u0456', '\u043d', '.']"}], "[\n  [\"\u2581\u0411\", \"\u041d\u043e\"],\n  [\"\u0456\", null],\n  [\"\u0440\u0430\", null],\n  [\"\u049b\", null],\n  [\"\u2581\u0414\", \"\u0412\u0422\u041e\"],\n  [\"\u0421\", null],\n  [\"<0xD2>\", null],\n  [\"<0xB0>\", null],\n  [\"-\", null],\n  [\"\u043d\u044b\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\", null],\n  [\"\u049b\", \"\u0432\u043e\u0437\u0440\u043e\u0436\u0434\u0435\u043d\u0438\u0435\"],\n  [\"\u0430\", null],\n  [\"\u0439\", null],\n  [\"\u0442\u0430\", null],\n  [\"\u2581\u0431\u0430\", null],\n  [\"\u0441\", null],\n  [\"\u2581\u043a\", null],\n  [\"\u04e9\", null],\n  [\"\u0442\u0435\", null],\n  [\"\u0440\u0443\", null],\n  [\"\u0456\", null],\n  [\"\u043d\u0435\", null],\n  [\"\u2581\u043c\", \"\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c\"],\n  [\"\u04af\", null],\n  [\"\u043c\", null],\n  [\"\u043a\u0456\", null],\n  [\"\u043d\", null],\n  [\"\u0434\u0456\", null],\n  [\"\u043a\", null],\n  [\"\u2581\u0431\u0435\u0440\u0435\", \"\u0434\u0430\u044e\u0449\u0438\u0435\"],\n  [\"\u0442\u0456\", null],\n  [\"\u043d\", null],\n  [\"\u2581\u0431\u0430\", \"\u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0435\"],\n  [\"\u0441\u0442\u044b\", null],\n  [\"\u2581\", null],\n  [\"\u04af\", \"\u0442\u0440\u0438\"],\n  [\"\u0448\", null],\n  [\"\u2581\", null],\n  [\"\u04e9\", \"\u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f\"],\n  [\"\u0437\", null],\n  [\"\u0433\", null],\n  [\"\u0435\u0440\", null],\n  [\"\u0456\u0441\", null],\n  [\"\u2581\u0436\", \"\u0438\"],\n  [\"\u04d9\", null],\n  [\"\u043d\u0435\", null],\n  [\"\u2581\", null],\n  [\"\u04b1\", \"\u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u0438\"],\n  [\"\u0439\", null],\n  [\"\u044b\", null],\n  [\"\u043c\", null],\n  [\"\u043d\u044b\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\u043a\", \"\u043c\u043d\u043e\u0433\u043e\u0441\u0442\u043e\u0440\u043e\u043d\u043d\u0438\u0439\"],\n  [\"\u04e9\", null],\n  [\"\u043f\", null],\n  [\"\u0436\u0430\", null],\n  [\"\u049b\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u2581\u0441\u0438\", \"\u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\"],\n  [\"\u043f\u0430\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u2581\u0430\", \"\u0432\"],\n  [\"\u049b\", null],\n  [\"\u044b\", null],\n  [\"\u0440\", null],\n  [\"\u2581\u0441\u043e\", \"\u043a\u043e\u043d\u0435\u0447\u043d\u043e\u043c\"],\n  [\"\u04a3\", null],\n  [\"\u044b\", null],\n  [\"\u043d\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u2581\u043c\", \"\u0441\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u043d\u043e\"],\n  [\"\u04af\", null],\n  [\"\u043b\", null],\n  [\"\u0434\u0435\", null],\n  [\"\u2581\u0431\u0430\", \"\u0434\u0440\u0443\u0433\u0438\u043c\"],\n  [\"\u0441\", null],\n  [\"\u049b\", null],\n  [\"\u0430\", null],\n  [\"\u2581\u043d\", \"\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u043c\"],\n  [\"\u04d9\", null],\n  [\"\u0442\u0438\", null],\n  [\"\u0436\u0435\", null],\n  [\"\u043b\u0435\", null],\n  [\"\u0440\", null],\n  [\"\u0433\u0435\", null],\n  [\"\u2581\u0430\", \"\u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438\"],\n  [\"\u043b\u044b\", null],\n  [\"\u043f\", null],\n  [\"\u2581\u043a\", null],\n  [\"\u0435\", null],\n  [\"\u043b\u0443\", null],\n  [\"\u0456\", null],\n  [\"\u2581\u043c\", \"\u043c\u043e\u0436\u0435\u0442\"],\n  [\"\u04af\", null],\n  [\"\u043c\", null],\n  [\"\u043a\u0456\", null],\n  [\"\u043d\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u0421\u043e\u04a3\u0493\u044b \u043e\u043d \u0436\u044b\u043b \u0456\u0448\u0456\u043d\u0434\u0435 \u0442\u0440\u0430\u043d\u0441\u0448\u0435\u043a\u0430\u0440\u0430\u043b\u044b\u049b \u043a\u0430\u043f\u0438\u0442\u0430\u043b \u0430\u0493\u044b\u043d\u0434\u0430\u0440\u044b\u043d\u044b\u04a3 \u0436\u0430\u043b\u043f\u044b \u049b\u04b1\u043d\u044b 65% -\u0493\u0430 \u0442\u04e9\u043c\u0435\u043d\u0434\u0435\u0434\u0456, \u0431\u04b1\u043b \u04d9\u0441\u0456\u0440\u0435\u0441\u0435 \u0431\u0430\u043d\u043a \u0441\u0435\u043a\u0442\u043e\u0440\u044b\u043d\u0434\u0430 \u0445\u0430\u043b\u044b\u049b\u0430\u0440\u0430\u043b\u044b\u049b \u0431\u0435\u043b\u0441\u0435\u043d\u0434\u0456\u043b\u0456\u043a\u0442\u0456\u04a3 \u0442\u04e9\u043c\u0435\u043d\u0434\u0435\u0443\u0456\u043d\u0435\u043d \u0431\u043e\u043b\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0421\u043e', '\u04a3', '\u0493', '\u044b', '\u2581\u043e\u043d', '\u2581\u0436', '\u044b', '\u043b', '\u2581\u0456', '\u0448', '\u0456\u043d', '\u0434\u0435', '\u2581\u0442\u0440\u0430\u043d', '\u0441', '\u0448\u0435', '\u043a\u0430', '\u0440\u0430', '\u043b\u044b', '\u049b', '\u2581\u043a\u0430', '\u043f\u0438', '\u0442\u0430\u043b', '\u2581\u0430', '\u0493', '\u044b', '\u043d', '\u0434\u0430', '\u0440\u044b', '\u043d\u044b', '\u04a3', '\u2581', '\u0436\u0430', '\u043b', '\u043f\u044b', '\u2581', '\u049b', '\u04b1', '\u043d\u044b', '\u2581', '6', '5', '%', '\u2581-', '\u0493', '\u0430', '\u2581\u0442', '\u04e9', '\u043c\u0435\u043d', '\u0434\u0435', '\u0434\u0456', ',', '\u2581\u0431', '\u04b1', '\u043b', '\u2581', '\u04d9', '\u0441\u0456', '\u0440\u0435', '\u0441\u0435', '\u2581\u0431', '\u0430\u043d', '\u043a', '\u2581\u0441\u0435', '\u043a\u0442\u043e', '\u0440\u044b', '\u043d', '\u0434\u0430', '\u2581\u0445\u0430', '\u043b\u044b', '\u049b', '\u0430', '\u0440\u0430', '\u043b\u044b', '\u049b', '\u2581\u0431\u0435', '\u043b', '\u0441\u0435\u043d', '\u0434\u0456', '\u043b\u0456', '\u043a\u0442', '\u0456', '\u04a3', '\u2581\u0442', '\u04e9', '\u043c\u0435\u043d', '\u0434\u0435', '\u0443', '\u0456', '\u043d\u0435\u043d', '\u2581\u0431\u043e', '\u043b', '\u0434\u044b', '.']"}], "[\n[\"\u2581\u0421\u043e\", \"Over\"],\n[\"\u04a3\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u2581\u043e\u043d\", \"ten\"],\n[\"\u2581\u0436\", null],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\"\u2581\u0456\", null],\n[\"\u0448\", null],\n[\"\u0456\u043d\", null],\n[\"\u0434\u0435\", null],\n[\"\u2581\u0442\u0440\u0430\u043d\", \"cross\"],\n[\"\u0441\", null],\n[\"\u0448\u0435\", null],\n[\"\u043a\u0430\", null],\n[\"\u0440\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u049b\", null],\n[\"\u2581\u043a\u0430\", \"capital\"],\n[\"\u043f\u0438\", null],\n[\"\u0442\u0430\u043b\", null],\n[\"\u2581\u0430\", \"flows\"],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u0440\u044b\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\", null],\n[\"\u0436\u0430\", \"total\"],\n[\"\u043b\", null],\n[\"\u043f\u044b\", null],\n[\"\u2581\", null],\n[\"\u049b\", null],\n[\"\u04b1\", null],\n[\"\u043d\u044b\", null],\n[\"\u2581\", null],\n[\"6\", null],\n[\"5\", null],\n[\"%\", null],\n[\"\u2581-\", null],\n[\"\u0493\", null],\n[\"\u0430\", null],\n[\"\u2581\u0442\", \"decreased\"],\n[\"\u04e9\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u0434\u0435\", null],\n[\"\u0434\u0456\", null],\n[\",\", null],\n[\"\u2581\u0431\", \"which\"],\n[\"\u04b1\", null],\n[\"\u043b\", null],\n[\"\u2581\", null],\n[\"\u04d9\", \"especially\"],\n[\"\u0441\u0456\", null],\n[\"\u0440\u0435\", null],\n[\"\u0441\u0435\", null],\n[\"\u2581\u0431\", \"bank\"],\n[\"\u0430\u043d\", null],\n[\"\u043a\", null],\n[\"\u2581\u0441\u0435\", \"sector\"],\n[\"\u043a\u0442\u043e\", null],\n[\"\u0440\u044b\", null],\n[\"\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u2581\u0445\u0430\", \"international\"],\n[\"\u043b\u044b\", null],\n[\"\u049b\", null],\n[\"\u0430\", null],\n[\"\u0440\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u049b\", null],\n[\"\u2581\u0431\u0435\", \"activity\"],\n[\"\u043b\", null],\n[\"\u0441\u0435\u043d\", null],\n[\"\u0434\u0456\", null],\n[\"\u043b\u0456\", null],\n[\"\u043a\u0442\", null],\n[\"\u0456\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0442\", \"decline\"],\n[\"\u04e9\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u0434\u0435\", null],\n[\"\u0443\", null],\n[\"\u0456\", null],\n[\"\u043d\u0435\u043d\", null],\n[\"\u2581\u0431\u043e\", \"was\"],\n[\"\u043b\", null],\n[\"\u0434\u044b\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u0421\u043e\u04a3\u0493\u044b \u043e\u043d \u0436\u044b\u043b \u0456\u0448\u0456\u043d\u0434\u0435 \u0442\u0440\u0430\u043d\u0441\u0448\u0435\u043a\u0430\u0440\u0430\u043b\u044b\u049b \u043a\u0430\u043f\u0438\u0442\u0430\u043b \u0430\u0493\u044b\u043d\u0434\u0430\u0440\u044b\u043d\u044b\u04a3 \u0436\u0430\u043b\u043f\u044b \u049b\u04b1\u043d\u044b 65% -\u0493\u0430 \u0442\u04e9\u043c\u0435\u043d\u0434\u0435\u0434\u0456, \u0431\u04b1\u043b \u04d9\u0441\u0456\u0440\u0435\u0441\u0435 \u0431\u0430\u043d\u043a \u0441\u0435\u043a\u0442\u043e\u0440\u044b\u043d\u0434\u0430 \u0445\u0430\u043b\u044b\u049b\u0430\u0440\u0430\u043b\u044b\u049b \u0431\u0435\u043b\u0441\u0435\u043d\u0434\u0456\u043b\u0456\u043a\u0442\u0456\u04a3 \u0442\u04e9\u043c\u0435\u043d\u0434\u0435\u0443\u0456\u043d\u0435\u043d \u0431\u043e\u043b\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0421\u043e', '\u04a3', '\u0493', '\u044b', '\u2581\u043e\u043d', '\u2581\u0436', '\u044b', '\u043b', '\u2581\u0456', '\u0448', '\u0456\u043d', '\u0434\u0435', '\u2581\u0442\u0440\u0430\u043d', '\u0441', '\u0448\u0435', '\u043a\u0430', '\u0440\u0430', '\u043b\u044b', '\u049b', '\u2581\u043a\u0430', '\u043f\u0438', '\u0442\u0430\u043b', '\u2581\u0430', '\u0493', '\u044b', '\u043d', '\u0434\u0430', '\u0440\u044b', '\u043d\u044b', '\u04a3', '\u2581', '\u0436\u0430', '\u043b', '\u043f\u044b', '\u2581', '\u049b', '\u04b1', '\u043d\u044b', '\u2581', '6', '5', '%', '\u2581-', '\u0493', '\u0430', '\u2581\u0442', '\u04e9', '\u043c\u0435\u043d', '\u0434\u0435', '\u0434\u0456', ',', '\u2581\u0431', '\u04b1', '\u043b', '\u2581', '\u04d9', '\u0441\u0456', '\u0440\u0435', '\u0441\u0435', '\u2581\u0431', '\u0430\u043d', '\u043a', '\u2581\u0441\u0435', '\u043a\u0442\u043e', '\u0440\u044b', '\u043d', '\u0434\u0430', '\u2581\u0445\u0430', '\u043b\u044b', '\u049b', '\u0430', '\u0440\u0430', '\u043b\u044b', '\u049b', '\u2581\u0431\u0435', '\u043b', '\u0441\u0435\u043d', '\u0434\u0456', '\u043b\u0456', '\u043a\u0442', '\u0456', '\u04a3', '\u2581\u0442', '\u04e9', '\u043c\u0435\u043d', '\u0434\u0435', '\u0443', '\u0456', '\u043d\u0435\u043d', '\u2581\u0431\u043e', '\u043b', '\u0434\u044b', '.']"}], "[\n[\"\u2581\u0421\u043e\", \"\u6700\u8fd1\"],\n[\"\u04a3\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u2581\u043e\u043d\", \"\u5341\"],\n[\"\u2581\u0436\", \"\u5e74\"],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\"\u2581\u0456\", \"\u5185\"],\n[\"\u0448\", null],\n[\"\u0456\u043d\", null],\n[\"\u0434\u0435\", null],\n[\"\u2581\u0442\u0440\u0430\u043d\", \"\u8de8\"],\n[\"\u0441\", null],\n[\"\u0448\u0435\", null],\n[\"\u043a\u0430\", null],\n[\"\u0440\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u049b\", null],\n[\"\u2581\u043a\u0430\", \"\u8d44\u672c\"],\n[\"\u043f\u0438\", null],\n[\"\u0442\u0430\u043b\", null],\n[\"\u2581\u0430\", \"\u6d41\"],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u0440\u044b\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\", null],\n[\"\u0436\u0430\", \"\u603b\"],\n[\"\u043b\", null],\n[\"\u043f\u044b\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"\u4ef7\u503c\"],\n[\"\u04b1\", null],\n[\"\u043d\u044b\", null],\n[\"\u2581\", null],\n[\"6\", \"65\"],\n[\"5\", null],\n[\"%\", \"%\"],\n[\"\u2581-\", null],\n[\"\u0493\", \"\u4e0b\u964d\"],\n[\"\u0430\", null],\n[\"\u2581\u0442\", null],\n[\"\u04e9\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u0434\u0435\", null],\n[\"\u0434\u0456\", null],\n[\",\", null],\n[\"\u2581\u0431\", \"\u8fd9\"],\n[\"\u04b1\", null],\n[\"\u043b\", null],\n[\"\u2581\", null],\n[\"\u04d9\", \"\u7279\u522b\"],\n[\"\u0441\u0456\", null],\n[\"\u0440\u0435\", null],\n[\"\u0441\u0435\", null],\n[\"\u2581\u0431\", \"\u94f6\u884c\"],\n[\"\u0430\u043d\", null],\n[\"\u043a\", null],\n[\"\u2581\u0441\u0435\", \"\u90e8\u95e8\"],\n[\"\u043a\u0442\u043e\", null],\n[\"\u0440\u044b\", null],\n[\"\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u2581\u0445\u0430\", \"\u56fd\u9645\"],\n[\"\u043b\u044b\", null],\n[\"\u049b\", null],\n[\"\u0430\", null],\n[\"\u0440\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u049b\", null],\n[\"\u2581\u0431\u0435\", \"\u6d3b\u8dc3\u5ea6\"],\n[\"\u043b\", null],\n[\"\u0441\u0435\u043d\", null],\n[\"\u0434\u0456\", null],\n[\"\u043b\u0456\", null],\n[\"\u043a\u0442\", null],\n[\"\u0456\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0442\", \"\u4e0b\u964d\"],\n[\"\u04e9\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u0434\u0435\", null],\n[\"\u0443\", null],\n[\"\u0456\", null],\n[\"\u043d\u0435\u043d\", null],\n[\"\u2581\u0431\u043e\", \"\u9020\u6210\"],\n[\"\u043b\", null],\n[\"\u0434\u044b\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u0421\u043e\u04a3\u0493\u044b \u043e\u043d \u0436\u044b\u043b \u0456\u0448\u0456\u043d\u0434\u0435 \u0442\u0440\u0430\u043d\u0441\u0448\u0435\u043a\u0430\u0440\u0430\u043b\u044b\u049b \u043a\u0430\u043f\u0438\u0442\u0430\u043b \u0430\u0493\u044b\u043d\u0434\u0430\u0440\u044b\u043d\u044b\u04a3 \u0436\u0430\u043b\u043f\u044b \u049b\u04b1\u043d\u044b 65% -\u0493\u0430 \u0442\u04e9\u043c\u0435\u043d\u0434\u0435\u0434\u0456, \u0431\u04b1\u043b \u04d9\u0441\u0456\u0440\u0435\u0441\u0435 \u0431\u0430\u043d\u043a \u0441\u0435\u043a\u0442\u043e\u0440\u044b\u043d\u0434\u0430 \u0445\u0430\u043b\u044b\u049b\u0430\u0440\u0430\u043b\u044b\u049b \u0431\u0435\u043b\u0441\u0435\u043d\u0434\u0456\u043b\u0456\u043a\u0442\u0456\u04a3 \u0442\u04e9\u043c\u0435\u043d\u0434\u0435\u0443\u0456\u043d\u0435\u043d \u0431\u043e\u043b\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0421\u043e', '\u04a3', '\u0493', '\u044b', '\u2581\u043e\u043d', '\u2581\u0436', '\u044b', '\u043b', '\u2581\u0456', '\u0448', '\u0456\u043d', '\u0434\u0435', '\u2581\u0442\u0440\u0430\u043d', '\u0441', '\u0448\u0435', '\u043a\u0430', '\u0440\u0430', '\u043b\u044b', '\u049b', '\u2581\u043a\u0430', '\u043f\u0438', '\u0442\u0430\u043b', '\u2581\u0430', '\u0493', '\u044b', '\u043d', '\u0434\u0430', '\u0440\u044b', '\u043d\u044b', '\u04a3', '\u2581', '\u0436\u0430', '\u043b', '\u043f\u044b', '\u2581', '\u049b', '\u04b1', '\u043d\u044b', '\u2581', '6', '5', '%', '\u2581-', '\u0493', '\u0430', '\u2581\u0442', '\u04e9', '\u043c\u0435\u043d', '\u0434\u0435', '\u0434\u0456', ',', '\u2581\u0431', '\u04b1', '\u043b', '\u2581', '\u04d9', '\u0441\u0456', '\u0440\u0435', '\u0441\u0435', '\u2581\u0431', '\u0430\u043d', '\u043a', '\u2581\u0441\u0435', '\u043a\u0442\u043e', '\u0440\u044b', '\u043d', '\u0434\u0430', '\u2581\u0445\u0430', '\u043b\u044b', '\u049b', '\u0430', '\u0440\u0430', '\u043b\u044b', '\u049b', '\u2581\u0431\u0435', '\u043b', '\u0441\u0435\u043d', '\u0434\u0456', '\u043b\u0456', '\u043a\u0442', '\u0456', '\u04a3', '\u2581\u0442', '\u04e9', '\u043c\u0435\u043d', '\u0434\u0435', '\u0443', '\u0456', '\u043d\u0435\u043d', '\u2581\u0431\u043e', '\u043b', '\u0434\u044b', '.']"}], "[\n[\"\u2581\u0421\u043e\", \"\u0417\u0430\"],\n[\"\u04a3\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u2581\u043e\u043d\", \"\u0434\u0435\u0441\u044f\u0442\u044c\"],\n[\"\u2581\u0436\", \"\u043b\u0435\u0442\"],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\"\u2581\u0456\", null],\n[\"\u0448\", null],\n[\"\u0456\u043d\", null],\n[\"\u0434\u0435\", null],\n[\"\u2581\u0442\u0440\u0430\u043d\", \"\u0442\u0440\u0430\u043d\u0441\u0433\u0440\u0430\u043d\u0438\u0447\u043d\u044b\u0445\"],\n[\"\u0441\", null],\n[\"\u0448\u0435\", null],\n[\"\u043a\u0430\", null],\n[\"\u0440\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u049b\", null],\n[\"\u2581\u043a\u0430\", \"\u043a\u0430\u043f\u0438\u0442\u0430\u043b\u0430\"],\n[\"\u043f\u0438\", null],\n[\"\u0442\u0430\u043b\", null],\n[\"\u2581\u0430\", \"\u043f\u043e\u0442\u043e\u043a\u043e\u0432\"],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u0440\u044b\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\", null],\n[\"\u0436\u0430\", \"\u043e\u0431\u0449\u0430\u044f\"],\n[\"\u043b\", null],\n[\"\u043f\u044b\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"\u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u044c\"],\n[\"\u04b1\", null],\n[\"\u043d\u044b\", null],\n[\"\u2581\", null],\n[\"6\", null],\n[\"5\", null],\n[\"%\", null],\n[\"\u2581-\", null],\n[\"\u0493\", null],\n[\"\u0430\", null],\n[\"\u2581\u0442\", \"\u0441\u043d\u0438\u0437\u0438\u043b\u0430\u0441\u044c\"],\n[\"\u04e9\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u0434\u0435\", null],\n[\"\u0434\u0456\", null],\n[\",\", null],\n[\"\u2581\u0431\", \"\u044d\u0442\u043e\"],\n[\"\u04b1\", null],\n[\"\u043b\", null],\n[\"\u2581\", null],\n[\"\u04d9\", \"\u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\"],\n[\"\u0441\u0456\", null],\n[\"\u0440\u0435\", null],\n[\"\u0441\u0435\", null],\n[\"\u2581\u0431\", \"\u0431\u0430\u043d\u043a\u043e\u0432\u0441\u043a\u043e\u043c\"],\n[\"\u0430\u043d\", null],\n[\"\u043a\", null],\n[\"\u2581\u0441\u0435\", \"\u0441\u0435\u043a\u0442\u043e\u0440\u0435\"],\n[\"\u043a\u0442\u043e\", null],\n[\"\u0440\u044b\", null],\n[\"\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u2581\u0445\u0430\", \"\u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u043e\u0439\"],\n[\"\u043b\u044b\", null],\n[\"\u049b\", null],\n[\"\u0430\", null],\n[\"\u0440\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u049b\", null],\n[\"\u2581\u0431\u0435\", \"\u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438\"],\n[\"\u043b\", null],\n[\"\u0441\u0435\u043d\", null],\n[\"\u0434\u0456\", null],\n[\"\u043b\u0456\", null],\n[\"\u043a\u0442\", null],\n[\"\u0456\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0442\", \"\u0441\u043d\u0438\u0436\u0435\u043d\u0438\u044f\"],\n[\"\u04e9\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u0434\u0435\", null],\n[\"\u0443\", null],\n[\"\u0456\", null],\n[\"\u043d\u0435\u043d\", null],\n[\"\u2581\u0431\u043e\", \"\u043f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u043e\"],\n[\"\u043b\", null],\n[\"\u0434\u044b\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u041a\u0415\u041c\u0411\u0420\u0418\u0414\u0416 \u2013 \u0422\u0430\u043c\u044b\u0437 \u0430\u0439\u044b\u043d\u044b\u04a3 \u0441\u043e\u04a3\u044b\u043d\u0434\u0430 \u0424\u0440\u0430\u043d\u0446\u0438\u044f \u043f\u0440\u0435\u0437\u0438\u0434\u0435\u043d\u0442\u0456 \u042d\u043c\u043c\u0430\u043d\u0443\u044d\u043b\u044c \u041c\u0430\u043a\u0440\u043e\u043d \u0431\u0438\u043b\u0456\u043a\u0442\u0435 \u04e9\u0437\u0456\u043d \u043a\u04e9\u043a\u043a\u0435 \u043a\u04e9\u0442\u0435\u0440\u0435\u0442\u0456\u043d, \u044f \u0442\u04e9\u043c\u0435\u043d\u0433\u0435 \u049b\u04b1\u043b\u0434\u044b\u0440\u0430\u0442\u0430\u0442\u044b\u043d, \u0435\u04a3 \u0431\u0430\u0441\u0442\u044b\u0441\u044b \u2013 \u0435\u0443\u0440\u043e\u0430\u0439\u043c\u0430\u049b\u0442\u044b\u04a3 \u0431\u043e\u043b\u0430\u0448\u0430\u0493\u044b\u043d \u0430\u0439\u049b\u044b\u043d\u0434\u0430\u0439\u0442\u044b\u043d \u0435\u04a3\u0431\u0435\u043a \u043d\u0430\u0440\u044b\u0493\u044b\u043d \u0440\u0435\u0444\u043e\u0440\u043c\u0430\u043b\u0430\u0443 \u0436\u043e\u0441\u043f\u0430\u0440\u044b\u043d \u0436\u0430\u0440\u0438\u044f\u043b\u0430\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041a', '\u0415', '\u041c', '\u0411', '\u0420', '\u0418', '\u0414', '\u0416', '\u2581\u2013', '\u2581\u0422\u0430', '\u043c\u044b', '\u0437', '\u2581\u0430', '\u0439', '\u044b', '\u043d\u044b', '\u04a3', '\u2581\u0441\u043e', '\u04a3', '\u044b', '\u043d', '\u0434\u0430', '\u2581\u0424\u0440\u0430\u043d', '\u0446\u0438\u044f', '\u2581\u043f\u0440\u0435', '\u0437\u0438', '\u0434', '\u0435\u043d\u0442', '\u0456', '\u2581\u042d', '\u043c', '\u043c\u0430', '\u043d\u0443', '\u044d', '\u043b\u044c', '\u2581\u041c\u0430', '\u043a', '\u0440\u043e\u043d', '\u2581\u0431\u0438', '\u043b\u0456', '\u043a', '\u0442\u0435', '\u2581', '\u04e9', '\u0437\u0456', '\u043d', '\u2581\u043a', '\u04e9', '\u043a', '\u043a\u0435', '\u2581\u043a', '\u04e9', '\u0442\u0435', '\u0440\u0435', '\u0442\u0456', '\u043d', ',', '\u2581\u044f', '\u2581\u0442', '\u04e9', '\u043c\u0435\u043d', '\u0433\u0435', '\u2581', '\u049b', '\u04b1', '\u043b', '\u0434\u044b', '\u0440\u0430', '\u0442\u0430', '\u0442\u044b', '\u043d', ',', '\u2581\u0435', '\u04a3', '\u2581\u0431\u0430', '\u0441\u0442\u044b', '\u0441\u044b', '\u2581\u2013', '\u2581\u0435', '\u0443', '\u0440\u043e', '\u0430', '\u0439', '\u043c\u0430', '\u049b', '\u0442\u044b', '\u04a3', '\u2581\u0431\u043e', '\u043b\u0430', '\u0448\u0430', '\u0493', '\u044b', '\u043d', '\u2581\u0430', '\u0439', '\u049b', '\u044b', '\u043d', '\u0434\u0430', '\u0439', '\u0442\u044b', '\u043d', '\u2581\u0435', '\u04a3', '\u0431\u0435', '\u043a', '\u2581\u043d\u0430', '\u0440\u044b', '\u0493', '\u044b', '\u043d', '\u2581\u0440\u0435', '\u0444\u043e\u0440\u043c\u0430', '\u043b\u0430', '\u0443', '\u2581\u0436', '\u043e', '\u0441', '\u043f\u0430\u0440', '\u044b', '\u043d', '\u2581', '\u0436\u0430', '\u0440\u0438\u044f', '\u043b\u0430', '\u0434\u044b', '.']"}], "[\n  [\"\u2581\u041a\", \"CAMBRIDGE\"],\n  [\"\u0415\", null],\n  [\"\u041c\", null],\n  [\"\u0411\", null],\n  [\"\u0420\", null],\n  [\"\u0418\", null],\n  [\"\u0414\", null],\n  [\"\u0416\", null],\n  [\"\u2581\u2013\", null],\n  [\"\u2581\u0422\u0430\", \"August\"],\n  [\"\u043c\u044b\", null],\n  [\"\u0437\", null],\n  [\"\u2581\u0430\", null],\n  [\"\u0439\", null],\n  [\"\u044b\", null],\n  [\"\u043d\u044b\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\u0441\u043e\", \"end\"],\n  [\"\u04a3\", null],\n  [\"\u044b\", null],\n  [\"\u043d\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u2581\u0424\u0440\u0430\u043d\", \"France\"],\n  [\"\u0446\u0438\u044f\", null],\n  [\"\u2581\u043f\u0440\u0435\", \"President\"],\n  [\"\u0437\u0438\", null],\n  [\"\u0434\", null],\n  [\"\u0435\u043d\u0442\", null],\n  [\"\u0456\", null],\n  [\"\u2581\u042d\", \"Emmanuel\"],\n  [\"\u043c\", null],\n  [\"\u043c\u0430\", null],\n  [\"\u043d\u0443\", null],\n  [\"\u044d\", null],\n  [\"\u043b\u044c\", null],\n  [\"\u2581\u041c\u0430\", \"Macron\"],\n  [\"\u043a\", null],\n  [\"\u0440\u043e\u043d\", null],\n  [\"\u2581\u0431\u0438\", \"power\"],\n  [\"\u043b\u0456\", null],\n  [\"\u043a\", null],\n  [\"\u0442\u0435\", null],\n  [\"\u2581\", null],\n  [\"\u04e9\", \"himself\"],\n  [\"\u0437\u0456\", null],\n  [\"\u043d\", null],\n  [\"\u2581\u043a\", \"elevate\"],\n  [\"\u04e9\", null],\n  [\"\u043a\", null],\n  [\"\u043a\u0435\", null],\n  [\"\u2581\u043a\", \"raise\"],\n  [\"\u04e9\", null],\n  [\"\u0442\u0435\", null],\n  [\"\u0440\u0435\", null],\n  [\"\u0442\u0456\", null],\n  [\"\u043d\", null],\n  [\",\", null],\n  [\"\u2581\u044f\", \"or\"],\n  [\"\u2581\u0442\", \"lower\"],\n  [\"\u04e9\", null],\n  [\"\u043c\u0435\u043d\", null],\n  [\"\u0433\u0435\", null],\n  [\"\u2581\", null],\n  [\"\u049b\", \"bring\"],\n  [\"\u04b1\", null],\n  [\"\u043b\", null],\n  [\"\u0434\u044b\", null],\n  [\"\u0440\u0430\", null],\n  [\"\u0442\u0430\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u043d\", null],\n  [\",\", null],\n  [\"\u2581\u0435\", \"most\"],\n  [\"\u04a3\", null],\n  [\"\u2581\u0431\u0430\", \"importantly\"],\n  [\"\u0441\u0442\u044b\", null],\n  [\"\u0441\u044b\", null],\n  [\"\u2581\u2013\", null],\n  [\"\u2581\u0435\", \"eurozone\"],\n  [\"\u0443\", null],\n  [\"\u0440\u043e\", null],\n  [\"\u0430\", null],\n  [\"\u0439\", null],\n  [\"\u043c\u0430\", null],\n  [\"\u049b\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\u0431\u043e\", \"future\"],\n  [\"\u043b\u0430\", null],\n  [\"\u0448\u0430\", null],\n  [\"\u0493\", null],\n  [\"\u044b\", null],\n  [\"\u043d\", null],\n  [\"\u2581\u0430\", \"determine\"],\n  [\"\u0439\", null],\n  [\"\u049b\", null],\n  [\"\u044b\", null],\n  [\"\u043d\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u0439\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u043d\", null],\n  [\"\u2581\u0435\", \"labor\"],\n  [\"\u04a3\", null],\n  [\"\u0431\u0435\", null],\n  [\"\u043a\", null],\n  [\"\u2581\u043d\u0430\", \"market\"],\n  [\"\u0440\u044b\", null],\n  [\"\u0493\", null],\n  [\"\u044b\", null],\n  [\"\u043d\", null],\n  [\"\u2581\u0440\u0435\", \"reform\"],\n  [\"\u0444\u043e\u0440\u043c\u0430\", null],\n  [\"\u043b\u0430\", null],\n  [\"\u0443\", null],\n  [\"\u2581\u0436\", \"plan\"],\n  [\"\u043e\", null],\n  [\"\u0441\", null],\n  [\"\u043f\u0430\u0440\", null],\n  [\"\u044b\", null],\n  [\"\u043d\", null],\n  [\"\u2581\", null],\n  [\"\u0436\u0430\", \"announced\"],\n  [\"\u0440\u0438\u044f\", null],\n  [\"\u043b\u0430\", null],\n  [\"\u0434\u044b\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u041a\u0415\u041c\u0411\u0420\u0418\u0414\u0416 \u2013 \u0422\u0430\u043c\u044b\u0437 \u0430\u0439\u044b\u043d\u044b\u04a3 \u0441\u043e\u04a3\u044b\u043d\u0434\u0430 \u0424\u0440\u0430\u043d\u0446\u0438\u044f \u043f\u0440\u0435\u0437\u0438\u0434\u0435\u043d\u0442\u0456 \u042d\u043c\u043c\u0430\u043d\u0443\u044d\u043b\u044c \u041c\u0430\u043a\u0440\u043e\u043d \u0431\u0438\u043b\u0456\u043a\u0442\u0435 \u04e9\u0437\u0456\u043d \u043a\u04e9\u043a\u043a\u0435 \u043a\u04e9\u0442\u0435\u0440\u0435\u0442\u0456\u043d, \u044f \u0442\u04e9\u043c\u0435\u043d\u0433\u0435 \u049b\u04b1\u043b\u0434\u044b\u0440\u0430\u0442\u0430\u0442\u044b\u043d, \u0435\u04a3 \u0431\u0430\u0441\u0442\u044b\u0441\u044b \u2013 \u0435\u0443\u0440\u043e\u0430\u0439\u043c\u0430\u049b\u0442\u044b\u04a3 \u0431\u043e\u043b\u0430\u0448\u0430\u0493\u044b\u043d \u0430\u0439\u049b\u044b\u043d\u0434\u0430\u0439\u0442\u044b\u043d \u0435\u04a3\u0431\u0435\u043a \u043d\u0430\u0440\u044b\u0493\u044b\u043d \u0440\u0435\u0444\u043e\u0440\u043c\u0430\u043b\u0430\u0443 \u0436\u043e\u0441\u043f\u0430\u0440\u044b\u043d \u0436\u0430\u0440\u0438\u044f\u043b\u0430\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041a', '\u0415', '\u041c', '\u0411', '\u0420', '\u0418', '\u0414', '\u0416', '\u2581\u2013', '\u2581\u0422\u0430', '\u043c\u044b', '\u0437', '\u2581\u0430', '\u0439', '\u044b', '\u043d\u044b', '\u04a3', '\u2581\u0441\u043e', '\u04a3', '\u044b', '\u043d', '\u0434\u0430', '\u2581\u0424\u0440\u0430\u043d', '\u0446\u0438\u044f', '\u2581\u043f\u0440\u0435', '\u0437\u0438', '\u0434', '\u0435\u043d\u0442', '\u0456', '\u2581\u042d', '\u043c', '\u043c\u0430', '\u043d\u0443', '\u044d', '\u043b\u044c', '\u2581\u041c\u0430', '\u043a', '\u0440\u043e\u043d', '\u2581\u0431\u0438', '\u043b\u0456', '\u043a', '\u0442\u0435', '\u2581', '\u04e9', '\u0437\u0456', '\u043d', '\u2581\u043a', '\u04e9', '\u043a', '\u043a\u0435', '\u2581\u043a', '\u04e9', '\u0442\u0435', '\u0440\u0435', '\u0442\u0456', '\u043d', ',', '\u2581\u044f', '\u2581\u0442', '\u04e9', '\u043c\u0435\u043d', '\u0433\u0435', '\u2581', '\u049b', '\u04b1', '\u043b', '\u0434\u044b', '\u0440\u0430', '\u0442\u0430', '\u0442\u044b', '\u043d', ',', '\u2581\u0435', '\u04a3', '\u2581\u0431\u0430', '\u0441\u0442\u044b', '\u0441\u044b', '\u2581\u2013', '\u2581\u0435', '\u0443', '\u0440\u043e', '\u0430', '\u0439', '\u043c\u0430', '\u049b', '\u0442\u044b', '\u04a3', '\u2581\u0431\u043e', '\u043b\u0430', '\u0448\u0430', '\u0493', '\u044b', '\u043d', '\u2581\u0430', '\u0439', '\u049b', '\u044b', '\u043d', '\u0434\u0430', '\u0439', '\u0442\u044b', '\u043d', '\u2581\u0435', '\u04a3', '\u0431\u0435', '\u043a', '\u2581\u043d\u0430', '\u0440\u044b', '\u0493', '\u044b', '\u043d', '\u2581\u0440\u0435', '\u0444\u043e\u0440\u043c\u0430', '\u043b\u0430', '\u0443', '\u2581\u0436', '\u043e', '\u0441', '\u043f\u0430\u0440', '\u044b', '\u043d', '\u2581', '\u0436\u0430', '\u0440\u0438\u044f', '\u043b\u0430', '\u0434\u044b', '.']"}], "[\n[\"\u2581\u041a\", \"\u5251\"],\n[\"\u0415\", null],\n[\"\u041c\", null],\n[\"\u0411\", null],\n[\"\u0420\", null],\n[\"\u0418\", null],\n[\"\u0414\", null],\n[\"\u0416\", null],\n[\"\u2581\u2013\", null],\n[\"\u2581\u0422\u0430\", \"\u516b\"],\n[\"\u043c\u044b\", null],\n[\"\u0437\", null],\n[\"\u2581\u0430\", null],\n[\"\u0439\", null],\n[\"\u044b\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0441\u043e\", \"\u672b\"],\n[\"\u04a3\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u2581\u0424\u0440\u0430\u043d\", \"\u6cd5\"],\n[\"\u0446\u0438\u044f\", null],\n[\"\u2581\u043f\u0440\u0435\", \"\u603b\"],\n[\"\u0437\u0438\", null],\n[\"\u0434\", null],\n[\"\u0435\u043d\u0442\", null],\n[\"\u0456\", null],\n[\"\u2581\u042d\", \"\u57c3\"],\n[\"\u043c\", null],\n[\"\u043c\u0430\", null],\n[\"\u043d\u0443\", null],\n[\"\u044d\", null],\n[\"\u043b\u044c\", null],\n[\"\u2581\u041c\u0430\", \"\u9a6c\"],\n[\"\u043a\", null],\n[\"\u0440\u043e\u043d\", null],\n[\"\u2581\u0431\u0438\", \"\u6743\"],\n[\"\u043b\u0456\", null],\n[\"\u043a\", null],\n[\"\u0442\u0435\", null],\n[\"\u2581\", null],\n[\"\u04e9\", null],\n[\"\u0437\u0456\", null],\n[\"\u043d\", null],\n[\"\u2581\u043a\", \"\u63d0\"],\n[\"\u04e9\", null],\n[\"\u043a\", null],\n[\"\u043a\u0435\", null],\n[\"\u2581\u043a\", null],\n[\"\u04e9\", null],\n[\"\u0442\u0435\", null],\n[\"\u0440\u0435\", null],\n[\"\u0442\u0456\", null],\n[\"\u043d\", null],\n[\",\", null],\n[\"\u2581\u044f\", \"\u6216\"],\n[\"\u2581\u0442\", \"\u4e0b\"],\n[\"\u04e9\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u0433\u0435\", null],\n[\"\u2581\", null],\n[\"\u049b\", null],\n[\"\u04b1\", null],\n[\"\u043b\", null],\n[\"\u0434\u044b\", null],\n[\"\u0440\u0430\", null],\n[\"\u0442\u0430\", null],\n[\"\u0442\u044b\", null],\n[\"\u043d\", null],\n[\",\", null],\n[\"\u2581\u0435\", \"\u6700\"],\n[\"\u04a3\", null],\n[\"\u2581\u0431\u0430\", \"\u91cd\"],\n[\"\u0441\u0442\u044b\", null],\n[\"\u0441\u044b\", null],\n[\"\u2581\u2013\", null],\n[\"\u2581\u0435\", \"\u6b27\"],\n[\"\u0443\", null],\n[\"\u0440\u043e\", null],\n[\"\u0430\", null],\n[\"\u0439\", null],\n[\"\u043c\u0430\", null],\n[\"\u049b\", null],\n[\"\u0442\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0431\u043e\", \"\u672a\"],\n[\"\u043b\u0430\", null],\n[\"\u0448\u0430\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u2581\u0430\", \"\u51b3\"],\n[\"\u0439\", null],\n[\"\u049b\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u0439\", null],\n[\"\u0442\u044b\", null],\n[\"\u043d\", null],\n[\"\u2581\u0435\", \"\u52b3\"],\n[\"\u04a3\", null],\n[\"\u0431\u0435\", null],\n[\"\u043a\", null],\n[\"\u2581\u043d\u0430\", \"\u5e02\"],\n[\"\u0440\u044b\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u2581\u0440\u0435\", \"\u6539\"],\n[\"\u0444\u043e\u0440\u043c\u0430\", null],\n[\"\u043b\u0430\", null],\n[\"\u0443\", null],\n[\"\u2581\u0436\", \"\u8ba1\"],\n[\"\u043e\", null],\n[\"\u0441\", null],\n[\"\u043f\u0430\u0440\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u2581\", null],\n[\"\u0436\u0430\", \"\u5ba3\"],\n[\"\u0440\u0438\u044f\", null],\n[\"\u043b\u0430\", null],\n[\"\u0434\u044b\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u041a\u0415\u041c\u0411\u0420\u0418\u0414\u0416 \u2013 \u0422\u0430\u043c\u044b\u0437 \u0430\u0439\u044b\u043d\u044b\u04a3 \u0441\u043e\u04a3\u044b\u043d\u0434\u0430 \u0424\u0440\u0430\u043d\u0446\u0438\u044f \u043f\u0440\u0435\u0437\u0438\u0434\u0435\u043d\u0442\u0456 \u042d\u043c\u043c\u0430\u043d\u0443\u044d\u043b\u044c \u041c\u0430\u043a\u0440\u043e\u043d \u0431\u0438\u043b\u0456\u043a\u0442\u0435 \u04e9\u0437\u0456\u043d \u043a\u04e9\u043a\u043a\u0435 \u043a\u04e9\u0442\u0435\u0440\u0435\u0442\u0456\u043d, \u044f \u0442\u04e9\u043c\u0435\u043d\u0433\u0435 \u049b\u04b1\u043b\u0434\u044b\u0440\u0430\u0442\u0430\u0442\u044b\u043d, \u0435\u04a3 \u0431\u0430\u0441\u0442\u044b\u0441\u044b \u2013 \u0435\u0443\u0440\u043e\u0430\u0439\u043c\u0430\u049b\u0442\u044b\u04a3 \u0431\u043e\u043b\u0430\u0448\u0430\u0493\u044b\u043d \u0430\u0439\u049b\u044b\u043d\u0434\u0430\u0439\u0442\u044b\u043d \u0435\u04a3\u0431\u0435\u043a \u043d\u0430\u0440\u044b\u0493\u044b\u043d \u0440\u0435\u0444\u043e\u0440\u043c\u0430\u043b\u0430\u0443 \u0436\u043e\u0441\u043f\u0430\u0440\u044b\u043d \u0436\u0430\u0440\u0438\u044f\u043b\u0430\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041a', '\u0415', '\u041c', '\u0411', '\u0420', '\u0418', '\u0414', '\u0416', '\u2581\u2013', '\u2581\u0422\u0430', '\u043c\u044b', '\u0437', '\u2581\u0430', '\u0439', '\u044b', '\u043d\u044b', '\u04a3', '\u2581\u0441\u043e', '\u04a3', '\u044b', '\u043d', '\u0434\u0430', '\u2581\u0424\u0440\u0430\u043d', '\u0446\u0438\u044f', '\u2581\u043f\u0440\u0435', '\u0437\u0438', '\u0434', '\u0435\u043d\u0442', '\u0456', '\u2581\u042d', '\u043c', '\u043c\u0430', '\u043d\u0443', '\u044d', '\u043b\u044c', '\u2581\u041c\u0430', '\u043a', '\u0440\u043e\u043d', '\u2581\u0431\u0438', '\u043b\u0456', '\u043a', '\u0442\u0435', '\u2581', '\u04e9', '\u0437\u0456', '\u043d', '\u2581\u043a', '\u04e9', '\u043a', '\u043a\u0435', '\u2581\u043a', '\u04e9', '\u0442\u0435', '\u0440\u0435', '\u0442\u0456', '\u043d', ',', '\u2581\u044f', '\u2581\u0442', '\u04e9', '\u043c\u0435\u043d', '\u0433\u0435', '\u2581', '\u049b', '\u04b1', '\u043b', '\u0434\u044b', '\u0440\u0430', '\u0442\u0430', '\u0442\u044b', '\u043d', ',', '\u2581\u0435', '\u04a3', '\u2581\u0431\u0430', '\u0441\u0442\u044b', '\u0441\u044b', '\u2581\u2013', '\u2581\u0435', '\u0443', '\u0440\u043e', '\u0430', '\u0439', '\u043c\u0430', '\u049b', '\u0442\u044b', '\u04a3', '\u2581\u0431\u043e', '\u043b\u0430', '\u0448\u0430', '\u0493', '\u044b', '\u043d', '\u2581\u0430', '\u0439', '\u049b', '\u044b', '\u043d', '\u0434\u0430', '\u0439', '\u0442\u044b', '\u043d', '\u2581\u0435', '\u04a3', '\u0431\u0435', '\u043a', '\u2581\u043d\u0430', '\u0440\u044b', '\u0493', '\u044b', '\u043d', '\u2581\u0440\u0435', '\u0444\u043e\u0440\u043c\u0430', '\u043b\u0430', '\u0443', '\u2581\u0436', '\u043e', '\u0441', '\u043f\u0430\u0440', '\u044b', '\u043d', '\u2581', '\u0436\u0430', '\u0440\u0438\u044f', '\u043b\u0430', '\u0434\u044b', '.']"}], "[\n[\"\u2581\u041a\", \"\u041a\u0415\u041c\u0411\u0420\u0418\u0414\u0416\"],\n[\"\u0415\", null],\n[\"\u041c\", null],\n[\"\u0411\", null],\n[\"\u0420\", null],\n[\"\u0418\", null],\n[\"\u0414\", null],\n[\"\u0416\", null],\n[\"\u2581\u2013\", null],\n[\"\u2581\u0422\u0430\", \"\u0410\u0412\u0413\u0423\u0421\u0422\"],\n[\"\u043c\u044b\", null],\n[\"\u0437\", null],\n[\"\u2581\u0430\", null],\n[\"\u0439\", null],\n[\"\u044b\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0441\u043e\", \"\u041a\u041e\u041d\u0415\u0426\"],\n[\"\u04a3\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u2581\u0424\u0440\u0430\u043d\", \"\u0424\u0420\u0410\u041d\u0426\u0418\u042f\"],\n[\"\u0446\u0438\u044f\", null],\n[\"\u2581\u043f\u0440\u0435\", \"\u041f\u0420\u0415\u0417\u0418\u0414\u0415\u041d\u0422\"],\n[\"\u0437\u0438\", null],\n[\"\u0434\", null],\n[\"\u0435\u043d\u0442\", null],\n[\"\u0456\", null],\n[\"\u2581\u042d\", \"\u042d\u041c\u041c\u0410\u041d\u0423\u042d\u041b\u042c\"],\n[\"\u043c\", null],\n[\"\u043c\u0430\", null],\n[\"\u043d\u0443\", null],\n[\"\u044d\", null],\n[\"\u043b\u044c\", null],\n[\"\u2581\u041c\u0430\", \"\u041c\u0410\u041a\u0420\u041e\u041d\"],\n[\"\u043a\", null],\n[\"\u0440\u043e\u043d\", null],\n[\"\u2581\u0431\u0438\", \"\u0412\u041b\u0410\u0421\u0422\u042c\"],\n[\"\u043b\u0456\", null],\n[\"\u043a\", null],\n[\"\u0442\u0435\", null],\n[\"\u2581\", null],\n[\"\u04e9\", \"\u0421\u0415\u0411\u042f\"],\n[\"\u0437\u0456\", null],\n[\"\u043d\", null],\n[\"\u2581\u043a\", \"\u0412\u041e\u0417\u0412\u042b\u0421\u0418\u0422\u042c\"],\n[\"\u04e9\", null],\n[\"\u043a\", null],\n[\"\u043a\u0435\", null],\n[\"\u2581\u043a\", null],\n[\"\u04e9\", null],\n[\"\u0442\u0435\", null],\n[\"\u0440\u0435\", null],\n[\"\u0442\u0456\", null],\n[\"\u043d\", null],\n[\",\" , null],\n[\"\u2581\u044f\", \"\u0418\u041b\u0418\"],\n[\"\u2581\u0442\", \"\u0421\u041d\u0418\u0417\u0418\u0422\u042c\"],\n[\"\u04e9\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u0433\u0435\", null],\n[\"\u2581\", null],\n[\"\u049b\", null],\n[\"\u04b1\", null],\n[\"\u043b\", null],\n[\"\u0434\u044b\", null],\n[\"\u0440\u0430\", null],\n[\"\u0442\u0430\", null],\n[\"\u0442\u044b\", null],\n[\"\u043d\", null],\n[\",\" , null],\n[\"\u2581\u0435\", \"\u0413\u041b\u0410\u0412\u041d\u041e\u0415\"],\n[\"\u04a3\", null],\n[\"\u2581\u0431\u0430\", null],\n[\"\u0441\u0442\u044b\", null],\n[\"\u0441\u044b\", null],\n[\"\u2581\u2013\", null],\n[\"\u2581\u0435\", \"\u0415\u0412\u0420\u041e\u0417\u041e\u041d\u0410\"],\n[\"\u0443\", null],\n[\"\u0440\u043e\", null],\n[\"\u0430\", null],\n[\"\u0439\", null],\n[\"\u043c\u0430\", null],\n[\"\u049b\", null],\n[\"\u0442\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0431\u043e\", \"\u0411\u0423\u0414\u0423\u0429\u0415\u0415\"],\n[\"\u043b\u0430\", null],\n[\"\u0448\u0430\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u2581\u0430\", \"\u041e\u041f\u0420\u0415\u0414\u0415\u041b\u042f\u0422\u042c\"],\n[\"\u0439\", null],\n[\"\u049b\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u0439\", null],\n[\"\u0442\u044b\", null],\n[\"\u043d\", null],\n[\"\u2581\u0435\", \"\u0420\u042b\u041d\u041e\u041a\"],\n[\"\u04a3\", null],\n[\"\u0431\u0435\", null],\n[\"\u043a\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u0440\u044b\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u2581\u0440\u0435\", \"\u0420\u0415\u0424\u041e\u0420\u041c\u0418\u0420\u041e\u0412\u0410\u041d\u0418\u0415\"],\n[\"\u0444\u043e\u0440\u043c\u0430\", null],\n[\"\u043b\u0430\", null],\n[\"\u0443\", null],\n[\"\u2581\u0436\", \"\u041f\u041b\u0410\u041d\"],\n[\"\u043e\", null],\n[\"\u0441\", null],\n[\"\u043f\u0430\u0440\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u2581\", null],\n[\"\u0436\u0430\", \"\u041e\u0411\u042a\u042f\u0412\u0418\u0422\u042c\"],\n[\"\u0440\u0438\u044f\", null],\n[\"\u043b\u0430\", null],\n[\"\u0434\u044b\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u0418\u04d9, \u0448\u044b\u043d\u044b\u043c\u0435\u043d, \u041c\u0430\u043a\u0440\u043e\u043d \u0437\u0430\u043c\u0430\u043d\u0430\u0443\u0438, \u0436\u0430\u04bb\u0430\u043d\u0434\u0430\u043d\u0443\u0434\u044b \u049b\u043e\u043b\u0434\u0430\u0439\u0442\u044b\u043d \u043e\u0440\u0442\u0430\u0448\u044b\u043b \u0441\u043e\u043b\u0448\u044b\u043b \u0442\u043e\u043f\u0442\u044b\u04a3 \u043e\u0440\u0442\u0430\u0441\u044b\u043d\u0430\u043d \u0448\u044b\u049b\u049b\u0430\u043d.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0418', '\u04d9', ',', '\u2581\u0448', '\u044b', '\u043d\u044b', '\u043c\u0435\u043d', ',', '\u2581\u041c\u0430', '\u043a', '\u0440\u043e\u043d', '\u2581\u0437\u0430', '\u043c\u0430', '\u043d\u0430', '\u0443', '\u0438', ',', '\u2581', '\u0436\u0430', '\u04bb', '\u0430\u043d', '\u0434\u0430', '\u043d\u0443', '\u0434\u044b', '\u2581', '\u049b', '\u043e', '\u043b', '\u0434\u0430', '\u0439', '\u0442\u044b', '\u043d', '\u2581\u043e\u0440', '\u0442\u0430', '\u0448', '\u044b', '\u043b', '\u2581\u0441\u043e', '\u043b', '\u0448', '\u044b', '\u043b', '\u2581\u0442\u043e', '\u043f', '\u0442\u044b', '\u04a3', '\u2581\u043e\u0440', '\u0442\u0430', '\u0441\u044b', '\u043d\u0430\u043d', '\u2581\u0448', '\u044b', '\u049b', '\u049b', '\u0430\u043d', '.']"}], "[\n[\"\u2581\u0418\", \"Yes\"],\n[\"\u04d9\", null],\n[\",\", null],\n[\"\u2581\u0448\", \"indeed\"],\n[\"\u044b\", null],\n[\"\u043d\u044b\", null],\n[\"\u043c\u0435\u043d\", null],\n[\",\", null],\n[\"\u2581\u041c\u0430\", \"Macron\"],\n[\"\u043a\", null],\n[\"\u0440\u043e\u043d\", null],\n[\"\u2581\u0437\u0430\", \"modern\"],\n[\"\u043c\u0430\", null],\n[\"\u043d\u0430\", null],\n[\"\u0443\", null],\n[\"\u0438\", null],\n[\",\", null],\n[\"\u2581\", null],\n[\"\u0436\u0430\", \"globalization\"],\n[\"\u04bb\", null],\n[\"\u0430\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u043d\u0443\", null],\n[\"\u0434\u044b\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"supporting\"],\n[\"\u043e\", null],\n[\"\u043b\", null],\n[\"\u0434\u0430\", null],\n[\"\u0439\", null],\n[\"\u0442\u044b\", null],\n[\"\u043d\", null],\n[\"\u2581\u043e\u0440\", \"centrist\"],\n[\"\u0442\u0430\", null],\n[\"\u0448\", null],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\"\u2581\u0441\u043e\", \"left-wing\"],\n[\"\u043b\", null],\n[\"\u0448\", null],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\"\u2581\u0442\u043e\", \"group\"],\n[\"\u043f\", null],\n[\"\u0442\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\u043e\u0440\", \"center\"],\n[\"\u0442\u0430\", null],\n[\"\u0441\u044b\", null],\n[\"\u043d\u0430\u043d\", null],\n[\"\u2581\u0448\", \"emerged\"],\n[\"\u044b\", null],\n[\"\u049b\", null],\n[\"\u049b\", null],\n[\"\u0430\u043d\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u0418\u04d9, \u0448\u044b\u043d\u044b\u043c\u0435\u043d, \u041c\u0430\u043a\u0440\u043e\u043d \u0437\u0430\u043c\u0430\u043d\u0430\u0443\u0438, \u0436\u0430\u04bb\u0430\u043d\u0434\u0430\u043d\u0443\u0434\u044b \u049b\u043e\u043b\u0434\u0430\u0439\u0442\u044b\u043d \u043e\u0440\u0442\u0430\u0448\u044b\u043b \u0441\u043e\u043b\u0448\u044b\u043b \u0442\u043e\u043f\u0442\u044b\u04a3 \u043e\u0440\u0442\u0430\u0441\u044b\u043d\u0430\u043d \u0448\u044b\u049b\u049b\u0430\u043d.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0418', '\u04d9', ',', '\u2581\u0448', '\u044b', '\u043d\u044b', '\u043c\u0435\u043d', ',', '\u2581\u041c\u0430', '\u043a', '\u0440\u043e\u043d', '\u2581\u0437\u0430', '\u043c\u0430', '\u043d\u0430', '\u0443', '\u0438', ',', '\u2581', '\u0436\u0430', '\u04bb', '\u0430\u043d', '\u0434\u0430', '\u043d\u0443', '\u0434\u044b', '\u2581', '\u049b', '\u043e', '\u043b', '\u0434\u0430', '\u0439', '\u0442\u044b', '\u043d', '\u2581\u043e\u0440', '\u0442\u0430', '\u0448', '\u044b', '\u043b', '\u2581\u0441\u043e', '\u043b', '\u0448', '\u044b', '\u043b', '\u2581\u0442\u043e', '\u043f', '\u0442\u044b', '\u04a3', '\u2581\u043e\u0440', '\u0442\u0430', '\u0441\u044b', '\u043d\u0430\u043d', '\u2581\u0448', '\u044b', '\u049b', '\u049b', '\u0430\u043d', '.']"}], "[\n[\"\u2581\u0418\", \"\u662f\"],\n[\"\u04d9\", null],\n[\",\", null],\n[\"\u2581\u0448\", \"\u786e\u5b9e\"],\n[\"\u044b\", null],\n[\"\u043d\u044b\", null],\n[\"\u043c\u0435\u043d\", null],\n[\",\", null],\n[\"\u2581\u041c\u0430\", \"\u9a6c\u514b\u9f99\"],\n[\"\u043a\", null],\n[\"\u0440\u043e\u043d\", null],\n[\"\u2581\u0437\u0430\", \"\u73b0\u4ee3\"],\n[\"\u043c\u0430\", null],\n[\"\u043d\u0430\", null],\n[\"\u0443\", null],\n[\"\u0438\", null],\n[\",\", null],\n[\"\u2581\", null],\n[\"\u0436\u0430\", \"\u5168\u7403\u5316\"],\n[\"\u04bb\", null],\n[\"\u0430\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u043d\u0443\", null],\n[\"\u0434\u044b\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"\u652f\u6301\"],\n[\"\u043e\", null],\n[\"\u043b\", null],\n[\"\u0434\u0430\", null],\n[\"\u0439\", null],\n[\"\u0442\u044b\", null],\n[\"\u043d\", null],\n[\"\u2581\u043e\u0440\", \"\u4e2d\u95f4\"],\n[\"\u0442\u0430\", null],\n[\"\u0448\", null],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\"\u2581\u0441\u043e\", \"\u5de6\u7ffc\"],\n[\"\u043b\", null],\n[\"\u0448\", null],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\"\u2581\u0442\u043e\", \"\u7fa4\u4f53\"],\n[\"\u043f\", null],\n[\"\u0442\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\u043e\u0440\", \"\u4e2d\"],\n[\"\u0442\u0430\", null],\n[\"\u0441\u044b\", null],\n[\"\u043d\u0430\u043d\", null],\n[\"\u2581\u0448\", \"\u51fa\u8eab\"],\n[\"\u044b\", null],\n[\"\u049b\", null],\n[\"\u049b\", null],\n[\"\u0430\u043d\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u0418\u04d9, \u0448\u044b\u043d\u044b\u043c\u0435\u043d, \u041c\u0430\u043a\u0440\u043e\u043d \u0437\u0430\u043c\u0430\u043d\u0430\u0443\u0438, \u0436\u0430\u04bb\u0430\u043d\u0434\u0430\u043d\u0443\u0434\u044b \u049b\u043e\u043b\u0434\u0430\u0439\u0442\u044b\u043d \u043e\u0440\u0442\u0430\u0448\u044b\u043b \u0441\u043e\u043b\u0448\u044b\u043b \u0442\u043e\u043f\u0442\u044b\u04a3 \u043e\u0440\u0442\u0430\u0441\u044b\u043d\u0430\u043d \u0448\u044b\u049b\u049b\u0430\u043d.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0418', '\u04d9', ',', '\u2581\u0448', '\u044b', '\u043d\u044b', '\u043c\u0435\u043d', ',', '\u2581\u041c\u0430', '\u043a', '\u0440\u043e\u043d', '\u2581\u0437\u0430', '\u043c\u0430', '\u043d\u0430', '\u0443', '\u0438', ',', '\u2581', '\u0436\u0430', '\u04bb', '\u0430\u043d', '\u0434\u0430', '\u043d\u0443', '\u0434\u044b', '\u2581', '\u049b', '\u043e', '\u043b', '\u0434\u0430', '\u0439', '\u0442\u044b', '\u043d', '\u2581\u043e\u0440', '\u0442\u0430', '\u0448', '\u044b', '\u043b', '\u2581\u0441\u043e', '\u043b', '\u0448', '\u044b', '\u043b', '\u2581\u0442\u043e', '\u043f', '\u0442\u044b', '\u04a3', '\u2581\u043e\u0440', '\u0442\u0430', '\u0441\u044b', '\u043d\u0430\u043d', '\u2581\u0448', '\u044b', '\u049b', '\u049b', '\u0430\u043d', '.']"}], "[\n[\"\u2581\u0418\", \"\u0414\u0430\"],\n[\"\u04d9\", null],\n[\",\", null],\n[\"\u2581\u0448\", \"\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e\"],\n[\"\u044b\", null],\n[\"\u043d\u044b\", null],\n[\"\u043c\u0435\u043d\", null],\n[\",\", null],\n[\"\u2581\u041c\u0430\", \"\u041c\u0430\u043a\u0440\u043e\u043d\"],\n[\"\u043a\", null],\n[\"\u0440\u043e\u043d\", null],\n[\"\u2581\u0437\u0430\", \"\u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0439\"],\n[\"\u043c\u0430\", null],\n[\"\u043d\u0430\", null],\n[\"\u0443\", null],\n[\"\u0438\", null],\n[\",\", null],\n[\"\u2581\", null],\n[\"\u0436\u0430\", \"\u0433\u043b\u043e\u0431\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e\"],\n[\"\u04bb\", null],\n[\"\u0430\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u043d\u0443\", null],\n[\"\u0434\u044b\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"\u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u044e\u0449\u0438\u0439\"],\n[\"\u043e\", null],\n[\"\u043b\", null],\n[\"\u0434\u0430\", null],\n[\"\u0439\", null],\n[\"\u0442\u044b\", null],\n[\"\u043d\", null],\n[\"\u2581\u043e\u0440\", \"\u0446\u0435\u043d\u0442\u0440\u0438\u0441\u0442\u0441\u043a\u0438\u0439\"],\n[\"\u0442\u0430\", null],\n[\"\u0448\", null],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\"\u2581\u0441\u043e\", \"\u043b\u0435\u0432\u044b\u0439\"],\n[\"\u043b\", null],\n[\"\u0448\", null],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\"\u2581\u0442\u043e\", \"\u0433\u0440\u0443\u043f\u043f\u044b\"],\n[\"\u043f\", null],\n[\"\u0442\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\u043e\u0440\", \"\u0438\u0437\"],\n[\"\u0442\u0430\", null],\n[\"\u0441\u044b\", null],\n[\"\u043d\u0430\u043d\", null],\n[\"\u2581\u0448\", \"\u0432\u044b\u0448\u0435\u043b\"],\n[\"\u044b\", null],\n[\"\u049b\", null],\n[\"\u049b\", null],\n[\"\u0430\u043d\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u0411\u0456\u0440\u0430\u049b, Gallup \u0436\u0430\u0441\u0430\u0493\u0430\u043d \u0441\u0430\u0443\u0430\u043b\u043d\u0430\u043c\u0430 \u043d\u04d9\u0442\u0438\u0436\u0435\u043b\u0435\u0440\u0456\u043d\u0435 \u0441\u04d9\u0439\u043a\u0435\u0441, \u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0434\u044b\u049b\u0442\u0430\u0440\u0434\u044b\u04a3 \u0442\u0435\u043a 32% -\u044b\u043d\u044b\u04a3 \u0434\u04d9\u0441\u0442\u04af\u0440\u043b\u0456 \u0411\u0410\u049a \u049b\u0430\u0442\u044b\u0441\u0442\u044b \u0441\u0435\u043d\u0456\u043c\u0456 \u00ab\u0436\u043e\u0493\u0430\u0440\u044b\u00bb \u043d\u0435\u043c\u0435\u0441\u0435 \u00ab\u043e\u0440\u0442\u0430\u0448\u0430\u00bb \u0431\u043e\u043b\u044b\u043f \u0442\u0430\u0431\u044b\u043b\u0430\u0434\u044b, \u0431\u04b1\u043b \u04e9\u0442\u0435 \u0442\u04e9\u043c\u0435\u043d \u043d\u04d9\u0442\u0438\u0436\u0435.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0411', '\u0456', '\u0440\u0430', '\u049b', ',', '\u2581Gall', 'up', '\u2581', '\u0436\u0430', '\u0441\u0430', '\u0493', '\u0430\u043d', '\u2581\u0441\u0430', '\u0443', '\u0430\u043b', '\u043d\u0430', '\u043c\u0430', '\u2581\u043d', '\u04d9', '\u0442\u0438', '\u0436\u0435', '\u043b\u0435', '\u0440\u0456', '\u043d\u0435', '\u2581\u0441', '\u04d9', '\u0439', '\u043a\u0435', '\u0441', ',', '\u2581\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d', '\u0434\u044b', '\u049b', '\u0442\u0430\u0440', '\u0434\u044b', '\u04a3', '\u2581\u0442\u0435', '\u043a', '\u2581', '3', '2', '%', '\u2581-', '\u044b', '\u043d\u044b', '\u04a3', '\u2581\u0434', '\u04d9', '\u0441\u0442', '\u04af', '\u0440', '\u043b\u0456', '\u2581\u0411', '\u0410', '\u049a', '\u2581', '\u049b', '\u0430', '\u0442\u044b', '\u0441\u0442\u044b', '\u2581\u0441', '\u0435\u043d', '\u0456', '\u043c\u0456', '\u2581\u00ab', '\u0436', '\u043e', '\u0493', '\u0430\u0440', '\u044b', '\u00bb', '\u2581\u043d\u0435', '\u043c\u0435', '\u0441\u0435', '\u2581\u00ab', '\u043e\u0440', '\u0442\u0430', '\u0448\u0430', '\u00bb', '\u2581\u0431\u043e', '\u043b\u044b', '\u043f', '\u2581\u0442\u0430', '\u0431\u044b', '\u043b\u0430', '\u0434\u044b', ',', '\u2581\u0431', '\u04b1', '\u043b', '\u2581', '\u04e9', '\u0442\u0435', '\u2581\u0442', '\u04e9', '\u043c\u0435\u043d', '\u2581\u043d', '\u04d9', '\u0442\u0438', '\u0436\u0435', '.']"}], "[\n[\"\u2581\u0411\", \"However\"],\n[\"\u0456\", null],\n[\"\u0440\u0430\", null],\n[\"\u049b\", null],\n[\",\", null],\n[\"\u2581Gall\", \"Gallup\"],\n[\"up\", null],\n[\"\u2581\", null],\n[\"\u0436\u0430\", \"conducted\"],\n[\"\u0441\u0430\", null],\n[\"\u0493\", null],\n[\"\u0430\u043d\", null],\n[\"\u2581\u0441\u0430\", \"survey\"],\n[\"\u0443\", null],\n[\"\u0430\u043b\", null],\n[\"\u043d\u0430\", null],\n[\"\u043c\u0430\", null],\n[\"\u2581\u043d\", \"results\"],\n[\"\u04d9\", null],\n[\"\u0442\u0438\", null],\n[\"\u0436\u0435\", null],\n[\"\u043b\u0435\", null],\n[\"\u0440\u0456\", null],\n[\"\u043d\u0435\", null],\n[\"\u2581\u0441\", \"according\"],\n[\"\u04d9\", null],\n[\"\u0439\", null],\n[\"\u043a\u0435\", null],\n[\"\u0441\", null],\n[\",\", null],\n[\"\u2581\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\", \"Americans\"],\n[\"\u0434\u044b\", null],\n[\"\u049b\", null],\n[\"\u0442\u0430\u0440\", null],\n[\"\u0434\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0442\u0435\", \"only\"],\n[\"\u043a\", null],\n[\"\u2581\", null],\n[\"3\", null],\n[\"2\", null],\n[\"%\", null],\n[\"\u2581-\", null],\n[\"\u044b\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0434\", \"traditional\"],\n[\"\u04d9\", null],\n[\"\u0441\u0442\", null],\n[\"\u04af\", null],\n[\"\u0440\", null],\n[\"\u043b\u0456\", null],\n[\"\u2581\u0411\", \"media\"],\n[\"\u0410\", null],\n[\"\u049a\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"regarding\"],\n[\"\u0430\", null],\n[\"\u0442\u044b\", null],\n[\"\u0441\u0442\u044b\", null],\n[\"\u2581\u0441\", \"trust\"],\n[\"\u0435\u043d\", null],\n[\"\u0456\", null],\n[\"\u043c\u0456\", null],\n[\"\u2581\u00ab\", null],\n[\"\u0436\", \"high\"],\n[\"\u043e\", null],\n[\"\u0493\", null],\n[\"\u0430\u0440\", null],\n[\"\u044b\", null],\n[\"\u00bb\", null],\n[\"\u2581\u043d\u0435\", \"or\"],\n[\"\u043c\u0435\", null],\n[\"\u0441\u0435\", null],\n[\"\u2581\u00ab\", null],\n[\"\u043e\u0440\", \"medium\"],\n[\"\u0442\u0430\", null],\n[\"\u0448\u0430\", null],\n[\"\u00bb\", null],\n[\"\u2581\u0431\u043e\", \"is\"],\n[\"\u043b\u044b\", null],\n[\"\u043f\", null],\n[\"\u2581\u0442\u0430\", \"considered\"],\n[\"\u0431\u044b\", null],\n[\"\u043b\u0430\", null],\n[\"\u0434\u044b\", null],\n[\",\", null],\n[\"\u2581\u0431\", \"which\"],\n[\"\u04b1\", null],\n[\"\u043b\", null],\n[\"\u2581\", null],\n[\"\u04e9\", \"very\"],\n[\"\u0442\u0435\", null],\n[\"\u2581\u0442\", \"low\"],\n[\"\u04e9\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u2581\u043d\", \"result\"],\n[\"\u04d9\", null],\n[\"\u0442\u0438\", null],\n[\"\u0436\u0435\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u0411\u0456\u0440\u0430\u049b, Gallup \u0436\u0430\u0441\u0430\u0493\u0430\u043d \u0441\u0430\u0443\u0430\u043b\u043d\u0430\u043c\u0430 \u043d\u04d9\u0442\u0438\u0436\u0435\u043b\u0435\u0440\u0456\u043d\u0435 \u0441\u04d9\u0439\u043a\u0435\u0441, \u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0434\u044b\u049b\u0442\u0430\u0440\u0434\u044b\u04a3 \u0442\u0435\u043a 32% -\u044b\u043d\u044b\u04a3 \u0434\u04d9\u0441\u0442\u04af\u0440\u043b\u0456 \u0411\u0410\u049a \u049b\u0430\u0442\u044b\u0441\u0442\u044b \u0441\u0435\u043d\u0456\u043c\u0456 \u00ab\u0436\u043e\u0493\u0430\u0440\u044b\u00bb \u043d\u0435\u043c\u0435\u0441\u0435 \u00ab\u043e\u0440\u0442\u0430\u0448\u0430\u00bb \u0431\u043e\u043b\u044b\u043f \u0442\u0430\u0431\u044b\u043b\u0430\u0434\u044b, \u0431\u04b1\u043b \u04e9\u0442\u0435 \u0442\u04e9\u043c\u0435\u043d \u043d\u04d9\u0442\u0438\u0436\u0435.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0411', '\u0456', '\u0440\u0430', '\u049b', ',', '\u2581Gall', 'up', '\u2581', '\u0436\u0430', '\u0441\u0430', '\u0493', '\u0430\u043d', '\u2581\u0441\u0430', '\u0443', '\u0430\u043b', '\u043d\u0430', '\u043c\u0430', '\u2581\u043d', '\u04d9', '\u0442\u0438', '\u0436\u0435', '\u043b\u0435', '\u0440\u0456', '\u043d\u0435', '\u2581\u0441', '\u04d9', '\u0439', '\u043a\u0435', '\u0441', ',', '\u2581\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d', '\u0434\u044b', '\u049b', '\u0442\u0430\u0440', '\u0434\u044b', '\u04a3', '\u2581\u0442\u0435', '\u043a', '\u2581', '3', '2', '%', '\u2581-', '\u044b', '\u043d\u044b', '\u04a3', '\u2581\u0434', '\u04d9', '\u0441\u0442', '\u04af', '\u0440', '\u043b\u0456', '\u2581\u0411', '\u0410', '\u049a', '\u2581', '\u049b', '\u0430', '\u0442\u044b', '\u0441\u0442\u044b', '\u2581\u0441', '\u0435\u043d', '\u0456', '\u043c\u0456', '\u2581\u00ab', '\u0436', '\u043e', '\u0493', '\u0430\u0440', '\u044b', '\u00bb', '\u2581\u043d\u0435', '\u043c\u0435', '\u0441\u0435', '\u2581\u00ab', '\u043e\u0440', '\u0442\u0430', '\u0448\u0430', '\u00bb', '\u2581\u0431\u043e', '\u043b\u044b', '\u043f', '\u2581\u0442\u0430', '\u0431\u044b', '\u043b\u0430', '\u0434\u044b', ',', '\u2581\u0431', '\u04b1', '\u043b', '\u2581', '\u04e9', '\u0442\u0435', '\u2581\u0442', '\u04e9', '\u043c\u0435\u043d', '\u2581\u043d', '\u04d9', '\u0442\u0438', '\u0436\u0435', '.']"}], "[\n  [\"\u2581\u0411\", \"\u4f46\"],\n  [\"\u0456\", null],\n  [\"\u0440\u0430\", null],\n  [\"\u049b\", null],\n  [\",\", null],\n  [\"\u2581Gall\", \"\u76d6\u6d1b\u666e\"],\n  [\"up\", null],\n  [\"\u2581\", null],\n  [\"\u0436\u0430\", \"\u8fdb\u884c\"],\n  [\"\u0441\u0430\", null],\n  [\"\u0493\", null],\n  [\"\u0430\u043d\", null],\n  [\"\u2581\u0441\u0430\", \"\u8c03\u67e5\"],\n  [\"\u0443\", null],\n  [\"\u0430\u043b\", null],\n  [\"\u043d\u0430\", null],\n  [\"\u043c\u0430\", null],\n  [\"\u2581\u043d\", \"\u7ed3\u679c\"],\n  [\"\u04d9\", null],\n  [\"\u0442\u0438\", null],\n  [\"\u0436\u0435\", null],\n  [\"\u043b\u0435\", null],\n  [\"\u0440\u0456\", null],\n  [\"\u043d\u0435\", null],\n  [\"\u2581\u0441\", \"\u6839\u636e\"],\n  [\"\u04d9\", null],\n  [\"\u0439\", null],\n  [\"\u043a\u0435\", null],\n  [\"\u0441\", null],\n  [\",\", null],\n  [\"\u2581\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\", \"\u7f8e\u56fd\u4eba\"],\n  [\"\u0434\u044b\", null],\n  [\"\u049b\", null],\n  [\"\u0442\u0430\u0440\", null],\n  [\"\u0434\u044b\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\u0442\u0435\", \"\u53ea\u6709\"],\n  [\"\u043a\", null],\n  [\"\u2581\", null],\n  [\"3\", null],\n  [\"2\", null],\n  [\"%\", null],\n  [\"\u2581-\", null],\n  [\"\u044b\", null],\n  [\"\u043d\u044b\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\u0434\", \"\u4f20\u7edf\"],\n  [\"\u04d9\", null],\n  [\"\u0441\u0442\", null],\n  [\"\u04af\", null],\n  [\"\u0440\", null],\n  [\"\u043b\u0456\", null],\n  [\"\u2581\u0411\", \"\u5a92\u4f53\"],\n  [\"\u0410\", null],\n  [\"\u049a\", null],\n  [\"\u2581\", null],\n  [\"\u049b\", \"\u5bf9\"],\n  [\"\u0430\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u0441\u0442\u044b\", null],\n  [\"\u2581\u0441\", \"\u4fe1\u4efb\"],\n  [\"\u0435\u043d\", null],\n  [\"\u0456\", null],\n  [\"\u043c\u0456\", null],\n  [\"\u2581\u00ab\", null],\n  [\"\u0436\", \"\u9ad8\"],\n  [\"\u043e\", null],\n  [\"\u0493\", null],\n  [\"\u0430\u0440\", null],\n  [\"\u044b\", null],\n  [\"\u00bb\", null],\n  [\"\u2581\u043d\u0435\", \"\u6216\"],\n  [\"\u043c\u0435\", null],\n  [\"\u0441\u0435\", null],\n  [\"\u2581\u00ab\", null],\n  [\"\u043e\u0440\", \"\u4e2d\u7b49\"],\n  [\"\u0442\u0430\", null],\n  [\"\u0448\u0430\", null],\n  [\"\u00bb\", null],\n  [\"\u2581\u0431\u043e\", \"\u662f\"],\n  [\"\u043b\u044b\", null],\n  [\"\u043f\", null],\n  [\"\u2581\u0442\u0430\", \"\u7ed3\u679c\"],\n  [\"\u0431\u044b\", null],\n  [\"\u043b\u0430\", null],\n  [\"\u0434\u044b\", null],\n  [\",\", null],\n  [\"\u2581\u0431\", \"\u8fd9\"],\n  [\"\u04b1\", null],\n  [\"\u043b\", null],\n  [\"\u2581\", null],\n  [\"\u04e9\", \"\u975e\u5e38\"],\n  [\"\u0442\u0435\", null],\n  [\"\u2581\u0442\", \"\u4f4e\"],\n  [\"\u04e9\", null],\n  [\"\u043c\u0435\u043d\", null],\n  [\"\u2581\u043d\", \"\u7ed3\u679c\"],\n  [\"\u04d9\", null],\n  [\"\u0442\u0438\", null],\n  [\"\u0436\u0435\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u0411\u0456\u0440\u0430\u049b, Gallup \u0436\u0430\u0441\u0430\u0493\u0430\u043d \u0441\u0430\u0443\u0430\u043b\u043d\u0430\u043c\u0430 \u043d\u04d9\u0442\u0438\u0436\u0435\u043b\u0435\u0440\u0456\u043d\u0435 \u0441\u04d9\u0439\u043a\u0435\u0441, \u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0434\u044b\u049b\u0442\u0430\u0440\u0434\u044b\u04a3 \u0442\u0435\u043a 32% -\u044b\u043d\u044b\u04a3 \u0434\u04d9\u0441\u0442\u04af\u0440\u043b\u0456 \u0411\u0410\u049a \u049b\u0430\u0442\u044b\u0441\u0442\u044b \u0441\u0435\u043d\u0456\u043c\u0456 \u00ab\u0436\u043e\u0493\u0430\u0440\u044b\u00bb \u043d\u0435\u043c\u0435\u0441\u0435 \u00ab\u043e\u0440\u0442\u0430\u0448\u0430\u00bb \u0431\u043e\u043b\u044b\u043f \u0442\u0430\u0431\u044b\u043b\u0430\u0434\u044b, \u0431\u04b1\u043b \u04e9\u0442\u0435 \u0442\u04e9\u043c\u0435\u043d \u043d\u04d9\u0442\u0438\u0436\u0435.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0411', '\u0456', '\u0440\u0430', '\u049b', ',', '\u2581Gall', 'up', '\u2581', '\u0436\u0430', '\u0441\u0430', '\u0493', '\u0430\u043d', '\u2581\u0441\u0430', '\u0443', '\u0430\u043b', '\u043d\u0430', '\u043c\u0430', '\u2581\u043d', '\u04d9', '\u0442\u0438', '\u0436\u0435', '\u043b\u0435', '\u0440\u0456', '\u043d\u0435', '\u2581\u0441', '\u04d9', '\u0439', '\u043a\u0435', '\u0441', ',', '\u2581\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d', '\u0434\u044b', '\u049b', '\u0442\u0430\u0440', '\u0434\u044b', '\u04a3', '\u2581\u0442\u0435', '\u043a', '\u2581', '3', '2', '%', '\u2581-', '\u044b', '\u043d\u044b', '\u04a3', '\u2581\u0434', '\u04d9', '\u0441\u0442', '\u04af', '\u0440', '\u043b\u0456', '\u2581\u0411', '\u0410', '\u049a', '\u2581', '\u049b', '\u0430', '\u0442\u044b', '\u0441\u0442\u044b', '\u2581\u0441', '\u0435\u043d', '\u0456', '\u043c\u0456', '\u2581\u00ab', '\u0436', '\u043e', '\u0493', '\u0430\u0440', '\u044b', '\u00bb', '\u2581\u043d\u0435', '\u043c\u0435', '\u0441\u0435', '\u2581\u00ab', '\u043e\u0440', '\u0442\u0430', '\u0448\u0430', '\u00bb', '\u2581\u0431\u043e', '\u043b\u044b', '\u043f', '\u2581\u0442\u0430', '\u0431\u044b', '\u043b\u0430', '\u0434\u044b', ',', '\u2581\u0431', '\u04b1', '\u043b', '\u2581', '\u04e9', '\u0442\u0435', '\u2581\u0442', '\u04e9', '\u043c\u0435\u043d', '\u2581\u043d', '\u04d9', '\u0442\u0438', '\u0436\u0435', '.']"}], "[\n[\"\u2581\u0411\", \"\u041e\u0434\u043d\u0430\u043a\u043e\"],\n[\"\u0456\", null],\n[\"\u0440\u0430\", null],\n[\"\u049b\", null],\n[\",\", null],\n[\"\u2581Gall\", \"Gallup\"],\n[\"up\", null],\n[\"\u2581\", null],\n[\"\u0436\u0430\", \"\u0441\u043e\u0433\u043b\u0430\u0441\u043d\u043e\"],\n[\"\u0441\u0430\", null],\n[\"\u0493\", null],\n[\"\u0430\u043d\", null],\n[\"\u2581\u0441\u0430\", \"\u043e\u043f\u0440\u043e\u0441\u0443\"],\n[\"\u0443\", null],\n[\"\u0430\u043b\", null],\n[\"\u043d\u0430\", null],\n[\"\u043c\u0430\", null],\n[\"\u2581\u043d\", \"\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u043c\"],\n[\"\u04d9\", null],\n[\"\u0442\u0438\", null],\n[\"\u0436\u0435\", null],\n[\"\u043b\u0435\", null],\n[\"\u0440\u0456\", null],\n[\"\u043d\u0435\", null],\n[\"\u2581\u0441\", null],\n[\"\u04d9\", null],\n[\"\u0439\", null],\n[\"\u043a\u0435\", null],\n[\"\u0441\", null],\n[\",\", null],\n[\"\u2581\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\", \"\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0446\u0435\u0432\"],\n[\"\u0434\u044b\", null],\n[\"\u049b\", null],\n[\"\u0442\u0430\u0440\", null],\n[\"\u0434\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0442\u0435\", \"\u0442\u043e\u043b\u044c\u043a\u043e\"],\n[\"\u043a\", null],\n[\"\u2581\", null],\n[\"3\", null],\n[\"2\", null],\n[\"%\", null],\n[\"\u2581-\", null],\n[\"\u044b\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\u0434\", \"\u0442\u0440\u0430\u0434\u0438\u0446\u0438\u043e\u043d\u043d\u044b\u043c\"],\n[\"\u04d9\", null],\n[\"\u0441\u0442\", null],\n[\"\u04af\", null],\n[\"\u0440\", null],\n[\"\u043b\u0456\", null],\n[\"\u2581\u0411\", \"\u0421\u041c\u0418\"],\n[\"\u0410\", null],\n[\"\u049a\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"\u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e\"],\n[\"\u0430\", null],\n[\"\u0442\u044b\", null],\n[\"\u0441\u0442\u044b\", null],\n[\"\u2581\u0441\", \"\u0434\u043e\u0432\u0435\u0440\u0438\u0435\"],\n[\"\u0435\u043d\", null],\n[\"\u0456\", null],\n[\"\u043c\u0456\", null],\n[\"\u2581\u00ab\", null],\n[\"\u0436\", \"\u0432\u044b\u0441\u043e\u043a\u043e\u0435\"],\n[\"\u043e\", null],\n[\"\u0493\", null],\n[\"\u0430\u0440\", null],\n[\"\u044b\", null],\n[\"\u00bb\", null],\n[\"\u2581\u043d\u0435\", \"\u0438\u043b\u0438\"],\n[\"\u043c\u0435\", null],\n[\"\u0441\u0435\", null],\n[\"\u2581\u00ab\", null],\n[\"\u043e\u0440\", \"\u0441\u0440\u0435\u0434\u043d\u0435\u0435\"],\n[\"\u0442\u0430\", null],\n[\"\u0448\u0430\", null],\n[\"\u00bb\", null],\n[\"\u2581\u0431\u043e\", \"\u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f\"],\n[\"\u043b\u044b\", null],\n[\"\u043f\", null],\n[\"\u2581\u0442\u0430\", null],\n[\"\u0431\u044b\", null],\n[\"\u043b\u0430\", null],\n[\"\u0434\u044b\", null],\n[\",\", null],\n[\"\u2581\u0431\", \"\u044d\u0442\u043e\"],\n[\"\u04b1\", null],\n[\"\u043b\", null],\n[\"\u2581\", null],\n[\"\u04e9\", \"\u043e\u0447\u0435\u043d\u044c\"],\n[\"\u0442\u0435\", null],\n[\"\u2581\u0442\", \"\u043d\u0438\u0437\u043a\u0438\u0439\"],\n[\"\u04e9\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u2581\u043d\", \"\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\"],\n[\"\u04d9\", null],\n[\"\u0442\u0438\", null],\n[\"\u0436\u0435\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u04e8\u043d\u0435\u0440\u043a\u04d9\u0441\u0456\u043f \u0442\u0430\u0431\u044b\u0441\u044b \u0442\u04e9\u043c\u0435\u043d \u0435\u043b\u0434\u0435\u0440\u0434\u0435\u0433\u0456 \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0430\u043b\u044b\u049b \u0434\u0430\u043c\u0443\u0434\u044b\u04a3 \u0435\u04a3 \u043a\u04af\u0448\u0442\u0456 \u044d\u0441\u043a\u0430\u043b\u0430\u0442\u043e\u0440\u044b\u043d\u0430 \u0430\u0439\u043d\u0430\u043b\u0434\u044b. \u041e\u0493\u0430\u043d \u043c\u044b\u043d\u0430\u0434\u0430\u0439 \u04af\u0448 \u0441\u0435\u0431\u0435\u043f \u0431\u0430\u0440.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '<0xD3>', '<0xA8>', '\u043d\u0435\u0440', '\u043a', '\u04d9', '\u0441\u0456', '\u043f', '\u2581\u0442\u0430', '\u0431\u044b', '\u0441\u044b', '\u2581\u0442', '\u04e9', '\u043c\u0435\u043d', '\u2581\u0435', '\u043b', '\u0434\u0435\u0440', '\u0434\u0435', '\u0433\u0456', '\u2581\u044d', '\u043a\u043e\u043d\u043e\u043c\u0438', '\u043a\u0430', '\u043b\u044b', '\u049b', '\u2581\u0434\u0430', '\u043c\u0443', '\u0434\u044b', '\u04a3', '\u2581\u0435', '\u04a3', '\u2581\u043a', '\u04af', '\u0448', '\u0442\u0456', '\u2581\u044d', '\u0441\u043a\u0430', '\u043b\u0430', '\u0442\u043e', '\u0440\u044b', '\u043d\u0430', '\u2581\u0430', '\u0439', '\u043d\u0430\u043b', '\u0434\u044b', '.', '\u2581\u041e', '\u0493', '\u0430\u043d', '\u2581\u043c', '\u044b', '\u043d\u0430', '\u0434\u0430', '\u0439', '\u2581', '\u04af', '\u0448', '\u2581\u0441\u0435', '\u0431\u0435', '\u043f', '\u2581\u0431', '\u0430\u0440', '.']"}], "[\n  [\"\u2581\", null],\n  [\"<0xD3>\", null],\n  [\"<0xA8>\", null],\n  [\"\u043d\u0435\u0440\", \"Industry\"],\n  [\"\u043a\", null],\n  [\"\u04d9\", null],\n  [\"\u0441\u0456\", null],\n  [\"\u043f\", null],\n  [\"\u2581\u0442\u0430\", null],\n  [\"\u0431\u044b\", null],\n  [\"\u0441\u044b\", null],\n  [\"\u2581\u0442\", null],\n  [\"\u04e9\", null],\n  [\"\u043c\u0435\u043d\", \"low\"],\n  [\"\u2581\u0435\", null],\n  [\"\u043b\", null],\n  [\"\u0434\u0435\u0440\", null],\n  [\"\u0434\u0435\", null],\n  [\"\u0433\u0456\", null],\n  [\"\u2581\u044d\", null],\n  [\"\u043a\u043e\u043d\u043e\u043c\u0438\", \"economic\"],\n  [\"\u043a\u0430\", null],\n  [\"\u043b\u044b\", null],\n  [\"\u049b\", null],\n  [\"\u2581\u0434\u0430\", null],\n  [\"\u043c\u0443\", \"development\"],\n  [\"\u0434\u044b\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\u0435\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\u043a\", null],\n  [\"\u04af\", null],\n  [\"\u0448\", null],\n  [\"\u0442\u0456\", \"strongest\"],\n  [\"\u2581\u044d\", null],\n  [\"\u0441\u043a\u0430\", \"escalator\"],\n  [\"\u043b\u0430\", null],\n  [\"\u0442\u043e\", null],\n  [\"\u0440\u044b\", null],\n  [\"\u043d\u0430\", null],\n  [\"\u2581\u0430\", null],\n  [\"\u0439\", null],\n  [\"\u043d\u0430\u043b\", \"became\"],\n  [\"\u0434\u044b\", null],\n  [\".\", null],\n  [\"\u2581\u041e\", null],\n  [\"\u0493\", null],\n  [\"\u0430\u043d\", \"There\"],\n  [\"\u2581\u043c\", null],\n  [\"\u044b\", null],\n  [\"\u043d\u0430\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u0439\", null],\n  [\"\u2581\", null],\n  [\"\u04af\", null],\n  [\"\u0448\", \"three\"],\n  [\"\u2581\u0441\u0435\", null],\n  [\"\u0431\u0435\", \"reasons\"],\n  [\"\u043f\", null],\n  [\"\u2581\u0431\", null],\n  [\"\u0430\u0440\", \"are\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u04e8\u043d\u0435\u0440\u043a\u04d9\u0441\u0456\u043f \u0442\u0430\u0431\u044b\u0441\u044b \u0442\u04e9\u043c\u0435\u043d \u0435\u043b\u0434\u0435\u0440\u0434\u0435\u0433\u0456 \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0430\u043b\u044b\u049b \u0434\u0430\u043c\u0443\u0434\u044b\u04a3 \u0435\u04a3 \u043a\u04af\u0448\u0442\u0456 \u044d\u0441\u043a\u0430\u043b\u0430\u0442\u043e\u0440\u044b\u043d\u0430 \u0430\u0439\u043d\u0430\u043b\u0434\u044b. \u041e\u0493\u0430\u043d \u043c\u044b\u043d\u0430\u0434\u0430\u0439 \u04af\u0448 \u0441\u0435\u0431\u0435\u043f \u0431\u0430\u0440.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '<0xD3>', '<0xA8>', '\u043d\u0435\u0440', '\u043a', '\u04d9', '\u0441\u0456', '\u043f', '\u2581\u0442\u0430', '\u0431\u044b', '\u0441\u044b', '\u2581\u0442', '\u04e9', '\u043c\u0435\u043d', '\u2581\u0435', '\u043b', '\u0434\u0435\u0440', '\u0434\u0435', '\u0433\u0456', '\u2581\u044d', '\u043a\u043e\u043d\u043e\u043c\u0438', '\u043a\u0430', '\u043b\u044b', '\u049b', '\u2581\u0434\u0430', '\u043c\u0443', '\u0434\u044b', '\u04a3', '\u2581\u0435', '\u04a3', '\u2581\u043a', '\u04af', '\u0448', '\u0442\u0456', '\u2581\u044d', '\u0441\u043a\u0430', '\u043b\u0430', '\u0442\u043e', '\u0440\u044b', '\u043d\u0430', '\u2581\u0430', '\u0439', '\u043d\u0430\u043b', '\u0434\u044b', '.', '\u2581\u041e', '\u0493', '\u0430\u043d', '\u2581\u043c', '\u044b', '\u043d\u0430', '\u0434\u0430', '\u0439', '\u2581', '\u04af', '\u0448', '\u2581\u0441\u0435', '\u0431\u0435', '\u043f', '\u2581\u0431', '\u0430\u0440', '.']"}], "[\n  [\"\u2581\", null],\n  [\"<0xD3>\", \"\u5de5\u4e1a\"],\n  [\"<0xA8>\", null],\n  [\"\u043d\u0435\u0440\", null],\n  [\"\u043a\", null],\n  [\"\u04d9\", null],\n  [\"\u0441\u0456\", null],\n  [\"\u043f\", null],\n  [\"\u2581\u0442\u0430\", null],\n  [\"\u0431\u044b\", null],\n  [\"\u0441\u044b\", null],\n  [\"\u2581\u0442\", \"\u4f4e\"],\n  [\"\u04e9\", null],\n  [\"\u043c\u0435\u043d\", null],\n  [\"\u2581\u0435\", \"\u56fd\u5bb6\"],\n  [\"\u043b\", null],\n  [\"\u0434\u0435\u0440\", null],\n  [\"\u0434\u0435\", null],\n  [\"\u0433\u0456\", null],\n  [\"\u2581\u044d\", \"\u7ecf\u6d4e\"],\n  [\"\u043a\u043e\u043d\u043e\u043c\u0438\", null],\n  [\"\u043a\u0430\", null],\n  [\"\u043b\u044b\", null],\n  [\"\u049b\", null],\n  [\"\u2581\u0434\u0430\", \"\u53d1\u5c55\"],\n  [\"\u043c\u0443\", null],\n  [\"\u0434\u044b\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\u0435\", \"\u6700\"],\n  [\"\u04a3\", null],\n  [\"\u2581\u043a\", \"\u5f3a\u52b2\"],\n  [\"\u04af\", null],\n  [\"\u0448\", null],\n  [\"\u0442\u0456\", null],\n  [\"\u2581\u044d\", \"\u7535\u68af\"],\n  [\"\u0441\u043a\u0430\", null],\n  [\"\u043b\u0430\", null],\n  [\"\u0442\u043e\", null],\n  [\"\u0440\u044b\", null],\n  [\"\u043d\u0430\", null],\n  [\"\u2581\u0430\", \"\u53d8\u6210\"],\n  [\"\u0439\", null],\n  [\"\u043d\u0430\u043b\", null],\n  [\"\u0434\u044b\", null],\n  [\".\", null],\n  [\"\u2581\u041e\", \"\u8fd9\"],\n  [\"\u0493\", null],\n  [\"\u0430\u043d\", null],\n  [\"\u2581\u043c\", \"\u6709\"],\n  [\"\u044b\", null],\n  [\"\u043d\u0430\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u0439\", null],\n  [\"\u2581\", null],\n  [\"\u04af\", \"\u4e09\"],\n  [\"\u0448\", null],\n  [\"\u2581\u0441\u0435\", \"\u4e2a\"],\n  [\"\u0431\u0435\", null],\n  [\"\u043f\", null],\n  [\"\u2581\u0431\", \"\u539f\u56e0\"],\n  [\"\u0430\u0440\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u04e8\u043d\u0435\u0440\u043a\u04d9\u0441\u0456\u043f \u0442\u0430\u0431\u044b\u0441\u044b \u0442\u04e9\u043c\u0435\u043d \u0435\u043b\u0434\u0435\u0440\u0434\u0435\u0433\u0456 \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0430\u043b\u044b\u049b \u0434\u0430\u043c\u0443\u0434\u044b\u04a3 \u0435\u04a3 \u043a\u04af\u0448\u0442\u0456 \u044d\u0441\u043a\u0430\u043b\u0430\u0442\u043e\u0440\u044b\u043d\u0430 \u0430\u0439\u043d\u0430\u043b\u0434\u044b. \u041e\u0493\u0430\u043d \u043c\u044b\u043d\u0430\u0434\u0430\u0439 \u04af\u0448 \u0441\u0435\u0431\u0435\u043f \u0431\u0430\u0440.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581', '<0xD3>', '<0xA8>', '\u043d\u0435\u0440', '\u043a', '\u04d9', '\u0441\u0456', '\u043f', '\u2581\u0442\u0430', '\u0431\u044b', '\u0441\u044b', '\u2581\u0442', '\u04e9', '\u043c\u0435\u043d', '\u2581\u0435', '\u043b', '\u0434\u0435\u0440', '\u0434\u0435', '\u0433\u0456', '\u2581\u044d', '\u043a\u043e\u043d\u043e\u043c\u0438', '\u043a\u0430', '\u043b\u044b', '\u049b', '\u2581\u0434\u0430', '\u043c\u0443', '\u0434\u044b', '\u04a3', '\u2581\u0435', '\u04a3', '\u2581\u043a', '\u04af', '\u0448', '\u0442\u0456', '\u2581\u044d', '\u0441\u043a\u0430', '\u043b\u0430', '\u0442\u043e', '\u0440\u044b', '\u043d\u0430', '\u2581\u0430', '\u0439', '\u043d\u0430\u043b', '\u0434\u044b', '.', '\u2581\u041e', '\u0493', '\u0430\u043d', '\u2581\u043c', '\u044b', '\u043d\u0430', '\u0434\u0430', '\u0439', '\u2581', '\u04af', '\u0448', '\u2581\u0441\u0435', '\u0431\u0435', '\u043f', '\u2581\u0431', '\u0430\u0440', '.']"}], "[\n  [\"\u2581\", null],\n  [\"<0xD3>\", \"\u041f\u0440\u043e\u043c\u044b\u0448\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u044c\"],\n  [\"<0xA8>\", null],\n  [\"\u043d\u0435\u0440\", null],\n  [\"\u043a\", null],\n  [\"\u04d9\", null],\n  [\"\u0441\u0456\", null],\n  [\"\u043f\", null],\n  [\"\u2581\u0442\u0430\", null],\n  [\"\u0431\u044b\", null],\n  [\"\u0441\u044b\", null],\n  [\"\u2581\u0442\", null],\n  [\"\u04e9\", null],\n  [\"\u043c\u0435\u043d\", null],\n  [\"\u2581\u0435\", null],\n  [\"\u043b\", null],\n  [\"\u0434\u0435\u0440\", null],\n  [\"\u0434\u0435\", null],\n  [\"\u0433\u0456\", null],\n  [\"\u2581\u044d\", null],\n  [\"\u043a\u043e\u043d\u043e\u043c\u0438\", \"\u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e\"],\n  [\"\u043a\u0430\", null],\n  [\"\u043b\u044b\", null],\n  [\"\u049b\", null],\n  [\"\u2581\u0434\u0430\", null],\n  [\"\u043c\u0443\", null],\n  [\"\u0434\u044b\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\u0435\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\u043a\", null],\n  [\"\u04af\", null],\n  [\"\u0448\", null],\n  [\"\u0442\u0456\", null],\n  [\"\u2581\u044d\", null],\n  [\"\u0441\u043a\u0430\", \"\u044d\u0441\u043a\u0430\u043b\u0430\u0442\u043e\u0440\u043e\u043c\"],\n  [\"\u043b\u0430\", null],\n  [\"\u0442\u043e\", null],\n  [\"\u0440\u044b\", null],\n  [\"\u043d\u0430\", null],\n  [\"\u2581\u0430\", null],\n  [\"\u0439\", null],\n  [\"\u043d\u0430\u043b\", null],\n  [\"\u0434\u044b\", null],\n  [\".\", null],\n  [\"\u2581\u041e\", \"\u0414\u043b\u044f\"],\n  [\"\u0493\", null],\n  [\"\u0430\u043d\", null],\n  [\"\u2581\u043c\", null],\n  [\"\u044b\", null],\n  [\"\u043d\u0430\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u0439\", null],\n  [\"\u2581\", null],\n  [\"\u04af\", \"\u0442\u0440\u0438\"],\n  [\"\u0448\", null],\n  [\"\u2581\u0441\u0435\", \"\u043f\u0440\u0438\u0447\u0438\u043d\u044b\"],\n  [\"\u0431\u0435\", null],\n  [\"\u043f\", null],\n  [\"\u2581\u0431\", null],\n  [\"\u0430\u0440\", \"\u0435\u0441\u0442\u044c\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u0410\u043b \u0416\u0430\u043f\u043e\u043d\u0438\u044f \u0431\u0430\u044f\u0443 \u0434\u0430\u043c\u0443 \u0436\u043e\u043b\u044b\u043d\u0434\u0430 \u043a\u0456\u0431\u0456\u0440\u0442\u0456\u043a\u0442\u0435\u043f \u049b\u0430\u043b\u0434\u044b, \u043f\u0440\u0435\u043c\u044c\u0435\u0440-\u043c\u0438\u043d\u0438\u0441\u0442\u0440 \u0421\u0438\u043d\u0434\u0437\u043e \u0410\u0431\u0435\u043d\u0456\u04a3 \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0430\u043b\u044b\u049b \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u044f\u0441\u044b\u043d\u044b\u04a3 \u04af\u0448\u0456\u043d\u0448\u0456 \u0431\u0430\u0493\u044b\u0442\u044b \u2013 \u0441\u0430\u0443\u0434\u0430\u043d\u044b \u043b\u0438\u0431\u0435\u0440\u0430\u043b\u0434\u0430\u043d\u0434\u044b\u0440\u0443 \u04d9\u0440\u0435\u043a\u0435\u0442\u0456 \u0435\u0448 \u043d\u04d9\u0442\u0438\u0436\u0435 \u0431\u0435\u0440\u043c\u0435\u0434\u0456.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0410\u043b', '\u2581\u0416', '\u0430', '\u043f\u043e', '\u043d\u0438\u044f', '\u2581\u0431\u0430', '\u044f', '\u0443', '\u2581\u0434\u0430', '\u043c\u0443', '\u2581\u0436', '\u043e', '\u043b\u044b', '\u043d', '\u0434\u0430', '\u2581\u043a', '\u0456', '\u0431\u0456', '\u0440', '\u0442\u0456', '\u043a', '\u0442\u0435', '\u043f', '\u2581', '\u049b', '\u0430\u043b', '\u0434\u044b', ',', '\u2581\u043f\u0440\u0435', '\u043c', '\u044c', '\u0435\u0440', '-', '\u043c\u0438\u043d\u0438', '\u0441\u0442', '\u0440', '\u2581\u0421\u0438', '\u043d\u0434', '\u0437\u043e', '\u2581\u0410', '\u0431', '\u0435\u043d', '\u0456', '\u04a3', '\u2581\u044d', '\u043a\u043e\u043d\u043e\u043c\u0438', '\u043a\u0430', '\u043b\u044b', '\u049b', '\u2581\u0441\u0442\u0440\u0430', '\u0442\u0435', '\u0433\u0438', '\u044f', '\u0441\u044b', '\u043d\u044b', '\u04a3', '\u2581', '\u04af', '\u0448', '\u0456\u043d', '\u0448\u0456', '\u2581\u0431\u0430', '\u0493', '\u044b', '\u0442\u044b', '\u2581\u2013', '\u2581\u0441\u0430', '\u0443', '\u0434\u0430', '\u043d\u044b', '\u2581\u043b\u0438', '\u0431\u0435', '\u0440\u0430\u043b', '\u0434\u0430\u043d', '\u0434\u044b', '\u0440\u0443', '\u2581', '\u04d9', '\u0440\u0435', '\u043a', '\u0435\u0442', '\u0456', '\u2581\u0435', '\u0448', '\u2581\u043d', '\u04d9', '\u0442\u0438', '\u0436\u0435', '\u2581\u0431', '\u0435\u0440', '\u043c\u0435', '\u0434\u0456', '.']"}], "[\n[\"\u2581\u0410\u043b\", \"Meanwhile\"],\n[\"\u2581\u0416\", \"Japan\"],\n[\"\u0430\", null],\n[\"\u043f\u043e\", null],\n[\"\u043d\u0438\u044f\", null],\n[\"\u2581\u0431\u0430\", \"slow\"],\n[\"\u044f\", null],\n[\"\u0443\", null],\n[\"\u2581\u0434\u0430\", \"development\"],\n[\"\u043c\u0443\", null],\n[\"\u2581\u0436\", \"path\"],\n[\"\u043e\", null],\n[\"\u043b\u044b\", null],\n[\"\u043d\", null],\n[\"\u0434\u0430\", null],\n[\"\u2581\u043a\", \"stagnated\"],\n[\"\u0456\", null],\n[\"\u0431\u0456\", null],\n[\"\u0440\", null],\n[\"\u0442\u0456\", null],\n[\"\u043a\", null],\n[\"\u0442\u0435\", null],\n[\"\u043f\", null],\n[\"\u2581\", null],\n[\"\u049b\", null],\n[\"\u0430\u043b\", null],\n[\"\u0434\u044b\", null],\n[\",\", null],\n[\"\u2581\u043f\u0440\u0435\", \"Prime\"],\n[\"\u043c\", null],\n[\"\u044c\", null],\n[\"\u0435\u0440\", null],\n[\"-\", null],\n[\"\u043c\u0438\u043d\u0438\", \"Minister\"],\n[\"\u0441\u0442\", null],\n[\"\u0440\", null],\n[\"\u2581\u0421\u0438\", \"Shinzo\"],\n[\"\u043d\u0434\", null],\n[\"\u0437\u043e\", null],\n[\"\u2581\u0410\", \"Abe's\"],\n[\"\u0431\", null],\n[\"\u0435\u043d\", null],\n[\"\u0456\", null],\n[\"\u04a3\", null],\n[\"\u2581\u044d\", \"economic\"],\n[\"\u043a\u043e\u043d\u043e\u043c\u0438\", null],\n[\"\u043a\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u049b\", null],\n[\"\u2581\u0441\u0442\u0440\u0430\", \"strategy's\"],\n[\"\u0442\u0435\", null],\n[\"\u0433\u0438\", null],\n[\"\u044f\", null],\n[\"\u0441\u044b\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", null],\n[\"\u2581\", null],\n[\"\u04af\", \"third\"],\n[\"\u0448\", null],\n[\"\u0456\u043d\", null],\n[\"\u0448\u0456\", null],\n[\"\u2581\u0431\u0430\", \"direction\"],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u0442\u044b\", null],\n[\"\u2581\u2013\", null],\n[\"\u2581\u0441\u0430\", \"trade\"],\n[\"\u0443\", null],\n[\"\u0434\u0430\", null],\n[\"\u043d\u044b\", null],\n[\"\u2581\u043b\u0438\", \"liberalization\"],\n[\"\u0431\u0435\", null],\n[\"\u0440\u0430\u043b\", null],\n[\"\u0434\u0430\u043d\", null],\n[\"\u0434\u044b\", null],\n[\"\u0440\u0443\", null],\n[\"\u2581\", null],\n[\"\u04d9\", \"attempt\"],\n[\"\u0440\u0435\", null],\n[\"\u043a\", null],\n[\"\u0435\u0442\", null],\n[\"\u0456\", null],\n[\"\u2581\u0435\", \"no\"],\n[\"\u0448\", null],\n[\"\u2581\u043d\", \"result\"],\n[\"\u04d9\", null],\n[\"\u0442\u0438\", null],\n[\"\u0436\u0435\", null],\n[\"\u2581\u0431\", \"yielded\"],\n[\"\u0435\u0440\", null],\n[\"\u043c\u0435\", null],\n[\"\u0434\u0456\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u0410\u043b \u0416\u0430\u043f\u043e\u043d\u0438\u044f \u0431\u0430\u044f\u0443 \u0434\u0430\u043c\u0443 \u0436\u043e\u043b\u044b\u043d\u0434\u0430 \u043a\u0456\u0431\u0456\u0440\u0442\u0456\u043a\u0442\u0435\u043f \u049b\u0430\u043b\u0434\u044b, \u043f\u0440\u0435\u043c\u044c\u0435\u0440-\u043c\u0438\u043d\u0438\u0441\u0442\u0440 \u0421\u0438\u043d\u0434\u0437\u043e \u0410\u0431\u0435\u043d\u0456\u04a3 \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0430\u043b\u044b\u049b \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u044f\u0441\u044b\u043d\u044b\u04a3 \u04af\u0448\u0456\u043d\u0448\u0456 \u0431\u0430\u0493\u044b\u0442\u044b \u2013 \u0441\u0430\u0443\u0434\u0430\u043d\u044b \u043b\u0438\u0431\u0435\u0440\u0430\u043b\u0434\u0430\u043d\u0434\u044b\u0440\u0443 \u04d9\u0440\u0435\u043a\u0435\u0442\u0456 \u0435\u0448 \u043d\u04d9\u0442\u0438\u0436\u0435 \u0431\u0435\u0440\u043c\u0435\u0434\u0456.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0410\u043b', '\u2581\u0416', '\u0430', '\u043f\u043e', '\u043d\u0438\u044f', '\u2581\u0431\u0430', '\u044f', '\u0443', '\u2581\u0434\u0430', '\u043c\u0443', '\u2581\u0436', '\u043e', '\u043b\u044b', '\u043d', '\u0434\u0430', '\u2581\u043a', '\u0456', '\u0431\u0456', '\u0440', '\u0442\u0456', '\u043a', '\u0442\u0435', '\u043f', '\u2581', '\u049b', '\u0430\u043b', '\u0434\u044b', ',', '\u2581\u043f\u0440\u0435', '\u043c', '\u044c', '\u0435\u0440', '-', '\u043c\u0438\u043d\u0438', '\u0441\u0442', '\u0440', '\u2581\u0421\u0438', '\u043d\u0434', '\u0437\u043e', '\u2581\u0410', '\u0431', '\u0435\u043d', '\u0456', '\u04a3', '\u2581\u044d', '\u043a\u043e\u043d\u043e\u043c\u0438', '\u043a\u0430', '\u043b\u044b', '\u049b', '\u2581\u0441\u0442\u0440\u0430', '\u0442\u0435', '\u0433\u0438', '\u044f', '\u0441\u044b', '\u043d\u044b', '\u04a3', '\u2581', '\u04af', '\u0448', '\u0456\u043d', '\u0448\u0456', '\u2581\u0431\u0430', '\u0493', '\u044b', '\u0442\u044b', '\u2581\u2013', '\u2581\u0441\u0430', '\u0443', '\u0434\u0430', '\u043d\u044b', '\u2581\u043b\u0438', '\u0431\u0435', '\u0440\u0430\u043b', '\u0434\u0430\u043d', '\u0434\u044b', '\u0440\u0443', '\u2581', '\u04d9', '\u0440\u0435', '\u043a', '\u0435\u0442', '\u0456', '\u2581\u0435', '\u0448', '\u2581\u043d', '\u04d9', '\u0442\u0438', '\u0436\u0435', '\u2581\u0431', '\u0435\u0440', '\u043c\u0435', '\u0434\u0456', '.']"}], "[\n  [\"\u2581\u0410\u043b\", \"\u800c\"],\n  [\"\u2581\u0416\", \"\u65e5\"],\n  [\"\u0430\", null],\n  [\"\u043f\u043e\", null],\n  [\"\u043d\u0438\u044f\", null],\n  [\"\u2581\u0431\u0430\", \"\u7f13\"],\n  [\"\u044f\", null],\n  [\"\u0443\", null],\n  [\"\u2581\u0434\u0430\", \"\u53d1\"],\n  [\"\u043c\u0443\", null],\n  [\"\u2581\u0436\", \"\u9053\"],\n  [\"\u043e\", null],\n  [\"\u043b\u044b\", null],\n  [\"\u043d\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u2581\u043a\", \"\u505c\"],\n  [\"\u0456\", null],\n  [\"\u0431\u0456\", null],\n  [\"\u0440\", null],\n  [\"\u0442\u0456\", null],\n  [\"\u043a\", null],\n  [\"\u0442\u0435\", null],\n  [\"\u043f\", null],\n  [\"\u2581\", null],\n  [\"\u049b\", null],\n  [\"\u0430\u043b\", null],\n  [\"\u0434\u044b\", null],\n  [\",\", null],\n  [\"\u2581\u043f\u0440\u0435\", \"\u9996\"],\n  [\"\u043c\", null],\n  [\"\u044c\", null],\n  [\"\u0435\u0440\", null],\n  [\"-\", null],\n  [\"\u043c\u0438\u043d\u0438\", \"\u76f8\"],\n  [\"\u0441\u0442\", null],\n  [\"\u0440\", null],\n  [\"\u2581\u0421\u0438\", \"\u5b89\"],\n  [\"\u043d\u0434\", null],\n  [\"\u0437\u043e\", null],\n  [\"\u2581\u0410\", \"\u500d\"],\n  [\"\u0431\", null],\n  [\"\u0435\u043d\", null],\n  [\"\u0456\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\u044d\", \"\u7ecf\"],\n  [\"\u043a\u043e\u043d\u043e\u043c\u0438\", null],\n  [\"\u043a\u0430\", null],\n  [\"\u043b\u044b\", null],\n  [\"\u049b\", null],\n  [\"\u2581\u0441\u0442\u0440\u0430\", \"\u6218\"],\n  [\"\u0442\u0435\", null],\n  [\"\u0433\u0438\", null],\n  [\"\u044f\", null],\n  [\"\u0441\u044b\", null],\n  [\"\u043d\u044b\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\", null],\n  [\"\u04af\", \"\u7b2c\"],\n  [\"\u0448\", null],\n  [\"\u0456\u043d\", null],\n  [\"\u0448\u0456\", null],\n  [\"\u2581\u0431\u0430\", \"\u65b9\"],\n  [\"\u0493\", null],\n  [\"\u044b\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u2581\u2013\", null],\n  [\"\u2581\u0441\u0430\", \"\u8d38\"],\n  [\"\u0443\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u043d\u044b\", null],\n  [\"\u2581\u043b\u0438\", \"\u81ea\"],\n  [\"\u0431\u0435\", null],\n  [\"\u0440\u0430\u043b\", null],\n  [\"\u0434\u0430\u043d\", null],\n  [\"\u0434\u044b\", null],\n  [\"\u0440\u0443\", null],\n  [\"\u2581\", null],\n  [\"\u04d9\", \"\u52aa\"],\n  [\"\u0440\u0435\", null],\n  [\"\u043a\", null],\n  [\"\u0435\u0442\", null],\n  [\"\u0456\", null],\n  [\"\u2581\u0435\", \"\u6ca1\"],\n  [\"\u0448\", null],\n  [\"\u2581\u043d\", \"\u7ed3\"],\n  [\"\u04d9\", null],\n  [\"\u0442\u0438\", null],\n  [\"\u0436\u0435\", null],\n  [\"\u2581\u0431\", \"\u679c\"],\n  [\"\u0435\u0440\", null],\n  [\"\u043c\u0435\", null],\n  [\"\u0434\u0456\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u0410\u043b \u0416\u0430\u043f\u043e\u043d\u0438\u044f \u0431\u0430\u044f\u0443 \u0434\u0430\u043c\u0443 \u0436\u043e\u043b\u044b\u043d\u0434\u0430 \u043a\u0456\u0431\u0456\u0440\u0442\u0456\u043a\u0442\u0435\u043f \u049b\u0430\u043b\u0434\u044b, \u043f\u0440\u0435\u043c\u044c\u0435\u0440-\u043c\u0438\u043d\u0438\u0441\u0442\u0440 \u0421\u0438\u043d\u0434\u0437\u043e \u0410\u0431\u0435\u043d\u0456\u04a3 \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0430\u043b\u044b\u049b \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u044f\u0441\u044b\u043d\u044b\u04a3 \u04af\u0448\u0456\u043d\u0448\u0456 \u0431\u0430\u0493\u044b\u0442\u044b \u2013 \u0441\u0430\u0443\u0434\u0430\u043d\u044b \u043b\u0438\u0431\u0435\u0440\u0430\u043b\u0434\u0430\u043d\u0434\u044b\u0440\u0443 \u04d9\u0440\u0435\u043a\u0435\u0442\u0456 \u0435\u0448 \u043d\u04d9\u0442\u0438\u0436\u0435 \u0431\u0435\u0440\u043c\u0435\u0434\u0456.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0410\u043b', '\u2581\u0416', '\u0430', '\u043f\u043e', '\u043d\u0438\u044f', '\u2581\u0431\u0430', '\u044f', '\u0443', '\u2581\u0434\u0430', '\u043c\u0443', '\u2581\u0436', '\u043e', '\u043b\u044b', '\u043d', '\u0434\u0430', '\u2581\u043a', '\u0456', '\u0431\u0456', '\u0440', '\u0442\u0456', '\u043a', '\u0442\u0435', '\u043f', '\u2581', '\u049b', '\u0430\u043b', '\u0434\u044b', ',', '\u2581\u043f\u0440\u0435', '\u043c', '\u044c', '\u0435\u0440', '-', '\u043c\u0438\u043d\u0438', '\u0441\u0442', '\u0440', '\u2581\u0421\u0438', '\u043d\u0434', '\u0437\u043e', '\u2581\u0410', '\u0431', '\u0435\u043d', '\u0456', '\u04a3', '\u2581\u044d', '\u043a\u043e\u043d\u043e\u043c\u0438', '\u043a\u0430', '\u043b\u044b', '\u049b', '\u2581\u0441\u0442\u0440\u0430', '\u0442\u0435', '\u0433\u0438', '\u044f', '\u0441\u044b', '\u043d\u044b', '\u04a3', '\u2581', '\u04af', '\u0448', '\u0456\u043d', '\u0448\u0456', '\u2581\u0431\u0430', '\u0493', '\u044b', '\u0442\u044b', '\u2581\u2013', '\u2581\u0441\u0430', '\u0443', '\u0434\u0430', '\u043d\u044b', '\u2581\u043b\u0438', '\u0431\u0435', '\u0440\u0430\u043b', '\u0434\u0430\u043d', '\u0434\u044b', '\u0440\u0443', '\u2581', '\u04d9', '\u0440\u0435', '\u043a', '\u0435\u0442', '\u0456', '\u2581\u0435', '\u0448', '\u2581\u043d', '\u04d9', '\u0442\u0438', '\u0436\u0435', '\u2581\u0431', '\u0435\u0440', '\u043c\u0435', '\u0434\u0456', '.']"}], "[\n  [\"\u2581\u0410\u043b\", \"\u0410\"],\n  [\"\u2581\u0416\", \"\u042f\u043f\u043e\u043d\u0438\u044f\"],\n  [\"\u0430\", null],\n  [\"\u043f\u043e\", null],\n  [\"\u043d\u0438\u044f\", null],\n  [\"\u2581\u0431\u0430\", \"\u043c\u0435\u0434\u043b\u0435\u043d\u043d\u043e\"],\n  [\"\u044f\", null],\n  [\"\u0443\", null],\n  [\"\u2581\u0434\u0430\", \"\u0440\u0430\u0437\u0432\u0438\u0442\u0438\u044f\"],\n  [\"\u043c\u0443\", null],\n  [\"\u2581\u0436\", \"\u043f\u0443\u0442\u0438\"],\n  [\"\u043e\", null],\n  [\"\u043b\u044b\", null],\n  [\"\u043d\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u2581\u043a\", \"\u0437\u0430\u0441\u0442\u043e\u043f\u043e\u0440\u0438\u043b\u0430\u0441\u044c\"],\n  [\"\u0456\", null],\n  [\"\u0431\u0456\", null],\n  [\"\u0440\", null],\n  [\"\u0442\u0456\", null],\n  [\"\u043a\", null],\n  [\"\u0442\u0435\", null],\n  [\"\u043f\", null],\n  [\"\u2581\", null],\n  [\"\u049b\", null],\n  [\"\u0430\u043b\", null],\n  [\"\u0434\u044b\", null],\n  [\",\", null],\n  [\"\u2581\u043f\u0440\u0435\", \"\u043f\u0440\u0435\u043c\u044c\u0435\u0440-\u043c\u0438\u043d\u0438\u0441\u0442\u0440\u0430\"],\n  [\"\u043c\", null],\n  [\"\u044c\", null],\n  [\"\u0435\u0440\", null],\n  [\"-\", null],\n  [\"\u043c\u0438\u043d\u0438\", null],\n  [\"\u0441\u0442\", null],\n  [\"\u0440\", null],\n  [\"\u2581\u0421\u0438\", \"\u0421\u0438\u043d\u0434\u0437\u043e\"],\n  [\"\u043d\u0434\", null],\n  [\"\u0437\u043e\", null],\n  [\"\u2581\u0410\", \"\u0410\u0431\u044d\"],\n  [\"\u0431\", null],\n  [\"\u0435\u043d\", null],\n  [\"\u0456\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\u044d\", \"\u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0439\"],\n  [\"\u043a\u043e\u043d\u043e\u043c\u0438\", null],\n  [\"\u043a\u0430\", null],\n  [\"\u043b\u044b\", null],\n  [\"\u049b\", null],\n  [\"\u2581\u0441\u0442\u0440\u0430\", \"\u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0438\"],\n  [\"\u0442\u0435\", null],\n  [\"\u0433\u0438\", null],\n  [\"\u044f\", null],\n  [\"\u0441\u044b\", null],\n  [\"\u043d\u044b\", null],\n  [\"\u04a3\", null],\n  [\"\u2581\", null],\n  [\"\u04af\", \"\u0442\u0440\u0435\u0442\u044c\u0435\"],\n  [\"\u0448\", null],\n  [\"\u0456\u043d\", null],\n  [\"\u0448\u0456\", null],\n  [\"\u2581\u0431\u0430\", \"\u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435\"],\n  [\"\u0493\", null],\n  [\"\u044b\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u2581\u2013\", null],\n  [\"\u2581\u0441\u0430\", \"\u0442\u043e\u0440\u0433\u043e\u0432\u043b\u0438\"],\n  [\"\u0443\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u043d\u044b\", null],\n  [\"\u2581\u043b\u0438\", \"\u043b\u0438\u0431\u0435\u0440\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438\"],\n  [\"\u0431\u0435\", null],\n  [\"\u0440\u0430\u043b\", null],\n  [\"\u0434\u0430\u043d\", null],\n  [\"\u0434\u044b\", null],\n  [\"\u0440\u0443\", null],\n  [\"\u2581\", null],\n  [\"\u04d9\", \"\u043f\u043e\u043f\u044b\u0442\u043a\u0430\"],\n  [\"\u0440\u0435\", null],\n  [\"\u043a\", null],\n  [\"\u0435\u0442\", null],\n  [\"\u0456\", null],\n  [\"\u2581\u0435\", \"\u043d\u0438\u043a\u0430\u043a\u0438\u0445\"],\n  [\"\u0448\", null],\n  [\"\u2581\u043d\", \"\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\"],\n  [\"\u04d9\", null],\n  [\"\u0442\u0438\", null],\n  [\"\u0436\u0435\", null],\n  [\"\u2581\u0431\", \"\u043d\u0435 \u0434\u0430\u043b\u0430\"],\n  [\"\u0435\u0440\", null],\n  [\"\u043c\u0435\", null],\n  [\"\u0434\u0456\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u041e\u0441\u044b \u0443\u0430\u049b\u044b\u0442\u049b\u0430 \u0434\u0435\u0439\u0456\u043d \u0431\u0456\u043b\u0456\u043c \u0431\u0435\u0440\u0443 \u043c\u0435\u043d \u043e\u049b\u044b\u0442\u0443 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u044f\u043c\u0435\u043d \u0436\u0430\u0440\u044b\u0441\u0442\u0430 \u0430\u0440\u0442\u0442\u0430 \u049b\u0430\u043b\u044b\u043f \u043a\u0435\u043b\u0434\u0456.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041e', '\u0441\u044b', '\u2581\u0443', '\u0430', '\u049b', '\u044b', '\u0442', '\u049b', '\u0430', '\u2581\u0434\u0435\u0439', '\u0456\u043d', '\u2581\u0431\u0456', '\u043b\u0456', '\u043c', '\u2581\u0431', '\u0435\u0440', '\u0443', '\u2581\u043c', '\u0435\u043d', '\u2581\u043e', '\u049b', '\u044b', '\u0442\u0443', '\u2581\u0442\u0435\u0445', '\u043d\u043e\u043b\u043e\u0433\u0438', '\u044f', '\u043c\u0435\u043d', '\u2581\u0436', '\u0430\u0440', '\u044b', '\u0441\u0442\u0430', '\u2581\u0430\u0440', '\u0442', '\u0442\u0430', '\u2581', '\u049b', '\u0430', '\u043b\u044b', '\u043f', '\u2581\u043a', '\u0435', '\u043b', '\u0434\u0456', '.']"}], "[\n[\"\u2581\u041e\", \"Until\"],\n[\"\u0441\u044b\", null],\n[\"\u2581\u0443\", null],\n[\"\u0430\", null],\n[\"\u049b\", null],\n[\"\u044b\", null],\n[\"\u0442\", null],\n[\"\u049b\", null],\n[\"\u0430\", null],\n[\"\u2581\u0434\u0435\u0439\", null],\n[\"\u0456\u043d\", null],\n[\"\u2581\u0431\u0456\", \"education\"],\n[\"\u043b\u0456\", null],\n[\"\u043c\", null],\n[\"\u2581\u0431\", null],\n[\"\u0435\u0440\", null],\n[\"\u0443\", null],\n[\"\u2581\u043c\", \"and\"],\n[\"\u0435\u043d\", null],\n[\"\u2581\u043e\", \"learning\"],\n[\"\u049b\", null],\n[\"\u044b\", null],\n[\"\u0442\u0443\", null],\n[\"\u2581\u0442\u0435\u0445\", \"technology\"],\n[\"\u043d\u043e\u043b\u043e\u0433\u0438\", null],\n[\"\u044f\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u2581\u0436\", \"race\"],\n[\"\u0430\u0440\", null],\n[\"\u044b\", null],\n[\"\u0441\u0442\u0430\", null],\n[\"\u2581\u0430\u0440\", \"behind\"],\n[\"\u0442\", null],\n[\"\u0442\u0430\", null],\n[\"\u2581\", null],\n[\"\u049b\", \"lagged\"],\n[\"\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u043f\", null],\n[\"\u2581\u043a\", null],\n[\"\u0435\", null],\n[\"\u043b\", null],\n[\"\u0434\u0456\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u041e\u0441\u044b \u0443\u0430\u049b\u044b\u0442\u049b\u0430 \u0434\u0435\u0439\u0456\u043d \u0431\u0456\u043b\u0456\u043c \u0431\u0435\u0440\u0443 \u043c\u0435\u043d \u043e\u049b\u044b\u0442\u0443 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u044f\u043c\u0435\u043d \u0436\u0430\u0440\u044b\u0441\u0442\u0430 \u0430\u0440\u0442\u0442\u0430 \u049b\u0430\u043b\u044b\u043f \u043a\u0435\u043b\u0434\u0456.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041e', '\u0441\u044b', '\u2581\u0443', '\u0430', '\u049b', '\u044b', '\u0442', '\u049b', '\u0430', '\u2581\u0434\u0435\u0439', '\u0456\u043d', '\u2581\u0431\u0456', '\u043b\u0456', '\u043c', '\u2581\u0431', '\u0435\u0440', '\u0443', '\u2581\u043c', '\u0435\u043d', '\u2581\u043e', '\u049b', '\u044b', '\u0442\u0443', '\u2581\u0442\u0435\u0445', '\u043d\u043e\u043b\u043e\u0433\u0438', '\u044f', '\u043c\u0435\u043d', '\u2581\u0436', '\u0430\u0440', '\u044b', '\u0441\u0442\u0430', '\u2581\u0430\u0440', '\u0442', '\u0442\u0430', '\u2581', '\u049b', '\u0430', '\u043b\u044b', '\u043f', '\u2581\u043a', '\u0435', '\u043b', '\u0434\u0456', '.']"}], "[\n[\"\u2581\u041e\", \"\u5230\"],\n[\"\u0441\u044b\", null],\n[\"\u2581\u0443\", null],\n[\"\u0430\", null],\n[\"\u049b\", null],\n[\"\u044b\", null],\n[\"\u0442\", null],\n[\"\u049b\", null],\n[\"\u0430\", null],\n[\"\u2581\u0434\u0435\u0439\", null],\n[\"\u0456\u043d\", null],\n[\"\u2581\u0431\u0456\", null],\n[\"\u043b\u0456\", null],\n[\"\u043c\", null],\n[\"\u2581\u0431\", null],\n[\"\u0435\u0440\", null],\n[\"\u0443\", null],\n[\"\u2581\u043c\", null],\n[\"\u0435\u043d\", null],\n[\"\u2581\u043e\", null],\n[\"\u049b\", null],\n[\"\u044b\", null],\n[\"\u0442\u0443\", null],\n[\"\u2581\u0442\u0435\u0445\", \"\u6280\u672f\"],\n[\"\u043d\u043e\u043b\u043e\u0433\u0438\", null],\n[\"\u044f\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u2581\u0436\", null],\n[\"\u0430\u0440\", null],\n[\"\u044b\", null],\n[\"\u0441\u0442\u0430\", null],\n[\"\u2581\u0430\u0440\", null],\n[\"\u0442\", null],\n[\"\u0442\u0430\", null],\n[\"\u2581\", null],\n[\"\u049b\", null],\n[\"\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u043f\", null],\n[\"\u2581\u043a\", null],\n[\"\u0435\", null],\n[\"\u043b\", null],\n[\"\u0434\u0456\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u041e\u0441\u044b \u0443\u0430\u049b\u044b\u0442\u049b\u0430 \u0434\u0435\u0439\u0456\u043d \u0431\u0456\u043b\u0456\u043c \u0431\u0435\u0440\u0443 \u043c\u0435\u043d \u043e\u049b\u044b\u0442\u0443 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u044f\u043c\u0435\u043d \u0436\u0430\u0440\u044b\u0441\u0442\u0430 \u0430\u0440\u0442\u0442\u0430 \u049b\u0430\u043b\u044b\u043f \u043a\u0435\u043b\u0434\u0456.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041e', '\u0441\u044b', '\u2581\u0443', '\u0430', '\u049b', '\u044b', '\u0442', '\u049b', '\u0430', '\u2581\u0434\u0435\u0439', '\u0456\u043d', '\u2581\u0431\u0456', '\u043b\u0456', '\u043c', '\u2581\u0431', '\u0435\u0440', '\u0443', '\u2581\u043c', '\u0435\u043d', '\u2581\u043e', '\u049b', '\u044b', '\u0442\u0443', '\u2581\u0442\u0435\u0445', '\u043d\u043e\u043b\u043e\u0433\u0438', '\u044f', '\u043c\u0435\u043d', '\u2581\u0436', '\u0430\u0440', '\u044b', '\u0441\u0442\u0430', '\u2581\u0430\u0440', '\u0442', '\u0442\u0430', '\u2581', '\u049b', '\u0430', '\u043b\u044b', '\u043f', '\u2581\u043a', '\u0435', '\u043b', '\u0434\u0456', '.']"}], "[\n  [\"\u2581\u041e\", \"\u0414\u043e\"],\n  [\"\u0441\u044b\", null],\n  [\"\u2581\u0443\", null],\n  [\"\u0430\", null],\n  [\"\u049b\", null],\n  [\"\u044b\", null],\n  [\"\u0442\", null],\n  [\"\u049b\", null],\n  [\"\u0430\", null],\n  [\"\u2581\u0434\u0435\u0439\", null],\n  [\"\u0456\u043d\", null],\n  [\"\u2581\u0431\u0456\", \"\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435\"],\n  [\"\u043b\u0456\", null],\n  [\"\u043c\", null],\n  [\"\u2581\u0431\", null],\n  [\"\u0435\u0440\", null],\n  [\"\u0443\", null],\n  [\"\u2581\u043c\", null],\n  [\"\u0435\u043d\", null],\n  [\"\u2581\u043e\", \"\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\"],\n  [\"\u049b\", null],\n  [\"\u044b\", null],\n  [\"\u0442\u0443\", null],\n  [\"\u2581\u0442\u0435\u0445\", \"\u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0435\u0439\"],\n  [\"\u043d\u043e\u043b\u043e\u0433\u0438\", null],\n  [\"\u044f\", null],\n  [\"\u043c\u0435\u043d\", null],\n  [\"\u2581\u0436\", \"\u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u0438\"],\n  [\"\u0430\u0440\", null],\n  [\"\u044b\", null],\n  [\"\u0441\u0442\u0430\", null],\n  [\"\u2581\u0430\u0440\", \"\u043e\u0442\u0441\u0442\u0430\u0432\u0430\u043b\u0438\"],\n  [\"\u0442\", null],\n  [\"\u0442\u0430\", null],\n  [\"\u2581\", null],\n  [\"\u049b\", null],\n  [\"\u0430\", null],\n  [\"\u043b\u044b\", null],\n  [\"\u043f\", null],\n  [\"\u2581\u043a\", null],\n  [\"\u0435\", null],\n  [\"\u043b\", null],\n  [\"\u0434\u0456\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0418\u043c\u043c\u0443\u043d\u043d\u044b\u0435 \u043a\u043b\u0435\u0442\u043a\u0438 \u043f\u0440\u0435\u0432\u0440\u0430\u0442\u0438\u043b\u0438\u0441\u044c \u0432 \u043a\u043b\u0435\u0442\u043a\u0438, \u043f\u043e\u0432\u044b\u0448\u0430\u044e\u0449\u0438\u0435 \u0442\u043e\u043b\u0435\u0440\u0430\u043d\u0442\u043d\u043e\u0441\u0442\u044c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0418', '\u043c', '\u043c\u0443', '\u043d', '\u043d\u044b\u0435', '\u2581\u043a', '\u043b\u0435\u0442', '\u043a\u0438', '\u2581\u043f\u0440\u0435', '\u0432\u0440\u0430', '\u0442\u0438', '\u043b\u0438\u0441\u044c', '\u2581\u0432', '\u2581\u043a', '\u043b\u0435\u0442', '\u043a\u0438', ',', '\u2581\u043f\u043e', '\u0432\u044b', '\u0448\u0430', '\u044e\u0449\u0438\u0435', '\u2581\u0442\u043e', '\u043b\u0435', '\u0440\u0430\u043d', '\u0442', '\u043d\u043e\u0441\u0442\u044c', '.']"}], "[\n[\"\u2581\u0418\", \"Immune\"],\n[\"\u043c\", null],\n[\"\u043c\u0443\", null],\n[\"\u043d\", null],\n[\"\u043d\u044b\u0435\", null],\n[\"\u2581\u043a\", null],\n[\"\u043b\u0435\u0442\", \"cells\"],\n[\"\u043a\u0438\", null],\n[\"\u2581\u043f\u0440\u0435\", \"turned\"],\n[\"\u0432\u0440\u0430\", null],\n[\"\u0442\u0438\", null],\n[\"\u043b\u0438\u0441\u044c\", null],\n[\"\u2581\u0432\", \"into\"],\n[\"\u2581\u043a\", null],\n[\"\u043b\u0435\u0442\", \"cells\"],\n[\"\u043a\u0438\", null],\n[\",\", null],\n[\"\u2581\u043f\u043e\", \"increasing\"],\n[\"\u0432\u044b\", null],\n[\"\u0448\u0430\", null],\n[\"\u044e\u0449\u0438\u0435\", null],\n[\"\u2581\u0442\u043e\", \"tolerance\"],\n[\"\u043b\u0435\", null],\n[\"\u0440\u0430\u043d\", null],\n[\"\u0442\", null],\n[\"\u043d\u043e\u0441\u0442\u044c\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u0418\u043c\u043c\u0443\u043d\u043d\u044b\u0435 \u043a\u043b\u0435\u0442\u043a\u0438 \u043f\u0440\u0435\u0432\u0440\u0430\u0442\u0438\u043b\u0438\u0441\u044c \u0432 \u043a\u043b\u0435\u0442\u043a\u0438, \u043f\u043e\u0432\u044b\u0448\u0430\u044e\u0449\u0438\u0435 \u0442\u043e\u043b\u0435\u0440\u0430\u043d\u0442\u043d\u043e\u0441\u0442\u044c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0418', '\u043c', '\u043c\u0443', '\u043d', '\u043d\u044b\u0435', '\u2581\u043a', '\u043b\u0435\u0442', '\u043a\u0438', '\u2581\u043f\u0440\u0435', '\u0432\u0440\u0430', '\u0442\u0438', '\u043b\u0438\u0441\u044c', '\u2581\u0432', '\u2581\u043a', '\u043b\u0435\u0442', '\u043a\u0438', ',', '\u2581\u043f\u043e', '\u0432\u044b', '\u0448\u0430', '\u044e\u0449\u0438\u0435', '\u2581\u0442\u043e', '\u043b\u0435', '\u0440\u0430\u043d', '\u0442', '\u043d\u043e\u0441\u0442\u044c', '.']"}], "[\n  [\"\u2581\u0418\", \"\u514d\"],\n  [\"\u043c\", null],\n  [\"\u043c\u0443\", null],\n  [\"\u043d\", null],\n  [\"\u043d\u044b\u0435\", null],\n  [\"\u2581\u043a\", null],\n  [\"\u043b\u0435\u0442\", null],\n  [\"\u043a\u0438\", null],\n  [\"\u2581\u043f\u0440\u0435\", \"\u8f6c\u5316\"],\n  [\"\u0432\u0440\u0430\", null],\n  [\"\u0442\u0438\", null],\n  [\"\u043b\u0438\u0441\u044c\", null],\n  [\"\u2581\u0432\", \"\u6210\"],\n  [\"\u2581\u043a\", null],\n  [\"\u043b\u0435\u0442\", null],\n  [\"\u043a\u0438\", null],\n  [\",\", null],\n  [\"\u2581\u043f\u043e\", \"\u63d0\u9ad8\"],\n  [\"\u0432\u044b\", null],\n  [\"\u0448\u0430\", null],\n  [\"\u044e\u0449\u0438\u0435\", null],\n  [\"\u2581\u0442\u043e\", \"\u8010\u53d7\u6027\"],\n  [\"\u043b\u0435\", null],\n  [\"\u0440\u0430\u043d\", null],\n  [\"\u0442\", null],\n  [\"\u043d\u043e\u0441\u0442\u044c\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u0418\u043c\u043c\u0443\u043d\u043d\u044b\u0435 \u043a\u043b\u0435\u0442\u043a\u0438 \u043f\u0440\u0435\u0432\u0440\u0430\u0442\u0438\u043b\u0438\u0441\u044c \u0432 \u043a\u043b\u0435\u0442\u043a\u0438, \u043f\u043e\u0432\u044b\u0448\u0430\u044e\u0449\u0438\u0435 \u0442\u043e\u043b\u0435\u0440\u0430\u043d\u0442\u043d\u043e\u0441\u0442\u044c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0418', '\u043c', '\u043c\u0443', '\u043d', '\u043d\u044b\u0435', '\u2581\u043a', '\u043b\u0435\u0442', '\u043a\u0438', '\u2581\u043f\u0440\u0435', '\u0432\u0440\u0430', '\u0442\u0438', '\u043b\u0438\u0441\u044c', '\u2581\u0432', '\u2581\u043a', '\u043b\u0435\u0442', '\u043a\u0438', ',', '\u2581\u043f\u043e', '\u0432\u044b', '\u0448\u0430', '\u044e\u0449\u0438\u0435', '\u2581\u0442\u043e', '\u043b\u0435', '\u0440\u0430\u043d', '\u0442', '\u043d\u043e\u0441\u0442\u044c', '.']"}], "[\n  [\"\u2581\u0418\", \"\u0418\u043c\u043c\u0443\u043d\u0434\u044b\u049b\"],\n  [\"\u043c\", null],\n  [\"\u043c\u0443\", null],\n  [\"\u043d\", null],\n  [\"\u043d\u044b\u0435\", null],\n  [\"\u2581\u043a\", \"\u0436\u0430\u0441\u0443\u0448\u0430\u043b\u0430\u0440\"],\n  [\"\u043b\u0435\u0442\", null],\n  [\"\u043a\u0438\", null],\n  [\"\u2581\u043f\u0440\u0435\", \"\u0430\u0439\u043d\u0430\u043b\u0434\u044b\"],\n  [\"\u0432\u0440\u0430\", null],\n  [\"\u0442\u0438\", null],\n  [\"\u043b\u0438\u0441\u044c\", null],\n  [\"\u2581\u0432\", null],\n  [\"\u2581\u043a\", \"\u0436\u0430\u0441\u0443\u0448\u0430\u043b\u0430\u0440\u0493\u0430\"],\n  [\"\u043b\u0435\u0442\", null],\n  [\"\u043a\u0438\", null],\n  [\",\", null],\n  [\"\u2581\u043f\u043e\", \"\u0430\u0440\u0442\u0442\u044b\u0440\u0430\u0442\u044b\u043d\"],\n  [\"\u0432\u044b\", null],\n  [\"\u0448\u0430\", null],\n  [\"\u044e\u0449\u0438\u0435\", null],\n  [\"\u2581\u0442\u043e\", \"\u0442\u04e9\u0437\u0456\u043c\u0434\u0456\u043b\u0456\u043a\u0442\u0456\"],\n  [\"\u043b\u0435\", null],\n  [\"\u0440\u0430\u043d\", null],\n  [\"\u0442\", null],\n  [\"\u043d\u043e\u0441\u0442\u044c\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0421\u0442\u043e\u0438\u0442 \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c, \u0447\u0442\u043e \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044f \u0441 \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u044b\u043c\u0438 \u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u043c\u0438 \u0443\u0434\u043e\u0441\u0442\u043e\u0432\u0435\u0440\u0435\u043d\u0438\u044f\u043c\u0438 \u043e\u0441\u0442\u0430\u0435\u0442\u0441\u044f \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u0440\u0435\u0447\u0438\u0432\u043e\u0439.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0421', '\u0442\u043e', '\u0438\u0442', '\u2581\u043e\u0442', '\u043c\u0435', '\u0442\u0438', '\u0442\u044c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0441\u0438', '\u0442\u0443', '\u0430', '\u0446\u0438\u044f', '\u2581\u0441', '\u2581\u043c\u0435\u0436\u0434\u0443', '\u043d\u0430\u0440\u043e\u0434', '\u043d\u044b\u043c\u0438', '\u2581\u0432\u043e', '\u0434\u0438', '\u0442\u0435\u043b\u044c', '\u0441\u043a\u0438', '\u043c\u0438', '\u2581\u0443', '\u0434\u043e', '\u0441\u0442\u043e', '\u0432\u0435', '\u0440\u0435', '\u043d\u0438\u044f', '\u043c\u0438', '\u2581\u043e\u0441\u0442\u0430', '\u0435\u0442\u0441\u044f', '\u2581\u043f\u0440\u043e\u0442\u0438', '\u0432\u043e', '\u0440\u0435', '\u0447\u0438', '\u0432\u043e\u0439', '.']"}], "[\n    [\"\u2581\u0421\", \"It\"],\n    [\"\u0442\u043e\", null],\n    [\"\u0438\u0442\", null],\n    [\"\u2581\u043e\u0442\", null],\n    [\"\u043c\u0435\", null],\n    [\"\u0442\u0438\", null],\n    [\"\u0442\u044c\", null],\n    [\",\", null],\n    [\"\u2581\u0447\u0442\u043e\", null],\n    [\"\u2581\u0441\u0438\", \"situation\"],\n    [\"\u0442\u0443\", null],\n    [\"\u0430\", null],\n    [\"\u0446\u0438\u044f\", null],\n    [\"\u2581\u0441\", null],\n    [\"\u2581\u043c\u0435\u0436\u0434\u0443\", \"international\"],\n    [\"\u043d\u0430\u0440\u043e\u0434\", null],\n    [\"\u043d\u044b\u043c\u0438\", null],\n    [\"\u2581\u0432\u043e\", \"driver's\"],\n    [\"\u0434\u0438\", null],\n    [\"\u0442\u0435\u043b\u044c\", null],\n    [\"\u0441\u043a\u0438\", null],\n    [\"\u043c\u0438\", null],\n    [\"\u2581\u0443\", \"licenses\"],\n    [\"\u0434\u043e\", null],\n    [\"\u0441\u0442\u043e\", null],\n    [\"\u0432\u0435\", null],\n    [\"\u0440\u0435\", null],\n    [\"\u043d\u0438\u044f\", null],\n    [\"\u043c\u0438\", null],\n    [\"\u2581\u043e\u0441\u0442\u0430\", \"remains\"],\n    [\"\u0435\u0442\u0441\u044f\", null],\n    [\"\u2581\u043f\u0440\u043e\u0442\u0438\", \"contradictory\"],\n    [\"\u0432\u043e\", null],\n    [\"\u0440\u0435\", null],\n    [\"\u0447\u0438\", null],\n    [\"\u0432\u043e\u0439\", null],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u0421\u0442\u043e\u0438\u0442 \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c, \u0447\u0442\u043e \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044f \u0441 \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u044b\u043c\u0438 \u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u043c\u0438 \u0443\u0434\u043e\u0441\u0442\u043e\u0432\u0435\u0440\u0435\u043d\u0438\u044f\u043c\u0438 \u043e\u0441\u0442\u0430\u0435\u0442\u0441\u044f \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u0440\u0435\u0447\u0438\u0432\u043e\u0439.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0421', '\u0442\u043e', '\u0438\u0442', '\u2581\u043e\u0442', '\u043c\u0435', '\u0442\u0438', '\u0442\u044c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0441\u0438', '\u0442\u0443', '\u0430', '\u0446\u0438\u044f', '\u2581\u0441', '\u2581\u043c\u0435\u0436\u0434\u0443', '\u043d\u0430\u0440\u043e\u0434', '\u043d\u044b\u043c\u0438', '\u2581\u0432\u043e', '\u0434\u0438', '\u0442\u0435\u043b\u044c', '\u0441\u043a\u0438', '\u043c\u0438', '\u2581\u0443', '\u0434\u043e', '\u0441\u0442\u043e', '\u0432\u0435', '\u0440\u0435', '\u043d\u0438\u044f', '\u043c\u0438', '\u2581\u043e\u0441\u0442\u0430', '\u0435\u0442\u0441\u044f', '\u2581\u043f\u0440\u043e\u0442\u0438', '\u0432\u043e', '\u0440\u0435', '\u0447\u0438', '\u0432\u043e\u0439', '.']"}], "[\n[\"\u2581\u0421\", \"\u503c\u5f97\"],\n[\"\u0442\u043e\", null],\n[\"\u0438\u0442\", null],\n[\"\u2581\u043e\u0442\", null],\n[\"\u043c\u0435\", null],\n[\"\u0442\u0438\", null],\n[\"\u0442\u044c\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0441\u0438\", \"\u60c5\u51b5\"],\n[\"\u0442\u0443\", null],\n[\"\u0430\", null],\n[\"\u0446\u0438\u044f\", null],\n[\"\u2581\u0441\", null],\n[\"\u2581\u043c\u0435\u0436\u0434\u0443\", \"\u56fd\u9645\"],\n[\"\u043d\u0430\u0440\u043e\u0434\", null],\n[\"\u043d\u044b\u043c\u0438\", null],\n[\"\u2581\u0432\u043e\", \"\u9a7e\u9a76\"],\n[\"\u0434\u0438\", null],\n[\"\u0442\u0435\u043b\u044c\", null],\n[\"\u0441\u043a\u0438\", null],\n[\"\u043c\u0438\", null],\n[\"\u2581\u0443\", \"\u6267\u7167\"],\n[\"\u0434\u043e\", null],\n[\"\u0441\u0442\u043e\", null],\n[\"\u0432\u0435\", null],\n[\"\u0440\u0435\", null],\n[\"\u043d\u0438\u044f\", null],\n[\"\u043c\u0438\", null],\n[\"\u2581\u043e\u0441\u0442\u0430\", \"\u4ecd\u7136\"],\n[\"\u0435\u0442\u0441\u044f\", null],\n[\"\u2581\u043f\u0440\u043e\u0442\u0438\", \"\u5b58\u5728\"],\n[\"\u0432\u043e\", null],\n[\"\u0440\u0435\", null],\n[\"\u0447\u0438\", null],\n[\"\u0432\u043e\u0439\", \"\u77db\u76fe\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u0421\u0442\u043e\u0438\u0442 \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c, \u0447\u0442\u043e \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044f \u0441 \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u044b\u043c\u0438 \u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u043c\u0438 \u0443\u0434\u043e\u0441\u0442\u043e\u0432\u0435\u0440\u0435\u043d\u0438\u044f\u043c\u0438 \u043e\u0441\u0442\u0430\u0435\u0442\u0441\u044f \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u0440\u0435\u0447\u0438\u0432\u043e\u0439.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0421', '\u0442\u043e', '\u0438\u0442', '\u2581\u043e\u0442', '\u043c\u0435', '\u0442\u0438', '\u0442\u044c', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0441\u0438', '\u0442\u0443', '\u0430', '\u0446\u0438\u044f', '\u2581\u0441', '\u2581\u043c\u0435\u0436\u0434\u0443', '\u043d\u0430\u0440\u043e\u0434', '\u043d\u044b\u043c\u0438', '\u2581\u0432\u043e', '\u0434\u0438', '\u0442\u0435\u043b\u044c', '\u0441\u043a\u0438', '\u043c\u0438', '\u2581\u0443', '\u0434\u043e', '\u0441\u0442\u043e', '\u0432\u0435', '\u0440\u0435', '\u043d\u0438\u044f', '\u043c\u0438', '\u2581\u043e\u0441\u0442\u0430', '\u0435\u0442\u0441\u044f', '\u2581\u043f\u0440\u043e\u0442\u0438', '\u0432\u043e', '\u0440\u0435', '\u0447\u0438', '\u0432\u043e\u0439', '.']"}], "[\n    [\"\u2581\u0421\", \"\u0410\u0439\u0442\u0430\"],\n    [\"\u0442\u043e\", null],\n    [\"\u0438\u0442\", null],\n    [\"\u2581\u043e\u0442\", null],\n    [\"\u043c\u0435\", null],\n    [\"\u0442\u0438\", null],\n    [\"\u0442\u044c\", null],\n    [\",\", null],\n    [\"\u2581\u0447\u0442\u043e\", null],\n    [\"\u2581\u0441\u0438\", \"\u0436\u0430\u0493\u0434\u0430\u0439\"],\n    [\"\u0442\u0443\", null],\n    [\"\u0430\", null],\n    [\"\u0446\u0438\u044f\", null],\n    [\"\u2581\u0441\", null],\n    [\"\u2581\u043c\u0435\u0436\u0434\u0443\", \"\u0445\u0430\u043b\u044b\u049b\u0430\u0440\u0430\u043b\u044b\u049b\"],\n    [\"\u043d\u0430\u0440\u043e\u0434\", null],\n    [\"\u043d\u044b\u043c\u0438\", null],\n    [\"\u2581\u0432\u043e\", \"\u0436\u04af\u0440\u0433\u0456\u0437\u0443\u0448\u0456\"],\n    [\"\u0434\u0438\", null],\n    [\"\u0442\u0435\u043b\u044c\", null],\n    [\"\u0441\u043a\u0438\", null],\n    [\"\u043c\u0438\", null],\n    [\"\u2581\u0443\", \"\u043a\u0443\u04d9\u043b\u0456\u043a\u0442\u0435\u0440\u0433\u0435\"],\n    [\"\u0434\u043e\", null],\n    [\"\u0441\u0442\u043e\", null],\n    [\"\u0432\u0435\", null],\n    [\"\u0440\u0435\", null],\n    [\"\u043d\u0438\u044f\", null],\n    [\"\u043c\u0438\", null],\n    [\"\u2581\u043e\u0441\u0442\u0430\", \"\u049b\u0430\u043b\u0430\u0434\u044b\"],\n    [\"\u0435\u0442\u0441\u044f\", null],\n    [\"\u2581\u043f\u0440\u043e\u0442\u0438\", \"\u049b\u0430\u0439\u0448\u044b\u043b\u044b\u049b\u0442\u044b\"],\n    [\"\u0432\u043e\", null],\n    [\"\u0440\u0435\", null],\n    [\"\u0447\u0438\", null],\n    [\"\u0432\u043e\u0439\", null],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041e\u0431 \u044d\u0442\u043e\u043c \u0432 \u0447\u0435\u0442\u0432\u0435\u0440\u0433, 3 \u0430\u0432\u0433\u0443\u0441\u0442\u0430, \u0441\u043e\u043e\u0431\u0449\u0438\u043b\u0438 \u0432 \u0432\u0435\u0434\u043e\u043c\u0441\u0442\u0432\u0435, \u043f\u0435\u0440\u0435\u0434\u0430\u0435\u0442 \u0420\u0418\u0410 \u041d\u043e\u0432\u043e\u0441\u0442\u0438.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041e\u0431', '\u2581\u044d\u0442\u043e\u043c', '\u2581\u0432', '\u2581\u0447\u0435', '\u0442\u0432\u0435\u0440', '\u0433', ',', '\u2581', '3', '\u2581\u0430\u0432\u0433\u0443\u0441\u0442\u0430', ',', '\u2581\u0441\u043e', '\u043e\u0431', '\u0449\u0438', '\u043b\u0438', '\u2581\u0432', '\u2581\u0432\u0435', '\u0434\u043e\u043c', '\u0441\u0442\u0432\u0435', ',', '\u2581\u043f\u0435\u0440\u0435', '\u0434\u0430', '\u0435\u0442', '\u2581\u0420', '\u0418', '\u0410', '\u2581\u041d\u043e', '\u0432\u043e', '\u0441\u0442\u0438', '.']"}], "[\n[\"\u2581\u041e\u0431\", null],\n[\"\u2581\u044d\u0442\u043e\u043c\", \"this\"],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0447\u0435\", null],\n[\"\u0442\u0432\u0435\u0440\", null],\n[\"\u0433\", null],\n[\",\", null],\n[\"\u2581\", null],\n[\"3\", null],\n[\"\u2581\u0430\u0432\u0433\u0443\u0441\u0442\u0430\", \"August\"],\n[\",\", null],\n[\"\u2581\u0441\u043e\", null],\n[\"\u043e\u0431\", null],\n[\"\u0449\u0438\", null],\n[\"\u043b\u0438\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0432\u0435\", null],\n[\"\u0434\u043e\u043c\", null],\n[\"\u0441\u0442\u0432\u0435\", null],\n[\",\", null],\n[\"\u2581\u043f\u0435\u0440\u0435\", null],\n[\"\u0434\u0430\", null],\n[\"\u0435\u0442\", null],\n[\"\u2581\u0420\", null],\n[\"\u0418\", null],\n[\"\u0410\", null],\n[\"\u2581\u041d\u043e\", null],\n[\"\u0432\u043e\", null],\n[\"\u0441\u0442\u0438\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u041e\u0431 \u044d\u0442\u043e\u043c \u0432 \u0447\u0435\u0442\u0432\u0435\u0440\u0433, 3 \u0430\u0432\u0433\u0443\u0441\u0442\u0430, \u0441\u043e\u043e\u0431\u0449\u0438\u043b\u0438 \u0432 \u0432\u0435\u0434\u043e\u043c\u0441\u0442\u0432\u0435, \u043f\u0435\u0440\u0435\u0434\u0430\u0435\u0442 \u0420\u0418\u0410 \u041d\u043e\u0432\u043e\u0441\u0442\u0438.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041e\u0431', '\u2581\u044d\u0442\u043e\u043c', '\u2581\u0432', '\u2581\u0447\u0435', '\u0442\u0432\u0435\u0440', '\u0433', ',', '\u2581', '3', '\u2581\u0430\u0432\u0433\u0443\u0441\u0442\u0430', ',', '\u2581\u0441\u043e', '\u043e\u0431', '\u0449\u0438', '\u043b\u0438', '\u2581\u0432', '\u2581\u0432\u0435', '\u0434\u043e\u043c', '\u0441\u0442\u0432\u0435', ',', '\u2581\u043f\u0435\u0440\u0435', '\u0434\u0430', '\u0435\u0442', '\u2581\u0420', '\u0418', '\u0410', '\u2581\u041d\u043e', '\u0432\u043e', '\u0441\u0442\u0438', '.']"}], "[\n  [\"\u2581\u041e\u0431\", \"\u5173\u4e8e\"],\n  [\"\u2581\u044d\u0442\u043e\u043c\", null],\n  [\"\u2581\u0432\", null],\n  [\"\u2581\u0447\u0435\", null],\n  [\"\u0442\u0432\u0435\u0440\", null],\n  [\"\u0433\", \"\u661f\u671f\u56db\"],\n  [\",\", null],\n  [\"\u2581\", null],\n  [\"3\", null],\n  [\"\u2581\u0430\u0432\u0433\u0443\u0441\u0442\u0430\", \"8\u6708\"],\n  [\",\", null],\n  [\"\u2581\u0441\u043e\", null],\n  [\"\u043e\u0431\", null],\n  [\"\u0449\u0438\", null],\n  [\"\u043b\u0438\", \"\u62a5\u544a\"],\n  [\"\u2581\u0432\", null],\n  [\"\u2581\u0432\u0435\", null],\n  [\"\u0434\u043e\u043c\", null],\n  [\"\u0441\u0442\u0432\u0435\", \"\u90e8\u95e8\"],\n  [\",\", null],\n  [\"\u2581\u043f\u0435\u0440\u0435\", null],\n  [\"\u0434\u0430\", null],\n  [\"\u0435\u0442\", \"\u8f6c\u53d1\"],\n  [\"\u2581\u0420\", null],\n  [\"\u0418\", null],\n  [\"\u0410\", null],\n  [\"\u2581\u041d\u043e\", null],\n  [\"\u0432\u043e\", null],\n  [\"\u0441\u0442\u0438\", \"\u4fc4\u7f57\u65af\u65b0\u95fb\u793e\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u041e\u0431 \u044d\u0442\u043e\u043c \u0432 \u0447\u0435\u0442\u0432\u0435\u0440\u0433, 3 \u0430\u0432\u0433\u0443\u0441\u0442\u0430, \u0441\u043e\u043e\u0431\u0449\u0438\u043b\u0438 \u0432 \u0432\u0435\u0434\u043e\u043c\u0441\u0442\u0432\u0435, \u043f\u0435\u0440\u0435\u0434\u0430\u0435\u0442 \u0420\u0418\u0410 \u041d\u043e\u0432\u043e\u0441\u0442\u0438.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041e\u0431', '\u2581\u044d\u0442\u043e\u043c', '\u2581\u0432', '\u2581\u0447\u0435', '\u0442\u0432\u0435\u0440', '\u0433', ',', '\u2581', '3', '\u2581\u0430\u0432\u0433\u0443\u0441\u0442\u0430', ',', '\u2581\u0441\u043e', '\u043e\u0431', '\u0449\u0438', '\u043b\u0438', '\u2581\u0432', '\u2581\u0432\u0435', '\u0434\u043e\u043c', '\u0441\u0442\u0432\u0435', ',', '\u2581\u043f\u0435\u0440\u0435', '\u0434\u0430', '\u0435\u0442', '\u2581\u0420', '\u0418', '\u0410', '\u2581\u041d\u043e', '\u0432\u043e', '\u0441\u0442\u0438', '.']"}], "[\n  [\"\u2581\u041e\u0431\", \"\u0411\u04b1\u043b\"],\n  [\"\u2581\u044d\u0442\u043e\u043c\", null],\n  [\"\u2581\u0432\", null],\n  [\"\u2581\u0447\u0435\", \"\u0431\u0435\u0439\u0441\u0435\u043d\u0431\u0456\"],\n  [\"\u0442\u0432\u0435\u0440\", null],\n  [\"\u0433\", null],\n  [\",\", null],\n  [\"\u2581\", null],\n  [\"3\", null],\n  [\"\u2581\u0430\u0432\u0433\u0443\u0441\u0442\u0430\", \"\u0442\u0430\u043c\u044b\u0437\"],\n  [\",\", null],\n  [\"\u2581\u0441\u043e\", \"\u0445\u0430\u0431\u0430\u0440\u043b\u0430\u0434\u044b\"],\n  [\"\u043e\u0431\", null],\n  [\"\u0449\u0438\", null],\n  [\"\u043b\u0438\", null],\n  [\"\u2581\u0432\", null],\n  [\"\u2581\u0432\u0435\", \"\u0432\u0435\u0434\u043e\u043c\u0441\u0442\u0432\u043e\"],\n  [\"\u0434\u043e\u043c\", null],\n  [\"\u0441\u0442\u0432\u0435\", null],\n  [\",\", null],\n  [\"\u2581\u043f\u0435\u0440\u0435\", \"\u0445\u0430\u0431\u0430\u0440\u043b\u0430\u0439\u0434\u044b\"],\n  [\"\u0434\u0430\", null],\n  [\"\u0435\u0442\", null],\n  [\"\u2581\u0420\", \"\u0420\u0418\u0410\"],\n  [\"\u0418\", null],\n  [\"\u0410\", null],\n  [\"\u2581\u041d\u043e\", \"\u0416\u0430\u04a3\u0430\u043b\u044b\u049b\u0442\u0430\u0440\u044b\"],\n  [\"\u0432\u043e\", null],\n  [\"\u0441\u0442\u0438\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0417\u0430 \u0447\u0435\u0442\u044b\u0440\u0435 \u0434\u043d\u044f \u044f \u0432\u0441\u0442\u0440\u0435\u0442\u0438\u043b \u0434\u0432\u0443\u0445 \u0447\u0435\u043b\u043e\u0432\u0435\u043a \u0441 \u0431\u0435\u043b\u044b\u043c\u0438 \u0442\u0430\u0431\u043b\u0438\u0447\u043a\u0430\u043c\u0438 \u043d\u0430 \u0448\u0435\u0435: \"\u042f \u043c\u0443\u0441\u043e\u0440\u0438\u043b\".\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0417\u0430', '\u2581\u0447\u0435', '\u0442\u044b', '\u0440\u0435', '\u2581\u0434', '\u043d\u044f', '\u2581\u044f', '\u2581\u0432', '\u0441\u0442\u0440\u0435', '\u0442\u0438', '\u043b', '\u2581\u0434\u0432\u0443', '\u0445', '\u2581\u0447\u0435\u043b\u043e\u0432\u0435\u043a', '\u2581\u0441', '\u2581\u0431\u0435', '\u043b\u044b', '\u043c\u0438', '\u2581\u0442\u0430', '\u0431\u043b\u0438', '\u0447\u043a\u0430', '\u043c\u0438', '\u2581\u043d\u0430', '\u2581', '\u0448\u0435', '\u0435', ':', '\u2581\"', '\u042f', '\u2581\u043c\u0443', '\u0441\u043e', '\u0440\u0438', '\u043b', '\".']"}], "[\n[\"\u2581\u0417\u0430\", \"In\"],\n[\"\u2581\u0447\u0435\", \"four\"],\n[\"\u0442\u044b\", null],\n[\"\u0440\u0435\", null],\n[\"\u2581\u0434\", \"days\"],\n[\"\u043d\u044f\", null],\n[\"\u2581\u044f\", \"I\"],\n[\"\u2581\u0432\", null],\n[\"\u0441\u0442\u0440\u0435\", \"met\"],\n[\"\u0442\u0438\", null],\n[\"\u043b\", null],\n[\"\u2581\u0434\u0432\u0443\", \"two\"],\n[\"\u0445\", null],\n[\"\u2581\u0447\u0435\u043b\u043e\u0432\u0435\u043a\", \"people\"],\n[\"\u2581\u0441\", \"with\"],\n[\"\u2581\u0431\u0435\", \"white\"],\n[\"\u043b\u044b\", null],\n[\"\u043c\u0438\", null],\n[\"\u2581\u0442\u0430\", \"signs\"],\n[\"\u0431\u043b\u0438\", null],\n[\"\u0447\u043a\u0430\", null],\n[\"\u043c\u0438\", null],\n[\"\u2581\u043d\u0430\", \"on\"],\n[\"\u2581\", null],\n[\"\u0448\u0435\", \"neck\"],\n[\"\u0435\", null],\n[\":\", null],\n[\"\u2581\\\"\", null],\n[\"\u042f\", \"I\"],\n[\"\u2581\u043c\u0443\", \"littered\"],\n[\"\u0441\u043e\", null],\n[\"\u0440\u0438\", null],\n[\"\u043b\", null],\n[\"\\\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u0417\u0430 \u0447\u0435\u0442\u044b\u0440\u0435 \u0434\u043d\u044f \u044f \u0432\u0441\u0442\u0440\u0435\u0442\u0438\u043b \u0434\u0432\u0443\u0445 \u0447\u0435\u043b\u043e\u0432\u0435\u043a \u0441 \u0431\u0435\u043b\u044b\u043c\u0438 \u0442\u0430\u0431\u043b\u0438\u0447\u043a\u0430\u043c\u0438 \u043d\u0430 \u0448\u0435\u0435: \"\u042f \u043c\u0443\u0441\u043e\u0440\u0438\u043b\".\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0417\u0430', '\u2581\u0447\u0435', '\u0442\u044b', '\u0440\u0435', '\u2581\u0434', '\u043d\u044f', '\u2581\u044f', '\u2581\u0432', '\u0441\u0442\u0440\u0435', '\u0442\u0438', '\u043b', '\u2581\u0434\u0432\u0443', '\u0445', '\u2581\u0447\u0435\u043b\u043e\u0432\u0435\u043a', '\u2581\u0441', '\u2581\u0431\u0435', '\u043b\u044b', '\u043c\u0438', '\u2581\u0442\u0430', '\u0431\u043b\u0438', '\u0447\u043a\u0430', '\u043c\u0438', '\u2581\u043d\u0430', '\u2581', '\u0448\u0435', '\u0435', ':', '\u2581\"', '\u042f', '\u2581\u043c\u0443', '\u0441\u043e', '\u0440\u0438', '\u043b', '\".']"}], "[\n[\"\u2581\u0417\u0430\", \"\u5728\"],\n[\"\u2581\u0447\u0435\", \"\u56db\"],\n[\"\u0442\u044b\", null],\n[\"\u0440\u0435\", null],\n[\"\u2581\u0434\", \"\u5929\"],\n[\"\u043d\u044f\", null],\n[\"\u2581\u044f\", \"\u6211\"],\n[\"\u2581\u0432\", null],\n[\"\u0441\u0442\u0440\u0435\", \"\u9047\u89c1\"],\n[\"\u0442\u0438\", null],\n[\"\u043b\", null],\n[\"\u2581\u0434\u0432\u0443\", \"\u4e24\"],\n[\"\u0445\", null],\n[\"\u2581\u0447\u0435\u043b\u043e\u0432\u0435\u043a\", \"\u4eba\"],\n[\"\u2581\u0441\", null],\n[\"\u2581\u0431\u0435\", \"\u767d\"],\n[\"\u043b\u044b\", null],\n[\"\u043c\u0438\", null],\n[\"\u2581\u0442\u0430\", \"\u724c\"],\n[\"\u0431\u043b\u0438\", null],\n[\"\u0447\u043a\u0430\", null],\n[\"\u043c\u0438\", null],\n[\"\u2581\u043d\u0430\", \"\u5728\"],\n[\"\u2581\", null],\n[\"\u0448\u0435\", \"\u8116\"],\n[\"\u0435\", null],\n[\":\", null],\n[\"\u2581\\\"\", null],\n[\"\u042f\", \"\u6211\"],\n[\"\u2581\u043c\u0443\", \"\u4e71\u6254\"],\n[\"\u0441\u043e\", null],\n[\"\u0440\u0438\", null],\n[\"\u043b\", null],\n[\"\\\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u0417\u0430 \u0447\u0435\u0442\u044b\u0440\u0435 \u0434\u043d\u044f \u044f \u0432\u0441\u0442\u0440\u0435\u0442\u0438\u043b \u0434\u0432\u0443\u0445 \u0447\u0435\u043b\u043e\u0432\u0435\u043a \u0441 \u0431\u0435\u043b\u044b\u043c\u0438 \u0442\u0430\u0431\u043b\u0438\u0447\u043a\u0430\u043c\u0438 \u043d\u0430 \u0448\u0435\u0435: \"\u042f \u043c\u0443\u0441\u043e\u0440\u0438\u043b\".\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0417\u0430', '\u2581\u0447\u0435', '\u0442\u044b', '\u0440\u0435', '\u2581\u0434', '\u043d\u044f', '\u2581\u044f', '\u2581\u0432', '\u0441\u0442\u0440\u0435', '\u0442\u0438', '\u043b', '\u2581\u0434\u0432\u0443', '\u0445', '\u2581\u0447\u0435\u043b\u043e\u0432\u0435\u043a', '\u2581\u0441', '\u2581\u0431\u0435', '\u043b\u044b', '\u043c\u0438', '\u2581\u0442\u0430', '\u0431\u043b\u0438', '\u0447\u043a\u0430', '\u043c\u0438', '\u2581\u043d\u0430', '\u2581', '\u0448\u0435', '\u0435', ':', '\u2581\"', '\u042f', '\u2581\u043c\u0443', '\u0441\u043e', '\u0440\u0438', '\u043b', '\".']"}], "[\n[\"\u2581\u0417\u0430\", \"\u0422\u04e9\u0440\u0442\"],\n[\"\u2581\u0447\u0435\", null],\n[\"\u0442\u044b\", null],\n[\"\u0440\u0435\", null],\n[\"\u2581\u0434\", null],\n[\"\u043d\u044f\", null],\n[\"\u2581\u044f\", null],\n[\"\u2581\u0432\", null],\n[\"\u0441\u0442\u0440\u0435\", \"\u043a\u0435\u0437\u0434\u0435\u0441\u0442\u0456\u0440\u0434\u0456\u043c\"],\n[\"\u0442\u0438\", null],\n[\"\u043b\", null],\n[\"\u2581\u0434\u0432\u0443\", \"\u0435\u043a\u0456\"],\n[\"\u0445\", null],\n[\"\u2581\u0447\u0435\u043b\u043e\u0432\u0435\u043a\", \"\u0430\u0434\u0430\u043c\u0434\u044b\"],\n[\"\u2581\u0441\", null],\n[\"\u2581\u0431\u0435\", \"\u0430\u049b\"],\n[\"\u043b\u044b\", null],\n[\"\u043c\u0438\", null],\n[\"\u2581\u0442\u0430\", \"\u0442\u0430\u049b\u0442\u0430\u0439\u0448\u0430\u043b\u0430\u0440\u043c\u0435\u043d\"],\n[\"\u0431\u043b\u0438\", null],\n[\"\u0447\u043a\u0430\", null],\n[\"\u043c\u0438\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\", null],\n[\"\u0448\u0435\", \"\u043c\u043e\u0439\u043d\u044b\u043d\u0434\u0430\"],\n[\"\u0435\", null],\n[\":\", null],\n[\"\u2581\\\"\", null],\n[\"\u042f\", \"\u041c\u0435\u043d\"],\n[\"\u2581\u043c\u0443\", \"\u049b\u043e\u049b\u044b\u0441\"],\n[\"\u0441\u043e\", null],\n[\"\u0440\u0438\", null],\n[\"\u043b\", \"\u0442\u0430\u0441\u0442\u0430\u0434\u044b\u043c\"],\n[\"\\\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0428\u0430\u0442\u0435\u0440 \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0430\u0434\u044b \u0431\u0443\u0434\u0435\u0442 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u043d\u0430 \u043f\u0440\u043e\u0442\u044f\u0436\u0435\u043d\u0438\u0438 \u0432\u0441\u0435\u0433\u043e \u043b\u0435\u0442\u043d\u0435\u0433\u043e \u0441\u0435\u0437\u043e\u043d\u0430 \u0434\u043e 25 \u0441\u0435\u043d\u0442\u044f\u0431\u0440\u044f, \u0441\u043e\u043e\u0431\u0449\u0430\u0435\u0442 \u043f\u0440\u0435\u0441\u0441-\u0441\u043b\u0443\u0436\u0431\u0430 \u0434\u0438\u0440\u0435\u043a\u0446\u0438\u0438 \u0417\u0438\u043c\u043d\u0435\u0439 \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0430\u0434\u044b-2019.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0428\u0430', '\u0442\u0435\u0440', '\u2581\u0443\u043d\u0438', '\u0432\u0435\u0440', '\u0441\u0438', '\u0430', '\u0434\u044b', '\u2581\u0431\u0443', '\u0434\u0435', '\u0442', '\u2581\u0440\u0430\u0431\u043e\u0442\u0430', '\u0442\u044c', '\u2581\u043d\u0430', '\u2581\u043f\u0440\u043e', '\u0442\u044f', '\u0436\u0435', '\u043d\u0438\u0438', '\u2581\u0432\u0441\u0435', '\u0433\u043e', '\u2581\u043b\u0435\u0442', '\u043d\u0435\u0433\u043e', '\u2581\u0441\u0435\u0437\u043e', '\u043d\u0430', '\u2581\u0434\u043e', '\u2581', '2', '5', '\u2581\u0441\u0435\u043d\u0442\u044f\u0431\u0440\u044f', ',', '\u2581\u0441\u043e', '\u043e\u0431', '\u0449\u0430', '\u0435\u0442', '\u2581\u043f\u0440\u0435', '\u0441\u0441', '-', '\u0441\u043b\u0443', '\u0436', '\u0431\u0430', '\u2581\u0434\u0438', '\u0440\u0435', '\u043a', '\u0446\u0438\u0438', '\u2581\u0417', '\u0438\u043c', '\u043d\u0435\u0439', '\u2581\u0443\u043d\u0438', '\u0432\u0435\u0440', '\u0441\u0438', '\u0430', '\u0434\u044b', '-', '2', '0', '1', '9', '.']"}], "[\n[\"\u2581\u0428\u0430\", \"Tent\"],\n[\"\u0442\u0435\u0440\", null],\n[\"\u2581\u0443\u043d\u0438\", \"Universiade\"],\n[\"\u0432\u0435\u0440\", null],\n[\"\u0441\u0438\", null],\n[\"\u0430\", null],\n[\"\u0434\u044b\", null],\n[\"\u2581\u0431\u0443\", \"will\"],\n[\"\u0434\u0435\", null],\n[\"\u0442\", null],\n[\"\u2581\u0440\u0430\u0431\u043e\u0442\u0430\", \"operate\"],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u043f\u0440\u043e\", \"throughout\"],\n[\"\u0442\u044f\", null],\n[\"\u0436\u0435\", null],\n[\"\u043d\u0438\u0438\", null],\n[\"\u2581\u0432\u0441\u0435\", \"entire\"],\n[\"\u0433\u043e\", null],\n[\"\u2581\u043b\u0435\u0442\", \"summer\"],\n[\"\u043d\u0435\u0433\u043e\", null],\n[\"\u2581\u0441\u0435\u0437\u043e\", \"season\"],\n[\"\u043d\u0430\", null],\n[\"\u2581\u0434\u043e\", \"until\"],\n[\"\u2581\", null],\n[\"2\", null],\n[\"5\", null],\n[\"\u2581\u0441\u0435\u043d\u0442\u044f\u0431\u0440\u044f\", \"September\"],\n[\",\", null],\n[\"\u2581\u0441\u043e\", \"reports\"],\n[\"\u043e\u0431\", null],\n[\"\u0449\u0430\", null],\n[\"\u0435\u0442\", null],\n[\"\u2581\u043f\u0440\u0435\", \"press\"],\n[\"\u0441\u0441\", null],\n[\"-\", null],\n[\"\u0441\u043b\u0443\", \"service\"],\n[\"\u0436\", null],\n[\"\u0431\u0430\", null],\n[\"\u2581\u0434\u0438\", \"directorate\"],\n[\"\u0440\u0435\", null],\n[\"\u043a\", null],\n[\"\u0446\u0438\u0438\", null],\n[\"\u2581\u0417\", \"Winter\"],\n[\"\u0438\u043c\", null],\n[\"\u043d\u0435\u0439\", null],\n[\"\u2581\u0443\u043d\u0438\", \"Universiade\"],\n[\"\u0432\u0435\u0440\", null],\n[\"\u0441\u0438\", null],\n[\"\u0430\", null],\n[\"\u0434\u044b\", null],\n[\"-\", null],\n[\"2\", null],\n[\"0\", null],\n[\"1\", null],\n[\"9\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u0428\u0430\u0442\u0435\u0440 \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0430\u0434\u044b \u0431\u0443\u0434\u0435\u0442 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u043d\u0430 \u043f\u0440\u043e\u0442\u044f\u0436\u0435\u043d\u0438\u0438 \u0432\u0441\u0435\u0433\u043e \u043b\u0435\u0442\u043d\u0435\u0433\u043e \u0441\u0435\u0437\u043e\u043d\u0430 \u0434\u043e 25 \u0441\u0435\u043d\u0442\u044f\u0431\u0440\u044f, \u0441\u043e\u043e\u0431\u0449\u0430\u0435\u0442 \u043f\u0440\u0435\u0441\u0441-\u0441\u043b\u0443\u0436\u0431\u0430 \u0434\u0438\u0440\u0435\u043a\u0446\u0438\u0438 \u0417\u0438\u043c\u043d\u0435\u0439 \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0430\u0434\u044b-2019.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0428\u0430', '\u0442\u0435\u0440', '\u2581\u0443\u043d\u0438', '\u0432\u0435\u0440', '\u0441\u0438', '\u0430', '\u0434\u044b', '\u2581\u0431\u0443', '\u0434\u0435', '\u0442', '\u2581\u0440\u0430\u0431\u043e\u0442\u0430', '\u0442\u044c', '\u2581\u043d\u0430', '\u2581\u043f\u0440\u043e', '\u0442\u044f', '\u0436\u0435', '\u043d\u0438\u0438', '\u2581\u0432\u0441\u0435', '\u0433\u043e', '\u2581\u043b\u0435\u0442', '\u043d\u0435\u0433\u043e', '\u2581\u0441\u0435\u0437\u043e', '\u043d\u0430', '\u2581\u0434\u043e', '\u2581', '2', '5', '\u2581\u0441\u0435\u043d\u0442\u044f\u0431\u0440\u044f', ',', '\u2581\u0441\u043e', '\u043e\u0431', '\u0449\u0430', '\u0435\u0442', '\u2581\u043f\u0440\u0435', '\u0441\u0441', '-', '\u0441\u043b\u0443', '\u0436', '\u0431\u0430', '\u2581\u0434\u0438', '\u0440\u0435', '\u043a', '\u0446\u0438\u0438', '\u2581\u0417', '\u0438\u043c', '\u043d\u0435\u0439', '\u2581\u0443\u043d\u0438', '\u0432\u0435\u0440', '\u0441\u0438', '\u0430', '\u0434\u044b', '-', '2', '0', '1', '9', '.']"}], "[\n[\"\u2581\u0428\u0430\", \"\u4e16\u754c\u5927\u5b66\u751f\"],\n[\"\u0442\u0435\u0440\", null],\n[\"\u2581\u0443\u043d\u0438\", \"\u8fd0\u52a8\u4f1a\"],\n[\"\u0432\u0435\u0440\", null],\n[\"\u0441\u0438\", null],\n[\"\u0430\", null],\n[\"\u0434\u044b\", null],\n[\"\u2581\u0431\u0443\", \"\u5c06\"],\n[\"\u0434\u0435\", null],\n[\"\u0442\", null],\n[\"\u2581\u0440\u0430\u0431\u043e\u0442\u0430\", \"\u8fd0\u8425\"],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u043f\u0440\u043e\", \"\u6574\u4e2a\"],\n[\"\u0442\u044f\", null],\n[\"\u0436\u0435\", null],\n[\"\u043d\u0438\u0438\", null],\n[\"\u2581\u0432\u0441\u0435\", null],\n[\"\u0433\u043e\", null],\n[\"\u2581\u043b\u0435\u0442\", \"\u590f\u5b63\"],\n[\"\u043d\u0435\u0433\u043e\", null],\n[\"\u2581\u0441\u0435\u0437\u043e\", \"\u5b63\u8282\"],\n[\"\u043d\u0430\", null],\n[\"\u2581\u0434\u043e\", \"\u76f4\u5230\"],\n[\"\u2581\", null],\n[\"2\", null],\n[\"5\", null],\n[\"\u2581\u0441\u0435\u043d\u0442\u044f\u0431\u0440\u044f\", \"9\u670825\u65e5\"],\n[\",\", null],\n[\"\u2581\u0441\u043e\", \"\u636e\"],\n[\"\u043e\u0431\", null],\n[\"\u0449\u0430\", null],\n[\"\u0435\u0442\", null],\n[\"\u2581\u043f\u0440\u0435\", \"\u65b0\u95fb\"],\n[\"\u0441\u0441\", null],\n[\"-\", null],\n[\"\u0441\u043b\u0443\", \"\u670d\u52a1\"],\n[\"\u0436\", null],\n[\"\u0431\u0430\", null],\n[\"\u2581\u0434\u0438\", \"2019\u5e74\"],\n[\"\u0440\u0435\", null],\n[\"\u043a\", null],\n[\"\u0446\u0438\u0438\", null],\n[\"\u2581\u0417\", \"\u51ac\u5b63\"],\n[\"\u0438\u043c\", null],\n[\"\u043d\u0435\u0439\", null],\n[\"\u2581\u0443\u043d\u0438\", \"\u4e16\u754c\u5927\u5b66\u751f\"],\n[\"\u0432\u0435\u0440\", null],\n[\"\u0441\u0438\", null],\n[\"\u0430\", null],\n[\"\u0434\u044b\", null],\n[\"-\", null],\n[\"2\", null],\n[\"0\", null],\n[\"1\", null],\n[\"9\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u0428\u0430\u0442\u0435\u0440 \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0430\u0434\u044b \u0431\u0443\u0434\u0435\u0442 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u043d\u0430 \u043f\u0440\u043e\u0442\u044f\u0436\u0435\u043d\u0438\u0438 \u0432\u0441\u0435\u0433\u043e \u043b\u0435\u0442\u043d\u0435\u0433\u043e \u0441\u0435\u0437\u043e\u043d\u0430 \u0434\u043e 25 \u0441\u0435\u043d\u0442\u044f\u0431\u0440\u044f, \u0441\u043e\u043e\u0431\u0449\u0430\u0435\u0442 \u043f\u0440\u0435\u0441\u0441-\u0441\u043b\u0443\u0436\u0431\u0430 \u0434\u0438\u0440\u0435\u043a\u0446\u0438\u0438 \u0417\u0438\u043c\u043d\u0435\u0439 \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0430\u0434\u044b-2019.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0428\u0430', '\u0442\u0435\u0440', '\u2581\u0443\u043d\u0438', '\u0432\u0435\u0440', '\u0441\u0438', '\u0430', '\u0434\u044b', '\u2581\u0431\u0443', '\u0434\u0435', '\u0442', '\u2581\u0440\u0430\u0431\u043e\u0442\u0430', '\u0442\u044c', '\u2581\u043d\u0430', '\u2581\u043f\u0440\u043e', '\u0442\u044f', '\u0436\u0435', '\u043d\u0438\u0438', '\u2581\u0432\u0441\u0435', '\u0433\u043e', '\u2581\u043b\u0435\u0442', '\u043d\u0435\u0433\u043e', '\u2581\u0441\u0435\u0437\u043e', '\u043d\u0430', '\u2581\u0434\u043e', '\u2581', '2', '5', '\u2581\u0441\u0435\u043d\u0442\u044f\u0431\u0440\u044f', ',', '\u2581\u0441\u043e', '\u043e\u0431', '\u0449\u0430', '\u0435\u0442', '\u2581\u043f\u0440\u0435', '\u0441\u0441', '-', '\u0441\u043b\u0443', '\u0436', '\u0431\u0430', '\u2581\u0434\u0438', '\u0440\u0435', '\u043a', '\u0446\u0438\u0438', '\u2581\u0417', '\u0438\u043c', '\u043d\u0435\u0439', '\u2581\u0443\u043d\u0438', '\u0432\u0435\u0440', '\u0441\u0438', '\u0430', '\u0434\u044b', '-', '2', '0', '1', '9', '.']"}], "[\n  [\"\u2581\u0428\u0430\", \"\u0428\u0430\u0442\u044b\u0440\"],\n  [\"\u0442\u0435\u0440\", null],\n  [\"\u2581\u0443\u043d\u0438\", \"\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0430\u0434\u0430\"],\n  [\"\u0432\u0435\u0440\", null],\n  [\"\u0441\u0438\", null],\n  [\"\u0430\", null],\n  [\"\u0434\u044b\", null],\n  [\"\u2581\u0431\u0443\", \"\u0431\u043e\u043b\u0430\u0434\u044b\"],\n  [\"\u0434\u0435\", null],\n  [\"\u0442\", null],\n  [\"\u2581\u0440\u0430\u0431\u043e\u0442\u0430\", \"\u0436\u04b1\u043c\u044b\u0441\"],\n  [\"\u0442\u044c\", null],\n  [\"\u2581\u043d\u0430\", null],\n  [\"\u2581\u043f\u0440\u043e\", \"\u0431\u043e\u0439\u044b\"],\n  [\"\u0442\u044f\", null],\n  [\"\u0436\u0435\", null],\n  [\"\u043d\u0438\u0438\", null],\n  [\"\u2581\u0432\u0441\u0435\", \"\u0431\u0430\u0440\u043b\u044b\u049b\"],\n  [\"\u0433\u043e\", null],\n  [\"\u2581\u043b\u0435\u0442\", \"\u0436\u0430\u0437\u0493\u044b\"],\n  [\"\u043d\u0435\u0433\u043e\", null],\n  [\"\u2581\u0441\u0435\u0437\u043e\", \"\u043c\u0430\u0443\u0441\u044b\u043c\"],\n  [\"\u043d\u0430\", null],\n  [\"\u2581\u0434\u043e\", \"\u0434\u0435\u0439\u0456\u043d\"],\n  [\"\u2581\", null],\n  [\"2\", null],\n  [\"5\", null],\n  [\"\u2581\u0441\u0435\u043d\u0442\u044f\u0431\u0440\u044f\", \"\u049b\u044b\u0440\u043a\u04af\u0439\u0435\u043a\"],\n  [\",\", null],\n  [\"\u2581\u0441\u043e\", \"\u0445\u0430\u0431\u0430\u0440\u043b\u0430\u0439\u0434\u044b\"],\n  [\"\u043e\u0431\", null],\n  [\"\u0449\u0430\", null],\n  [\"\u0435\u0442\", null],\n  [\"\u2581\u043f\u0440\u0435\", \"\u0431\u0430\u0441\u043f\u0430\u0441\u04e9\u0437\"],\n  [\"\u0441\u0441\", null],\n  [\"-\", null],\n  [\"\u0441\u043b\u0443\", \"\u049b\u044b\u0437\u043c\u0435\u0442\u0456\"],\n  [\"\u0436\", null],\n  [\"\u0431\u0430\", null],\n  [\"\u2581\u0434\u0438\", \"\u0434\u0438\u0440\u0435\u043a\u0446\u0438\u044f\u0441\u044b\"],\n  [\"\u0440\u0435\", null],\n  [\"\u043a\", null],\n  [\"\u0446\u0438\u0438\", null],\n  [\"\u2581\u0417\", \"\u049a\u044b\u0441\u049b\u044b\"],\n  [\"\u0438\u043c\", null],\n  [\"\u043d\u0435\u0439\", null],\n  [\"\u2581\u0443\u043d\u0438\", \"\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0430\u0434\u0430\"],\n  [\"\u0432\u0435\u0440\", null],\n  [\"\u0441\u0438\", null],\n  [\"\u0430\", null],\n  [\"\u0434\u044b\", null],\n  [\"-\", null],\n  [\"2\", null],\n  [\"0\", null],\n  [\"1\", null],\n  [\"9\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u00ab\u041f\u0421\u0416\u00bb \u043e\u0436\u0438\u0434\u0430\u0435\u0442, \u0447\u0442\u043e \u043f\u0440\u0438\u0431\u044b\u0442\u0438\u0435 \u041d\u0435\u0439\u043c\u0430\u0440\u0430 \u0441\u0442\u0430\u043d\u0435\u0442 \u0431\u043b\u0430\u0433\u043e\u043f\u0440\u0438\u044f\u0442\u043d\u043e\u0439 \u043f\u043e\u0447\u0432\u043e\u0439 \u0434\u043b\u044f \u0441\u043f\u043e\u043d\u0441\u043e\u0440\u0441\u043a\u0438\u0445 \u0441\u0434\u0435\u043b\u043e\u043a, \u043f\u0440\u043e\u0434\u0430\u0436\u0438 \u0430\u0442\u0440\u0438\u0431\u0443\u0442\u0438\u043a\u0438 \u0438 \u0444\u0443\u0442\u0431\u043e\u043b\u043e\u043a \u0441 \u043b\u043e\u0433\u043e\u0442\u0438\u043f\u043e\u043c \u043a\u043e\u043c\u0430\u043d\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00ab', '\u041f', '\u0421', '\u0416', '\u00bb', '\u2581\u043e', '\u0436\u0438', '\u0434\u0430', '\u0435\u0442', ',', '\u2581\u0447\u0442\u043e', '\u2581\u043f\u0440\u0438', '\u0431\u044b', '\u0442\u0438\u0435', '\u2581\u041d', '\u0435\u0439', '\u043c\u0430', '\u0440\u0430', '\u2581\u0441\u0442\u0430', '\u043d\u0435', '\u0442', '\u2581\u0431', '\u043b\u0430\u0433\u043e', '\u043f', '\u0440\u0438\u044f', '\u0442', '\u043d\u043e\u0439', '\u2581\u043f\u043e', '\u0447', '\u0432\u043e\u0439', '\u2581\u0434\u043b\u044f', '\u2581\u0441\u043f\u043e', '\u043d', '\u0441\u043e', '\u0440', '\u0441\u043a\u0438\u0445', '\u2581\u0441', '\u0434\u0435', '\u043b\u043e\u043a', ',', '\u2581\u043f\u0440\u043e', '\u0434\u0430', '\u0436\u0438', '\u2581\u0430', '\u0442\u0440\u0438', '\u0431\u0443', '\u0442\u0438', '\u043a\u0438', '\u2581\u0438', '\u2581\u0444\u0443\u0442\u0431\u043e', '\u043b\u043e\u043a', '\u2581\u0441', '\u2581\u043b\u043e', '\u0433\u043e', '\u0442\u0438', '\u043f\u043e', '\u043c', '\u2581\u043a\u043e\u043c\u0430\u043d', '\u0434\u044b', '.']"}], "[\n[\"\u2581\u00ab\", null],\n[\"\u041f\", \"PSG\"],\n[\"\u0421\", null],\n[\"\u0416\", null],\n[\"\u00bb\", null],\n[\"\u2581\u043e\", \"expects\"],\n[\"\u0436\u0438\", null],\n[\"\u0434\u0430\", null],\n[\"\u0435\u0442\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u043f\u0440\u0438\", \"arrival\"],\n[\"\u0431\u044b\", null],\n[\"\u0442\u0438\u0435\", null],\n[\"\u2581\u041d\", \"Neymar\"],\n[\"\u0435\u0439\", null],\n[\"\u043c\u0430\", null],\n[\"\u0440\u0430\", null],\n[\"\u2581\u0441\u0442\u0430\", \"will\"],\n[\"\u043d\u0435\", null],\n[\"\u0442\", null],\n[\"\u2581\u0431\", \"favorable\"],\n[\"\u043b\u0430\u0433\u043e\", null],\n[\"\u043f\", null],\n[\"\u0440\u0438\u044f\", null],\n[\"\u0442\", null],\n[\"\u043d\u043e\u0439\", null],\n[\"\u2581\u043f\u043e\", \"ground\"],\n[\"\u0447\", null],\n[\"\u0432\u043e\u0439\", null],\n[\"\u2581\u0434\u043b\u044f\", null],\n[\"\u2581\u0441\u043f\u043e\", \"sponsorship\"],\n[\"\u043d\", null],\n[\"\u0441\u043e\", null],\n[\"\u0440\", null],\n[\"\u0441\u043a\u0438\u0445\", null],\n[\"\u2581\u0441\", \"deals\"],\n[\"\u0434\u0435\", null],\n[\"\u043b\u043e\u043a\", null],\n[\",\", null],\n[\"\u2581\u043f\u0440\u043e\", \"sale\"],\n[\"\u0434\u0430\", null],\n[\"\u0436\u0438\", null],\n[\"\u2581\u0430\", \"merchandise\"],\n[\"\u0442\u0440\u0438\", null],\n[\"\u0431\u0443\", null],\n[\"\u0442\u0438\", null],\n[\"\u043a\u0438\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0444\u0443\u0442\u0431\u043e\", \"jerseys\"],\n[\"\u043b\u043e\u043a\", null],\n[\"\u2581\u0441\", null],\n[\"\u2581\u043b\u043e\", \"logo\"],\n[\"\u0433\u043e\", null],\n[\"\u0442\u0438\", null],\n[\"\u043f\u043e\", null],\n[\"\u043c\", null],\n[\"\u2581\u043a\u043e\u043c\u0430\u043d\", \"team\"],\n[\"\u0434\u044b\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u00ab\u041f\u0421\u0416\u00bb \u043e\u0436\u0438\u0434\u0430\u0435\u0442, \u0447\u0442\u043e \u043f\u0440\u0438\u0431\u044b\u0442\u0438\u0435 \u041d\u0435\u0439\u043c\u0430\u0440\u0430 \u0441\u0442\u0430\u043d\u0435\u0442 \u0431\u043b\u0430\u0433\u043e\u043f\u0440\u0438\u044f\u0442\u043d\u043e\u0439 \u043f\u043e\u0447\u0432\u043e\u0439 \u0434\u043b\u044f \u0441\u043f\u043e\u043d\u0441\u043e\u0440\u0441\u043a\u0438\u0445 \u0441\u0434\u0435\u043b\u043e\u043a, \u043f\u0440\u043e\u0434\u0430\u0436\u0438 \u0430\u0442\u0440\u0438\u0431\u0443\u0442\u0438\u043a\u0438 \u0438 \u0444\u0443\u0442\u0431\u043e\u043b\u043e\u043a \u0441 \u043b\u043e\u0433\u043e\u0442\u0438\u043f\u043e\u043c \u043a\u043e\u043c\u0430\u043d\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00ab', '\u041f', '\u0421', '\u0416', '\u00bb', '\u2581\u043e', '\u0436\u0438', '\u0434\u0430', '\u0435\u0442', ',', '\u2581\u0447\u0442\u043e', '\u2581\u043f\u0440\u0438', '\u0431\u044b', '\u0442\u0438\u0435', '\u2581\u041d', '\u0435\u0439', '\u043c\u0430', '\u0440\u0430', '\u2581\u0441\u0442\u0430', '\u043d\u0435', '\u0442', '\u2581\u0431', '\u043b\u0430\u0433\u043e', '\u043f', '\u0440\u0438\u044f', '\u0442', '\u043d\u043e\u0439', '\u2581\u043f\u043e', '\u0447', '\u0432\u043e\u0439', '\u2581\u0434\u043b\u044f', '\u2581\u0441\u043f\u043e', '\u043d', '\u0441\u043e', '\u0440', '\u0441\u043a\u0438\u0445', '\u2581\u0441', '\u0434\u0435', '\u043b\u043e\u043a', ',', '\u2581\u043f\u0440\u043e', '\u0434\u0430', '\u0436\u0438', '\u2581\u0430', '\u0442\u0440\u0438', '\u0431\u0443', '\u0442\u0438', '\u043a\u0438', '\u2581\u0438', '\u2581\u0444\u0443\u0442\u0431\u043e', '\u043b\u043e\u043a', '\u2581\u0441', '\u2581\u043b\u043e', '\u0433\u043e', '\u0442\u0438', '\u043f\u043e', '\u043c', '\u2581\u043a\u043e\u043c\u0430\u043d', '\u0434\u044b', '.']"}], "[\n[\"\u2581\u00ab\", null],\n[\"\u041f\", \"\u5df4\u9ece\u5723\u65e5\u8033\u66fc\"],\n[\"\u0421\", null],\n[\"\u0416\", null],\n[\"\u00bb\", null],\n[\"\u2581\u043e\", \"\u671f\u5f85\"],\n[\"\u0436\u0438\", null],\n[\"\u0434\u0430\", null],\n[\"\u0435\u0442\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u043f\u0440\u0438\", \"\u5230\u6765\"],\n[\"\u0431\u044b\", null],\n[\"\u0442\u0438\u0435\", null],\n[\"\u2581\u041d\", \"\u5185\u9a6c\u5c14\"],\n[\"\u0435\u0439\", null],\n[\"\u043c\u0430\", null],\n[\"\u0440\u0430\", null],\n[\"\u2581\u0441\u0442\u0430\", \"\u5c06\u6210\u4e3a\"],\n[\"\u043d\u0435\", null],\n[\"\u0442\", null],\n[\"\u2581\u0431\", \"\u6709\u5229\"],\n[\"\u043b\u0430\u0433\u043e\", null],\n[\"\u043f\", null],\n[\"\u0440\u0438\u044f\", null],\n[\"\u0442\", null],\n[\"\u043d\u043e\u0439\", null],\n[\"\u2581\u043f\u043e\", \"\u571f\u58e4\"],\n[\"\u0447\", null],\n[\"\u0432\u043e\u0439\", null],\n[\"\u2581\u0434\u043b\u044f\", null],\n[\"\u2581\u0441\u043f\u043e\", \"\u8d5e\u52a9\"],\n[\"\u043d\", null],\n[\"\u0441\u043e\", null],\n[\"\u0440\", null],\n[\"\u0441\u043a\u0438\u0445\", null],\n[\"\u2581\u0441\", \"\u4ea4\u6613\"],\n[\"\u0434\u0435\", null],\n[\"\u043b\u043e\u043a\", null],\n[\",\", null],\n[\"\u2581\u043f\u0440\u043e\", \"\u9500\u552e\"],\n[\"\u0434\u0430\", null],\n[\"\u0436\u0438\", null],\n[\"\u2581\u0430\", \"\u7eaa\u5ff5\u54c1\"],\n[\"\u0442\u0440\u0438\", null],\n[\"\u0431\u0443\", null],\n[\"\u0442\u0438\", null],\n[\"\u043a\u0438\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0444\u0443\u0442\u0431\u043e\", \"\u7403\u8863\"],\n[\"\u043b\u043e\u043a\", null],\n[\"\u2581\u0441\", null],\n[\"\u2581\u043b\u043e\", \"\u6807\u5fd7\"],\n[\"\u0433\u043e\", null],\n[\"\u0442\u0438\", null],\n[\"\u043f\u043e\", null],\n[\"\u043c\", null],\n[\"\u2581\u043a\u043e\u043c\u0430\u043d\", \"\u7403\u961f\"],\n[\"\u0434\u044b\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u00ab\u041f\u0421\u0416\u00bb \u043e\u0436\u0438\u0434\u0430\u0435\u0442, \u0447\u0442\u043e \u043f\u0440\u0438\u0431\u044b\u0442\u0438\u0435 \u041d\u0435\u0439\u043c\u0430\u0440\u0430 \u0441\u0442\u0430\u043d\u0435\u0442 \u0431\u043b\u0430\u0433\u043e\u043f\u0440\u0438\u044f\u0442\u043d\u043e\u0439 \u043f\u043e\u0447\u0432\u043e\u0439 \u0434\u043b\u044f \u0441\u043f\u043e\u043d\u0441\u043e\u0440\u0441\u043a\u0438\u0445 \u0441\u0434\u0435\u043b\u043e\u043a, \u043f\u0440\u043e\u0434\u0430\u0436\u0438 \u0430\u0442\u0440\u0438\u0431\u0443\u0442\u0438\u043a\u0438 \u0438 \u0444\u0443\u0442\u0431\u043e\u043b\u043e\u043a \u0441 \u043b\u043e\u0433\u043e\u0442\u0438\u043f\u043e\u043c \u043a\u043e\u043c\u0430\u043d\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00ab', '\u041f', '\u0421', '\u0416', '\u00bb', '\u2581\u043e', '\u0436\u0438', '\u0434\u0430', '\u0435\u0442', ',', '\u2581\u0447\u0442\u043e', '\u2581\u043f\u0440\u0438', '\u0431\u044b', '\u0442\u0438\u0435', '\u2581\u041d', '\u0435\u0439', '\u043c\u0430', '\u0440\u0430', '\u2581\u0441\u0442\u0430', '\u043d\u0435', '\u0442', '\u2581\u0431', '\u043b\u0430\u0433\u043e', '\u043f', '\u0440\u0438\u044f', '\u0442', '\u043d\u043e\u0439', '\u2581\u043f\u043e', '\u0447', '\u0432\u043e\u0439', '\u2581\u0434\u043b\u044f', '\u2581\u0441\u043f\u043e', '\u043d', '\u0441\u043e', '\u0440', '\u0441\u043a\u0438\u0445', '\u2581\u0441', '\u0434\u0435', '\u043b\u043e\u043a', ',', '\u2581\u043f\u0440\u043e', '\u0434\u0430', '\u0436\u0438', '\u2581\u0430', '\u0442\u0440\u0438', '\u0431\u0443', '\u0442\u0438', '\u043a\u0438', '\u2581\u0438', '\u2581\u0444\u0443\u0442\u0431\u043e', '\u043b\u043e\u043a', '\u2581\u0441', '\u2581\u043b\u043e', '\u0433\u043e', '\u0442\u0438', '\u043f\u043e', '\u043c', '\u2581\u043a\u043e\u043c\u0430\u043d', '\u0434\u044b', '.']"}], "[\n[\"\u2581\u00ab\", null],\n[\"\u041f\", \"\u041f\u0421\u0416\"],\n[\"\u0421\", null],\n[\"\u0416\", null],\n[\"\u00bb\", null],\n[\"\u2581\u043e\", \"\u043a\u04af\u0442\u0435\u0434\u0456\"],\n[\"\u0436\u0438\", null],\n[\"\u0434\u0430\", null],\n[\"\u0435\u0442\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u043f\u0440\u0438\", \"\u043a\u0435\u043b\u0443\u0456\"],\n[\"\u0431\u044b\", null],\n[\"\u0442\u0438\u0435\", null],\n[\"\u2581\u041d\", \"\u041d\u0435\u0439\u043c\u0430\u0440\u0434\u044b\u04a3\"],\n[\"\u0435\u0439\", null],\n[\"\u043c\u0430\", null],\n[\"\u0440\u0430\", null],\n[\"\u2581\u0441\u0442\u0430\", \"\u0431\u043e\u043b\u0430\u0434\u044b\"],\n[\"\u043d\u0435\", null],\n[\"\u0442\", null],\n[\"\u2581\u0431\", \"\u049b\u043e\u043b\u0430\u0439\u043b\u044b\"],\n[\"\u043b\u0430\u0433\u043e\", null],\n[\"\u043f\", null],\n[\"\u0440\u0438\u044f\", null],\n[\"\u0442\", null],\n[\"\u043d\u043e\u0439\", null],\n[\"\u2581\u043f\u043e\", \"\u043d\u0435\u0433\u0456\u0437\"],\n[\"\u0447\", null],\n[\"\u0432\u043e\u0439\", null],\n[\"\u2581\u0434\u043b\u044f\", null],\n[\"\u2581\u0441\u043f\u043e\", \"\u0434\u0435\u043c\u0435\u0443\u0448\u0456\u043b\u0456\u043a\"],\n[\"\u043d\", null],\n[\"\u0441\u043e\", null],\n[\"\u0440\", null],\n[\"\u0441\u043a\u0438\u0445\", null],\n[\"\u2581\u0441\", \"\u043a\u0435\u043b\u0456\u0441\u0456\u043c\u0434\u0435\u0440\"],\n[\"\u0434\u0435\", null],\n[\"\u043b\u043e\u043a\", null],\n[\",\", null],\n[\"\u2581\u043f\u0440\u043e\", \"\u0441\u0430\u0442\u0443\"],\n[\"\u0434\u0430\", null],\n[\"\u0436\u0438\", null],\n[\"\u2581\u0430\", \"\u0430\u0442\u0440\u0438\u0431\u0443\u0442\u0438\u043a\u0430\"],\n[\"\u0442\u0440\u0438\", null],\n[\"\u0431\u0443\", null],\n[\"\u0442\u0438\", null],\n[\"\u043a\u0438\", null],\n[\"\u2581\u0438\", \"\u0436\u04d9\u043d\u0435\"],\n[\"\u2581\u0444\u0443\u0442\u0431\u043e\", \"\u0444\u0443\u0442\u0431\u043e\u043b\u043a\u0430\u043b\u0430\u0440\"],\n[\"\u043b\u043e\u043a\", null],\n[\"\u2581\u0441\", null],\n[\"\u2581\u043b\u043e\", \"\u043b\u043e\u0433\u043e\u0442\u0438\u043f\u0456\"],\n[\"\u0433\u043e\", null],\n[\"\u0442\u0438\", null],\n[\"\u043f\u043e\", null],\n[\"\u043c\", null],\n[\"\u2581\u043a\u043e\u043c\u0430\u043d\", \"\u043a\u043e\u043c\u0430\u043d\u0434\u0430\u043d\u044b\u04a3\"],\n[\"\u0434\u044b\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\"\u041c\u0430\u043c\u0430\u0435\u0432 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0442\u043e\u0440\u043e\u043c \u0440\u0430\u0437\u0431\u043e\u044f, \u0447\u0442\u043e \u0442\u0430\u043a\u0436\u0435 \u043e\u0431\u0440\u0430\u0437\u0443\u0435\u0442 \u0441\u043e\u0441\u0442\u0430\u0432 \u043f\u0440\u0435\u0441\u0442\u0443\u043f\u043b\u0435\u043d\u0438\u044f, \u0438, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e, \u0442\u0430\u043a\u043e\u0435 \u043b\u0438\u0446\u043e \u0434\u043e\u043b\u0436\u043d\u043e \u0431\u044b\u0442\u044c \u043f\u0440\u0438\u0432\u043b\u0435\u0447\u0435\u043d\u043e \u043a \u0443\u0433\u043e\u043b\u043e\u0432\u043d\u043e\u0439 \u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0441\u0442\u0438\", - \u043f\u043e\u044f\u0441\u043d\u0438\u043b\u0430 \"\u042a\" \u043f\u0440\u0435\u0441\u0441-\u0441\u0435\u043a\u0440\u0435\u0442\u0430\u0440\u044c \u041c\u043e\u0441\u0433\u043e\u0440\u0441\u0443\u0434\u0430 \u0423\u043b\u044c\u044f\u043d\u0430 \u0421\u043e\u043b\u043e\u043f\u043e\u0432\u0430.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\"', '\u041c\u0430', '\u043c\u0430', '\u0435\u0432', '\u2581\u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f', '\u2581\u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430', '\u0442\u043e', '\u0440\u043e\u043c', '\u2581\u0440\u0430\u0437', '\u0431\u043e', '\u044f', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0442\u0430\u043a\u0436\u0435', '\u2581\u043e\u0431\u0440\u0430', '\u0437\u0443', '\u0435\u0442', '\u2581\u0441\u043e\u0441\u0442\u0430\u0432', '\u2581\u043f\u0440\u0435', '\u0441\u0442\u0443\u043f', '\u043b\u0435\u043d\u0438\u044f', ',', '\u2581\u0438', ',', '\u2581\u0441\u043e', '\u043e\u0442', '\u0432\u0435\u0442', '\u0441\u0442\u0432\u0435\u043d', '\u043d\u043e', ',', '\u2581\u0442\u0430\u043a\u043e', '\u0435', '\u2581\u043b\u0438', '\u0446\u043e', '\u2581\u0434\u043e\u043b', '\u0436\u043d\u043e', '\u2581\u0431\u044b', '\u0442\u044c', '\u2581\u043f\u0440\u0438', '\u0432', '\u043b\u0435', '\u0447\u0435', '\u043d\u043e', '\u2581\u043a', '\u2581\u0443', '\u0433\u043e', '\u043b\u043e\u0432', '\u043d\u043e\u0439', '\u2581\u043e\u0442', '\u0432\u0435\u0442', '\u0441\u0442\u0432\u0435\u043d', '\u043d\u043e\u0441\u0442\u0438', '\",', '\u2581-', '\u2581\u043f\u043e', '\u044f', '\u0441\u043d\u0438', '\u043b\u0430', '\u2581\"', '\u042a', '\"', '\u2581\u043f\u0440\u0435', '\u0441\u0441', '-', '\u0441\u0435', '\u043a\u0440\u0435', '\u0442\u0430', '\u0440\u044c', '\u2581\u041c\u043e', '\u0441', '\u0433\u043e\u0440', '\u0441\u0443', '\u0434\u0430', '\u2581\u0423', '\u043b\u044c', '\u044f', '\u043d\u0430', '\u2581\u0421', '\u043e\u043b\u043e', '\u043f\u043e', '\u0432\u0430', '.']"}], "[\n[\"\u2581\\\"\", null],\n[\"\u041c\u0430\", \"Mamaev\"],\n[\"\u043c\u0430\", null],\n[\"\u0435\u0432\", null],\n[\"\u2581\u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f\", \"is\"],\n[\"\u2581\u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\", \"organizer\"],\n[\"\u0442\u043e\", null],\n[\"\u0440\u043e\u043c\", null],\n[\"\u2581\u0440\u0430\u0437\", \"robbery\"],\n[\"\u0431\u043e\", null],\n[\"\u044f\", null],\n[\",\" , null],\n[\"\u2581\u0447\u0442\u043e\", \"which\"],\n[\"\u2581\u0442\u0430\u043a\u0436\u0435\", \"also\"],\n[\"\u2581\u043e\u0431\u0440\u0430\", \"constitutes\"],\n[\"\u0437\u0443\", null],\n[\"\u0435\u0442\", null],\n[\"\u2581\u0441\u043e\u0441\u0442\u0430\u0432\", \"elements\"],\n[\"\u2581\u043f\u0440\u0435\", \"crime\"],\n[\"\u0441\u0442\u0443\u043f\", null],\n[\"\u043b\u0435\u043d\u0438\u044f\", null],\n[\",\", null],\n[\"\u2581\u0438\", \"and\"],\n[\",\", null],\n[\"\u2581\u0441\u043e\", \"accordingly\"],\n[\"\u043e\u0442\", null],\n[\"\u0432\u0435\u0442\", null],\n[\"\u0441\u0442\u0432\u0435\u043d\", null],\n[\"\u043d\u043e\", null],\n[\",\", null],\n[\"\u2581\u0442\u0430\u043a\u043e\", \"such\"],\n[\"\u0435\", null],\n[\"\u2581\u043b\u0438\", \"person\"],\n[\"\u0446\u043e\", null],\n[\"\u2581\u0434\u043e\u043b\", \"should\"],\n[\"\u0436\u043d\u043e\", null],\n[\"\u2581\u0431\u044b\", \"be\"],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043f\u0440\u0438\", \"brought\"],\n[\"\u0432\", null],\n[\"\u043b\u0435\", null],\n[\"\u0447\u0435\", null],\n[\"\u043d\u043e\", null],\n[\"\u2581\u043a\", \"to\"],\n[\"\u2581\u0443\", \"criminal\"],\n[\"\u0433\u043e\", null],\n[\"\u043b\u043e\u0432\", null],\n[\"\u043d\u043e\u0439\", null],\n[\"\u2581\u043e\u0442\", \"responsibility\"],\n[\"\u0432\u0435\u0442\", null],\n[\"\u0441\u0442\u0432\u0435\u043d\", null],\n[\"\u043d\u043e\u0441\u0442\u0438\", null],\n[\"\\\"\", null],\n[\"\u2581-\", null],\n[\"\u2581\u043f\u043e\", \"explained\"],\n[\"\u044f\", null],\n[\"\u0441\u043d\u0438\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\\\"\", null],\n[\"\u042a\", \"Kommersant\"],\n[\"\\\"\", null],\n[\"\u2581\u043f\u0440\u0435\", \"press\"],\n[\"\u0441\u0441\", null],\n[\"-\", null],\n[\"\u0441\u0435\", \"secretary\"],\n[\"\u043a\u0440\u0435\", null],\n[\"\u0442\u0430\", null],\n[\"\u0440\u044c\", null],\n[\"\u2581\u041c\u043e\", \"Moscow\"],\n[\"\u0441\", null],\n[\"\u0433\u043e\u0440\", null],\n[\"\u0441\u0443\", \"City\"],\n[\"\u0434\u0430\", \"Court\"],\n[\"\u2581\u0423\", \"Ulyana\"],\n[\"\u043b\u044c\", null],\n[\"\u044f\", null],\n[\"\u043d\u0430\", null],\n[\"\u2581\u0421\", \"Solopova\"],\n[\"\u043e\u043b\u043e\", null],\n[\"\u043f\u043e\", null],\n[\"\u0432\u0430\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\"\u041c\u0430\u043c\u0430\u0435\u0432 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0442\u043e\u0440\u043e\u043c \u0440\u0430\u0437\u0431\u043e\u044f, \u0447\u0442\u043e \u0442\u0430\u043a\u0436\u0435 \u043e\u0431\u0440\u0430\u0437\u0443\u0435\u0442 \u0441\u043e\u0441\u0442\u0430\u0432 \u043f\u0440\u0435\u0441\u0442\u0443\u043f\u043b\u0435\u043d\u0438\u044f, \u0438, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e, \u0442\u0430\u043a\u043e\u0435 \u043b\u0438\u0446\u043e \u0434\u043e\u043b\u0436\u043d\u043e \u0431\u044b\u0442\u044c \u043f\u0440\u0438\u0432\u043b\u0435\u0447\u0435\u043d\u043e \u043a \u0443\u0433\u043e\u043b\u043e\u0432\u043d\u043e\u0439 \u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0441\u0442\u0438\", - \u043f\u043e\u044f\u0441\u043d\u0438\u043b\u0430 \"\u042a\" \u043f\u0440\u0435\u0441\u0441-\u0441\u0435\u043a\u0440\u0435\u0442\u0430\u0440\u044c \u041c\u043e\u0441\u0433\u043e\u0440\u0441\u0443\u0434\u0430 \u0423\u043b\u044c\u044f\u043d\u0430 \u0421\u043e\u043b\u043e\u043f\u043e\u0432\u0430.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\"', '\u041c\u0430', '\u043c\u0430', '\u0435\u0432', '\u2581\u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f', '\u2581\u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430', '\u0442\u043e', '\u0440\u043e\u043c', '\u2581\u0440\u0430\u0437', '\u0431\u043e', '\u044f', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0442\u0430\u043a\u0436\u0435', '\u2581\u043e\u0431\u0440\u0430', '\u0437\u0443', '\u0435\u0442', '\u2581\u0441\u043e\u0441\u0442\u0430\u0432', '\u2581\u043f\u0440\u0435', '\u0441\u0442\u0443\u043f', '\u043b\u0435\u043d\u0438\u044f', ',', '\u2581\u0438', ',', '\u2581\u0441\u043e', '\u043e\u0442', '\u0432\u0435\u0442', '\u0441\u0442\u0432\u0435\u043d', '\u043d\u043e', ',', '\u2581\u0442\u0430\u043a\u043e', '\u0435', '\u2581\u043b\u0438', '\u0446\u043e', '\u2581\u0434\u043e\u043b', '\u0436\u043d\u043e', '\u2581\u0431\u044b', '\u0442\u044c', '\u2581\u043f\u0440\u0438', '\u0432', '\u043b\u0435', '\u0447\u0435', '\u043d\u043e', '\u2581\u043a', '\u2581\u0443', '\u0433\u043e', '\u043b\u043e\u0432', '\u043d\u043e\u0439', '\u2581\u043e\u0442', '\u0432\u0435\u0442', '\u0441\u0442\u0432\u0435\u043d', '\u043d\u043e\u0441\u0442\u0438', '\",', '\u2581-', '\u2581\u043f\u043e', '\u044f', '\u0441\u043d\u0438', '\u043b\u0430', '\u2581\"', '\u042a', '\"', '\u2581\u043f\u0440\u0435', '\u0441\u0441', '-', '\u0441\u0435', '\u043a\u0440\u0435', '\u0442\u0430', '\u0440\u044c', '\u2581\u041c\u043e', '\u0441', '\u0433\u043e\u0440', '\u0441\u0443', '\u0434\u0430', '\u2581\u0423', '\u043b\u044c', '\u044f', '\u043d\u0430', '\u2581\u0421', '\u043e\u043b\u043e', '\u043f\u043e', '\u0432\u0430', '.']"}], "[\n  [\"\u2581\\\"\", null],\n  [\"\u041c\u0430\", \"\u9a6c\u9a6c\u8036\u592b\"],\n  [\"\u043c\u0430\", null],\n  [\"\u0435\u0432\", null],\n  [\"\u2581\u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f\", \"\u662f\"],\n  [\"\u2581\u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\", \"\u7ec4\u7ec7\u8005\"],\n  [\"\u0442\u043e\", null],\n  [\"\u0440\u043e\u043c\", null],\n  [\"\u2581\u0440\u0430\u0437\", \"\u62a2\u52ab\"],\n  [\"\u0431\u043e\", null],\n  [\"\u044f\", null],\n  [\",\", null],\n  [\"\u2581\u0447\u0442\u043e\", \"\u8fd9\"],\n  [\"\u2581\u0442\u0430\u043a\u0436\u0435\", \"\u4e5f\"],\n  [\"\u2581\u043e\u0431\u0440\u0430\", \"\u6784\u6210\"],\n  [\"\u0437\u0443\", null],\n  [\"\u0435\u0442\", null],\n  [\"\u2581\u0441\u043e\u0441\u0442\u0430\u0432\", \"\u72af\u7f6a\"],\n  [\"\u2581\u043f\u0440\u0435\", null],\n  [\"\u0441\u0442\u0443\u043f\", null],\n  [\"\u043b\u0435\u043d\u0438\u044f\", null],\n  [\",\", null],\n  [\"\u2581\u0438\", \"\u5e76\u4e14\"],\n  [\",\", null],\n  [\"\u2581\u0441\u043e\", \"\u76f8\u5e94\u5730\"],\n  [\"\u043e\u0442\", null],\n  [\"\u0432\u0435\u0442\", null],\n  [\"\u0441\u0442\u0432\u0435\u043d\", null],\n  [\"\u043d\u043e\", null],\n  [\",\", null],\n  [\"\u2581\u0442\u0430\u043a\u043e\", \"\u8fd9\u6837\"],\n  [\"\u0435\", null],\n  [\"\u2581\u043b\u0438\", \"\u4eba\"],\n  [\"\u0446\u043e\", null],\n  [\"\u2581\u0434\u043e\u043b\", \"\u5e94\u8be5\"],\n  [\"\u0436\u043d\u043e\", null],\n  [\"\u2581\u0431\u044b\", \"\u88ab\"],\n  [\"\u0442\u044c\", null],\n  [\"\u2581\u043f\u0440\u0438\", \"\u8ffd\u7a76\"],\n  [\"\u0432\", null],\n  [\"\u043b\u0435\", null],\n  [\"\u0447\u0435\", null],\n  [\"\u043d\u043e\", null],\n  [\"\u2581\u043a\", null],\n  [\"\u2581\u0443\", \"\u5211\u4e8b\"],\n  [\"\u0433\u043e\", null],\n  [\"\u043b\u043e\u0432\", null],\n  [\"\u043d\u043e\u0439\", null],\n  [\"\u2581\u043e\u0442\", \"\u8d23\u4efb\"],\n  [\"\u0432\u0435\u0442\", null],\n  [\"\u0441\u0442\u0432\u0435\u043d\", null],\n  [\"\u043d\u043e\u0441\u0442\u0438\", null],\n  [\"\\\"\", null],\n  [\"\u2581-\", null],\n  [\"\u2581\u043f\u043e\", \"\u89e3\u91ca\"],\n  [\"\u044f\", null],\n  [\"\u0441\u043d\u0438\", null],\n  [\"\u043b\u0430\", null],\n  [\"\u2581\\\"\", null],\n  [\"\u042a\", \"\u300a\u5546\u4e1a\u62a5\u300b\"],\n  [\"\\\"\", null],\n  [\"\u2581\u043f\u0440\u0435\", \"\u65b0\u95fb\"],\n  [\"\u0441\u0441\", null],\n  [\"-\", null],\n  [\"\u0441\u0435\", \"\u79d8\u4e66\"],\n  [\"\u043a\u0440\u0435\", null],\n  [\"\u0442\u0430\", null],\n  [\"\u0440\u044c\", null],\n  [\"\u2581\u041c\u043e\", \"\u83ab\u65af\u79d1\"],\n  [\"\u0441\", null],\n  [\"\u0433\u043e\u0440\", \"\u5e02\"],\n  [\"\u0441\u0443\", \"\u6cd5\u9662\"],\n  [\"\u0434\u0430\", null],\n  [\"\u2581\u0423\", \"\u4e4c\u8389\u4e9a\u5a1c\"],\n  [\"\u043b\u044c\", null],\n  [\"\u044f\", null],\n  [\"\u043d\u0430\", null],\n  [\"\u2581\u0421\", \"\u7d22\u6d1b\u6ce2\u5a03\"],\n  [\"\u043e\u043b\u043e\", null],\n  [\"\u043f\u043e\", null],\n  [\"\u0432\u0430\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\"\u041c\u0430\u043c\u0430\u0435\u0432 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0442\u043e\u0440\u043e\u043c \u0440\u0430\u0437\u0431\u043e\u044f, \u0447\u0442\u043e \u0442\u0430\u043a\u0436\u0435 \u043e\u0431\u0440\u0430\u0437\u0443\u0435\u0442 \u0441\u043e\u0441\u0442\u0430\u0432 \u043f\u0440\u0435\u0441\u0442\u0443\u043f\u043b\u0435\u043d\u0438\u044f, \u0438, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e, \u0442\u0430\u043a\u043e\u0435 \u043b\u0438\u0446\u043e \u0434\u043e\u043b\u0436\u043d\u043e \u0431\u044b\u0442\u044c \u043f\u0440\u0438\u0432\u043b\u0435\u0447\u0435\u043d\u043e \u043a \u0443\u0433\u043e\u043b\u043e\u0432\u043d\u043e\u0439 \u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0441\u0442\u0438\", - \u043f\u043e\u044f\u0441\u043d\u0438\u043b\u0430 \"\u042a\" \u043f\u0440\u0435\u0441\u0441-\u0441\u0435\u043a\u0440\u0435\u0442\u0430\u0440\u044c \u041c\u043e\u0441\u0433\u043e\u0440\u0441\u0443\u0434\u0430 \u0423\u043b\u044c\u044f\u043d\u0430 \u0421\u043e\u043b\u043e\u043f\u043e\u0432\u0430.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\"', '\u041c\u0430', '\u043c\u0430', '\u0435\u0432', '\u2581\u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f', '\u2581\u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430', '\u0442\u043e', '\u0440\u043e\u043c', '\u2581\u0440\u0430\u0437', '\u0431\u043e', '\u044f', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0442\u0430\u043a\u0436\u0435', '\u2581\u043e\u0431\u0440\u0430', '\u0437\u0443', '\u0435\u0442', '\u2581\u0441\u043e\u0441\u0442\u0430\u0432', '\u2581\u043f\u0440\u0435', '\u0441\u0442\u0443\u043f', '\u043b\u0435\u043d\u0438\u044f', ',', '\u2581\u0438', ',', '\u2581\u0441\u043e', '\u043e\u0442', '\u0432\u0435\u0442', '\u0441\u0442\u0432\u0435\u043d', '\u043d\u043e', ',', '\u2581\u0442\u0430\u043a\u043e', '\u0435', '\u2581\u043b\u0438', '\u0446\u043e', '\u2581\u0434\u043e\u043b', '\u0436\u043d\u043e', '\u2581\u0431\u044b', '\u0442\u044c', '\u2581\u043f\u0440\u0438', '\u0432', '\u043b\u0435', '\u0447\u0435', '\u043d\u043e', '\u2581\u043a', '\u2581\u0443', '\u0433\u043e', '\u043b\u043e\u0432', '\u043d\u043e\u0439', '\u2581\u043e\u0442', '\u0432\u0435\u0442', '\u0441\u0442\u0432\u0435\u043d', '\u043d\u043e\u0441\u0442\u0438', '\",', '\u2581-', '\u2581\u043f\u043e', '\u044f', '\u0441\u043d\u0438', '\u043b\u0430', '\u2581\"', '\u042a', '\"', '\u2581\u043f\u0440\u0435', '\u0441\u0441', '-', '\u0441\u0435', '\u043a\u0440\u0435', '\u0442\u0430', '\u0440\u044c', '\u2581\u041c\u043e', '\u0441', '\u0433\u043e\u0440', '\u0441\u0443', '\u0434\u0430', '\u2581\u0423', '\u043b\u044c', '\u044f', '\u043d\u0430', '\u2581\u0421', '\u043e\u043b\u043e', '\u043f\u043e', '\u0432\u0430', '.']"}], "[\n[\"\u2581\\\"\", null],\n[\"\u041c\u0430\", \"\u041c\u0430\u043c\u0430\u0435\u0432\"],\n[\"\u043c\u0430\", null],\n[\"\u0435\u0432\", null],\n[\"\u2581\u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f\", \"\u0431\u043e\u043b\u044b\u043f \u0442\u0430\u0431\u044b\u043b\u0430\u0434\u044b\"],\n[\"\u2581\u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\", \"\u04b1\u0439\u044b\u043c\u0434\u0430\u0441\u0442\u044b\u0440\u0443\u0448\u044b\"],\n[\"\u0442\u043e\", null],\n[\"\u0440\u043e\u043c\", null],\n[\"\u2581\u0440\u0430\u0437\", \"\u0442\u043e\u043d\u0430\u0443\"],\n[\"\u0431\u043e\", null],\n[\"\u044f\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", null],\n[\"\u2581\u0442\u0430\u043a\u0436\u0435\", \"\u0441\u043e\u043d\u044b\u043c\u0435\u043d \u049b\u0430\u0442\u0430\u0440\"],\n[\"\u2581\u043e\u0431\u0440\u0430\", \"\u049b\u04b1\u0440\u0430\u0439\u0434\u044b\"],\n[\"\u0437\u0443\", null],\n[\"\u0435\u0442\", null],\n[\"\u2581\u0441\u043e\u0441\u0442\u0430\u0432\", \"\u049b\u04b1\u0440\u0430\u043c\u044b\u043d\"],\n[\"\u2581\u043f\u0440\u0435\", \"\u049b\u044b\u043b\u043c\u044b\u0441\"],\n[\"\u0441\u0442\u0443\u043f\", null],\n[\"\u043b\u0435\u043d\u0438\u044f\", null],\n[\",\", null],\n[\"\u2581\u0438\", null],\n[\",\", null],\n[\"\u2581\u0441\u043e\", \"\u0442\u0438\u0456\u0441\u0456\u043d\u0448\u0435\"],\n[\"\u043e\u0442\", null],\n[\"\u0432\u0435\u0442\", null],\n[\"\u0441\u0442\u0432\u0435\u043d\", null],\n[\"\u043d\u043e\", null],\n[\",\", null],\n[\"\u2581\u0442\u0430\u043a\u043e\", \"\u043c\u04b1\u043d\u0434\u0430\u0439\"],\n[\"\u0435\", null],\n[\"\u2581\u043b\u0438\", \"\u0430\u0434\u0430\u043c\"],\n[\"\u0446\u043e\", null],\n[\"\u2581\u0434\u043e\u043b\", \"\u0442\u0438\u0456\u0441\"],\n[\"\u0436\u043d\u043e\", null],\n[\"\u2581\u0431\u044b\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043f\u0440\u0438\", \"\u049b\u044b\u043b\u043c\u044b\u0441\u0442\u044b\u049b\"],\n[\"\u0432\", null],\n[\"\u043b\u0435\", null],\n[\"\u0447\u0435\", null],\n[\"\u043d\u043e\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u0443\", null],\n[\"\u0433\u043e\", null],\n[\"\u043b\u043e\u0432\", null],\n[\"\u043d\u043e\u0439\", null],\n[\"\u2581\u043e\u0442\", \"\u0436\u0430\u0443\u0430\u043f\u043a\u0435\u0440\u0448\u0456\u043b\u0456\u043a\u043a\u0435\"],\n[\"\u0432\u0435\u0442\", null],\n[\"\u0441\u0442\u0432\u0435\u043d\", null],\n[\"\u043d\u043e\u0441\u0442\u0438\", null],\n[\"\\\"\", null],\n[\"\u2581-\", null],\n[\"\u2581\u043f\u043e\", \"\u0442\u04af\u0441\u0456\u043d\u0434\u0456\u0440\u0434\u0456\"],\n[\"\u044f\", null],\n[\"\u0441\u043d\u0438\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\\\"\", null],\n[\"\u042a\", null],\n[\"\\\"\", null],\n[\"\u2581\u043f\u0440\u0435\", \"\u0431\u0430\u0441\u043f\u0430\u0441\u04e9\u0437\"],\n[\"\u0441\u0441\", null],\n[\"-\", null],\n[\"\u0441\u0435\", \"\u0445\u0430\u0442\u0448\u044b\u0441\u044b\"],\n[\"\u043a\u0440\u0435\", null],\n[\"\u0442\u0430\", null],\n[\"\u0440\u044c\", null],\n[\"\u2581\u041c\u043e\", \"\u041c\u04d9\u0441\u043a\u0435\u0443\"],\n[\"\u0441\", null],\n[\"\u0433\u043e\u0440\", null],\n[\"\u0441\u0443\", \"\u049b\u0430\u043b\u0430\u043b\u044b\u049b\"],\n[\"\u0434\u0430\", \"\u0441\u043e\u0442\u044b\u043d\u044b\u04a3\"],\n[\"\u2581\u0423\", \"\u0423\u043b\u044c\u044f\u043d\u0430\"],\n[\"\u043b\u044c\", null],\n[\"\u044f\", null],\n[\"\u043d\u0430\", null],\n[\"\u2581\u0421\", \"\u0421\u043e\u043b\u043e\u043f\u043e\u0432\u0430\"],\n[\"\u043e\u043b\u043e\", null],\n[\"\u043f\u043e\", null],\n[\"\u0432\u0430\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0412 \u0441\u0432\u043e\u0435\u0439 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0439 \u043a\u043d\u0438\u0433\u0435 \"\u041d\u0430\u0440\u043a\u043e-\u0436\u0443\u0440\u043d\u0430\u043b\u0438\u0441\u0442\u0438\u043a\u0430\" \u043e\u043d \u043d\u0430\u043f\u0438\u0441\u0430\u043b, \u0447\u0442\u043e \u0440\u0435\u043f\u043e\u0440\u0442\u0435\u0440\u043e\u0432 \u0443\u0431\u0438\u0432\u0430\u044e\u0442 \u043d\u0435 \u043f\u0440\u043e\u0441\u0442\u043e \u0431\u0430\u043d\u0434\u044b \u043d\u0430\u0440\u043a\u043e\u0442\u043e\u0440\u0433\u043e\u0432\u0446\u0435\u0432; \u043e\u043d\u0438 \u0434\u0435\u043b\u0430\u044e\u0442 \u044d\u0442\u043e \u043f\u043e \u043f\u0440\u0438\u043a\u0430\u0437\u0430\u043c \u043f\u043e\u043b\u0438\u0442\u0438\u043a\u043e\u0432 \u0438 \u0441\u043b\u0443\u0436\u0431 \u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u0438, \u043d\u0430\u0445\u043e\u0434\u044f\u0449\u0438\u0445\u0441\u044f \u0432 \u0441\u0433\u043e\u0432\u043e\u0440\u0435 \u0441 \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u043f\u0440\u0435\u0441\u0442\u0443\u043f\u043d\u043e\u0441\u0442\u044c\u044e.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0412', '\u2581\u0441\u0432\u043e', '\u0435\u0439', '\u2581\u043f\u043e\u0441\u043b\u0435\u0434', '\u043d\u0435\u0439', '\u2581\u043a\u043d\u0438', '\u0433\u0435', '\u2581\"', '\u041d', '\u0430\u0440', '\u043a\u043e', '-', '\u0436', '\u0443\u0440', '\u043d\u0430\u043b\u0438', '\u0441\u0442\u0438', '\u043a\u0430', '\"', '\u2581\u043e\u043d', '\u2581\u043d\u0430', '\u043f\u0438\u0441\u0430', '\u043b', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0440\u0435', '\u043f\u043e\u0440', '\u0442\u0435', '\u0440\u043e\u0432', '\u2581\u0443\u0431\u0438', '\u0432\u0430', '\u044e\u0442', '\u2581\u043d\u0435', '\u2581\u043f\u0440\u043e', '\u0441\u0442\u043e', '\u2581\u0431', '\u0430\u043d', '\u0434\u044b', '\u2581\u043d\u0430', '\u0440', '\u043a\u043e', '\u0442\u043e\u0440', '\u0433\u043e\u0432', '\u0446\u0435\u0432', ';', '\u2581\u043e\u043d\u0438', '\u2581\u0434\u0435', '\u043b\u0430', '\u044e\u0442', '\u2581\u044d\u0442\u043e', '\u2581\u043f\u043e', '\u2581\u043f\u0440\u0438', '\u043a\u0430\u0437\u0430', '\u043c', '\u2581\u043f\u043e\u043b\u0438\u0442\u0438', '\u043a\u043e\u0432', '\u2581\u0438', '\u2581\u0441\u043b\u0443\u0436', '\u0431', '\u2581\u0431\u0435', '\u0437\u043e', '\u043f\u0430', '\u0441', '\u043d\u043e\u0441\u0442\u0438', ',', '\u2581\u043d\u0430', '\u0445\u043e\u0434\u044f', '\u0449\u0438', '\u0445', '\u0441\u044f', '\u2581\u0432', '\u2581\u0441', '\u0433\u043e\u0432\u043e', '\u0440\u0435', '\u2581\u0441', '\u2581\u043e\u0440\u0433\u0430\u043d\u0438', '\u0437\u043e', '\u0432\u0430\u043d', '\u043d\u043e\u0439', '\u2581\u043f\u0440\u0435', '\u0441\u0442\u0443\u043f', '\u043d\u043e\u0441\u0442\u044c', '\u044e', '.']"}], "[\n[\"\u2581\u0412\", \"In\"],\n[\"\u2581\u0441\u0432\u043e\", \"his\"],\n[\"\u0435\u0439\", null],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\u0434\", \"latest\"],\n[\"\u043d\u0435\u0439\", null],\n[\"\u2581\u043a\u043d\u0438\", \"book\"],\n[\"\u0433\u0435\", null],\n[\"\u2581\\\"\", null],\n[\"\u041d\", \"Narco\"],\n[\"\u0430\u0440\", null],\n[\"\u043a\u043e\", null],\n[\"-\", null],\n[\"\u0436\", null],\n[\"\u0443\u0440\", null],\n[\"\u043d\u0430\u043b\u0438\", null],\n[\"\u0441\u0442\u0438\", null],\n[\"\u043a\u0430\", null],\n[\"\\\"\", null],\n[\"\u2581\u043e\u043d\", \"he\"],\n[\"\u2581\u043d\u0430\", \"wrote\"],\n[\"\u043f\u0438\u0441\u0430\", null],\n[\"\u043b\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\", \"that\"],\n[\"\u2581\u0440\u0435\", \"reporters\"],\n[\"\u043f\u043e\u0440\", null],\n[\"\u0442\u0435\", null],\n[\"\u0440\u043e\u0432\", null],\n[\"\u2581\u0443\u0431\u0438\", \"are killed\"],\n[\"\u0432\u0430\", null],\n[\"\u044e\u0442\", null],\n[\"\u2581\u043d\u0435\", \"not\"],\n[\"\u2581\u043f\u0440\u043e\", \"just\"],\n[\"\u0441\u0442\u043e\", null],\n[\"\u2581\u0431\", \"by\"],\n[\"\u0430\u043d\", null],\n[\"\u0434\u044b\", null],\n[\"\u2581\u043d\u0430\", \"drug\"],\n[\"\u0440\", null],\n[\"\u043a\u043e\", null],\n[\"\u0442\u043e\u0440\", null],\n[\"\u0433\u043e\u0432\", null],\n[\"\u0446\u0435\u0432\", null],\n[\";\", null],\n[\"\u2581\u043e\u043d\u0438\", \"they\"],\n[\"\u2581\u0434\u0435\", \"do\"],\n[\"\u043b\u0430\", null],\n[\"\u044e\u0442\", null],\n[\"\u2581\u044d\u0442\u043e\", \"this\"],\n[\"\u2581\u043f\u043e\", \"on\"],\n[\"\u2581\u043f\u0440\u0438\", \"orders\"],\n[\"\u043a\u0430\u0437\u0430\", null],\n[\"\u043c\", null],\n[\"\u2581\u043f\u043e\u043b\u0438\u0442\u0438\", \"politicians\"],\n[\"\u043a\u043e\u0432\", null],\n[\"\u2581\u0438\", \"and\"],\n[\"\u2581\u0441\u043b\u0443\u0436\", \"security\"],\n[\"\u0431\", null],\n[\"\u2581\u0431\u0435\", null],\n[\"\u0437\u043e\", null],\n[\"\u043f\u0430\", null],\n[\"\u0441\", null],\n[\"\u043d\u043e\u0441\u0442\u0438\", null],\n[\",\", null],\n[\"\u2581\u043d\u0430\", \"who\"],\n[\"\u0445\u043e\u0434\u044f\", \"are\"],\n[\"\u0449\u0438\", null],\n[\"\u0445\", null],\n[\"\u0441\u044f\", null],\n[\"\u2581\u0432\", \"in\"],\n[\"\u2581\u0441\", \"collusion\"],\n[\"\u0433\u043e\u0432\u043e\", null],\n[\"\u0440\u0435\", null],\n[\"\u2581\u0441\", \"with\"],\n[\"\u2581\u043e\u0440\u0433\u0430\u043d\u0438\", \"organized\"],\n[\"\u0437\u043e\", null],\n[\"\u0432\u0430\u043d\", null],\n[\"\u043d\u043e\u0439\", null],\n[\"\u2581\u043f\u0440\u0435\", \"crime\"],\n[\"\u0441\u0442\u0443\u043f\", null],\n[\"\u043d\u043e\u0441\u0442\u044c\", null],\n[\"\u044e\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u0412 \u0441\u0432\u043e\u0435\u0439 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0439 \u043a\u043d\u0438\u0433\u0435 \"\u041d\u0430\u0440\u043a\u043e-\u0436\u0443\u0440\u043d\u0430\u043b\u0438\u0441\u0442\u0438\u043a\u0430\" \u043e\u043d \u043d\u0430\u043f\u0438\u0441\u0430\u043b, \u0447\u0442\u043e \u0440\u0435\u043f\u043e\u0440\u0442\u0435\u0440\u043e\u0432 \u0443\u0431\u0438\u0432\u0430\u044e\u0442 \u043d\u0435 \u043f\u0440\u043e\u0441\u0442\u043e \u0431\u0430\u043d\u0434\u044b \u043d\u0430\u0440\u043a\u043e\u0442\u043e\u0440\u0433\u043e\u0432\u0446\u0435\u0432; \u043e\u043d\u0438 \u0434\u0435\u043b\u0430\u044e\u0442 \u044d\u0442\u043e \u043f\u043e \u043f\u0440\u0438\u043a\u0430\u0437\u0430\u043c \u043f\u043e\u043b\u0438\u0442\u0438\u043a\u043e\u0432 \u0438 \u0441\u043b\u0443\u0436\u0431 \u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u0438, \u043d\u0430\u0445\u043e\u0434\u044f\u0449\u0438\u0445\u0441\u044f \u0432 \u0441\u0433\u043e\u0432\u043e\u0440\u0435 \u0441 \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u043f\u0440\u0435\u0441\u0442\u0443\u043f\u043d\u043e\u0441\u0442\u044c\u044e.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0412', '\u2581\u0441\u0432\u043e', '\u0435\u0439', '\u2581\u043f\u043e\u0441\u043b\u0435\u0434', '\u043d\u0435\u0439', '\u2581\u043a\u043d\u0438', '\u0433\u0435', '\u2581\"', '\u041d', '\u0430\u0440', '\u043a\u043e', '-', '\u0436', '\u0443\u0440', '\u043d\u0430\u043b\u0438', '\u0441\u0442\u0438', '\u043a\u0430', '\"', '\u2581\u043e\u043d', '\u2581\u043d\u0430', '\u043f\u0438\u0441\u0430', '\u043b', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0440\u0435', '\u043f\u043e\u0440', '\u0442\u0435', '\u0440\u043e\u0432', '\u2581\u0443\u0431\u0438', '\u0432\u0430', '\u044e\u0442', '\u2581\u043d\u0435', '\u2581\u043f\u0440\u043e', '\u0441\u0442\u043e', '\u2581\u0431', '\u0430\u043d', '\u0434\u044b', '\u2581\u043d\u0430', '\u0440', '\u043a\u043e', '\u0442\u043e\u0440', '\u0433\u043e\u0432', '\u0446\u0435\u0432', ';', '\u2581\u043e\u043d\u0438', '\u2581\u0434\u0435', '\u043b\u0430', '\u044e\u0442', '\u2581\u044d\u0442\u043e', '\u2581\u043f\u043e', '\u2581\u043f\u0440\u0438', '\u043a\u0430\u0437\u0430', '\u043c', '\u2581\u043f\u043e\u043b\u0438\u0442\u0438', '\u043a\u043e\u0432', '\u2581\u0438', '\u2581\u0441\u043b\u0443\u0436', '\u0431', '\u2581\u0431\u0435', '\u0437\u043e', '\u043f\u0430', '\u0441', '\u043d\u043e\u0441\u0442\u0438', ',', '\u2581\u043d\u0430', '\u0445\u043e\u0434\u044f', '\u0449\u0438', '\u0445', '\u0441\u044f', '\u2581\u0432', '\u2581\u0441', '\u0433\u043e\u0432\u043e', '\u0440\u0435', '\u2581\u0441', '\u2581\u043e\u0440\u0433\u0430\u043d\u0438', '\u0437\u043e', '\u0432\u0430\u043d', '\u043d\u043e\u0439', '\u2581\u043f\u0440\u0435', '\u0441\u0442\u0443\u043f', '\u043d\u043e\u0441\u0442\u044c', '\u044e', '.']"}], "[\n  [\"\u2581\u0412\", null],\n  [\"\u2581\u0441\u0432\u043e\", \"\u4ed6\u7684\"],\n  [\"\u0435\u0439\", null],\n  [\"\u2581\u043f\u043e\u0441\u043b\u0435\u0434\", \"\u6700\u65b0\"],\n  [\"\u043d\u0435\u0439\", null],\n  [\"\u2581\u043a\u043d\u0438\", \"\u4e66\"],\n  [\"\u0433\u0435\", null],\n  [\"\u2581\\\"\", null],\n  [\"\u041d\", \"\u6bd2\u54c1\"],\n  [\"\u0430\u0440\", null],\n  [\"\u043a\u043e\", null],\n  [\"-\", null],\n  [\"\u0436\", \"\u65b0\u95fb\"],\n  [\"\u0443\u0440\", null],\n  [\"\u043d\u0430\u043b\u0438\", null],\n  [\"\u0441\u0442\u0438\", null],\n  [\"\u043a\u0430\", null],\n  [\"\\\"\", null],\n  [\"\u2581\u043e\u043d\", \"\u4ed6\"],\n  [\"\u2581\u043d\u0430\", \"\u5199\"],\n  [\"\u043f\u0438\u0441\u0430\", null],\n  [\"\u043b\", null],\n  [\",\", null],\n  [\"\u2581\u0447\u0442\u043e\", \"\u90a3\"],\n  [\"\u2581\u0440\u0435\", \"\u8bb0\u8005\"],\n  [\"\u043f\u043e\u0440\", null],\n  [\"\u0442\u0435\", null],\n  [\"\u0440\u043e\u0432\", null],\n  [\"\u2581\u0443\u0431\u0438\", \"\u88ab\u6740\"],\n  [\"\u0432\u0430\", null],\n  [\"\u044e\u0442\", null],\n  [\"\u2581\u043d\u0435\", \"\u4e0d\u4ec5\"],\n  [\"\u2581\u043f\u0440\u043e\", \"\u4ec5\u4ec5\"],\n  [\"\u0441\u0442\u043e\", null],\n  [\"\u2581\u0431\", \"\u6bd2\u54c1\"],\n  [\"\u0430\u043d\", null],\n  [\"\u0434\u044b\", null],\n  [\"\u2581\u043d\u0430\", \"\u8d29\u5b50\"],\n  [\"\u0440\", null],\n  [\"\u043a\u043e\", null],\n  [\"\u0442\u043e\u0440\", null],\n  [\"\u0433\u043e\u0432\", null],\n  [\"\u0446\u0435\u0432\", null],\n  [\";\", null],\n  [\"\u2581\u043e\u043d\u0438\", \"\u4ed6\u4eec\"],\n  [\"\u2581\u0434\u0435\", \"\u8fd9\u6837\u505a\"],\n  [\"\u043b\u0430\", null],\n  [\"\u044e\u0442\", null],\n  [\"\u2581\u044d\u0442\u043e\", \"\u662f\"],\n  [\"\u2581\u043f\u043e\", \"\u6839\u636e\"],\n  [\"\u2581\u043f\u0440\u0438\", \"\u547d\u4ee4\"],\n  [\"\u043a\u0430\u0437\u0430\", null],\n  [\"\u043c\", null],\n  [\"\u2581\u043f\u043e\u043b\u0438\u0442\u0438\", \"\u653f\u5ba2\"],\n  [\"\u043a\u043e\u0432\", null],\n  [\"\u2581\u0438\", \"\u548c\"],\n  [\"\u2581\u0441\u043b\u0443\u0436\", \"\u5b89\u5168\"],\n  [\"\u0431\", null],\n  [\"\u2581\u0431\u0435\", \"\u90e8\u95e8\"],\n  [\"\u0437\u043e\", null],\n  [\"\u043f\u0430\", null],\n  [\"\u0441\", null],\n  [\"\u043d\u043e\u0441\u0442\u0438\", null],\n  [\",\", null],\n  [\"\u2581\u043d\u0430\", \"\u4e0e\"],\n  [\"\u0445\u043e\u0434\u044f\", null],\n  [\"\u0449\u0438\", null],\n  [\"\u0445\", null],\n  [\"\u0441\u044f\", null],\n  [\"\u2581\u0432\", null],\n  [\"\u2581\u0441\", \"\u52fe\u7ed3\"],\n  [\"\u0433\u043e\u0432\u043e\", null],\n  [\"\u0440\u0435\", null],\n  [\"\u2581\u0441\", \"\u7684\"],\n  [\"\u2581\u043e\u0440\u0433\u0430\u043d\u0438\", \"\u6709\u7ec4\u7ec7\"],\n  [\"\u0437\u043e\", null],\n  [\"\u0432\u0430\u043d\", null],\n  [\"\u043d\u043e\u0439\", null],\n  [\"\u2581\u043f\u0440\u0435\", \"\u72af\u7f6a\"],\n  [\"\u0441\u0442\u0443\u043f\", null],\n  [\"\u043d\u043e\u0441\u0442\u044c\", null],\n  [\"\u044e\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u0412 \u0441\u0432\u043e\u0435\u0439 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0439 \u043a\u043d\u0438\u0433\u0435 \"\u041d\u0430\u0440\u043a\u043e-\u0436\u0443\u0440\u043d\u0430\u043b\u0438\u0441\u0442\u0438\u043a\u0430\" \u043e\u043d \u043d\u0430\u043f\u0438\u0441\u0430\u043b, \u0447\u0442\u043e \u0440\u0435\u043f\u043e\u0440\u0442\u0435\u0440\u043e\u0432 \u0443\u0431\u0438\u0432\u0430\u044e\u0442 \u043d\u0435 \u043f\u0440\u043e\u0441\u0442\u043e \u0431\u0430\u043d\u0434\u044b \u043d\u0430\u0440\u043a\u043e\u0442\u043e\u0440\u0433\u043e\u0432\u0446\u0435\u0432; \u043e\u043d\u0438 \u0434\u0435\u043b\u0430\u044e\u0442 \u044d\u0442\u043e \u043f\u043e \u043f\u0440\u0438\u043a\u0430\u0437\u0430\u043c \u043f\u043e\u043b\u0438\u0442\u0438\u043a\u043e\u0432 \u0438 \u0441\u043b\u0443\u0436\u0431 \u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u0438, \u043d\u0430\u0445\u043e\u0434\u044f\u0449\u0438\u0445\u0441\u044f \u0432 \u0441\u0433\u043e\u0432\u043e\u0440\u0435 \u0441 \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u043f\u0440\u0435\u0441\u0442\u0443\u043f\u043d\u043e\u0441\u0442\u044c\u044e.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0412', '\u2581\u0441\u0432\u043e', '\u0435\u0439', '\u2581\u043f\u043e\u0441\u043b\u0435\u0434', '\u043d\u0435\u0439', '\u2581\u043a\u043d\u0438', '\u0433\u0435', '\u2581\"', '\u041d', '\u0430\u0440', '\u043a\u043e', '-', '\u0436', '\u0443\u0440', '\u043d\u0430\u043b\u0438', '\u0441\u0442\u0438', '\u043a\u0430', '\"', '\u2581\u043e\u043d', '\u2581\u043d\u0430', '\u043f\u0438\u0441\u0430', '\u043b', ',', '\u2581\u0447\u0442\u043e', '\u2581\u0440\u0435', '\u043f\u043e\u0440', '\u0442\u0435', '\u0440\u043e\u0432', '\u2581\u0443\u0431\u0438', '\u0432\u0430', '\u044e\u0442', '\u2581\u043d\u0435', '\u2581\u043f\u0440\u043e', '\u0441\u0442\u043e', '\u2581\u0431', '\u0430\u043d', '\u0434\u044b', '\u2581\u043d\u0430', '\u0440', '\u043a\u043e', '\u0442\u043e\u0440', '\u0433\u043e\u0432', '\u0446\u0435\u0432', ';', '\u2581\u043e\u043d\u0438', '\u2581\u0434\u0435', '\u043b\u0430', '\u044e\u0442', '\u2581\u044d\u0442\u043e', '\u2581\u043f\u043e', '\u2581\u043f\u0440\u0438', '\u043a\u0430\u0437\u0430', '\u043c', '\u2581\u043f\u043e\u043b\u0438\u0442\u0438', '\u043a\u043e\u0432', '\u2581\u0438', '\u2581\u0441\u043b\u0443\u0436', '\u0431', '\u2581\u0431\u0435', '\u0437\u043e', '\u043f\u0430', '\u0441', '\u043d\u043e\u0441\u0442\u0438', ',', '\u2581\u043d\u0430', '\u0445\u043e\u0434\u044f', '\u0449\u0438', '\u0445', '\u0441\u044f', '\u2581\u0432', '\u2581\u0441', '\u0433\u043e\u0432\u043e', '\u0440\u0435', '\u2581\u0441', '\u2581\u043e\u0440\u0433\u0430\u043d\u0438', '\u0437\u043e', '\u0432\u0430\u043d', '\u043d\u043e\u0439', '\u2581\u043f\u0440\u0435', '\u0441\u0442\u0443\u043f', '\u043d\u043e\u0441\u0442\u044c', '\u044e', '.']"}], "[\n  [\"\u2581\u0412\", null],\n  [\"\u2581\u0441\u0432\u043e\", \"\u04e9\u0437\u0456\u043d\u0456\u04a3\"],\n  [\"\u0435\u0439\", null],\n  [\"\u2581\u043f\u043e\u0441\u043b\u0435\u0434\", \"\u0441\u043e\u04a3\u0493\u044b\"],\n  [\"\u043d\u0435\u0439\", null],\n  [\"\u2581\u043a\u043d\u0438\", \"\u043a\u0456\u0442\u0430\u0431\u044b\"],\n  [\"\u0433\u0435\", null],\n  [\"\u2581\\\"\", null],\n  [\"\u041d\", \"\u0415\u0441\u0456\u0440\u0442\u043a\u0456\"],\n  [\"\u0430\u0440\", null],\n  [\"\u043a\u043e\", null],\n  [\"-\", null],\n  [\"\u0436\", null],\n  [\"\u0443\u0440\", null],\n  [\"\u043d\u0430\u043b\u0438\", null],\n  [\"\u0441\u0442\u0438\", null],\n  [\"\u043a\u0430\", null],\n  [\"\\\"\", null],\n  [\"\u2581\u043e\u043d\", \"\u043e\u043b\"],\n  [\"\u2581\u043d\u0430\", null],\n  [\"\u043f\u0438\u0441\u0430\", \"\u0436\u0430\u0437\u0434\u044b\"],\n  [\"\u043b\", null],\n  [\",\", null],\n  [\"\u2581\u0447\u0442\u043e\", null],\n  [\"\u2581\u0440\u0435\", \"\u0442\u0456\u043b\u0448\u0456\u043b\u0435\u0440\u0434\u0456\"],\n  [\"\u043f\u043e\u0440\", null],\n  [\"\u0442\u0435\", null],\n  [\"\u0440\u043e\u0432\", null],\n  [\"\u2581\u0443\u0431\u0438\", \"\u04e9\u043b\u0442\u0456\u0440\u0435\u0442\u0456\u043d\"],\n  [\"\u0432\u0430\", null],\n  [\"\u044e\u0442\", null],\n  [\"\u2581\u043d\u0435\", null],\n  [\"\u2581\u043f\u0440\u043e\", \"\u0442\u0435\u043a\"],\n  [\"\u0441\u0442\u043e\", null],\n  [\"\u2581\u0431\", \"\u0435\u0441\u0456\u0440\u0442\u043a\u0456\"],\n  [\"\u0430\u043d\", null],\n  [\"\u0434\u044b\", null],\n  [\"\u2581\u043d\u0430\", null],\n  [\"\u0440\", null],\n  [\"\u043a\u043e\", null],\n  [\"\u0442\u043e\u0440\", null],\n  [\"\u0433\u043e\u0432\", null],\n  [\"\u0446\u0435\u0432\", null],\n  [\";\", null],\n  [\"\u2581\u043e\u043d\u0438\", \"\u043e\u043b\u0430\u0440\"],\n  [\"\u2581\u0434\u0435\", \"\u043c\u04b1\u043d\u044b\"],\n  [\"\u043b\u0430\", null],\n  [\"\u044e\u0442\", null],\n  [\"\u2581\u044d\u0442\u043e\", null],\n  [\"\u2581\u043f\u043e\", null],\n  [\"\u2581\u043f\u0440\u0438\", \"\u0431\u04b1\u0439\u0440\u044b\u0493\u044b\u043c\u0435\u043d\"],\n  [\"\u043a\u0430\u0437\u0430\", null],\n  [\"\u043c\", null],\n  [\"\u2581\u043f\u043e\u043b\u0438\u0442\u0438\", \"\u0441\u0430\u044f\u0441\u0430\u0442\u043a\u0435\u0440\u043b\u0435\u0440\"],\n  [\"\u043a\u043e\u0432\", null],\n  [\"\u2581\u0438\", \"\u043c\u0435\u043d\"],\n  [\"\u2581\u0441\u043b\u0443\u0436\", \"\u049b\u0430\u0443\u0456\u043f\u0441\u0456\u0437\u0434\u0456\u043a\"],\n  [\"\u0431\", null],\n  [\"\u2581\u0431\u0435\", null],\n  [\"\u0437\u043e\", null],\n  [\"\u043f\u0430\", null],\n  [\"\u0441\", null],\n  [\"\u043d\u043e\u0441\u0442\u0438\", null],\n  [\",\", null],\n  [\"\u2581\u043d\u0430\", \"\u04b1\u0439\u044b\u043c\u0434\u0430\u0441\u049b\u0430\u043d\"],\n  [\"\u0445\u043e\u0434\u044f\", null],\n  [\"\u0449\u0438\", null],\n  [\"\u0445\", null],\n  [\"\u0441\u044f\", null],\n  [\"\u2581\u0432\", null],\n  [\"\u2581\u0441\", null],\n  [\"\u0433\u043e\u0432\u043e\", \"\u0441\u04e9\u0437\"],\n  [\"\u0440\u0435\", null],\n  [\"\u2581\u0441\", null],\n  [\"\u2581\u043e\u0440\u0433\u0430\u043d\u0438\", \"\u049b\u044b\u043b\u043c\u044b\u0441\u043f\u0435\u043d\"],\n  [\"\u0437\u043e\", null],\n  [\"\u0432\u0430\u043d\", null],\n  [\"\u043d\u043e\u0439\", null],\n  [\"\u2581\u043f\u0440\u0435\", null],\n  [\"\u0441\u0442\u0443\u043f\", null],\n  [\"\u043d\u043e\u0441\u0442\u044c\", null],\n  [\"\u044e\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0418\u0442\u0430\u043b\u0438\u044f \u0443\u0436\u0435\u0441\u0442\u043e\u0447\u0430\u0435\u0442 \u0442\u043e\u043d \u043f\u043e \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044e \u043a \u043a\u043e\u0440\u0430\u0431\u043b\u044f\u043c \u041d\u041f\u041e, \u0441\u043f\u0430\u0441\u0430\u044e\u0449\u0438\u0445 \u043c\u0438\u0433\u0440\u0430\u043d\u0442\u043e\u0432.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0418', '\u0442\u0430', '\u043b\u0438\u044f', '\u2581\u0443', '\u0436\u0435', '\u0441\u0442\u043e', '\u0447\u0430', '\u0435\u0442', '\u2581\u0442\u043e', '\u043d', '\u2581\u043f\u043e', '\u2581\u043e\u0442\u043d\u043e', '\u0448\u0435', '\u043d\u0438\u044e', '\u2581\u043a', '\u2581\u043a\u043e', '\u0440\u0430\u0431', '\u043b\u044f', '\u043c', '\u2581\u041d', '\u041f', '\u041e', ',', '\u2581\u0441', '\u043f\u0430', '\u0441\u0430', '\u044e\u0449\u0438\u0445', '\u2581\u043c\u0438', '\u0433\u0440\u0430', '\u043d', '\u0442\u043e\u0432', '.']"}], "[\n  [\"\u2581\u0418\", \"Italy\"],\n  [\"\u0442\u0430\", null],\n  [\"\u043b\u0438\u044f\", null],\n  [\"\u2581\u0443\", \"toughens\"],\n  [\"\u0436\u0435\", null],\n  [\"\u0441\u0442\u043e\", null],\n  [\"\u0447\u0430\", null],\n  [\"\u0435\u0442\", null],\n  [\"\u2581\u0442\u043e\", \"tone\"],\n  [\"\u043d\", null],\n  [\"\u2581\u043f\u043e\", null],\n  [\"\u2581\u043e\u0442\u043d\u043e\", \"towards\"],\n  [\"\u0448\u0435\", null],\n  [\"\u043d\u0438\u044e\", null],\n  [\"\u2581\u043a\", null],\n  [\"\u2581\u043a\u043e\", \"ships\"],\n  [\"\u0440\u0430\u0431\", null],\n  [\"\u043b\u044f\", null],\n  [\"\u043c\", null],\n  [\"\u2581\u041d\", \"NGO\"],\n  [\"\u041f\", null],\n  [\"\u041e\", null],\n  [\",\", null],\n  [\"\u2581\u0441\", \"rescuing\"],\n  [\"\u043f\u0430\", null],\n  [\"\u0441\u0430\", null],\n  [\"\u044e\u0449\u0438\u0445\", null],\n  [\"\u2581\u043c\u0438\", \"migrants\"],\n  [\"\u0433\u0440\u0430\", null],\n  [\"\u043d\", null],\n  [\"\u0442\u043e\u0432\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u0418\u0442\u0430\u043b\u0438\u044f \u0443\u0436\u0435\u0441\u0442\u043e\u0447\u0430\u0435\u0442 \u0442\u043e\u043d \u043f\u043e \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044e \u043a \u043a\u043e\u0440\u0430\u0431\u043b\u044f\u043c \u041d\u041f\u041e, \u0441\u043f\u0430\u0441\u0430\u044e\u0449\u0438\u0445 \u043c\u0438\u0433\u0440\u0430\u043d\u0442\u043e\u0432.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0418', '\u0442\u0430', '\u043b\u0438\u044f', '\u2581\u0443', '\u0436\u0435', '\u0441\u0442\u043e', '\u0447\u0430', '\u0435\u0442', '\u2581\u0442\u043e', '\u043d', '\u2581\u043f\u043e', '\u2581\u043e\u0442\u043d\u043e', '\u0448\u0435', '\u043d\u0438\u044e', '\u2581\u043a', '\u2581\u043a\u043e', '\u0440\u0430\u0431', '\u043b\u044f', '\u043c', '\u2581\u041d', '\u041f', '\u041e', ',', '\u2581\u0441', '\u043f\u0430', '\u0441\u0430', '\u044e\u0449\u0438\u0445', '\u2581\u043c\u0438', '\u0433\u0440\u0430', '\u043d', '\u0442\u043e\u0432', '.']"}], "[\n[\"\u2581\u0418\", \"\u610f\"],\n[\"\u0442\u0430\", null],\n[\"\u043b\u0438\u044f\", null],\n[\"\u2581\u0443\", null],\n[\"\u0436\u0435\", null],\n[\"\u0441\u0442\u043e\", null],\n[\"\u0447\u0430\", null],\n[\"\u0435\u0442\", null],\n[\"\u2581\u0442\u043e\", null],\n[\"\u043d\", null],\n[\"\u2581\u043f\u043e\", null],\n[\"\u2581\u043e\u0442\u043d\u043e\", \"\u5173\u7cfb\"],\n[\"\u0448\u0435\", null],\n[\"\u043d\u0438\u044e\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u043a\u043e\", \"\u8239\u53ea\"],\n[\"\u0440\u0430\u0431\", null],\n[\"\u043b\u044f\", null],\n[\"\u043c\", null],\n[\"\u2581\u041d\", \"\u975e\u653f\u5e9c\u7ec4\u7ec7\"],\n[\"\u041f\", null],\n[\"\u041e\", null],\n[\",\", null],\n[\"\u2581\u0441\", \"\u62ef\u6551\"],\n[\"\u043f\u0430\", null],\n[\"\u0441\u0430\", null],\n[\"\u044e\u0449\u0438\u0445\", null],\n[\"\u2581\u043c\u0438\", \"\u79fb\u6c11\"],\n[\"\u0433\u0440\u0430\", null],\n[\"\u043d\", null],\n[\"\u0442\u043e\u0432\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u0418\u0442\u0430\u043b\u0438\u044f \u0443\u0436\u0435\u0441\u0442\u043e\u0447\u0430\u0435\u0442 \u0442\u043e\u043d \u043f\u043e \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044e \u043a \u043a\u043e\u0440\u0430\u0431\u043b\u044f\u043c \u041d\u041f\u041e, \u0441\u043f\u0430\u0441\u0430\u044e\u0449\u0438\u0445 \u043c\u0438\u0433\u0440\u0430\u043d\u0442\u043e\u0432.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0418', '\u0442\u0430', '\u043b\u0438\u044f', '\u2581\u0443', '\u0436\u0435', '\u0441\u0442\u043e', '\u0447\u0430', '\u0435\u0442', '\u2581\u0442\u043e', '\u043d', '\u2581\u043f\u043e', '\u2581\u043e\u0442\u043d\u043e', '\u0448\u0435', '\u043d\u0438\u044e', '\u2581\u043a', '\u2581\u043a\u043e', '\u0440\u0430\u0431', '\u043b\u044f', '\u043c', '\u2581\u041d', '\u041f', '\u041e', ',', '\u2581\u0441', '\u043f\u0430', '\u0441\u0430', '\u044e\u0449\u0438\u0445', '\u2581\u043c\u0438', '\u0433\u0440\u0430', '\u043d', '\u0442\u043e\u0432', '.']"}], "[\n[\"\u2581\u0418\", \"\u0418\u0442\u0430\u043b\u0438\u044f\"],\n[\"\u0442\u0430\", null],\n[\"\u043b\u0438\u044f\", null],\n[\"\u2581\u0443\", \"\u043a\u04af\u0448\u0435\u0439\u0442\u0443\u0434\u0435\"],\n[\"\u0436\u0435\", null],\n[\"\u0441\u0442\u043e\", null],\n[\"\u0447\u0430\", null],\n[\"\u0435\u0442\", null],\n[\"\u2581\u0442\u043e\", null],\n[\"\u043d\", \"\u0442\u043e\u043d\u044b\u043d\"],\n[\"\u2581\u043f\u043e\", null],\n[\"\u2581\u043e\u0442\u043d\u043e\", \"\u049b\u0430\u0442\u044b\u0441\u0442\u044b\"],\n[\"\u0448\u0435\", null],\n[\"\u043d\u0438\u044e\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u043a\u043e\", \"\u043a\u0435\u043c\u0435\u043b\u0435\u0440\u0456\u043d\u0435\"],\n[\"\u0440\u0430\u0431\", null],\n[\"\u043b\u044f\", null],\n[\"\u043c\", null],\n[\"\u2581\u041d\", \"\u04ae\u0415\u04b0\"],\n[\"\u041f\", null],\n[\"\u041e\", null],\n[\",\", null],\n[\"\u2581\u0441\", \"\u049b\u04b1\u0442\u049b\u0430\u0440\u0430\u0442\u044b\u043d\"],\n[\"\u043f\u0430\", null],\n[\"\u0441\u0430\", null],\n[\"\u044e\u0449\u0438\u0445\", null],\n[\"\u2581\u043c\u0438\", \"\u043c\u0438\u0433\u0440\u0430\u043d\u0442\u0442\u0430\u0440\u0493\u0430\"],\n[\"\u0433\u0440\u0430\", null],\n[\"\u043d\", null],\n[\"\u0442\u043e\u0432\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0412 \u043e\u043a\u0442\u044f\u0431\u0440\u0435 \u043f\u0440\u043e\u0435\u0437\u0434 \u0432 \u0435\u043a\u0430\u0442\u0435\u0440\u0438\u043d\u0431\u0443\u0440\u0433\u0441\u043a\u043e\u043c \u043c\u0435\u0442\u0440\u043e\u043f\u043e\u043b\u0438\u0442\u0435\u043d\u0435 \u043c\u043e\u0436\u043d\u043e \u0431\u0443\u0434\u0435\u0442 \u043e\u043f\u043b\u0430\u0442\u0438\u0442\u044c \u0431\u0430\u043d\u043a\u043e\u0432\u0441\u043a\u043e\u0439 \u043a\u0430\u0440\u0442\u043e\u0439, \u0437\u0430\u044f\u0432\u0438\u043b \u0433\u0435\u043d\u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440 \u041e\u0410\u041e \"\u0418-\u0421\u0435\u0442\u044c\" \u041f\u0430\u0432\u0435\u043b \u0412\u0435\u0434\u0435\u0440\u043d\u0438\u043a\u043e\u0432 \u0432 \u0438\u043d\u0442\u0435\u0440\u0432\u044c\u044e \"\u041e\u0431\u043b\u0430\u0441\u0442\u043d\u043e\u0439 \u0433\u0430\u0437\u0435\u0442\u0435\".\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0412', '\u2581\u043e\u043a\u0442', '\u044f', '\u0431\u0440\u0435', '\u2581\u043f\u0440\u043e\u0435', '\u0437\u0434', '\u2581\u0432', '\u2581\u0435', '\u043a\u0430', '\u0442\u0435\u0440\u0438', '\u043d', '\u0431\u0443\u0440\u0433', '\u0441\u043a\u043e\u043c', '\u2581\u043c\u0435\u0442', '\u0440\u043e', '\u043f\u043e', '\u043b\u0438', '\u0442', '\u0435\u043d\u0435', '\u2581\u043c\u043e', '\u0436\u043d\u043e', '\u2581\u0431\u0443', '\u0434\u0435', '\u0442', '\u2581\u043e\u043f', '\u043b\u0430', '\u0442\u0438', '\u0442\u044c', '\u2581\u0431', '\u0430\u043d', '\u043a\u043e\u0432', '\u0441\u043a\u043e\u0439', '\u2581\u043a\u0430\u0440', '\u0442\u043e\u0439', ',', '\u2581\u0437\u0430', '\u044f\u0432\u0438', '\u043b', '\u2581\u0433', '\u0435\u043d', '\u0434\u0438', '\u0440\u0435', '\u043a\u0442\u043e\u0440', '\u2581\u041e', '\u0410', '\u041e', '\u2581\"', '\u0418', '-', '\u0421', '\u0435', '\u0442\u044c', '\"', '\u2581\u041f\u0430', '\u0432\u0435', '\u043b', '\u2581\u0412\u0435', '\u0434\u0435\u0440', '\u043d\u0438\u043a\u043e\u0432', '\u2581\u0432', '\u2581\u0438\u043d', '\u0442\u0435\u0440', '\u0432', '\u044c\u044e', '\u2581\"', '\u041e', '\u0431', '\u043b\u0430', '\u0441\u0442', '\u043d\u043e\u0439', '\u2581\u0433\u0430', '\u0437\u0435', '\u0442\u0435', '\".']"}], "[\n[\"\u2581\u0412\", \"In\"],\n[\"\u2581\u043e\u043a\u0442\", \"October\"],\n[\"\u044f\", null],\n[\"\u0431\u0440\u0435\", null],\n[\"\u2581\u043f\u0440\u043e\u0435\", \"fare\"],\n[\"\u0437\u0434\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0435\", \"Yekaterinburg\"],\n[\"\u043a\u0430\", null],\n[\"\u0442\u0435\u0440\u0438\", null],\n[\"\u043d\", null],\n[\"\u0431\u0443\u0440\u0433\", null],\n[\"\u0441\u043a\u043e\u043c\", null],\n[\"\u2581\u043c\u0435\u0442\", \"metro\"],\n[\"\u0440\u043e\", null],\n[\"\u043f\u043e\", null],\n[\"\u043b\u0438\", null],\n[\"\u0442\", null],\n[\"\u0435\u043d\u0435\", null],\n[\"\u2581\u043c\u043e\", \"can\"],\n[\"\u0436\u043d\u043e\", null],\n[\"\u2581\u0431\u0443\", \"be\"],\n[\"\u0434\u0435\", null],\n[\"\u0442\", null],\n[\"\u2581\u043e\u043f\", \"paid\"],\n[\"\u043b\u0430\", null],\n[\"\u0442\u0438\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0431\", \"bank\"],\n[\"\u0430\u043d\", null],\n[\"\u043a\u043e\u0432\", null],\n[\"\u0441\u043a\u043e\u0439\", null],\n[\"\u2581\u043a\u0430\u0440\", \"card\"],\n[\"\u0442\u043e\u0439\", null],\n[\",\", null],\n[\"\u2581\u0437\u0430\", \"stated\"],\n[\"\u044f\u0432\u0438\", null],\n[\"\u043b\", null],\n[\"\u2581\u0433\", \"general\"],\n[\"\u0435\u043d\", null],\n[\"\u0434\u0438\", null],\n[\"\u0440\u0435\", null],\n[\"\u043a\u0442\u043e\u0440\", null],\n[\"\u2581\u041e\", \"JSC\"],\n[\"\u0410\", null],\n[\"\u041e\", null],\n[\"\u2581\\\"\", null],\n[\"\u0418\", \"I\"],\n[\"-\", null],\n[\"\u0421\", \"Set\"],\n[\"\u0435\", null],\n[\"\u0442\u044c\", null],\n[\"\\\"\", null],\n[\"\u2581\u041f\u0430\", \"Pavel\"],\n[\"\u0432\u0435\", null],\n[\"\u043b\", null],\n[\"\u2581\u0412\u0435\", \"Vedernikov\"],\n[\"\u0434\u0435\u0440\", null],\n[\"\u043d\u0438\u043a\u043e\u0432\", null],\n[\"\u2581\u0432\", \"in\"],\n[\"\u2581\u0438\u043d\", \"interview\"],\n[\"\u0442\u0435\u0440\", null],\n[\"\u0432\", null],\n[\"\u044c\u044e\", null],\n[\"\u2581\\\"\", null],\n[\"\u041e\", \"Regional\"],\n[\"\u0431\", null],\n[\"\u043b\u0430\", null],\n[\"\u0441\u0442\", null],\n[\"\u043d\u043e\u0439\", null],\n[\"\u2581\u0433\u0430\", \"Newspaper\"],\n[\"\u0437\u0435\", null],\n[\"\u0442\u0435\", null],\n[\"\\\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u0412 \u043e\u043a\u0442\u044f\u0431\u0440\u0435 \u043f\u0440\u043e\u0435\u0437\u0434 \u0432 \u0435\u043a\u0430\u0442\u0435\u0440\u0438\u043d\u0431\u0443\u0440\u0433\u0441\u043a\u043e\u043c \u043c\u0435\u0442\u0440\u043e\u043f\u043e\u043b\u0438\u0442\u0435\u043d\u0435 \u043c\u043e\u0436\u043d\u043e \u0431\u0443\u0434\u0435\u0442 \u043e\u043f\u043b\u0430\u0442\u0438\u0442\u044c \u0431\u0430\u043d\u043a\u043e\u0432\u0441\u043a\u043e\u0439 \u043a\u0430\u0440\u0442\u043e\u0439, \u0437\u0430\u044f\u0432\u0438\u043b \u0433\u0435\u043d\u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440 \u041e\u0410\u041e \"\u0418-\u0421\u0435\u0442\u044c\" \u041f\u0430\u0432\u0435\u043b \u0412\u0435\u0434\u0435\u0440\u043d\u0438\u043a\u043e\u0432 \u0432 \u0438\u043d\u0442\u0435\u0440\u0432\u044c\u044e \"\u041e\u0431\u043b\u0430\u0441\u0442\u043d\u043e\u0439 \u0433\u0430\u0437\u0435\u0442\u0435\".\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0412', '\u2581\u043e\u043a\u0442', '\u044f', '\u0431\u0440\u0435', '\u2581\u043f\u0440\u043e\u0435', '\u0437\u0434', '\u2581\u0432', '\u2581\u0435', '\u043a\u0430', '\u0442\u0435\u0440\u0438', '\u043d', '\u0431\u0443\u0440\u0433', '\u0441\u043a\u043e\u043c', '\u2581\u043c\u0435\u0442', '\u0440\u043e', '\u043f\u043e', '\u043b\u0438', '\u0442', '\u0435\u043d\u0435', '\u2581\u043c\u043e', '\u0436\u043d\u043e', '\u2581\u0431\u0443', '\u0434\u0435', '\u0442', '\u2581\u043e\u043f', '\u043b\u0430', '\u0442\u0438', '\u0442\u044c', '\u2581\u0431', '\u0430\u043d', '\u043a\u043e\u0432', '\u0441\u043a\u043e\u0439', '\u2581\u043a\u0430\u0440', '\u0442\u043e\u0439', ',', '\u2581\u0437\u0430', '\u044f\u0432\u0438', '\u043b', '\u2581\u0433', '\u0435\u043d', '\u0434\u0438', '\u0440\u0435', '\u043a\u0442\u043e\u0440', '\u2581\u041e', '\u0410', '\u041e', '\u2581\"', '\u0418', '-', '\u0421', '\u0435', '\u0442\u044c', '\"', '\u2581\u041f\u0430', '\u0432\u0435', '\u043b', '\u2581\u0412\u0435', '\u0434\u0435\u0440', '\u043d\u0438\u043a\u043e\u0432', '\u2581\u0432', '\u2581\u0438\u043d', '\u0442\u0435\u0440', '\u0432', '\u044c\u044e', '\u2581\"', '\u041e', '\u0431', '\u043b\u0430', '\u0441\u0442', '\u043d\u043e\u0439', '\u2581\u0433\u0430', '\u0437\u0435', '\u0442\u0435', '\".']"}], "[\n[\"\u2581\u0412\", null],\n[\"\u2581\u043e\u043a\u0442\", \"\u5341\u6708\"],\n[\"\u044f\", null],\n[\"\u0431\u0440\u0435\", null],\n[\"\u2581\u043f\u0440\u043e\u0435\", \"\u4e58\u8f66\"],\n[\"\u0437\u0434\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0435\", \"\u53f6\u5361\u6377\u7433\u5821\"],\n[\"\u043a\u0430\", null],\n[\"\u0442\u0435\u0440\u0438\", null],\n[\"\u043d\", null],\n[\"\u0431\u0443\u0440\u0433\", null],\n[\"\u0441\u043a\u043e\u043c\", null],\n[\"\u2581\u043c\u0435\u0442\", \"\u5730\u94c1\"],\n[\"\u0440\u043e\", null],\n[\"\u043f\u043e\", null],\n[\"\u043b\u0438\", null],\n[\"\u0442\", null],\n[\"\u0435\u043d\u0435\", null],\n[\"\u2581\u043c\u043e\", \"\u53ef\u4ee5\"],\n[\"\u0436\u043d\u043e\", null],\n[\"\u2581\u0431\u0443\", \"\u5c06\"],\n[\"\u0434\u0435\", null],\n[\"\u0442\", null],\n[\"\u2581\u043e\u043f\", \"\u652f\u4ed8\"],\n[\"\u043b\u0430\", null],\n[\"\u0442\u0438\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0431\", \"\u94f6\u884c\u5361\"],\n[\"\u0430\u043d\", null],\n[\"\u043a\u043e\u0432\", null],\n[\"\u0441\u043a\u043e\u0439\", null],\n[\"\u2581\u043a\u0430\u0440\", null],\n[\"\u0442\u043e\u0439\", null],\n[\",\", null],\n[\"\u2581\u0437\u0430\", \"\u5ba3\u5e03\"],\n[\"\u044f\u0432\u0438\", null],\n[\"\u043b\", null],\n[\"\u2581\u0433\", \"\u603b\u7ecf\u7406\"],\n[\"\u0435\u043d\", null],\n[\"\u0434\u0438\", null],\n[\"\u0440\u0435\", null],\n[\"\u043a\u0442\u043e\u0440\", null],\n[\"\u2581\u041e\", null],\n[\"\u0410\", null],\n[\"\u041e\", null],\n[\"\u2581\\\"\", null],\n[\"\u0418\", \"I-Set\"],\n[\"-\", null],\n[\"\u0421\", null],\n[\"\u0435\", null],\n[\"\u0442\u044c\", null],\n[\"\\\"\", null],\n[\"\u2581\u041f\u0430\", \"\u5e15\u7ef4\u5c14\"],\n[\"\u0432\u0435\", null],\n[\"\u043b\", null],\n[\"\u2581\u0412\u0435\", \"\u97e6\u5fb7\u5c3c\u79d1\u592b\"],\n[\"\u0434\u0435\u0440\", null],\n[\"\u043d\u0438\u043a\u043e\u0432\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0438\u043d\", \"\u63a5\u53d7\"],\n[\"\u0442\u0435\u0440\", null],\n[\"\u0432\", null],\n[\"\u044c\u044e\", null],\n[\"\u2581\\\"\", null],\n[\"\u041e\", \"\u5dde\u62a5\"],\n[\"\u0431\", null],\n[\"\u043b\u0430\", null],\n[\"\u0441\u0442\", null],\n[\"\u043d\u043e\u0439\", null],\n[\"\u2581\u0433\u0430\", null],\n[\"\u0437\u0435\", null],\n[\"\u0442\u0435\", null],\n[\"\\\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u0412 \u043e\u043a\u0442\u044f\u0431\u0440\u0435 \u043f\u0440\u043e\u0435\u0437\u0434 \u0432 \u0435\u043a\u0430\u0442\u0435\u0440\u0438\u043d\u0431\u0443\u0440\u0433\u0441\u043a\u043e\u043c \u043c\u0435\u0442\u0440\u043e\u043f\u043e\u043b\u0438\u0442\u0435\u043d\u0435 \u043c\u043e\u0436\u043d\u043e \u0431\u0443\u0434\u0435\u0442 \u043e\u043f\u043b\u0430\u0442\u0438\u0442\u044c \u0431\u0430\u043d\u043a\u043e\u0432\u0441\u043a\u043e\u0439 \u043a\u0430\u0440\u0442\u043e\u0439, \u0437\u0430\u044f\u0432\u0438\u043b \u0433\u0435\u043d\u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440 \u041e\u0410\u041e \"\u0418-\u0421\u0435\u0442\u044c\" \u041f\u0430\u0432\u0435\u043b \u0412\u0435\u0434\u0435\u0440\u043d\u0438\u043a\u043e\u0432 \u0432 \u0438\u043d\u0442\u0435\u0440\u0432\u044c\u044e \"\u041e\u0431\u043b\u0430\u0441\u0442\u043d\u043e\u0439 \u0433\u0430\u0437\u0435\u0442\u0435\".\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0412', '\u2581\u043e\u043a\u0442', '\u044f', '\u0431\u0440\u0435', '\u2581\u043f\u0440\u043e\u0435', '\u0437\u0434', '\u2581\u0432', '\u2581\u0435', '\u043a\u0430', '\u0442\u0435\u0440\u0438', '\u043d', '\u0431\u0443\u0440\u0433', '\u0441\u043a\u043e\u043c', '\u2581\u043c\u0435\u0442', '\u0440\u043e', '\u043f\u043e', '\u043b\u0438', '\u0442', '\u0435\u043d\u0435', '\u2581\u043c\u043e', '\u0436\u043d\u043e', '\u2581\u0431\u0443', '\u0434\u0435', '\u0442', '\u2581\u043e\u043f', '\u043b\u0430', '\u0442\u0438', '\u0442\u044c', '\u2581\u0431', '\u0430\u043d', '\u043a\u043e\u0432', '\u0441\u043a\u043e\u0439', '\u2581\u043a\u0430\u0440', '\u0442\u043e\u0439', ',', '\u2581\u0437\u0430', '\u044f\u0432\u0438', '\u043b', '\u2581\u0433', '\u0435\u043d', '\u0434\u0438', '\u0440\u0435', '\u043a\u0442\u043e\u0440', '\u2581\u041e', '\u0410', '\u041e', '\u2581\"', '\u0418', '-', '\u0421', '\u0435', '\u0442\u044c', '\"', '\u2581\u041f\u0430', '\u0432\u0435', '\u043b', '\u2581\u0412\u0435', '\u0434\u0435\u0440', '\u043d\u0438\u043a\u043e\u0432', '\u2581\u0432', '\u2581\u0438\u043d', '\u0442\u0435\u0440', '\u0432', '\u044c\u044e', '\u2581\"', '\u041e', '\u0431', '\u043b\u0430', '\u0441\u0442', '\u043d\u043e\u0439', '\u2581\u0433\u0430', '\u0437\u0435', '\u0442\u0435', '\".']"}], "[\n[\"\u2581\u0412\", null],\n[\"\u2581\u043e\u043a\u0442\", \"\u049a\u0430\u0437\u0430\u043d\"],\n[\"\u044f\", null],\n[\"\u0431\u0440\u0435\", null],\n[\"\u2581\u043f\u0440\u043e\u0435\", \"\u0436\u043e\u043b\"],\n[\"\u0437\u0434\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0435\", \"\u0435\u043a\u0430\u0442\u0435\u0440\u0438\u043d\u0431\u0443\u0440\u0433\"],\n[\"\u043a\u0430\", null],\n[\"\u0442\u0435\u0440\u0438\", null],\n[\"\u043d\", null],\n[\"\u0431\u0443\u0440\u0433\", null],\n[\"\u0441\u043a\u043e\u043c\", null],\n[\"\u2581\u043c\u0435\u0442\", \"\u043c\u0435\u0442\u0440\u043e\"],\n[\"\u0440\u043e\", null],\n[\"\u043f\u043e\", null],\n[\"\u043b\u0438\", null],\n[\"\u0442\", null],\n[\"\u0435\u043d\u0435\", null],\n[\"\u2581\u043c\u043e\", \"\u0431\u043e\u043b\u0430\u0434\u044b\"],\n[\"\u0436\u043d\u043e\", null],\n[\"\u2581\u0431\u0443\", null],\n[\"\u0434\u0435\", null],\n[\"\u0442\", null],\n[\"\u2581\u043e\u043f\", \"\u0442\u04e9\u043b\u0435\u0443\"],\n[\"\u043b\u0430\", null],\n[\"\u0442\u0438\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0431\", \"\u0431\u0430\u043d\u043a\"],\n[\"\u0430\u043d\", null],\n[\"\u043a\u043e\u0432\", null],\n[\"\u0441\u043a\u043e\u0439\", null],\n[\"\u2581\u043a\u0430\u0440\", \"\u043a\u0430\u0440\u0442\u0430\u0441\u044b\u043c\u0435\u043d\"],\n[\"\u0442\u043e\u0439\", null],\n[\",\" , null],\n[\"\u2581\u0437\u0430\", \"\u043c\u04d9\u043b\u0456\u043c\u0434\u0435\u0434\u0456\"],\n[\"\u044f\u0432\u0438\", null],\n[\"\u043b\", null],\n[\"\u2581\u0433\", \"\u0431\u0430\u0441\"],\n[\"\u0435\u043d\", null],\n[\"\u0434\u0438\", null],\n[\"\u0440\u0435\", null],\n[\"\u043a\u0442\u043e\u0440\", \"\u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\"],\n[\"\u2581\u041e\", \"\u0410\u0410\u049a\"],\n[\"\u0410\", null],\n[\"\u041e\", null],\n[\"\u2581\\\"\", null],\n[\"\u0418\", \"\u0418\"],\n[\"-\", null],\n[\"\u0421\", \"\u0421\u0435\u0442\u044c\"],\n[\"\u0435\", null],\n[\"\u0442\u044c\", null],\n[\"\\\"\", null],\n[\"\u2581\u041f\u0430\", \"\u041f\u0430\u0432\u0435\u043b\"],\n[\"\u0432\u0435\", null],\n[\"\u043b\", null],\n[\"\u2581\u0412\u0435\", \"\u0412\u0435\u0434\u0435\u0440\u043d\u0438\u043a\u043e\u0432\"],\n[\"\u0434\u0435\u0440\", null],\n[\"\u043d\u0438\u043a\u043e\u0432\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0438\u043d\", \"\u0441\u04b1\u0445\u0431\u0430\u0442\u044b\u043d\u0434\u0430\"],\n[\"\u0442\u0435\u0440\", null],\n[\"\u0432\", null],\n[\"\u044c\u044e\", null],\n[\"\u2581\\\"\", null],\n[\"\u041e\", \"\u041e\u0431\u043b\u044b\u0441\u0442\u044b\u049b\"],\n[\"\u0431\", null],\n[\"\u043b\u0430\", null],\n[\"\u0441\u0442\", null],\n[\"\u043d\u043e\u0439\", null],\n[\"\u2581\u0433\u0430\", \"\u0433\u0430\u0437\u0435\u0442\u043a\u0435\"],\n[\"\u0437\u0435\", null],\n[\"\u0442\u0435\", null],\n[\"\\\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0411\u0435\u043b\u043e-\u0437\u0435\u043b\u0451\u043d\u044b\u0439 Sikorsky VH-3D, \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0439 \u043a\u0430\u043a \u00abMarine One\u00bb, \u043a\u043e\u0433\u0434\u0430 \u0430 \u0431\u043e\u0440\u0442\u0443 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u043f\u0440\u0435\u0437\u0438\u0434\u0435\u043d\u0442, \u0441\u0442\u0430\u043b \u0446\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u043c \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u043c \u043f\u0440\u043e\u0432\u0435\u0434\u0451\u043d\u043d\u043e\u0433\u043e 17 \u0438\u044e\u043b\u044f \u0432 \u0411\u0435\u043b\u043e\u043c \u0434\u043e\u043c\u0435 \u043c\u0435\u0440\u043e\u043f\u0440\u0438\u044f\u0442\u0438\u044f, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043b\u0438\u0441\u044c \u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0441\u043a\u0438\u0435 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b \u0441\u0442\u0440\u043e\u0438\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u0430.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0411\u0435', '\u043b\u043e', '-', '\u0437\u0435', '\u043b\u0451', '\u043d\u044b\u0439', '\u2581S', 'ik', 'ors', 'ky', '\u2581V', 'H', '-', '3', 'D', ',', '\u2581\u0438\u0437\u0432\u0435\u0441\u0442', '\u043d\u044b\u0439', '\u2581\u043a\u0430\u043a', '\u2581\u00ab', 'Mar', 'ine', '\u2581One', '\u00bb,', '\u2581\u043a\u043e\u0433\u0434\u0430', '\u2581\u0430', '\u2581\u0431', '\u043e\u0440', '\u0442\u0443', '\u2581\u043d\u0430\u0445\u043e\u0434\u0438', '\u0442\u0441\u044f', '\u2581\u043f\u0440\u0435', '\u0437\u0438', '\u0434', '\u0435\u043d\u0442', ',', '\u2581\u0441\u0442\u0430\u043b', '\u2581\u0446\u0435\u043d\u0442', '\u0440\u0430\u043b\u044c', '\u043d\u044b\u043c', '\u2581\u044d', '\u043b\u0435', '\u043c\u0435\u043d', '\u0442\u043e\u043c', '\u2581\u043f\u0440\u043e', '\u0432\u0435\u0434', '\u0451\u043d', '\u043d\u043e\u0433\u043e', '\u2581', '1', '7', '\u2581\u0438\u044e\u043b\u044f', '\u2581\u0432', '\u2581\u0411\u0435', '\u043b\u043e\u043c', '\u2581\u0434\u043e', '\u043c\u0435', '\u2581\u043c\u0435', '\u0440\u043e', '\u043f', '\u0440\u0438\u044f', '\u0442\u0438\u044f', ',', '\u2581\u043d\u0430', '\u2581\u043a\u043e\u0442\u043e', '\u0440\u043e\u043c', '\u2581\u0434\u0435', '\u043c\u043e\u043d', '\u0441\u0442\u0440\u0438', '\u0440\u043e\u0432\u0430', '\u043b\u0438\u0441\u044c', '\u2581\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d', '\u0441\u043a\u0438\u0435', '\u2581\u043f\u0440\u043e\u0433\u0440\u0430', '\u043c', '\u043c\u044b', '\u2581\u0441\u0442', '\u0440\u043e\u0438', '\u0442\u0435\u043b\u044c', '\u0441\u0442\u0432\u0430', '.']"}], "[\n[\"\u2581\u0411\u0435\", \"White\"],\n[\"\u043b\u043e\", null],\n[\"-\", null],\n[\"\u0437\u0435\", null],\n[\"\u043b\u0451\", null],\n[\"\u043d\u044b\u0439\", null],\n[\"\u2581S\", \"Sikorsky\"],\n[\"ik\", null],\n[\"ors\", null],\n[\"ky\", null],\n[\"\u2581V\", \"VH\"],\n[\"H\", null],\n[\"-\", null],\n[\"3\", null],\n[\"D\", null],\n[\",\", null],\n[\"\u2581\u0438\u0437\u0432\u0435\u0441\u0442\", \"known\"],\n[\"\u043d\u044b\u0439\", null],\n[\"\u2581\u043a\u0430\u043a\", null],\n[\"\u2581\u00ab\", null],\n[\"Mar\", \"Marine\"],\n[\"ine\", null],\n[\"\u2581One\", \"One\"],\n[\"\u00bb,\", null],\n[\"\u2581\u043a\u043e\u0433\u0434\u0430\", \"when\"],\n[\"\u2581\u0430\", null],\n[\"\u2581\u0431\", \"on\"],\n[\"\u043e\u0440\", null],\n[\"\u0442\u0443\", null],\n[\"\u2581\u043d\u0430\u0445\u043e\u0434\u0438\", \"is\"],\n[\"\u0442\u0441\u044f\", null],\n[\"\u2581\u043f\u0440\u0435\", \"president\"],\n[\"\u0437\u0438\", null],\n[\"\u0434\", null],\n[\"\u0435\u043d\u0442\", null],\n[\",\", null],\n[\"\u2581\u0441\u0442\u0430\u043b\", \"became\"],\n[\"\u2581\u0446\u0435\u043d\u0442\", \"central\"],\n[\"\u0440\u0430\u043b\u044c\", null],\n[\"\u043d\u044b\u043c\", null],\n[\"\u2581\u044d\", \"element\"],\n[\"\u043b\u0435\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u0442\u043e\u043c\", null],\n[\"\u2581\u043f\u0440\u043e\", \"held\"],\n[\"\u0432\u0435\u0434\", null],\n[\"\u0451\u043d\", null],\n[\"\u043d\u043e\u0433\u043e\", null],\n[\"\u2581\", null],\n[\"1\", null],\n[\"7\", null],\n[\"\u2581\u0438\u044e\u043b\u044f\", \"July\"],\n[\"\u2581\u0432\", \"in\"],\n[\"\u2581\u0411\u0435\", \"White\"],\n[\"\u043b\u043e\u043c\", null],\n[\"\u2581\u0434\u043e\", \"House\"],\n[\"\u043c\u0435\", null],\n[\"\u2581\u043c\u0435\", \"event\"],\n[\"\u0440\u043e\", null],\n[\"\u043f\", null],\n[\"\u0440\u0438\u044f\", null],\n[\"\u0442\u0438\u044f\", null],\n[\",\", null],\n[\"\u2581\u043d\u0430\", \"at\"],\n[\"\u2581\u043a\u043e\u0442\u043e\", \"which\"],\n[\"\u0440\u043e\u043c\", null],\n[\"\u2581\u0434\u0435\", \"were\"],\n[\"\u043c\u043e\u043d\", null],\n[\"\u0441\u0442\u0440\u0438\", null],\n[\"\u0440\u043e\u0432\u0430\", null],\n[\"\u043b\u0438\u0441\u044c\", null],\n[\"\u2581\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\", \"American\"],\n[\"\u0441\u043a\u0438\u0435\", null],\n[\"\u2581\u043f\u0440\u043e\u0433\u0440\u0430\", \"programs\"],\n[\"\u043c\", null],\n[\"\u043c\u044b\", null],\n[\"\u2581\u0441\u0442\", \"construction\"],\n[\"\u0440\u043e\u0438\", null],\n[\"\u0442\u0435\u043b\u044c\", null],\n[\"\u0441\u0442\u0432\u0430\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u0411\u0435\u043b\u043e-\u0437\u0435\u043b\u0451\u043d\u044b\u0439 Sikorsky VH-3D, \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0439 \u043a\u0430\u043a \u00abMarine One\u00bb, \u043a\u043e\u0433\u0434\u0430 \u0430 \u0431\u043e\u0440\u0442\u0443 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u043f\u0440\u0435\u0437\u0438\u0434\u0435\u043d\u0442, \u0441\u0442\u0430\u043b \u0446\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u043c \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u043c \u043f\u0440\u043e\u0432\u0435\u0434\u0451\u043d\u043d\u043e\u0433\u043e 17 \u0438\u044e\u043b\u044f \u0432 \u0411\u0435\u043b\u043e\u043c \u0434\u043e\u043c\u0435 \u043c\u0435\u0440\u043e\u043f\u0440\u0438\u044f\u0442\u0438\u044f, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043b\u0438\u0441\u044c \u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0441\u043a\u0438\u0435 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b \u0441\u0442\u0440\u043e\u0438\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u0430.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0411\u0435', '\u043b\u043e', '-', '\u0437\u0435', '\u043b\u0451', '\u043d\u044b\u0439', '\u2581S', 'ik', 'ors', 'ky', '\u2581V', 'H', '-', '3', 'D', ',', '\u2581\u0438\u0437\u0432\u0435\u0441\u0442', '\u043d\u044b\u0439', '\u2581\u043a\u0430\u043a', '\u2581\u00ab', 'Mar', 'ine', '\u2581One', '\u00bb,', '\u2581\u043a\u043e\u0433\u0434\u0430', '\u2581\u0430', '\u2581\u0431', '\u043e\u0440', '\u0442\u0443', '\u2581\u043d\u0430\u0445\u043e\u0434\u0438', '\u0442\u0441\u044f', '\u2581\u043f\u0440\u0435', '\u0437\u0438', '\u0434', '\u0435\u043d\u0442', ',', '\u2581\u0441\u0442\u0430\u043b', '\u2581\u0446\u0435\u043d\u0442', '\u0440\u0430\u043b\u044c', '\u043d\u044b\u043c', '\u2581\u044d', '\u043b\u0435', '\u043c\u0435\u043d', '\u0442\u043e\u043c', '\u2581\u043f\u0440\u043e', '\u0432\u0435\u0434', '\u0451\u043d', '\u043d\u043e\u0433\u043e', '\u2581', '1', '7', '\u2581\u0438\u044e\u043b\u044f', '\u2581\u0432', '\u2581\u0411\u0435', '\u043b\u043e\u043c', '\u2581\u0434\u043e', '\u043c\u0435', '\u2581\u043c\u0435', '\u0440\u043e', '\u043f', '\u0440\u0438\u044f', '\u0442\u0438\u044f', ',', '\u2581\u043d\u0430', '\u2581\u043a\u043e\u0442\u043e', '\u0440\u043e\u043c', '\u2581\u0434\u0435', '\u043c\u043e\u043d', '\u0441\u0442\u0440\u0438', '\u0440\u043e\u0432\u0430', '\u043b\u0438\u0441\u044c', '\u2581\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d', '\u0441\u043a\u0438\u0435', '\u2581\u043f\u0440\u043e\u0433\u0440\u0430', '\u043c', '\u043c\u044b', '\u2581\u0441\u0442', '\u0440\u043e\u0438', '\u0442\u0435\u043b\u044c', '\u0441\u0442\u0432\u0430', '.']"}], "[\n[\"\u2581\u0411\u0435\", \"\u767d\"],\n[\"\u043b\u043e\", null],\n[\"-\", null],\n[\"\u0437\u0435\", null],\n[\"\u043b\u0451\", null],\n[\"\u043d\u044b\u0439\", null],\n[\"\u2581S\", \"\u897f\"],\n[\"ik\", null],\n[\"ors\", null],\n[\"ky\", null],\n[\"\u2581V\", \"VH\"],\n[\"H\", null],\n[\"-\", null],\n[\"3\", null],\n[\"D\", null],\n[\",\", null],\n[\"\u2581\u0438\u0437\u0432\u0435\u0441\u0442\", \"\u77e5\u540d\"],\n[\"\u043d\u044b\u0439\", null],\n[\"\u2581\u043a\u0430\u043a\", null],\n[\"\u2581\u00ab\", null],\n[\"Mar\", \"\u6d77\u519b\"],\n[\"ine\", null],\n[\"\u2581One\", \"\u4e00\u53f7\"],\n[\"\u00bb,\", null],\n[\"\u2581\u043a\u043e\u0433\u0434\u0430\", \"\u5f53\"],\n[\"\u2581\u0430\", null],\n[\"\u2581\u0431\", \"\u673a\"],\n[\"\u043e\u0440\", null],\n[\"\u0442\u0443\", null],\n[\"\u2581\u043d\u0430\u0445\u043e\u0434\u0438\", \"\u642d\u8f7d\"],\n[\"\u0442\u0441\u044f\", null],\n[\"\u2581\u043f\u0440\u0435\", \"\u603b\"],\n[\"\u0437\u0438\", null],\n[\"\u0434\", null],\n[\"\u0435\u043d\u0442\", null],\n[\",\", null],\n[\"\u2581\u0441\u0442\u0430\u043b\", \"\u6210\u4e3a\"],\n[\"\u2581\u0446\u0435\u043d\u0442\", \"\u4e2d\"],\n[\"\u0440\u0430\u043b\u044c\", null],\n[\"\u043d\u044b\u043c\", null],\n[\"\u2581\u044d\", \"\u8981\"],\n[\"\u043b\u0435\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u0442\u043e\u043c\", null],\n[\"\u2581\u043f\u0440\u043e\", \"\u4e3e\"],\n[\"\u0432\u0435\u0434\", null],\n[\"\u0451\u043d\", null],\n[\"\u043d\u043e\u0433\u043e\", null],\n[\"\u2581\", null],\n[\"1\", \"1\"],\n[\"7\", \"7\"],\n[\"\u2581\u0438\u044e\u043b\u044f\", \"\u6708\"],\n[\"\u2581\u0432\", \"\u5728\"],\n[\"\u2581\u0411\u0435\", \"\u767d\"],\n[\"\u043b\u043e\u043c\", null],\n[\"\u2581\u0434\u043e\", \"\u5bab\"],\n[\"\u043c\u0435\", null],\n[\"\u2581\u043c\u0435\", \"\u6d3b\"],\n[\"\u0440\u043e\", null],\n[\"\u043f\", null],\n[\"\u0440\u0438\u044f\", null],\n[\"\u0442\u0438\u044f\", null],\n[\",\", null],\n[\"\u2581\u043d\u0430\", \"\u5c55\u793a\"],\n[\"\u2581\u043a\u043e\u0442\u043e\", null],\n[\"\u0440\u043e\u043c\", null],\n[\"\u2581\u0434\u0435\", \"\u5c55\"],\n[\"\u043c\u043e\u043d\", null],\n[\"\u0441\u0442\u0440\u0438\", null],\n[\"\u0440\u043e\u0432\u0430\", null],\n[\"\u043b\u0438\u0441\u044c\", null],\n[\"\u2581\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\", \"\u7f8e\u56fd\"],\n[\"\u0441\u043a\u0438\u0435\", null],\n[\"\u2581\u043f\u0440\u043e\u0433\u0440\u0430\", \"\u5efa\u8bbe\"],\n[\"\u043c\", null],\n[\"\u043c\u044b\", null],\n[\"\u2581\u0441\u0442\", \"\u9879\"],\n[\"\u0440\u043e\u0438\", null],\n[\"\u0442\u0435\u043b\u044c\", null],\n[\"\u0441\u0442\u0432\u0430\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u0411\u0435\u043b\u043e-\u0437\u0435\u043b\u0451\u043d\u044b\u0439 Sikorsky VH-3D, \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0439 \u043a\u0430\u043a \u00abMarine One\u00bb, \u043a\u043e\u0433\u0434\u0430 \u0430 \u0431\u043e\u0440\u0442\u0443 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u043f\u0440\u0435\u0437\u0438\u0434\u0435\u043d\u0442, \u0441\u0442\u0430\u043b \u0446\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u043c \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u043c \u043f\u0440\u043e\u0432\u0435\u0434\u0451\u043d\u043d\u043e\u0433\u043e 17 \u0438\u044e\u043b\u044f \u0432 \u0411\u0435\u043b\u043e\u043c \u0434\u043e\u043c\u0435 \u043c\u0435\u0440\u043e\u043f\u0440\u0438\u044f\u0442\u0438\u044f, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043b\u0438\u0441\u044c \u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0441\u043a\u0438\u0435 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b \u0441\u0442\u0440\u043e\u0438\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u0430.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0411\u0435', '\u043b\u043e', '-', '\u0437\u0435', '\u043b\u0451', '\u043d\u044b\u0439', '\u2581S', 'ik', 'ors', 'ky', '\u2581V', 'H', '-', '3', 'D', ',', '\u2581\u0438\u0437\u0432\u0435\u0441\u0442', '\u043d\u044b\u0439', '\u2581\u043a\u0430\u043a', '\u2581\u00ab', 'Mar', 'ine', '\u2581One', '\u00bb,', '\u2581\u043a\u043e\u0433\u0434\u0430', '\u2581\u0430', '\u2581\u0431', '\u043e\u0440', '\u0442\u0443', '\u2581\u043d\u0430\u0445\u043e\u0434\u0438', '\u0442\u0441\u044f', '\u2581\u043f\u0440\u0435', '\u0437\u0438', '\u0434', '\u0435\u043d\u0442', ',', '\u2581\u0441\u0442\u0430\u043b', '\u2581\u0446\u0435\u043d\u0442', '\u0440\u0430\u043b\u044c', '\u043d\u044b\u043c', '\u2581\u044d', '\u043b\u0435', '\u043c\u0435\u043d', '\u0442\u043e\u043c', '\u2581\u043f\u0440\u043e', '\u0432\u0435\u0434', '\u0451\u043d', '\u043d\u043e\u0433\u043e', '\u2581', '1', '7', '\u2581\u0438\u044e\u043b\u044f', '\u2581\u0432', '\u2581\u0411\u0435', '\u043b\u043e\u043c', '\u2581\u0434\u043e', '\u043c\u0435', '\u2581\u043c\u0435', '\u0440\u043e', '\u043f', '\u0440\u0438\u044f', '\u0442\u0438\u044f', ',', '\u2581\u043d\u0430', '\u2581\u043a\u043e\u0442\u043e', '\u0440\u043e\u043c', '\u2581\u0434\u0435', '\u043c\u043e\u043d', '\u0441\u0442\u0440\u0438', '\u0440\u043e\u0432\u0430', '\u043b\u0438\u0441\u044c', '\u2581\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d', '\u0441\u043a\u0438\u0435', '\u2581\u043f\u0440\u043e\u0433\u0440\u0430', '\u043c', '\u043c\u044b', '\u2581\u0441\u0442', '\u0440\u043e\u0438', '\u0442\u0435\u043b\u044c', '\u0441\u0442\u0432\u0430', '.']"}], "[\n  [\"\u2581\u0411\u0435\", null],\n  [\"\u043b\u043e\", \"\u0410\u049b-\u0436\u0430\u0441\u044b\u043b\"],\n  [\"-\", null],\n  [\"\u0437\u0435\", null],\n  [\"\u043b\u0451\", null],\n  [\"\u043d\u044b\u0439\", null],\n  [\"\u2581S\", null],\n  [\"ik\", null],\n  [\"ors\", null],\n  [\"ky\", null],\n  [\"\u2581V\", null],\n  [\"H\", null],\n  [\"-\", null],\n  [\"3\", null],\n  [\"D\", null],\n  [\",\", null],\n  [\"\u2581\u0438\u0437\u0432\u0435\u0441\u0442\", \"\u0431\u0435\u043b\u0433\u0456\u043b\u0456\"],\n  [\"\u043d\u044b\u0439\", null],\n  [\"\u2581\u043a\u0430\u043a\", null],\n  [\"\u2581\u00ab\", null],\n  [\"Mar\", null],\n  [\"ine\", null],\n  [\"\u2581One\", \"One\"],\n  [\"\u00bb,\", null],\n  [\"\u2581\u043a\u043e\u0433\u0434\u0430\", \"\u043a\u0435\u0437\u0434\u0435\"],\n  [\"\u2581\u0430\", null],\n  [\"\u2581\u0431\", null],\n  [\"\u043e\u0440\", \"\u0431\u043e\u0440\u0442\u044b\u043d\u0434\u0430\"],\n  [\"\u0442\u0443\", null],\n  [\"\u2581\u043d\u0430\u0445\u043e\u0434\u0438\", \"\u0431\u043e\u043b\u0493\u0430\u043d\"],\n  [\"\u0442\u0441\u044f\", null],\n  [\"\u2581\u043f\u0440\u0435\", null],\n  [\"\u0437\u0438\", \"\u043f\u0440\u0435\u0437\u0438\u0434\u0435\u043d\u0442\"],\n  [\"\u0434\", null],\n  [\"\u0435\u043d\u0442\", null],\n  [\",\", null],\n  [\"\u2581\u0441\u0442\u0430\u043b\", \"\u0431\u043e\u043b\u0434\u044b\"],\n  [\"\u2581\u0446\u0435\u043d\u0442\", \"\u043e\u0440\u0442\u0430\u043b\u044b\u049b\"],\n  [\"\u0440\u0430\u043b\u044c\", null],\n  [\"\u043d\u044b\u043c\", null],\n  [\"\u2581\u044d\", null],\n  [\"\u043b\u0435\", \"\u044d\u043b\u0435\u043c\u0435\u043d\u0442\u0456\"],\n  [\"\u043c\u0435\u043d\", null],\n  [\"\u0442\u043e\u043c\", null],\n  [\"\u2581\u043f\u0440\u043e\", null],\n  [\"\u0432\u0435\u0434\", \"\u04e9\u0442\u043a\u0456\u0437\u0456\u043b\u0433\u0435\u043d\"],\n  [\"\u0451\u043d\", null],\n  [\"\u043d\u043e\u0433\u043e\", null],\n  [\"\u2581\", null],\n  [\"1\", null],\n  [\"7\", null],\n  [\"\u2581\u0438\u044e\u043b\u044f\", \"\u0448\u0456\u043b\u0434\u0435\u0434\u0435\"],\n  [\"\u2581\u0432\", null],\n  [\"\u2581\u0411\u0435\", \"\u0410\u049b\"],\n  [\"\u043b\u043e\u043c\", null],\n  [\"\u2581\u0434\u043e\", \"\u04af\u0439\u0434\u0435\"],\n  [\"\u043c\u0435\", null],\n  [\"\u2581\u043c\u0435\", null],\n  [\"\u0440\u043e\", \"\u0456\u0441-\u0448\u0430\u0440\u0430\"],\n  [\"\u043f\", null],\n  [\"\u0440\u0438\u044f\", null],\n  [\"\u0442\u0438\u044f\", null],\n  [\",\", null],\n  [\"\u2581\u043d\u0430\", null],\n  [\"\u2581\u043a\u043e\u0442\u043e\", \"\u043e\u043d\u0434\u0430\"],\n  [\"\u0440\u043e\u043c\", null],\n  [\"\u2581\u0434\u0435\", null],\n  [\"\u043c\u043e\u043d\", \"\u043a\u04e9\u0440\u0441\u0435\u0442\u0456\u043b\u0434\u0456\"],\n  [\"\u0441\u0442\u0440\u0438\", null],\n  [\"\u0440\u043e\u0432\u0430\", null],\n  [\"\u043b\u0438\u0441\u044c\", null],\n  [\"\u2581\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\", \"\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0434\u044b\u049b\"],\n  [\"\u0441\u043a\u0438\u0435\", null],\n  [\"\u2581\u043f\u0440\u043e\u0433\u0440\u0430\", \"\u0431\u0430\u0493\u0434\u0430\u0440\u043b\u0430\u043c\u0430\u043b\u0430\u0440\u044b\"],\n  [\"\u043c\", null],\n  [\"\u043c\u044b\", null],\n  [\"\u2581\u0441\u0442\", null],\n  [\"\u0440\u043e\u0438\", \"\u049b\u04b1\u0440\u044b\u043b\u044b\u0441\"],\n  [\"\u0442\u0435\u043b\u044c\", null],\n  [\"\u0441\u0442\u0432\u0430\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\"\u0422\u043e\u0432\u0430\u0440\u0438\u0449 \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0432\" \u043d\u0430 \u0410\u043c\u0430\u0437\u043e\u043d\u0435 \u043e\u0431\u0440\u0430\u0449\u0430\u0435\u0442\u0441\u044f \u043a \u0434\u0440\u0443\u0433\u043e\u043c\u0443 \u0442\u0438\u043f\u0443 \u043d\u043e\u0441\u0442\u0430\u043b\u044c\u0433\u0438\u0438: \u0432\u043e\u0441\u043f\u043e\u043c\u0438\u043d\u0430\u043d\u0438\u044f\u043c \u043e \u0445\u043e\u043b\u043e\u0434\u043d\u043e\u0439 \u0432\u043e\u0439\u043d\u044b \u0438\u0437 \"\u0410\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0446\u0435\u0432\", \"\u0412\u0437\u0440\u044b\u0432\u043d\u043e\u0439 \u0431\u043b\u043e\u043d\u0434\u0438\u043d\u043a\u0438\" \u0438\u043b\u0438 \u0434\u0430\u0436\u0435 \"\u0411\u043b\u0435\u0441\u043a\u0430\" \u0441 \u0435\u0435 \u043e\u0431\u0440\u0430\u0437\u0430\u043c\u0438 \u0431\u043e\u0440\u044c\u0431\u044b \u0412\u043e\u0441\u0442\u043e\u043a\u0430 \u0438 \u0417\u0430\u043f\u0430\u0434\u0430.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\"', '\u0422', '\u043e', '\u0432\u0430', '\u0440\u0438', '\u0449', '\u2581\u0434\u0435', '\u0442\u0435', '\u043a\u0442\u0438\u0432', '\"', '\u2581\u043d\u0430', '\u2581\u0410', '\u043c\u0430', '\u0437\u043e', '\u043d\u0435', '\u2581\u043e\u0431\u0440\u0430', '\u0449\u0430', '\u0435\u0442\u0441\u044f', '\u2581\u043a', '\u2581\u0434\u0440\u0443', '\u0433\u043e', '\u043c\u0443', '\u2581\u0442\u0438', '\u043f\u0443', '\u2581\u043d\u043e', '\u0441\u0442\u0430', '\u043b\u044c', '\u0433\u0438', '\u0438', ':', '\u2581\u0432\u043e', '\u0441\u043f\u043e', '\u043c\u0438\u043d\u0430', '\u043d\u0438\u044f', '\u043c', '\u2581\u043e', '\u2581\u0445\u043e', '\u043b\u043e', '\u0434', '\u043d\u043e\u0439', '\u2581\u0432\u043e\u0439\u043d\u044b', '\u2581\u0438\u0437', '\u2581\"', '\u0410', '\u043c\u0435\u0440\u0438\u043a\u0430\u043d', '\u0446\u0435\u0432', '\",', '\u2581\"', '\u0412', '\u0437', '\u0440\u044b', '\u0432', '\u043d\u043e\u0439', '\u2581\u0431', '\u043b\u043e', '\u043d', '\u0434\u0438\u043d', '\u043a\u0438', '\"', '\u2581\u0438\u043b\u0438', '\u2581\u0434\u0430', '\u0436\u0435', '\u2581\"', '\u0411', '\u043b\u0435', '\u0441\u043a\u0430', '\"', '\u2581\u0441', '\u2581\u0435', '\u0435', '\u2581\u043e\u0431\u0440\u0430', '\u0437\u0430', '\u043c\u0438', '\u2581\u0431', '\u043e\u0440', '\u044c', '\u0431\u044b', '\u2581\u0412\u043e', '\u0441\u0442\u043e', '\u043a\u0430', '\u2581\u0438', '\u2581\u0417\u0430', '\u043f\u0430\u0434\u0430', '.']"}], "[\n  [\"\u2581\\\"\", null],\n  [\"\u0422\", \"Comrade\"],\n  [\"\u043e\", null],\n  [\"\u0432\u0430\", null],\n  [\"\u0440\u0438\", null],\n  [\"\u0449\", null],\n  [\"\u2581\u0434\u0435\", \"Detective\"],\n  [\"\u0442\u0435\", null],\n  [\"\u043a\u0442\u0438\u0432\", null],\n  [\"\\\"\", null],\n  [\"\u2581\u043d\u0430\", \"on\"],\n  [\"\u2581\u0410\", \"Amazon\"],\n  [\"\u043c\u0430\", null],\n  [\"\u0437\u043e\", null],\n  [\"\u043d\u0435\", null],\n  [\"\u2581\u043e\u0431\u0440\u0430\", \"appeals\"],\n  [\"\u0449\u0430\", null],\n  [\"\u0435\u0442\u0441\u044f\", null],\n  [\"\u2581\u043a\", \"to\"],\n  [\"\u2581\u0434\u0440\u0443\", \"another\"],\n  [\"\u0433\u043e\", null],\n  [\"\u043c\u0443\", null],\n  [\"\u2581\u0442\u0438\", \"type\"],\n  [\"\u043f\u0443\", null],\n  [\"\u2581\u043d\u043e\", \"nostalgia\"],\n  [\"\u0441\u0442\u0430\", null],\n  [\"\u043b\u044c\", null],\n  [\"\u0433\u0438\", null],\n  [\"\u0438\", null],\n  [\":\", null],\n  [\"\u2581\u0432\u043e\", \"memories\"],\n  [\"\u0441\u043f\u043e\", null],\n  [\"\u043c\u0438\u043d\u0430\", null],\n  [\"\u043d\u0438\u044f\", null],\n  [\"\u043c\", null],\n  [\"\u2581\u043e\", \"of\"],\n  [\"\u2581\u0445\u043e\", \"Cold\"],\n  [\"\u043b\u043e\", null],\n  [\"\u0434\", null],\n  [\"\u043d\u043e\u0439\", null],\n  [\"\u2581\u0432\u043e\u0439\u043d\u044b\", \"War\"],\n  [\"\u2581\u0438\u0437\", \"from\"],\n  [\"\u2581\\\"\", null],\n  [\"\u0410\", \"The Americans\"],\n  [\"\u043c\u0435\u0440\u0438\u043a\u0430\u043d\", null],\n  [\"\u0446\u0435\u0432\", null],\n  [\"\\\",\", null],\n  [\"\u2581\\\"\", null],\n  [\"\u0412\", \"Atomic\"],\n  [\"\u0437\", null],\n  [\"\u0440\u044b\", null],\n  [\"\u0432\", null],\n  [\"\u043d\u043e\u0439\", null],\n  [\"\u2581\u0431\", \"Blonde\"],\n  [\"\u043b\u043e\", null],\n  [\"\u043d\", null],\n  [\"\u0434\u0438\u043d\", null],\n  [\"\u043a\u0438\", null],\n  [\"\\\"\", null],\n  [\"\u2581\u0438\u043b\u0438\", \"or\"],\n  [\"\u2581\u0434\u0430\", \"even\"],\n  [\"\u0436\u0435\", null],\n  [\"\u2581\\\"\", null],\n  [\"\u0411\", \"GLOW\"],\n  [\"\u043b\u0435\", null],\n  [\"\u0441\u043a\u0430\", null],\n  [\"\\\"\", null],\n  [\"\u2581\u0441\", \"with\"],\n  [\"\u2581\u0435\", \"its\"],\n  [\"\u0435\", null],\n  [\"\u2581\u043e\u0431\u0440\u0430\", \"images\"],\n  [\"\u0437\u0430\", null],\n  [\"\u043c\u0438\", null],\n  [\"\u2581\u0431\", \"struggle\"],\n  [\"\u043e\u0440\", null],\n  [\"\u044c\", null],\n  [\"\u0431\u044b\", null],\n  [\"\u2581\u0412\u043e\", \"East\"],\n  [\"\u0441\u0442\u043e\", null],\n  [\"\u043a\u0430\", null],\n  [\"\u2581\u0438\", \"and\"],\n  [\"\u2581\u0417\u0430\", \"West\"],\n  [\"\u043f\u0430\u0434\u0430\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\"\u0422\u043e\u0432\u0430\u0440\u0438\u0449 \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0432\" \u043d\u0430 \u0410\u043c\u0430\u0437\u043e\u043d\u0435 \u043e\u0431\u0440\u0430\u0449\u0430\u0435\u0442\u0441\u044f \u043a \u0434\u0440\u0443\u0433\u043e\u043c\u0443 \u0442\u0438\u043f\u0443 \u043d\u043e\u0441\u0442\u0430\u043b\u044c\u0433\u0438\u0438: \u0432\u043e\u0441\u043f\u043e\u043c\u0438\u043d\u0430\u043d\u0438\u044f\u043c \u043e \u0445\u043e\u043b\u043e\u0434\u043d\u043e\u0439 \u0432\u043e\u0439\u043d\u044b \u0438\u0437 \"\u0410\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0446\u0435\u0432\", \"\u0412\u0437\u0440\u044b\u0432\u043d\u043e\u0439 \u0431\u043b\u043e\u043d\u0434\u0438\u043d\u043a\u0438\" \u0438\u043b\u0438 \u0434\u0430\u0436\u0435 \"\u0411\u043b\u0435\u0441\u043a\u0430\" \u0441 \u0435\u0435 \u043e\u0431\u0440\u0430\u0437\u0430\u043c\u0438 \u0431\u043e\u0440\u044c\u0431\u044b \u0412\u043e\u0441\u0442\u043e\u043a\u0430 \u0438 \u0417\u0430\u043f\u0430\u0434\u0430.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\"', '\u0422', '\u043e', '\u0432\u0430', '\u0440\u0438', '\u0449', '\u2581\u0434\u0435', '\u0442\u0435', '\u043a\u0442\u0438\u0432', '\"', '\u2581\u043d\u0430', '\u2581\u0410', '\u043c\u0430', '\u0437\u043e', '\u043d\u0435', '\u2581\u043e\u0431\u0440\u0430', '\u0449\u0430', '\u0435\u0442\u0441\u044f', '\u2581\u043a', '\u2581\u0434\u0440\u0443', '\u0433\u043e', '\u043c\u0443', '\u2581\u0442\u0438', '\u043f\u0443', '\u2581\u043d\u043e', '\u0441\u0442\u0430', '\u043b\u044c', '\u0433\u0438', '\u0438', ':', '\u2581\u0432\u043e', '\u0441\u043f\u043e', '\u043c\u0438\u043d\u0430', '\u043d\u0438\u044f', '\u043c', '\u2581\u043e', '\u2581\u0445\u043e', '\u043b\u043e', '\u0434', '\u043d\u043e\u0439', '\u2581\u0432\u043e\u0439\u043d\u044b', '\u2581\u0438\u0437', '\u2581\"', '\u0410', '\u043c\u0435\u0440\u0438\u043a\u0430\u043d', '\u0446\u0435\u0432', '\",', '\u2581\"', '\u0412', '\u0437', '\u0440\u044b', '\u0432', '\u043d\u043e\u0439', '\u2581\u0431', '\u043b\u043e', '\u043d', '\u0434\u0438\u043d', '\u043a\u0438', '\"', '\u2581\u0438\u043b\u0438', '\u2581\u0434\u0430', '\u0436\u0435', '\u2581\"', '\u0411', '\u043b\u0435', '\u0441\u043a\u0430', '\"', '\u2581\u0441', '\u2581\u0435', '\u0435', '\u2581\u043e\u0431\u0440\u0430', '\u0437\u0430', '\u043c\u0438', '\u2581\u0431', '\u043e\u0440', '\u044c', '\u0431\u044b', '\u2581\u0412\u043e', '\u0441\u0442\u043e', '\u043a\u0430', '\u2581\u0438', '\u2581\u0417\u0430', '\u043f\u0430\u0434\u0430', '.']"}], "[\n[\"\u2581\\\"\", null],\n[\"\u0422\", \"\u540c\u5fd7\"],\n[\"\u043e\", null],\n[\"\u0432\u0430\", null],\n[\"\u0440\u0438\", null],\n[\"\u0449\", null],\n[\"\u2581\u0434\u0435\", \"\u4fa6\u63a2\"],\n[\"\u0442\u0435\", null],\n[\"\u043a\u0442\u0438\u0432\", null],\n[\"\\\"\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u0410\", \"\u4e9a\u9a6c\u900a\"],\n[\"\u043c\u0430\", null],\n[\"\u0437\u043e\", null],\n[\"\u043d\u0435\", null],\n[\"\u2581\u043e\u0431\u0440\u0430\", \"\u6d89\u53ca\"],\n[\"\u0449\u0430\", null],\n[\"\u0435\u0442\u0441\u044f\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u0434\u0440\u0443\", \"\u53e6\u4e00\u79cd\"],\n[\"\u0433\u043e\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u0442\u0438\", \"\u7c7b\u578b\"],\n[\"\u043f\u0443\", null],\n[\"\u2581\u043d\u043e\", \"\u6000\u65e7\"],\n[\"\u0441\u0442\u0430\", null],\n[\"\u043b\u044c\", null],\n[\"\u0433\u0438\", null],\n[\"\u0438\", null],\n[\":\", null],\n[\"\u2581\u0432\u043e\", \"\u56de\u5fc6\"],\n[\"\u0441\u043f\u043e\", null],\n[\"\u043c\u0438\u043d\u0430\", null],\n[\"\u043d\u0438\u044f\", null],\n[\"\u043c\", null],\n[\"\u2581\u043e\", null],\n[\"\u2581\u0445\u043e\", \"\u51b7\u6218\"],\n[\"\u043b\u043e\", null],\n[\"\u0434\", null],\n[\"\u043d\u043e\u0439\", null],\n[\"\u2581\u0432\u043e\u0439\u043d\u044b\", null],\n[\"\u2581\u0438\u0437\", null],\n[\"\u2581\\\"\", null],\n[\"\u0410\", \"\u7f8e\u56fd\u4eba\"],\n[\"\u043c\u0435\u0440\u0438\u043a\u0430\u043d\", null],\n[\"\u0446\u0435\u0432\", null],\n[\"\\\"\", null],\n[\",\", null],\n[\"\u2581\\\"\", null],\n[\"\u0412\", \"\u91d1\u53d1\u7206\u90ed\"],\n[\"\u0437\", null],\n[\"\u0440\u044b\", null],\n[\"\u0432\", null],\n[\"\u043d\u043e\u0439\", null],\n[\"\u2581\u0431\", null],\n[\"\u043b\u043e\", null],\n[\"\u043d\", null],\n[\"\u0434\u0438\u043d\", null],\n[\"\u043a\u0438\", null],\n[\"\\\"\", null],\n[\"\u2581\u0438\u043b\u0438\", null],\n[\"\u2581\u0434\u0430\", \"\u751a\u81f3\"],\n[\"\u0436\u0435\", null],\n[\"\u2581\\\"\", null],\n[\"\u0411\", \"\u5149\u5f69\"],\n[\"\u043b\u0435\", null],\n[\"\u0441\u043a\u0430\", null],\n[\"\\\"\", null],\n[\"\u2581\u0441\", null],\n[\"\u2581\u0435\", \"\u5b83\u7684\"],\n[\"\u0435\", null],\n[\"\u2581\u043e\u0431\u0440\u0430\", \"\u5f62\u8c61\"],\n[\"\u0437\u0430\", null],\n[\"\u043c\u0438\", null],\n[\"\u2581\u0431\", \"\u6597\u4e89\"],\n[\"\u043e\u0440\", null],\n[\"\u044c\", null],\n[\"\u0431\u044b\", null],\n[\"\u2581\u0412\u043e\", \"\u4e1c\u65b9\"],\n[\"\u0441\u0442\u043e\", null],\n[\"\u043a\u0430\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0417\u0430\", \"\u897f\u65b9\"],\n[\"\u043f\u0430\u0434\u0430\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\"\u0422\u043e\u0432\u0430\u0440\u0438\u0449 \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0432\" \u043d\u0430 \u0410\u043c\u0430\u0437\u043e\u043d\u0435 \u043e\u0431\u0440\u0430\u0449\u0430\u0435\u0442\u0441\u044f \u043a \u0434\u0440\u0443\u0433\u043e\u043c\u0443 \u0442\u0438\u043f\u0443 \u043d\u043e\u0441\u0442\u0430\u043b\u044c\u0433\u0438\u0438: \u0432\u043e\u0441\u043f\u043e\u043c\u0438\u043d\u0430\u043d\u0438\u044f\u043c \u043e \u0445\u043e\u043b\u043e\u0434\u043d\u043e\u0439 \u0432\u043e\u0439\u043d\u044b \u0438\u0437 \"\u0410\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0446\u0435\u0432\", \"\u0412\u0437\u0440\u044b\u0432\u043d\u043e\u0439 \u0431\u043b\u043e\u043d\u0434\u0438\u043d\u043a\u0438\" \u0438\u043b\u0438 \u0434\u0430\u0436\u0435 \"\u0411\u043b\u0435\u0441\u043a\u0430\" \u0441 \u0435\u0435 \u043e\u0431\u0440\u0430\u0437\u0430\u043c\u0438 \u0431\u043e\u0440\u044c\u0431\u044b \u0412\u043e\u0441\u0442\u043e\u043a\u0430 \u0438 \u0417\u0430\u043f\u0430\u0434\u0430.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\"', '\u0422', '\u043e', '\u0432\u0430', '\u0440\u0438', '\u0449', '\u2581\u0434\u0435', '\u0442\u0435', '\u043a\u0442\u0438\u0432', '\"', '\u2581\u043d\u0430', '\u2581\u0410', '\u043c\u0430', '\u0437\u043e', '\u043d\u0435', '\u2581\u043e\u0431\u0440\u0430', '\u0449\u0430', '\u0435\u0442\u0441\u044f', '\u2581\u043a', '\u2581\u0434\u0440\u0443', '\u0433\u043e', '\u043c\u0443', '\u2581\u0442\u0438', '\u043f\u0443', '\u2581\u043d\u043e', '\u0441\u0442\u0430', '\u043b\u044c', '\u0433\u0438', '\u0438', ':', '\u2581\u0432\u043e', '\u0441\u043f\u043e', '\u043c\u0438\u043d\u0430', '\u043d\u0438\u044f', '\u043c', '\u2581\u043e', '\u2581\u0445\u043e', '\u043b\u043e', '\u0434', '\u043d\u043e\u0439', '\u2581\u0432\u043e\u0439\u043d\u044b', '\u2581\u0438\u0437', '\u2581\"', '\u0410', '\u043c\u0435\u0440\u0438\u043a\u0430\u043d', '\u0446\u0435\u0432', '\",', '\u2581\"', '\u0412', '\u0437', '\u0440\u044b', '\u0432', '\u043d\u043e\u0439', '\u2581\u0431', '\u043b\u043e', '\u043d', '\u0434\u0438\u043d', '\u043a\u0438', '\"', '\u2581\u0438\u043b\u0438', '\u2581\u0434\u0430', '\u0436\u0435', '\u2581\"', '\u0411', '\u043b\u0435', '\u0441\u043a\u0430', '\"', '\u2581\u0441', '\u2581\u0435', '\u0435', '\u2581\u043e\u0431\u0440\u0430', '\u0437\u0430', '\u043c\u0438', '\u2581\u0431', '\u043e\u0440', '\u044c', '\u0431\u044b', '\u2581\u0412\u043e', '\u0441\u0442\u043e', '\u043a\u0430', '\u2581\u0438', '\u2581\u0417\u0430', '\u043f\u0430\u0434\u0430', '.']"}], "[\n  [\"\u2581\\\"\", null],\n  [\"\u0422\", \"\u0416\u043e\u043b\u0434\u0430\u0441\"],\n  [\"\u043e\", null],\n  [\"\u0432\u0430\", null],\n  [\"\u0440\u0438\", null],\n  [\"\u0449\", null],\n  [\"\u2581\u0434\u0435\", \"\u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0432\"],\n  [\"\u0442\u0435\", null],\n  [\"\u043a\u0442\u0438\u0432\", null],\n  [\"\\\"\", null],\n  [\"\u2581\u043d\u0430\", null],\n  [\"\u2581\u0410\", \"Amazon\"],\n  [\"\u043c\u0430\", null],\n  [\"\u0437\u043e\", null],\n  [\"\u043d\u0435\", null],\n  [\"\u2581\u043e\u0431\u0440\u0430\", \"\u0436\u04af\u0433\u0456\u043d\u0435\u0434\u0456\"],\n  [\"\u0449\u0430\", null],\n  [\"\u0435\u0442\u0441\u044f\", null],\n  [\"\u2581\u043a\", null],\n  [\"\u2581\u0434\u0440\u0443\", \"\u0431\u0430\u0441\u049b\u0430\"],\n  [\"\u0433\u043e\", null],\n  [\"\u043c\u0443\", null],\n  [\"\u2581\u0442\u0438\", \"\u0442\u04af\u0440\u0456\u043d\u0435\"],\n  [\"\u043f\u0443\", null],\n  [\"\u2581\u043d\u043e\", \"\u043d\u043e\u0441\u0442\u0430\u043b\u044c\u0433\u0438\u044f\u043d\u044b\u04a3\"],\n  [\"\u0441\u0442\u0430\", null],\n  [\"\u043b\u044c\", null],\n  [\"\u0433\u0438\", null],\n  [\"\u0438\", null],\n  [\":\", null],\n  [\"\u2581\u0432\u043e\", \"\u0435\u0441\u0442\u0435\u043b\u0456\u043a\u0442\u0435\u0440\u0433\u0435\"],\n  [\"\u0441\u043f\u043e\", null],\n  [\"\u043c\u0438\u043d\u0430\", null],\n  [\"\u043d\u0438\u044f\", null],\n  [\"\u043c\", null],\n  [\"\u2581\u043e\", null],\n  [\"\u2581\u0445\u043e\", \"\u0441\u0443\u044b\u049b\"],\n  [\"\u043b\u043e\", null],\n  [\"\u0434\", null],\n  [\"\u043d\u043e\u0439\", null],\n  [\"\u2581\u0432\u043e\u0439\u043d\u044b\", \"\u0441\u043e\u0493\u044b\u0441\"],\n  [\"\u2581\u0438\u0437\", null],\n  [\"\u2581\\\"\", null],\n  [\"\u0410\", \"\u0410\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0434\u044b\u049b\u0442\u0430\u0440\"],\n  [\"\u043c\u0435\u0440\u0438\u043a\u0430\u043d\", null],\n  [\"\u0446\u0435\u0432\", null],\n  [\"\\\",\", null],\n  [\"\u2581\\\"\", null],\n  [\"\u0412\", \"\u0416\u0430\u0440\u044b\u043b\u0493\u044b\u0448\"],\n  [\"\u0437\", null],\n  [\"\u0440\u044b\", null],\n  [\"\u0432\", null],\n  [\"\u043d\u043e\u0439\", null],\n  [\"\u2581\u0431\", \"\u0441\u0430\u0440\u044b\"],\n  [\"\u043b\u043e\", null],\n  [\"\u043d\", null],\n  [\"\u0434\u0438\u043d\", null],\n  [\"\u043a\u0438\", null],\n  [\"\\\"\", null],\n  [\"\u2581\u0438\u043b\u0438\", \"\u043d\u0435\u043c\u0435\u0441\u0435\"],\n  [\"\u2581\u0434\u0430\", \"\u0442\u0456\u043f\u0442\u0456\"],\n  [\"\u0436\u0435\", null],\n  [\"\u2581\\\"\", null],\n  [\"\u0411\", \"\u0416\u044b\u043b\u0442\u044b\u0440\"],\n  [\"\u043b\u0435\", null],\n  [\"\u0441\u043a\u0430\", null],\n  [\"\\\"\", null],\n  [\"\u2581\u0441\", null],\n  [\"\u2581\u0435\", \"\u043e\u043d\u044b\u04a3\"],\n  [\"\u0435\", null],\n  [\"\u2581\u043e\u0431\u0440\u0430\", \"\u0431\u0435\u0439\u043d\u0435\u043b\u0435\u0440\u0456\u043c\u0435\u043d\"],\n  [\"\u0437\u0430\", null],\n  [\"\u043c\u0438\", null],\n  [\"\u2581\u0431\", \"\u043a\u04af\u0440\u0435\u0441\u0456\u043d\u0456\u04a3\"],\n  [\"\u043e\u0440\", null],\n  [\"\u044c\", null],\n  [\"\u0431\u044b\", null],\n  [\"\u2581\u0412\u043e\", \"\u0428\u044b\u0493\u044b\u0441\"],\n  [\"\u0441\u0442\u043e\", null],\n  [\"\u043a\u0430\", null],\n  [\"\u2581\u0438\", \"\u0436\u04d9\u043d\u0435\"],\n  [\"\u2581\u0417\u0430\", \"\u0411\u0430\u0442\u044b\u0441\"],\n  [\"\u043f\u0430\u0434\u0430\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041b\u0430\u0437\u0430\u0440\u0443\u0441 \u0438\u0437\u043c\u0435\u043d\u0438\u043b\u0430 \u0435\u0435 \u0441\u043c\u044b\u0441\u043b: \u0432 \u0435\u0435 \u0441\u0442\u0438\u0445\u043e\u0442\u0432\u043e\u0440\u0435\u043d\u0438\u0438 \u0436\u0435\u043d\u0441\u043a\u0430\u044f \u0444\u0438\u0433\u0443\u0440\u0430 \u0431\u043e\u043b\u0435\u0435 \u043d\u0435 \u0430\u0431\u0441\u0442\u0440\u0430\u043a\u0442\u043d\u0430, \u0443 \u043d\u0435\u0435 \u0435\u0441\u0442\u044c \u0433\u043e\u043b\u043e\u0441.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041b\u0430', '\u0437\u0430', '\u0440\u0443', '\u0441', '\u2581\u0438\u0437', '\u043c\u0435\u043d\u0438', '\u043b\u0430', '\u2581\u0435', '\u0435', '\u2581\u0441', '\u043c\u044b', '\u0441', '\u043b', ':', '\u2581\u0432', '\u2581\u0435', '\u0435', '\u2581\u0441\u0442\u0438', '\u0445\u043e', '\u0442\u0432\u043e', '\u0440\u0435', '\u043d\u0438\u0438', '\u2581\u0436\u0435\u043d', '\u0441\u043a\u0430\u044f', '\u2581\u0444\u0438', '\u0433\u0443', '\u0440\u0430', '\u2581\u0431\u043e\u043b\u0435\u0435', '\u2581\u043d\u0435', '\u2581\u0430', '\u0431', '\u0441\u0442\u0440\u0430', '\u043a\u0442', '\u043d\u0430', ',', '\u2581\u0443', '\u2581\u043d\u0435', '\u0435', '\u2581\u0435', '\u0441\u0442\u044c', '\u2581\u0433\u043e\u043b\u043e', '\u0441', '.']"}], "[\n  [\"\u2581\u041b\u0430\", \"Lazarus\"],\n  [\"\u0437\u0430\", null],\n  [\"\u0440\u0443\", null],\n  [\"\u0441\", null],\n  [\"\u2581\u0438\u0437\", \"changed\"],\n  [\"\u043c\u0435\u043d\u0438\", null],\n  [\"\u043b\u0430\", null],\n  [\"\u2581\u0435\", \"its\"],\n  [\"\u0435\", null],\n  [\"\u2581\u0441\", \"meaning\"],\n  [\"\u043c\u044b\", null],\n  [\"\u0441\", null],\n  [\"\u043b\", null],\n  [\":\", null],\n  [\"\u2581\u0432\", \"in\"],\n  [\"\u2581\u0435\", \"her\"],\n  [\"\u0435\", null],\n  [\"\u2581\u0441\u0442\u0438\", \"poem\"],\n  [\"\u0445\u043e\", null],\n  [\"\u0442\u0432\u043e\", null],\n  [\"\u0440\u0435\", null],\n  [\"\u043d\u0438\u0438\", null],\n  [\"\u2581\u0436\u0435\u043d\", \"female\"],\n  [\"\u0441\u043a\u0430\u044f\", null],\n  [\"\u2581\u0444\u0438\", \"figure\"],\n  [\"\u0433\u0443\", null],\n  [\"\u0440\u0430\", null],\n  [\"\u2581\u0431\u043e\u043b\u0435\u0435\", \"is no longer\"],\n  [\"\u2581\u043d\u0435\", null],\n  [\"\u2581\u0430\", \"abstract\"],\n  [\"\u0431\", null],\n  [\"\u0441\u0442\u0440\u0430\", null],\n  [\"\u043a\u0442\", null],\n  [\"\u043d\u0430\", null],\n  [\",\", null],\n  [\"\u2581\u0443\", \"she\"],\n  [\"\u2581\u043d\u0435\", \"has\"],\n  [\"\u0435\", null],\n  [\"\u2581\u0435\", \"a\"],\n  [\"\u0441\u0442\u044c\", null],\n  [\"\u2581\u0433\u043e\u043b\u043e\", \"voice\"],\n  [\"\u0441\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u041b\u0430\u0437\u0430\u0440\u0443\u0441 \u0438\u0437\u043c\u0435\u043d\u0438\u043b\u0430 \u0435\u0435 \u0441\u043c\u044b\u0441\u043b: \u0432 \u0435\u0435 \u0441\u0442\u0438\u0445\u043e\u0442\u0432\u043e\u0440\u0435\u043d\u0438\u0438 \u0436\u0435\u043d\u0441\u043a\u0430\u044f \u0444\u0438\u0433\u0443\u0440\u0430 \u0431\u043e\u043b\u0435\u0435 \u043d\u0435 \u0430\u0431\u0441\u0442\u0440\u0430\u043a\u0442\u043d\u0430, \u0443 \u043d\u0435\u0435 \u0435\u0441\u0442\u044c \u0433\u043e\u043b\u043e\u0441.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041b\u0430', '\u0437\u0430', '\u0440\u0443', '\u0441', '\u2581\u0438\u0437', '\u043c\u0435\u043d\u0438', '\u043b\u0430', '\u2581\u0435', '\u0435', '\u2581\u0441', '\u043c\u044b', '\u0441', '\u043b', ':', '\u2581\u0432', '\u2581\u0435', '\u0435', '\u2581\u0441\u0442\u0438', '\u0445\u043e', '\u0442\u0432\u043e', '\u0440\u0435', '\u043d\u0438\u0438', '\u2581\u0436\u0435\u043d', '\u0441\u043a\u0430\u044f', '\u2581\u0444\u0438', '\u0433\u0443', '\u0440\u0430', '\u2581\u0431\u043e\u043b\u0435\u0435', '\u2581\u043d\u0435', '\u2581\u0430', '\u0431', '\u0441\u0442\u0440\u0430', '\u043a\u0442', '\u043d\u0430', ',', '\u2581\u0443', '\u2581\u043d\u0435', '\u0435', '\u2581\u0435', '\u0441\u0442\u044c', '\u2581\u0433\u043e\u043b\u043e', '\u0441', '.']"}], "[\n[\"\u2581\u041b\u0430\", \"\u62c9\"],\n[\"\u0437\u0430\", null],\n[\"\u0440\u0443\", null],\n[\"\u0441\", null],\n[\"\u2581\u0438\u0437\", \"\u6539\u53d8\"],\n[\"\u043c\u0435\u043d\u0438\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\u0435\", null],\n[\"\u0435\", null],\n[\"\u2581\u0441\", null],\n[\"\u043c\u044b\", null],\n[\"\u0441\", null],\n[\"\u043b\", null],\n[\":\", null],\n[\"\u2581\u0432\", \"\u5728\"],\n[\"\u2581\u0435\", null],\n[\"\u0435\", null],\n[\"\u2581\u0441\u0442\u0438\", \"\u8bd7\"],\n[\"\u0445\u043e\", null],\n[\"\u0442\u0432\u043e\", null],\n[\"\u0440\u0435\", null],\n[\"\u043d\u0438\u0438\", null],\n[\"\u2581\u0436\u0435\u043d\", \"\u5973\u6027\"],\n[\"\u0441\u043a\u0430\u044f\", null],\n[\"\u2581\u0444\u0438\", \"\u5f62\u8c61\"],\n[\"\u0433\u0443\", null],\n[\"\u0440\u0430\", null],\n[\"\u2581\u0431\u043e\u043b\u0435\u0435\", \"\u4e0d\u518d\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0430\", \"\u62bd\u8c61\"],\n[\"\u0431\", null],\n[\"\u0441\u0442\u0440\u0430\", null],\n[\"\u043a\u0442\", null],\n[\"\u043d\u0430\", null],\n[\",\", null],\n[\"\u2581\u0443\", \"\u5979\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u0435\", null],\n[\"\u2581\u0435\", \"\u6709\"],\n[\"\u0441\u0442\u044c\", null],\n[\"\u2581\u0433\u043e\u043b\u043e\", \"\u58f0\u97f3\"],\n[\"\u0441\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u041b\u0430\u0437\u0430\u0440\u0443\u0441 \u0438\u0437\u043c\u0435\u043d\u0438\u043b\u0430 \u0435\u0435 \u0441\u043c\u044b\u0441\u043b: \u0432 \u0435\u0435 \u0441\u0442\u0438\u0445\u043e\u0442\u0432\u043e\u0440\u0435\u043d\u0438\u0438 \u0436\u0435\u043d\u0441\u043a\u0430\u044f \u0444\u0438\u0433\u0443\u0440\u0430 \u0431\u043e\u043b\u0435\u0435 \u043d\u0435 \u0430\u0431\u0441\u0442\u0440\u0430\u043a\u0442\u043d\u0430, \u0443 \u043d\u0435\u0435 \u0435\u0441\u0442\u044c \u0433\u043e\u043b\u043e\u0441.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041b\u0430', '\u0437\u0430', '\u0440\u0443', '\u0441', '\u2581\u0438\u0437', '\u043c\u0435\u043d\u0438', '\u043b\u0430', '\u2581\u0435', '\u0435', '\u2581\u0441', '\u043c\u044b', '\u0441', '\u043b', ':', '\u2581\u0432', '\u2581\u0435', '\u0435', '\u2581\u0441\u0442\u0438', '\u0445\u043e', '\u0442\u0432\u043e', '\u0440\u0435', '\u043d\u0438\u0438', '\u2581\u0436\u0435\u043d', '\u0441\u043a\u0430\u044f', '\u2581\u0444\u0438', '\u0433\u0443', '\u0440\u0430', '\u2581\u0431\u043e\u043b\u0435\u0435', '\u2581\u043d\u0435', '\u2581\u0430', '\u0431', '\u0441\u0442\u0440\u0430', '\u043a\u0442', '\u043d\u0430', ',', '\u2581\u0443', '\u2581\u043d\u0435', '\u0435', '\u2581\u0435', '\u0441\u0442\u044c', '\u2581\u0433\u043e\u043b\u043e', '\u0441', '.']"}], "[\n[\"\u2581\u041b\u0430\", \"\u041b\u0430\u0437\u0430\u0440\u0443\u0441\"],\n[\"\u0437\u0430\", null],\n[\"\u0440\u0443\", null],\n[\"\u0441\", null],\n[\"\u2581\u0438\u0437\", \"\u04e9\u0437\u0433\u0435\u0440\u0442\u0442\u0456\"],\n[\"\u043c\u0435\u043d\u0438\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\u0435\", \"\u043e\u043d\u044b\u04a3\"],\n[\"\u0435\", null],\n[\"\u2581\u0441\", \"\u043c\u0430\u0493\u044b\u043d\u0430\u0441\u044b\u043d\"],\n[\"\u043c\u044b\", null],\n[\"\u0441\", null],\n[\"\u043b\", null],\n[\":\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0435\", \"\u043e\u043d\u044b\u04a3\"],\n[\"\u0435\", null],\n[\"\u2581\u0441\u0442\u0438\", \"\u04e9\u043b\u0435\u04a3\u0456\u043d\u0434\u0435\"],\n[\"\u0445\u043e\", null],\n[\"\u0442\u0432\u043e\", null],\n[\"\u0440\u0435\", null],\n[\"\u043d\u0438\u0438\", null],\n[\"\u2581\u0436\u0435\u043d\", \"\u04d9\u0439\u0435\u043b\"],\n[\"\u0441\u043a\u0430\u044f\", null],\n[\"\u2581\u0444\u0438\", \"\u0431\u0435\u0439\u043d\u0435\u0441\u0456\"],\n[\"\u0433\u0443\", null],\n[\"\u0440\u0430\", null],\n[\"\u2581\u0431\u043e\u043b\u0435\u0435\", \"\u0435\u043d\u0434\u0456\"],\n[\"\u2581\u043d\u0435\", null],\n[\"\u2581\u0430\", \"\u0430\u0431\u0441\u0442\u0440\u0430\u043a\u0442\u044b\u043b\u044b\"],\n[\"\u0431\", null],\n[\"\u0441\u0442\u0440\u0430\", null],\n[\"\u043a\u0442\", null],\n[\"\u043d\u0430\", null],\n[\",\", null],\n[\"\u2581\u0443\", null],\n[\"\u2581\u043d\u0435\", \"\u043e\u043d\u044b\u04a3\"],\n[\"\u0435\", null],\n[\"\u2581\u0435\", \"\u0434\u0430\u0443\u044b\u0441\u044b\"],\n[\"\u0441\u0442\u044c\", null],\n[\"\u2581\u0433\u043e\u043b\u043e\", \"\u0431\u0430\u0440\"],\n[\"\u0441\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u00ab\u042f \u043f\u043e\u0447\u0443\u0432\u0441\u0442\u0432\u043e\u0432\u0430\u043b\u0430 \u0441\u0435\u0431\u044f \u0433\u043e\u043b\u043e\u0439\u00bb, - \u0441\u043a\u0430\u0437\u0430\u043b\u0430 \u0424\u0435\u0440\u0440\u0435\u043b, \u0441\u043e \u0441\u043c\u0435\u0445\u043e\u043c \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u044f \u043e \u0442\u043e\u043c, \u043a\u0430\u043a \u0432 \u043f\u0435\u0440\u0432\u044b\u0439 \u0440\u0430\u0437 \u0443\u0432\u0438\u0434\u0435\u043b\u0430 \u0433\u043e\u0442\u043e\u0432\u044b\u0439 \u043f\u0440\u043e\u0434\u0443\u043a\u0442.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00ab', '\u042f', '\u2581\u043f\u043e', '\u0447\u0443', '\u0432', '\u0441\u0442\u0432\u043e', '\u0432\u0430\u043b\u0430', '\u2581\u0441\u0435', '\u0431', '\u044f', '\u2581\u0433\u043e\u043b\u043e', '\u0439', '\u00bb,', '\u2581-', '\u2581\u0441', '\u043a\u0430\u0437\u0430', '\u043b\u0430', '\u2581\u0424', '\u0435\u0440', '\u0440\u0435', '\u043b', ',', '\u2581\u0441\u043e', '\u2581\u0441', '\u043c\u0435', '\u0445\u043e', '\u043c', '\u2581\u0440\u0430\u0441', '\u0441\u043a\u0430', '\u0437\u044b\u0432\u0430', '\u044f', '\u2581\u043e', '\u2581\u0442\u043e\u043c', ',', '\u2581\u043a\u0430\u043a', '\u2581\u0432', '\u2581\u043f\u0435\u0440', '\u0432\u044b\u0439', '\u2581\u0440\u0430\u0437', '\u2581\u0443', '\u0432\u0438', '\u0434\u0435\u043b\u0430', '\u2581\u0433\u043e', '\u0442\u043e', '\u0432\u044b\u0439', '\u2581\u043f\u0440\u043e', '\u0434\u0443', '\u043a\u0442', '.']"}], "[\n[\"\u2581\u00ab\", null],\n[\"\u042f\", \"I\"],\n[\"\u2581\u043f\u043e\", \"felt\"],\n[\"\u0447\u0443\", null],\n[\"\u0432\", null],\n[\"\u0441\u0442\u0432\u043e\", null],\n[\"\u0432\u0430\u043b\u0430\", null],\n[\"\u2581\u0441\u0435\", null],\n[\"\u0431\", null],\n[\"\u044f\", null],\n[\"\u2581\u0433\u043e\u043b\u043e\", \"naked\"],\n[\"\u0439\", null],\n[\"\u00bb,\", null],\n[\"\u2581-\", null],\n[\"\u2581\u0441\", \"said\"],\n[\"\u043a\u0430\u0437\u0430\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\u0424\", \"Ferrell\"],\n[\"\u0435\u0440\", null],\n[\"\u0440\u0435\", null],\n[\"\u043b\", null],\n[\",\", null],\n[\"\u2581\u0441\u043e\", \"with\"],\n[\"\u2581\u0441\", \"laughter\"],\n[\"\u043c\u0435\", null],\n[\"\u0445\u043e\", null],\n[\"\u043c\", null],\n[\"\u2581\u0440\u0430\u0441\", \"recounting\"],\n[\"\u0441\u043a\u0430\", null],\n[\"\u0437\u044b\u0432\u0430\", null],\n[\"\u044f\", null],\n[\"\u2581\u043e\", \"about\"],\n[\"\u2581\u0442\u043e\u043c\", \"how\"],\n[\",\", null],\n[\"\u2581\u043a\u0430\u043a\", null],\n[\"\u2581\u0432\", \"for\"],\n[\"\u2581\u043f\u0435\u0440\", \"first\"],\n[\"\u0432\u044b\u0439\", null],\n[\"\u2581\u0440\u0430\u0437\", \"time\"],\n[\"\u2581\u0443\", \"saw\"],\n[\"\u0432\u0438\", null],\n[\"\u0434\u0435\u043b\u0430\", null],\n[\"\u2581\u0433\u043e\", \"finished\"],\n[\"\u0442\u043e\", null],\n[\"\u0432\u044b\u0439\", null],\n[\"\u2581\u043f\u0440\u043e\", \"product\"],\n[\"\u0434\u0443\", null],\n[\"\u043a\u0442\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u00ab\u042f \u043f\u043e\u0447\u0443\u0432\u0441\u0442\u0432\u043e\u0432\u0430\u043b\u0430 \u0441\u0435\u0431\u044f \u0433\u043e\u043b\u043e\u0439\u00bb, - \u0441\u043a\u0430\u0437\u0430\u043b\u0430 \u0424\u0435\u0440\u0440\u0435\u043b, \u0441\u043e \u0441\u043c\u0435\u0445\u043e\u043c \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u044f \u043e \u0442\u043e\u043c, \u043a\u0430\u043a \u0432 \u043f\u0435\u0440\u0432\u044b\u0439 \u0440\u0430\u0437 \u0443\u0432\u0438\u0434\u0435\u043b\u0430 \u0433\u043e\u0442\u043e\u0432\u044b\u0439 \u043f\u0440\u043e\u0434\u0443\u043a\u0442.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00ab', '\u042f', '\u2581\u043f\u043e', '\u0447\u0443', '\u0432', '\u0441\u0442\u0432\u043e', '\u0432\u0430\u043b\u0430', '\u2581\u0441\u0435', '\u0431', '\u044f', '\u2581\u0433\u043e\u043b\u043e', '\u0439', '\u00bb,', '\u2581-', '\u2581\u0441', '\u043a\u0430\u0437\u0430', '\u043b\u0430', '\u2581\u0424', '\u0435\u0440', '\u0440\u0435', '\u043b', ',', '\u2581\u0441\u043e', '\u2581\u0441', '\u043c\u0435', '\u0445\u043e', '\u043c', '\u2581\u0440\u0430\u0441', '\u0441\u043a\u0430', '\u0437\u044b\u0432\u0430', '\u044f', '\u2581\u043e', '\u2581\u0442\u043e\u043c', ',', '\u2581\u043a\u0430\u043a', '\u2581\u0432', '\u2581\u043f\u0435\u0440', '\u0432\u044b\u0439', '\u2581\u0440\u0430\u0437', '\u2581\u0443', '\u0432\u0438', '\u0434\u0435\u043b\u0430', '\u2581\u0433\u043e', '\u0442\u043e', '\u0432\u044b\u0439', '\u2581\u043f\u0440\u043e', '\u0434\u0443', '\u043a\u0442', '.']"}], "[\n[\"\u2581\u00ab\", null],\n[\"\u042f\", \"\u6211\"],\n[\"\u2581\u043f\u043e\", null],\n[\"\u0447\u0443\", null],\n[\"\u0432\", null],\n[\"\u0441\u0442\u0432\u043e\", null],\n[\"\u0432\u0430\u043b\u0430\", null],\n[\"\u2581\u0441\u0435\", null],\n[\"\u0431\", null],\n[\"\u044f\", null],\n[\"\u2581\u0433\u043e\u043b\u043e\", \"\u8d64\u88f8\"],\n[\"\u0439\", null],\n[\"\u00bb,\", null],\n[\"\u2581-\", null],\n[\"\u2581\u0441\", null],\n[\"\u043a\u0430\u0437\u0430\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\u0424\", \"\u8d39\"],\n[\"\u0435\u0440\", null],\n[\"\u0440\u0435\", null],\n[\"\u043b\", null],\n[\",\", null],\n[\"\u2581\u0441\u043e\", null],\n[\"\u2581\u0441\", null],\n[\"\u043c\u0435\", null],\n[\"\u0445\u043e\", null],\n[\"\u043c\", null],\n[\"\u2581\u0440\u0430\u0441\", null],\n[\"\u0441\u043a\u0430\", null],\n[\"\u0437\u044b\u0432\u0430\", null],\n[\"\u044f\", null],\n[\"\u2581\u043e\", null],\n[\"\u2581\u0442\u043e\u043c\", \"\u90a3\"],\n[\",\", null],\n[\"\u2581\u043a\u0430\u043a\", \"\u5982\u4f55\"],\n[\"\u2581\u0432\", null],\n[\"\u2581\u043f\u0435\u0440\", \"\u7b2c\u4e00\"],\n[\"\u0432\u044b\u0439\", null],\n[\"\u2581\u0440\u0430\u0437\", \"\u6b21\"],\n[\"\u2581\u0443\", null],\n[\"\u0432\u0438\", null],\n[\"\u0434\u0435\u043b\u0430\", null],\n[\"\u2581\u0433\u043e\", null],\n[\"\u0442\u043e\", null],\n[\"\u0432\u044b\u0439\", null],\n[\"\u2581\u043f\u0440\u043e\", \"\u4ea7\u54c1\"],\n[\"\u0434\u0443\", null],\n[\"\u043a\u0442\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u00ab\u042f \u043f\u043e\u0447\u0443\u0432\u0441\u0442\u0432\u043e\u0432\u0430\u043b\u0430 \u0441\u0435\u0431\u044f \u0433\u043e\u043b\u043e\u0439\u00bb, - \u0441\u043a\u0430\u0437\u0430\u043b\u0430 \u0424\u0435\u0440\u0440\u0435\u043b, \u0441\u043e \u0441\u043c\u0435\u0445\u043e\u043c \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u044f \u043e \u0442\u043e\u043c, \u043a\u0430\u043a \u0432 \u043f\u0435\u0440\u0432\u044b\u0439 \u0440\u0430\u0437 \u0443\u0432\u0438\u0434\u0435\u043b\u0430 \u0433\u043e\u0442\u043e\u0432\u044b\u0439 \u043f\u0440\u043e\u0434\u0443\u043a\u0442.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u00ab', '\u042f', '\u2581\u043f\u043e', '\u0447\u0443', '\u0432', '\u0441\u0442\u0432\u043e', '\u0432\u0430\u043b\u0430', '\u2581\u0441\u0435', '\u0431', '\u044f', '\u2581\u0433\u043e\u043b\u043e', '\u0439', '\u00bb,', '\u2581-', '\u2581\u0441', '\u043a\u0430\u0437\u0430', '\u043b\u0430', '\u2581\u0424', '\u0435\u0440', '\u0440\u0435', '\u043b', ',', '\u2581\u0441\u043e', '\u2581\u0441', '\u043c\u0435', '\u0445\u043e', '\u043c', '\u2581\u0440\u0430\u0441', '\u0441\u043a\u0430', '\u0437\u044b\u0432\u0430', '\u044f', '\u2581\u043e', '\u2581\u0442\u043e\u043c', ',', '\u2581\u043a\u0430\u043a', '\u2581\u0432', '\u2581\u043f\u0435\u0440', '\u0432\u044b\u0439', '\u2581\u0440\u0430\u0437', '\u2581\u0443', '\u0432\u0438', '\u0434\u0435\u043b\u0430', '\u2581\u0433\u043e', '\u0442\u043e', '\u0432\u044b\u0439', '\u2581\u043f\u0440\u043e', '\u0434\u0443', '\u043a\u0442', '.']"}], "[\n[\"\u2581\u00ab\", null],\n[\"\u042f\", \"\u041c\u0435\u043d\"],\n[\"\u2581\u043f\u043e\", null],\n[\"\u0447\u0443\", null],\n[\"\u0432\", null],\n[\"\u0441\u0442\u0432\u043e\", null],\n[\"\u0432\u0430\u043b\u0430\", null],\n[\"\u2581\u0441\u0435\", null],\n[\"\u0431\", null],\n[\"\u044f\", null],\n[\"\u2581\u0433\u043e\u043b\u043e\", null],\n[\"\u0439\", null],\n[\"\u00bb,\", null],\n[\"\u2581-\", null],\n[\"\u2581\u0441\", null],\n[\"\u043a\u0430\u0437\u0430\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\u0424\", null],\n[\"\u0435\u0440\", null],\n[\"\u0440\u0435\", null],\n[\"\u043b\", null],\n[\",\" , null],\n[\"\u2581\u0441\u043e\", null],\n[\"\u2581\u0441\", null],\n[\"\u043c\u0435\", null],\n[\"\u0445\u043e\", null],\n[\"\u043c\", null],\n[\"\u2581\u0440\u0430\u0441\", null],\n[\"\u0441\u043a\u0430\", null],\n[\"\u0437\u044b\u0432\u0430\", null],\n[\"\u044f\", null],\n[\"\u2581\u043e\", null],\n[\"\u2581\u0442\u043e\u043c\", \"\u0441\u043e\u043b\"],\n[\",\" , null],\n[\"\u2581\u043a\u0430\u043a\", \"\u049b\u0430\u043b\u0430\u0439\"],\n[\"\u2581\u0432\", null],\n[\"\u2581\u043f\u0435\u0440\", null],\n[\"\u0432\u044b\u0439\", null],\n[\"\u2581\u0440\u0430\u0437\", \"\u0440\u0435\u0442\"],\n[\"\u2581\u0443\", null],\n[\"\u0432\u0438\", null],\n[\"\u0434\u0435\u043b\u0430\", null],\n[\"\u2581\u0433\u043e\", null],\n[\"\u0442\u043e\", null],\n[\"\u0432\u044b\u0439\", null],\n[\"\u2581\u043f\u0440\u043e\", null],\n[\"\u0434\u0443\", null],\n[\"\u043a\u0442\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0421\u0430\u043c\u043e\u043b\u0435\u0442, \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u0430\u0449\u0438\u0439 \u0430\u0432\u0438\u0430\u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 \"\u0420\u043e\u0441\u0441\u0438\u044f\", \u043d\u0430 \u0431\u043e\u0440\u0442\u0443 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u043d\u0430\u0445\u043e\u0434\u044f\u0442\u0441\u044f \u0442\u0435\u043b\u0430 \u043f\u043e\u0433\u0438\u0431\u0448\u0438\u0445 \u0432 \u0410\u0431\u0445\u0430\u0437\u0438\u0438 \u0440\u043e\u0441\u0441\u0438\u044f\u043d\u043e\u043a, \u0432\u044b\u043b\u0435\u0442\u0435\u043b \u0432 20:15 \u043f\u043e \u043c\u043e\u0441\u043a\u043e\u0432\u0441\u043a\u043e\u043c\u0443 \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0438\u0437 \u0421\u043e\u0447\u0438 \u0432 \u0421\u0430\u043d\u043a\u0442-\u041f\u0435\u0442\u0435\u0440\u0431\u0443\u0440\u0433, \u043f\u0435\u0440\u0435\u0434\u0430\u0435\u0442 \u0422\u0410\u0421\u0421.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0421\u0430', '\u043c\u043e', '\u043b\u0435\u0442', ',', '\u2581\u043f\u0440\u0438', '\u043d\u0430\u0434', '\u043b\u0435', '\u0436\u0430', '\u0449\u0438', '\u0439', '\u2581\u0430', '\u0432\u0438', '\u0430', '\u043a\u043e\u043c', '\u043f\u0430', '\u043d\u0438\u0438', '\u2581\"', '\u0420', '\u043e', '\u0441\u0441\u0438', '\u044f', '\",', '\u2581\u043d\u0430', '\u2581\u0431', '\u043e\u0440', '\u0442\u0443', '\u2581\u043a\u043e\u0442\u043e', '\u0440\u043e\u0433\u043e', '\u2581\u043d\u0430', '\u0445\u043e\u0434\u044f', '\u0442\u0441\u044f', '\u2581\u0442\u0435', '\u043b\u0430', '\u2581\u043f\u043e', '\u0433\u0438', '\u0431', '\u0448\u0438\u0445', '\u2581\u0432', '\u2581\u0410', '\u0431', '\u0445\u0430', '\u0437\u0438', '\u0438', '\u2581\u0440\u043e', '\u0441\u0441\u0438', '\u044f', '\u043d\u043e\u043a', ',', '\u2581\u0432\u044b', '\u043b\u0435', '\u0442\u0435\u043b', '\u2581\u0432', '\u2581', '2', '0', ':', '1', '5', '\u2581\u043f\u043e', '\u2581\u043c\u043e', '\u0441\u043a\u043e\u0432', '\u0441\u043a\u043e', '\u043c\u0443', '\u2581\u0432\u0440\u0435', '\u043c\u0435\u043d\u0438', '\u2581\u0438\u0437', '\u2581\u0421\u043e', '\u0447\u0438', '\u2581\u0432', '\u2581\u0421\u0430\u043d', '\u043a\u0442', '-', '\u041f', '\u0435', '\u0442\u0435\u0440', '\u0431\u0443\u0440\u0433', ',', '\u2581\u043f\u0435\u0440\u0435', '\u0434\u0430', '\u0435\u0442', '\u2581\u0422', '\u0410', '\u0421', '\u0421', '.']"}], "[\n[\"\u2581\u0421\u0430\", \"Aircraft\"],\n[\"\u043c\u043e\", null],\n[\"\u043b\u0435\u0442\", null],\n[\",\", null],\n[\"\u2581\u043f\u0440\u0438\", \"belonging\"],\n[\"\u043d\u0430\u0434\", null],\n[\"\u043b\u0435\", null],\n[\"\u0436\u0430\", null],\n[\"\u0449\u0438\", null],\n[\"\u0439\", null],\n[\"\u2581\u0430\", \"airline\"],\n[\"\u0432\u0438\", null],\n[\"\u0430\", null],\n[\"\u043a\u043e\u043c\", null],\n[\"\u043f\u0430\", null],\n[\"\u043d\u0438\u0438\", null],\n[\"\u2581\\\"\", null],\n[\"\u0420\", \"Russia\"],\n[\"\u043e\", null],\n[\"\u0441\u0441\u0438\", null],\n[\"\u044f\", null],\n[\"\\\"\", null],\n[\",\", null],\n[\"\u2581\u043d\u0430\", \"on\"],\n[\"\u2581\u0431\", \"board\"],\n[\"\u043e\u0440\", null],\n[\"\u0442\u0443\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\", \"which\"],\n[\"\u0440\u043e\u0433\u043e\", null],\n[\"\u2581\u043d\u0430\", \"are\"],\n[\"\u0445\u043e\u0434\u044f\", null],\n[\"\u0442\u0441\u044f\", null],\n[\"\u2581\u0442\u0435\", \"bodies\"],\n[\"\u043b\u0430\", null],\n[\"\u2581\u043f\u043e\", \"deceased\"],\n[\"\u0433\u0438\", null],\n[\"\u0431\", null],\n[\"\u0448\u0438\u0445\", null],\n[\"\u2581\u0432\", \"in\"],\n[\"\u2581\u0410\", \"Abkhazia\"],\n[\"\u0431\", null],\n[\"\u0445\u0430\", null],\n[\"\u0437\u0438\", null],\n[\"\u0438\", null],\n[\"\u2581\u0440\u043e\", \"Russian\"],\n[\"\u0441\u0441\u0438\", null],\n[\"\u044f\", null],\n[\"\u043d\u043e\u043a\", null],\n[\",\", null],\n[\"\u2581\u0432\u044b\", \"departed\"],\n[\"\u043b\u0435\", null],\n[\"\u0442\u0435\u043b\", null],\n[\"\u2581\u0432\", \"at\"],\n[\"\u2581\", null],\n[\"2\", \"20\"],\n[\"0\", null],\n[\":\", null],\n[\"1\", \"15\"],\n[\"5\", null],\n[\"\u2581\u043f\u043e\", \"Moscow\"],\n[\"\u2581\u043c\u043e\", null],\n[\"\u0441\u043a\u043e\u0432\", null],\n[\"\u0441\u043a\u043e\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u0432\u0440\u0435\", \"time\"],\n[\"\u043c\u0435\u043d\u0438\", null],\n[\"\u2581\u0438\u0437\", \"from\"],\n[\"\u2581\u0421\u043e\", \"Sochi\"],\n[\"\u0447\u0438\", null],\n[\"\u2581\u0432\", \"to\"],\n[\"\u2581\u0421\u0430\u043d\", \"Saint\"],\n[\"\u043a\u0442\", null],\n[\"-\", null],\n[\"\u041f\", \"Petersburg\"],\n[\"\u0435\", null],\n[\"\u0442\u0435\u0440\", null],\n[\"\u0431\u0443\u0440\u0433\", null],\n[\",\", null],\n[\"\u2581\u043f\u0435\u0440\u0435\", \"reports\"],\n[\"\u0434\u0430\", null],\n[\"\u0435\u0442\", null],\n[\"\u2581\u0422\", \"TASS\"],\n[\"\u0410\", null],\n[\"\u0421\", null],\n[\"\u0421\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u0421\u0430\u043c\u043e\u043b\u0435\u0442, \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u0430\u0449\u0438\u0439 \u0430\u0432\u0438\u0430\u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 \"\u0420\u043e\u0441\u0441\u0438\u044f\", \u043d\u0430 \u0431\u043e\u0440\u0442\u0443 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u043d\u0430\u0445\u043e\u0434\u044f\u0442\u0441\u044f \u0442\u0435\u043b\u0430 \u043f\u043e\u0433\u0438\u0431\u0448\u0438\u0445 \u0432 \u0410\u0431\u0445\u0430\u0437\u0438\u0438 \u0440\u043e\u0441\u0441\u0438\u044f\u043d\u043e\u043a, \u0432\u044b\u043b\u0435\u0442\u0435\u043b \u0432 20:15 \u043f\u043e \u043c\u043e\u0441\u043a\u043e\u0432\u0441\u043a\u043e\u043c\u0443 \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0438\u0437 \u0421\u043e\u0447\u0438 \u0432 \u0421\u0430\u043d\u043a\u0442-\u041f\u0435\u0442\u0435\u0440\u0431\u0443\u0440\u0433, \u043f\u0435\u0440\u0435\u0434\u0430\u0435\u0442 \u0422\u0410\u0421\u0421.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0421\u0430', '\u043c\u043e', '\u043b\u0435\u0442', ',', '\u2581\u043f\u0440\u0438', '\u043d\u0430\u0434', '\u043b\u0435', '\u0436\u0430', '\u0449\u0438', '\u0439', '\u2581\u0430', '\u0432\u0438', '\u0430', '\u043a\u043e\u043c', '\u043f\u0430', '\u043d\u0438\u0438', '\u2581\"', '\u0420', '\u043e', '\u0441\u0441\u0438', '\u044f', '\",', '\u2581\u043d\u0430', '\u2581\u0431', '\u043e\u0440', '\u0442\u0443', '\u2581\u043a\u043e\u0442\u043e', '\u0440\u043e\u0433\u043e', '\u2581\u043d\u0430', '\u0445\u043e\u0434\u044f', '\u0442\u0441\u044f', '\u2581\u0442\u0435', '\u043b\u0430', '\u2581\u043f\u043e', '\u0433\u0438', '\u0431', '\u0448\u0438\u0445', '\u2581\u0432', '\u2581\u0410', '\u0431', '\u0445\u0430', '\u0437\u0438', '\u0438', '\u2581\u0440\u043e', '\u0441\u0441\u0438', '\u044f', '\u043d\u043e\u043a', ',', '\u2581\u0432\u044b', '\u043b\u0435', '\u0442\u0435\u043b', '\u2581\u0432', '\u2581', '2', '0', ':', '1', '5', '\u2581\u043f\u043e', '\u2581\u043c\u043e', '\u0441\u043a\u043e\u0432', '\u0441\u043a\u043e', '\u043c\u0443', '\u2581\u0432\u0440\u0435', '\u043c\u0435\u043d\u0438', '\u2581\u0438\u0437', '\u2581\u0421\u043e', '\u0447\u0438', '\u2581\u0432', '\u2581\u0421\u0430\u043d', '\u043a\u0442', '-', '\u041f', '\u0435', '\u0442\u0435\u0440', '\u0431\u0443\u0440\u0433', ',', '\u2581\u043f\u0435\u0440\u0435', '\u0434\u0430', '\u0435\u0442', '\u2581\u0422', '\u0410', '\u0421', '\u0421', '.']"}], "[\n[\"\u2581\u0421\u0430\", \"\u98de\u673a\"],\n[\"\u043c\u043e\", null],\n[\"\u043b\u0435\u0442\", null],\n[\",\", null],\n[\"\u2581\u043f\u0440\u0438\", \"\u5c5e\u4e8e\"],\n[\"\u043d\u0430\u0434\", null],\n[\"\u043b\u0435\", null],\n[\"\u0436\u0430\", null],\n[\"\u0449\u0438\", null],\n[\"\u0439\", null],\n[\"\u2581\u0430\", \"\u822a\u7a7a\"],\n[\"\u0432\u0438\", null],\n[\"\u0430\", null],\n[\"\u043a\u043e\u043c\", null],\n[\"\u043f\u0430\", null],\n[\"\u043d\u0438\u0438\", null],\n[\"\u2581\\\"\", null],\n[\"\u0420\", \"\u4fc4\u7f57\u65af\"],\n[\"\u043e\", null],\n[\"\u0441\u0441\u0438\", null],\n[\"\u044f\", null],\n[\"\\\"\", null],\n[\",\", null],\n[\"\u2581\u043d\u0430\", \"\u5728\"],\n[\"\u2581\u0431\", \"\u673a\"],\n[\"\u043e\u0440\", null],\n[\"\u0442\u0443\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\", \"\u5176\"],\n[\"\u0440\u043e\u0433\u043e\", null],\n[\"\u2581\u043d\u0430\", \"\u8f7d\u6709\"],\n[\"\u0445\u043e\u0434\u044f\", null],\n[\"\u0442\u0441\u044f\", null],\n[\"\u2581\u0442\u0435\", \"\u9057\u4f53\"],\n[\"\u043b\u0430\", null],\n[\"\u2581\u043f\u043e\", \"\u6b7b\u4ea1\"],\n[\"\u0433\u0438\", null],\n[\"\u0431\", null],\n[\"\u0448\u0438\u0445\", null],\n[\"\u2581\u0432\", \"\u5728\"],\n[\"\u2581\u0410\", \"\u963f\u5e03\u54c8\u5179\"],\n[\"\u0431\", null],\n[\"\u0445\u0430\", null],\n[\"\u0437\u0438\", null],\n[\"\u0438\", null],\n[\"\u2581\u0440\u043e\", \"\u4fc4\u7f57\u65af\"],\n[\"\u0441\u0441\u0438\", null],\n[\"\u044f\", null],\n[\"\u043d\u043e\u043a\", null],\n[\",\", null],\n[\"\u2581\u0432\u044b\", \"\u8d77\u98de\"],\n[\"\u043b\u0435\", null],\n[\"\u0442\u0435\u043b\", null],\n[\"\u2581\u0432\", \"\u5728\"],\n[\"\u2581\", null],\n[\"2\", \"20\"],\n[\"0\", null],\n[\":\", null],\n[\"1\", \"15\"],\n[\"5\", null],\n[\"\u2581\u043f\u043e\", \"\u6309\"],\n[\"\u2581\u043c\u043e\", \"\u83ab\u65af\u79d1\"],\n[\"\u0441\u043a\u043e\u0432\", null],\n[\"\u0441\u043a\u043e\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u0432\u0440\u0435\", \"\u65f6\u95f4\"],\n[\"\u043c\u0435\u043d\u0438\", null],\n[\"\u2581\u0438\u0437\", \"\u4ece\"],\n[\"\u2581\u0421\u043e\", \"\u7d22\u5951\"],\n[\"\u0447\u0438\", null],\n[\"\u2581\u0432\", \"\u5230\"],\n[\"\u2581\u0421\u0430\u043d\", \"\u5723\u5f7c\u5f97\u5821\"],\n[\"\u043a\u0442\", null],\n[\"-\", null],\n[\"\u041f\", null],\n[\"\u0435\", null],\n[\"\u0442\u0435\u0440\", null],\n[\"\u0431\u0443\u0440\u0433\", null],\n[\",\", null],\n[\"\u2581\u043f\u0435\u0440\u0435\", \"\u636e\"],\n[\"\u0434\u0430\", null],\n[\"\u0435\u0442\", null],\n[\"\u2581\u0422\", \"\u5854\u65af\u793e\"],\n[\"\u0410\", null],\n[\"\u0421\", null],\n[\"\u0421\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u0421\u0430\u043c\u043e\u043b\u0435\u0442, \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u0430\u0449\u0438\u0439 \u0430\u0432\u0438\u0430\u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 \"\u0420\u043e\u0441\u0441\u0438\u044f\", \u043d\u0430 \u0431\u043e\u0440\u0442\u0443 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u043d\u0430\u0445\u043e\u0434\u044f\u0442\u0441\u044f \u0442\u0435\u043b\u0430 \u043f\u043e\u0433\u0438\u0431\u0448\u0438\u0445 \u0432 \u0410\u0431\u0445\u0430\u0437\u0438\u0438 \u0440\u043e\u0441\u0441\u0438\u044f\u043d\u043e\u043a, \u0432\u044b\u043b\u0435\u0442\u0435\u043b \u0432 20:15 \u043f\u043e \u043c\u043e\u0441\u043a\u043e\u0432\u0441\u043a\u043e\u043c\u0443 \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0438\u0437 \u0421\u043e\u0447\u0438 \u0432 \u0421\u0430\u043d\u043a\u0442-\u041f\u0435\u0442\u0435\u0440\u0431\u0443\u0440\u0433, \u043f\u0435\u0440\u0435\u0434\u0430\u0435\u0442 \u0422\u0410\u0421\u0421.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0421\u0430', '\u043c\u043e', '\u043b\u0435\u0442', ',', '\u2581\u043f\u0440\u0438', '\u043d\u0430\u0434', '\u043b\u0435', '\u0436\u0430', '\u0449\u0438', '\u0439', '\u2581\u0430', '\u0432\u0438', '\u0430', '\u043a\u043e\u043c', '\u043f\u0430', '\u043d\u0438\u0438', '\u2581\"', '\u0420', '\u043e', '\u0441\u0441\u0438', '\u044f', '\",', '\u2581\u043d\u0430', '\u2581\u0431', '\u043e\u0440', '\u0442\u0443', '\u2581\u043a\u043e\u0442\u043e', '\u0440\u043e\u0433\u043e', '\u2581\u043d\u0430', '\u0445\u043e\u0434\u044f', '\u0442\u0441\u044f', '\u2581\u0442\u0435', '\u043b\u0430', '\u2581\u043f\u043e', '\u0433\u0438', '\u0431', '\u0448\u0438\u0445', '\u2581\u0432', '\u2581\u0410', '\u0431', '\u0445\u0430', '\u0437\u0438', '\u0438', '\u2581\u0440\u043e', '\u0441\u0441\u0438', '\u044f', '\u043d\u043e\u043a', ',', '\u2581\u0432\u044b', '\u043b\u0435', '\u0442\u0435\u043b', '\u2581\u0432', '\u2581', '2', '0', ':', '1', '5', '\u2581\u043f\u043e', '\u2581\u043c\u043e', '\u0441\u043a\u043e\u0432', '\u0441\u043a\u043e', '\u043c\u0443', '\u2581\u0432\u0440\u0435', '\u043c\u0435\u043d\u0438', '\u2581\u0438\u0437', '\u2581\u0421\u043e', '\u0447\u0438', '\u2581\u0432', '\u2581\u0421\u0430\u043d', '\u043a\u0442', '-', '\u041f', '\u0435', '\u0442\u0435\u0440', '\u0431\u0443\u0440\u0433', ',', '\u2581\u043f\u0435\u0440\u0435', '\u0434\u0430', '\u0435\u0442', '\u2581\u0422', '\u0410', '\u0421', '\u0421', '.']"}], "[\n[\"\u2581\u0421\u0430\", \"\u04b0\u0448\u0430\u049b\"],\n[\"\u043c\u043e\", null],\n[\"\u043b\u0435\u0442\", null],\n[\",\", null],\n[\"\u2581\u043f\u0440\u0438\", \"\u0442\u0438\u0435\u0441\u0456\u043b\u0456\"],\n[\"\u043d\u0430\u0434\", null],\n[\"\u043b\u0435\", null],\n[\"\u0436\u0430\", null],\n[\"\u0449\u0438\", null],\n[\"\u0439\", null],\n[\"\u2581\u0430\", \"\u04d9\u0443\u0435\"],\n[\"\u0432\u0438\", null],\n[\"\u0430\", null],\n[\"\u043a\u043e\u043c\", null],\n[\"\u043f\u0430\", null],\n[\"\u043d\u0438\u0438\", null],\n[\"\u2581\\\"\", null],\n[\"\u0420\", \"\u0420\u0435\u0441\u0435\u0439\"],\n[\"\u043e\", null],\n[\"\u0441\u0441\u0438\", null],\n[\"\u044f\", null],\n[\"\\\"\", null],\n[\",\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u0431\", \"\u0431\u043e\u0440\u0442\u044b\u043d\u0434\u0430\"],\n[\"\u043e\u0440\", null],\n[\"\u0442\u0443\", null],\n[\"\u2581\u043a\u043e\u0442\u043e\", null],\n[\"\u0440\u043e\u0433\u043e\", null],\n[\"\u2581\u043d\u0430\", \"\u043e\u0440\u043d\u0430\u043b\u0430\u0441\u049b\u0430\u043d\"],\n[\"\u0445\u043e\u0434\u044f\", null],\n[\"\u0442\u0441\u044f\", null],\n[\"\u2581\u0442\u0435\", \"\u0434\u0435\u043d\u0435\u043b\u0435\u0440\u0456\"],\n[\"\u043b\u0430\", null],\n[\"\u2581\u043f\u043e\", \"\u049b\u0430\u0437\u0430\"],\n[\"\u0433\u0438\", null],\n[\"\u0431\", null],\n[\"\u0448\u0438\u0445\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0410\", \"\u0410\u0431\u0445\u0430\u0437\u0438\u044f\u0434\u0430\"],\n[\"\u0431\", null],\n[\"\u0445\u0430\", null],\n[\"\u0437\u0438\", null],\n[\"\u0438\", null],\n[\"\u2581\u0440\u043e\", \"\u0440\u0435\u0441\u0435\u0439\u043b\u0456\u043a\"],\n[\"\u0441\u0441\u0438\", null],\n[\"\u044f\", null],\n[\"\u043d\u043e\u043a\", null],\n[\",\", null],\n[\"\u2581\u0432\u044b\", \"\u04b1\u0448\u044b\u043f\"],\n[\"\u043b\u0435\", null],\n[\"\u0442\u0435\u043b\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\", null],\n[\"2\", null],\n[\"0\", null],\n[\":\", null],\n[\"1\", null],\n[\"5\", null],\n[\"\u2581\u043f\u043e\", null],\n[\"\u2581\u043c\u043e\", \"\u041c\u04d9\u0441\u043a\u0435\u0443\"],\n[\"\u0441\u043a\u043e\u0432\", null],\n[\"\u0441\u043a\u043e\", null],\n[\"\u043c\u0443\", null],\n[\"\u2581\u0432\u0440\u0435\", \"\u0443\u0430\u049b\u044b\u0442\u044b\u043c\u0435\u043d\"],\n[\"\u043c\u0435\u043d\u0438\", null],\n[\"\u2581\u0438\u0437\", null],\n[\"\u2581\u0421\u043e\", \"\u0421\u043e\u0447\u0438\u0434\u0435\u043d\"],\n[\"\u0447\u0438\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0421\u0430\u043d\", \"\u0421\u0430\u043d\u043a\u0442-\u041f\u0435\u0442\u0435\u0440\u0431\u0443\u0440\u0433\u043a\u0435\"],\n[\"\u043a\u0442\", null],\n[\"-\", null],\n[\"\u041f\", null],\n[\"\u0435\", null],\n[\"\u0442\u0435\u0440\", null],\n[\"\u0431\u0443\u0440\u0433\", null],\n[\",\", null],\n[\"\u2581\u043f\u0435\u0440\u0435\", \"\u0445\u0430\u0431\u0430\u0440\u043b\u0430\u0439\u0434\u044b\"],\n[\"\u0434\u0430\", null],\n[\"\u0435\u0442\", null],\n[\"\u2581\u0422\", \"\u0422\u0410\u0421\u0421\"],\n[\"\u0410\", null],\n[\"\u0421\", null],\n[\"\u0421\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0412\u0440\u0435\u043c\u044f \u043f\u0440\u0438\u0431\u043b\u0438\u0436\u0430\u0435\u0442\u0441\u044f \u043a \u043e\u0434\u0438\u043d\u043d\u0430\u0434\u0446\u0430\u0442\u0438 \u0432\u0435\u0447\u0435\u0440\u0430; \u0442\u043e\u043b\u043f\u0430 \u0444\u0443\u0442\u0431\u043e\u043b\u0438\u0441\u0442\u043e\u0432 \u0438\u0437 \u041a\u0430\u043b\u0438\u0444\u043e\u0440\u043d\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442\u0430 \u0432 \u041b\u043e\u0441-\u0410\u043d\u0434\u0436\u0435\u043b\u0435\u0441\u0435 \u0437\u0430\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0435\u0442\u0441\u044f, \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u0431\u043e\u043b\u0442\u0430\u0442\u044c \u043d\u0430 \u043e\u0434\u043d\u043e\u0439 \u0441\u0442\u043e\u0440\u043e\u043d\u0435 \u043d\u043e\u0432\u044b\u0445 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u0430\u043d\u0434\u044b, \u0440\u0430\u0441\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u043d\u044b\u0445 \u043d\u0430 \u043a\u0430\u043c\u043f\u0443\u0441\u0435.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0412', '\u0440\u0435', '\u043c\u044f', '\u2581\u043f\u0440\u0438', '\u0431\u043b\u0438', '\u0436\u0430', '\u0435\u0442\u0441\u044f', '\u2581\u043a', '\u2581\u043e\u0434\u0438\u043d', '\u043d\u0430\u0434', '\u0446\u0430', '\u0442\u0438', '\u2581\u0432\u0435', '\u0447\u0435', '\u0440\u0430', ';', '\u2581\u0442\u043e', '\u043b', '\u043f\u0430', '\u2581\u0444\u0443\u0442\u0431\u043e', '\u043b\u0438', '\u0441\u0442\u043e\u0432', '\u2581\u0438\u0437', '\u2581\u041a\u0430', '\u043b\u0438', '\u0444\u043e\u0440', '\u043d\u0438\u0439', '\u0441\u043a\u043e\u0433\u043e', '\u2581\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435', '\u0442\u0430', '\u2581\u0432', '\u2581\u041b\u043e', '\u0441', '-', '\u0410', '\u043d\u0434', '\u0436\u0435', '\u043b\u0435', '\u0441\u0435', '\u2581\u0437\u0430', '\u0434\u0435\u0440', '\u0436\u0438\u0432\u0430', '\u0435\u0442\u0441\u044f', ',', '\u2581\u0447\u0442\u043e\u0431\u044b', '\u2581\u043f\u043e', '\u0431\u043e', '\u043b\u0442\u0430', '\u0442\u044c', '\u2581\u043d\u0430', '\u2581\u043e\u0434', '\u043d\u043e\u0439', '\u2581\u0441\u0442\u043e\u0440\u043e', '\u043d\u0435', '\u2581\u043d\u043e\u0432', '\u044b', '\u0445', '\u2581\u0442\u0440\u0435', '\u043d\u0438', '\u0440\u043e', '\u0432\u043e', '\u0447', '\u043d\u044b\u0445', '\u2581\u043f\u043e', '\u043b\u0435\u0439', '\u2581\u043a\u043e\u043c\u0430\u043d', '\u0434\u044b', ',', '\u2581\u0440\u0430\u0441\u043f\u043e', '\u043b\u043e\u0436\u0435\u043d', '\u043d\u044b\u0445', '\u2581\u043d\u0430', '\u2581\u043a\u0430', '\u043c', '\u043f\u0443', '\u0441\u0435', '.']"}], "[\n[\"\u2581\u0412\", \"Time\"],\n[\"\u0440\u0435\", null],\n[\"\u043c\u044f\", null],\n[\"\u2581\u043f\u0440\u0438\", \"approaches\"],\n[\"\u0431\u043b\u0438\", null],\n[\"\u0436\u0430\", null],\n[\"\u0435\u0442\u0441\u044f\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u043e\u0434\u0438\u043d\", \"eleven\"],\n[\"\u043d\u0430\u0434\", null],\n[\"\u0446\u0430\", null],\n[\"\u0442\u0438\", null],\n[\"\u2581\u0432\u0435\", \"evening\"],\n[\"\u0447\u0435\", null],\n[\"\u0440\u0430\", null],\n[\";\" , null],\n[\"\u2581\u0442\u043e\", \"crowd\"],\n[\"\u043b\", null],\n[\"\u043f\u0430\", null],\n[\"\u2581\u0444\u0443\u0442\u0431\u043e\", \"football\"],\n[\"\u043b\u0438\", null],\n[\"\u0441\u0442\u043e\u0432\", null],\n[\"\u2581\u0438\u0437\", \"from\"],\n[\"\u2581\u041a\u0430\", \"University\"],\n[\"\u043b\u0438\", null],\n[\"\u0444\u043e\u0440\", null],\n[\"\u043d\u0438\u0439\", null],\n[\"\u0441\u043a\u043e\u0433\u043e\", null],\n[\"\u2581\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\", null],\n[\"\u0442\u0430\", null],\n[\"\u2581\u0432\", \"in\"],\n[\"\u2581\u041b\u043e\", \"Los\"],\n[\"\u0441\", null],\n[\"-\", null],\n[\"\u0410\", \"Angeles\"],\n[\"\u043d\u0434\", null],\n[\"\u0436\u0435\", null],\n[\"\u043b\u0435\", null],\n[\"\u0441\u0435\", null],\n[\"\u2581\u0437\u0430\", \"lingers\"],\n[\"\u0434\u0435\u0440\", null],\n[\"\u0436\u0438\u0432\u0430\", null],\n[\"\u0435\u0442\u0441\u044f\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\u0431\u044b\", \"to\"],\n[\"\u2581\u043f\u043e\", \"chat\"],\n[\"\u0431\u043e\", null],\n[\"\u043b\u0442\u0430\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043d\u0430\", \"on\"],\n[\"\u2581\u043e\u0434\", \"one\"],\n[\"\u043d\u043e\u0439\", null],\n[\"\u2581\u0441\u0442\u043e\u0440\u043e\", \"side\"],\n[\"\u043d\u0435\", null],\n[\"\u2581\u043d\u043e\u0432\", \"new\"],\n[\"\u044b\", null],\n[\"\u0445\", null],\n[\"\u2581\u0442\u0440\u0435\", \"training\"],\n[\"\u043d\u0438\", null],\n[\"\u0440\u043e\", null],\n[\"\u0432\u043e\", null],\n[\"\u0447\", null],\n[\"\u043d\u044b\u0445\", null],\n[\"\u2581\u043f\u043e\", \"fields\"],\n[\"\u043b\u0435\u0439\", null],\n[\"\u2581\u043a\u043e\u043c\u0430\u043d\", \"team\"],\n[\"\u0434\u044b\", null],\n[\",\", null],\n[\"\u2581\u0440\u0430\u0441\u043f\u043e\", \"located\"],\n[\"\u043b\u043e\u0436\u0435\u043d\", null],\n[\"\u043d\u044b\u0445\", null],\n[\"\u2581\u043d\u0430\", \"on\"],\n[\"\u2581\u043a\u0430\", \"campus\"],\n[\"\u043c\", null],\n[\"\u043f\u0443\", null],\n[\"\u0441\u0435\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u0412\u0440\u0435\u043c\u044f \u043f\u0440\u0438\u0431\u043b\u0438\u0436\u0430\u0435\u0442\u0441\u044f \u043a \u043e\u0434\u0438\u043d\u043d\u0430\u0434\u0446\u0430\u0442\u0438 \u0432\u0435\u0447\u0435\u0440\u0430; \u0442\u043e\u043b\u043f\u0430 \u0444\u0443\u0442\u0431\u043e\u043b\u0438\u0441\u0442\u043e\u0432 \u0438\u0437 \u041a\u0430\u043b\u0438\u0444\u043e\u0440\u043d\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442\u0430 \u0432 \u041b\u043e\u0441-\u0410\u043d\u0434\u0436\u0435\u043b\u0435\u0441\u0435 \u0437\u0430\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0435\u0442\u0441\u044f, \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u0431\u043e\u043b\u0442\u0430\u0442\u044c \u043d\u0430 \u043e\u0434\u043d\u043e\u0439 \u0441\u0442\u043e\u0440\u043e\u043d\u0435 \u043d\u043e\u0432\u044b\u0445 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u0430\u043d\u0434\u044b, \u0440\u0430\u0441\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u043d\u044b\u0445 \u043d\u0430 \u043a\u0430\u043c\u043f\u0443\u0441\u0435.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0412', '\u0440\u0435', '\u043c\u044f', '\u2581\u043f\u0440\u0438', '\u0431\u043b\u0438', '\u0436\u0430', '\u0435\u0442\u0441\u044f', '\u2581\u043a', '\u2581\u043e\u0434\u0438\u043d', '\u043d\u0430\u0434', '\u0446\u0430', '\u0442\u0438', '\u2581\u0432\u0435', '\u0447\u0435', '\u0440\u0430', ';', '\u2581\u0442\u043e', '\u043b', '\u043f\u0430', '\u2581\u0444\u0443\u0442\u0431\u043e', '\u043b\u0438', '\u0441\u0442\u043e\u0432', '\u2581\u0438\u0437', '\u2581\u041a\u0430', '\u043b\u0438', '\u0444\u043e\u0440', '\u043d\u0438\u0439', '\u0441\u043a\u043e\u0433\u043e', '\u2581\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435', '\u0442\u0430', '\u2581\u0432', '\u2581\u041b\u043e', '\u0441', '-', '\u0410', '\u043d\u0434', '\u0436\u0435', '\u043b\u0435', '\u0441\u0435', '\u2581\u0437\u0430', '\u0434\u0435\u0440', '\u0436\u0438\u0432\u0430', '\u0435\u0442\u0441\u044f', ',', '\u2581\u0447\u0442\u043e\u0431\u044b', '\u2581\u043f\u043e', '\u0431\u043e', '\u043b\u0442\u0430', '\u0442\u044c', '\u2581\u043d\u0430', '\u2581\u043e\u0434', '\u043d\u043e\u0439', '\u2581\u0441\u0442\u043e\u0440\u043e', '\u043d\u0435', '\u2581\u043d\u043e\u0432', '\u044b', '\u0445', '\u2581\u0442\u0440\u0435', '\u043d\u0438', '\u0440\u043e', '\u0432\u043e', '\u0447', '\u043d\u044b\u0445', '\u2581\u043f\u043e', '\u043b\u0435\u0439', '\u2581\u043a\u043e\u043c\u0430\u043d', '\u0434\u044b', ',', '\u2581\u0440\u0430\u0441\u043f\u043e', '\u043b\u043e\u0436\u0435\u043d', '\u043d\u044b\u0445', '\u2581\u043d\u0430', '\u2581\u043a\u0430', '\u043c', '\u043f\u0443', '\u0441\u0435', '.']"}], "[\n[\"\u2581\u0412\", \"\u65f6\u95f4\"],\n[\"\u0440\u0435\", null],\n[\"\u043c\u044f\", null],\n[\"\u2581\u043f\u0440\u0438\", \"\u63a5\u8fd1\"],\n[\"\u0431\u043b\u0438\", null],\n[\"\u0436\u0430\", null],\n[\"\u0435\u0442\u0441\u044f\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u043e\u0434\u0438\u043d\", \"\u5341\u4e00\"],\n[\"\u043d\u0430\u0434\", null],\n[\"\u0446\u0430\", null],\n[\"\u0442\u0438\", null],\n[\"\u2581\u0432\u0435\", \"\u665a\u4e0a\"],\n[\"\u0447\u0435\", null],\n[\"\u0440\u0430\", null],\n[\"\uff1b\", null],\n[\"\u2581\u0442\u043e\", \"\u4e00\u7fa4\"],\n[\"\u043b\", null],\n[\"\u043f\u0430\", null],\n[\"\u2581\u0444\u0443\u0442\u0431\u043e\", \"\u8db3\u7403\u8fd0\u52a8\u5458\"],\n[\"\u043b\u0438\", null],\n[\"\u0441\u0442\u043e\u0432\", null],\n[\"\u2581\u0438\u0437\", \"\u6765\u81ea\"],\n[\"\u2581\u041a\u0430\", \"\u52a0\u5dde\"],\n[\"\u043b\u0438\", null],\n[\"\u0444\u043e\u0440\", null],\n[\"\u043d\u0438\u0439\", null],\n[\"\u0441\u043a\u043e\u0433\u043e\", null],\n[\"\u2581\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\", \"\u5927\u5b66\"],\n[\"\u0442\u0430\", null],\n[\"\u2581\u0432\", \"\u5728\"],\n[\"\u2581\u041b\u043e\", \"\u6d1b\u6749\u77f6\"],\n[\"\u0441\", null],\n[\"-\", null],\n[\"\u0410\", null],\n[\"\u043d\u0434\", null],\n[\"\u0436\u0435\", null],\n[\"\u043b\u0435\", null],\n[\"\u0441\u0435\", null],\n[\"\u2581\u0437\u0430\", \"\u9017\u7559\"],\n[\"\u0434\u0435\u0440\", null],\n[\"\u0436\u0438\u0432\u0430\", null],\n[\"\u0435\u0442\u0441\u044f\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\u0431\u044b\", \"\u4e3a\u4e86\"],\n[\"\u2581\u043f\u043e\", \"\u804a\u5929\"],\n[\"\u0431\u043e\", null],\n[\"\u043b\u0442\u0430\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043d\u0430\", \"\u5728\"],\n[\"\u2581\u043e\u0434\", \"\u4e00\"],\n[\"\u043d\u043e\u0439\", null],\n[\"\u2581\u0441\u0442\u043e\u0440\u043e\", \"\u4fa7\"],\n[\"\u043d\u0435\", null],\n[\"\u2581\u043d\u043e\u0432\", \"\u65b0\"],\n[\"\u044b\", null],\n[\"\u0445\", null],\n[\"\u2581\u0442\u0440\u0435\", \"\u8bad\u7ec3\"],\n[\"\u043d\u0438\", null],\n[\"\u0440\u043e\", null],\n[\"\u0432\u043e\", null],\n[\"\u0447\", null],\n[\"\u043d\u044b\u0445\", null],\n[\"\u2581\u043f\u043e\", \"\u573a\u5730\"],\n[\"\u043b\u0435\u0439\", null],\n[\"\u2581\u043a\u043e\u043c\u0430\u043d\", \"\u7403\u961f\"],\n[\"\u0434\u044b\", null],\n[\",\", null],\n[\"\u2581\u0440\u0430\u0441\u043f\u043e\", \"\u4f4d\u4e8e\"],\n[\"\u043b\u043e\u0436\u0435\u043d\", null],\n[\"\u043d\u044b\u0445\", null],\n[\"\u2581\u043d\u0430\", \"\u5728\"],\n[\"\u2581\u043a\u0430\", \"\u6821\u56ed\"],\n[\"\u043c\", null],\n[\"\u043f\u0443\", null],\n[\"\u0441\u0435\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u0412\u0440\u0435\u043c\u044f \u043f\u0440\u0438\u0431\u043b\u0438\u0436\u0430\u0435\u0442\u0441\u044f \u043a \u043e\u0434\u0438\u043d\u043d\u0430\u0434\u0446\u0430\u0442\u0438 \u0432\u0435\u0447\u0435\u0440\u0430; \u0442\u043e\u043b\u043f\u0430 \u0444\u0443\u0442\u0431\u043e\u043b\u0438\u0441\u0442\u043e\u0432 \u0438\u0437 \u041a\u0430\u043b\u0438\u0444\u043e\u0440\u043d\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442\u0430 \u0432 \u041b\u043e\u0441-\u0410\u043d\u0434\u0436\u0435\u043b\u0435\u0441\u0435 \u0437\u0430\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0435\u0442\u0441\u044f, \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u0431\u043e\u043b\u0442\u0430\u0442\u044c \u043d\u0430 \u043e\u0434\u043d\u043e\u0439 \u0441\u0442\u043e\u0440\u043e\u043d\u0435 \u043d\u043e\u0432\u044b\u0445 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u043f\u043e\u043b\u0435\u0439 \u043a\u043e\u043c\u0430\u043d\u0434\u044b, \u0440\u0430\u0441\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u043d\u044b\u0445 \u043d\u0430 \u043a\u0430\u043c\u043f\u0443\u0441\u0435.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0412', '\u0440\u0435', '\u043c\u044f', '\u2581\u043f\u0440\u0438', '\u0431\u043b\u0438', '\u0436\u0430', '\u0435\u0442\u0441\u044f', '\u2581\u043a', '\u2581\u043e\u0434\u0438\u043d', '\u043d\u0430\u0434', '\u0446\u0430', '\u0442\u0438', '\u2581\u0432\u0435', '\u0447\u0435', '\u0440\u0430', ';', '\u2581\u0442\u043e', '\u043b', '\u043f\u0430', '\u2581\u0444\u0443\u0442\u0431\u043e', '\u043b\u0438', '\u0441\u0442\u043e\u0432', '\u2581\u0438\u0437', '\u2581\u041a\u0430', '\u043b\u0438', '\u0444\u043e\u0440', '\u043d\u0438\u0439', '\u0441\u043a\u043e\u0433\u043e', '\u2581\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435', '\u0442\u0430', '\u2581\u0432', '\u2581\u041b\u043e', '\u0441', '-', '\u0410', '\u043d\u0434', '\u0436\u0435', '\u043b\u0435', '\u0441\u0435', '\u2581\u0437\u0430', '\u0434\u0435\u0440', '\u0436\u0438\u0432\u0430', '\u0435\u0442\u0441\u044f', ',', '\u2581\u0447\u0442\u043e\u0431\u044b', '\u2581\u043f\u043e', '\u0431\u043e', '\u043b\u0442\u0430', '\u0442\u044c', '\u2581\u043d\u0430', '\u2581\u043e\u0434', '\u043d\u043e\u0439', '\u2581\u0441\u0442\u043e\u0440\u043e', '\u043d\u0435', '\u2581\u043d\u043e\u0432', '\u044b', '\u0445', '\u2581\u0442\u0440\u0435', '\u043d\u0438', '\u0440\u043e', '\u0432\u043e', '\u0447', '\u043d\u044b\u0445', '\u2581\u043f\u043e', '\u043b\u0435\u0439', '\u2581\u043a\u043e\u043c\u0430\u043d', '\u0434\u044b', ',', '\u2581\u0440\u0430\u0441\u043f\u043e', '\u043b\u043e\u0436\u0435\u043d', '\u043d\u044b\u0445', '\u2581\u043d\u0430', '\u2581\u043a\u0430', '\u043c', '\u043f\u0443', '\u0441\u0435', '.']"}], "[\n[\"\u2581\u0412\", \"\u0423\u0430\u049b\u044b\u0442\"],\n[\"\u0440\u0435\", null],\n[\"\u043c\u044f\", null],\n[\"\u2581\u043f\u0440\u0438\", null],\n[\"\u0431\u043b\u0438\", null],\n[\"\u0436\u0430\", null],\n[\"\u0435\u0442\u0441\u044f\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u043e\u0434\u0438\u043d\", \"\u043e\u043d\"],\n[\"\u043d\u0430\u0434\", null],\n[\"\u0446\u0430\", null],\n[\"\u0442\u0438\", null],\n[\"\u2581\u0432\u0435\", \"\u043a\u0435\u0448\"],\n[\"\u0447\u0435\", null],\n[\"\u0440\u0430\", null],\n[\"\u2581;\", null],\n[\"\u2581\u0442\u043e\", \"\u0442\u043e\u0431\u044b\u0440\"],\n[\"\u043b\", null],\n[\"\u043f\u0430\", null],\n[\"\u2581\u0444\u0443\u0442\u0431\u043e\", \"\u0444\u0443\u0442\u0431\u043e\u043b\u0448\u044b\"],\n[\"\u043b\u0438\", null],\n[\"\u0441\u0442\u043e\u0432\", null],\n[\"\u2581\u0438\u0437\", null],\n[\"\u2581\u041a\u0430\", \"\u041a\u0430\u043b\u0438\u0444\u043e\u0440\u043d\u0438\u044f\"],\n[\"\u043b\u0438\", null],\n[\"\u0444\u043e\u0440\", null],\n[\"\u043d\u0438\u0439\", null],\n[\"\u0441\u043a\u043e\u0433\u043e\", null],\n[\"\u2581\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\", \"\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442\u0456\"],\n[\"\u0442\u0430\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u041b\u043e\", \"\u041b\u043e\u0441\"],\n[\"\u0441\", null],\n[\"-\", null],\n[\"\u0410\", \"\u0410\u043d\u0434\u0436\u0435\u043b\u0435\u0441\"],\n[\"\u043d\u0434\", null],\n[\"\u0436\u0435\", null],\n[\"\u043b\u0435\", null],\n[\"\u0441\u0435\", null],\n[\"\u2581\u0437\u0430\", \"\u043a\u0456\u0434\u0456\u0440\u0456\u043f\"],\n[\"\u0434\u0435\u0440\", null],\n[\"\u0436\u0438\u0432\u0430\", null],\n[\"\u0435\u0442\u0441\u044f\", null],\n[\",\", null],\n[\"\u2581\u0447\u0442\u043e\u0431\u044b\", null],\n[\"\u2581\u043f\u043e\", \"\u0441\u04e9\u0439\u043b\u0435\u0441\u0443\"],\n[\"\u0431\u043e\", null],\n[\"\u043b\u0442\u0430\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u043e\u0434\", \"\u0431\u0456\u0440\"],\n[\"\u043d\u043e\u0439\", null],\n[\"\u2581\u0441\u0442\u043e\u0440\u043e\", \"\u0436\u0430\u0493\u044b\u043d\u0434\u0430\"],\n[\"\u043d\u0435\", null],\n[\"\u2581\u043d\u043e\u0432\", \"\u0436\u0430\u04a3\u0430\"],\n[\"\u044b\", null],\n[\"\u0445\", null],\n[\"\u2581\u0442\u0440\u0435\", \"\u0436\u0430\u0442\u0442\u044b\u0493\u0443\"],\n[\"\u043d\u0438\", null],\n[\"\u0440\u043e\", null],\n[\"\u0432\u043e\", null],\n[\"\u0447\", null],\n[\"\u043d\u044b\u0445\", null],\n[\"\u2581\u043f\u043e\", \"\u0430\u043b\u0430\u04a3\u0434\u0430\u0440\u044b\"],\n[\"\u043b\u0435\u0439\", null],\n[\"\u2581\u043a\u043e\u043c\u0430\u043d\", \"\u043a\u043e\u043c\u0430\u043d\u0434\u0430\"],\n[\"\u0434\u044b\", null],\n[\",\", null],\n[\"\u2581\u0440\u0430\u0441\u043f\u043e\", \"\u043e\u0440\u043d\u0430\u043b\u0430\u0441\u049b\u0430\u043d\"],\n[\"\u043b\u043e\u0436\u0435\u043d\", null],\n[\"\u043d\u044b\u0445\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u043a\u0430\", \"\u043a\u0430\u043c\u043f\u0443\u0441\u0442\u0430\"],\n[\"\u043c\", null],\n[\"\u043f\u0443\", null],\n[\"\u0441\u0435\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0412 2012 \u0433. \u043c\u043e\u043d\u0443\u043c\u0435\u043d\u0442 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043b\u0438 \u0432 \u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0441\u043a\u043e\u043c \u0425\u044c\u044e\u0441\u0442\u043e\u043d\u0435 \u0432\u043e\u0437\u043b\u0435 \u0431\u044b\u0432\u0448\u0435\u0439 \u0448\u0442\u0430\u0431-\u043a\u0432\u0430\u0440\u0442\u0438\u0440\u044b \u041d\u0410\u0421\u0410.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0412', '\u2581', '2', '0', '1', '2', '\u2581\u0433', '.', '\u2581\u043c\u043e', '\u043d\u0443', '\u043c\u0435\u043d\u0442', '\u2581\u0443', '\u0441\u0442\u0430', '\u043d\u043e\u0432\u0438', '\u043b\u0438', '\u2581\u0432', '\u2581\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d', '\u0441\u043a\u043e\u043c', '\u2581\u0425', '\u044c\u044e', '\u0441\u0442\u043e', '\u043d\u0435', '\u2581\u0432\u043e\u0437', '\u043b\u0435', '\u2581\u0431\u044b', '\u0432', '\u0448\u0435\u0439', '\u2581\u0448\u0442\u0430', '\u0431', '-', '\u043a', '\u0432\u0430\u0440', '\u0442\u0438', '\u0440\u044b', '\u2581\u041d', '\u0410', '\u0421', '\u0410', '.']"}], "[\n[\"\u2581\u0412\", \"In\"],\n[\"\u2581\", null],\n[\"2\", null],\n[\"0\", null],\n[\"1\", null],\n[\"2\", null],\n[\"\u2581\u0433\", null],\n[\".\", null],\n[\"\u2581\u043c\u043e\", \"monument\"],\n[\"\u043d\u0443\", null],\n[\"\u043c\u0435\u043d\u0442\", null],\n[\"\u2581\u0443\", \"installed\"],\n[\"\u0441\u0442\u0430\", null],\n[\"\u043d\u043e\u0432\u0438\", null],\n[\"\u043b\u0438\", null],\n[\"\u2581\u0432\", \"in\"],\n[\"\u2581\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\", \"American\"],\n[\"\u0441\u043a\u043e\u043c\", null],\n[\"\u2581\u0425\", \"Houston\"],\n[\"\u044c\u044e\", null],\n[\"\u0441\u0442\u043e\", null],\n[\"\u043d\u0435\", null],\n[\"\u2581\u0432\u043e\u0437\", \"near\"],\n[\"\u043b\u0435\", null],\n[\"\u2581\u0431\u044b\", \"former\"],\n[\"\u0432\", null],\n[\"\u0448\u0435\u0439\", null],\n[\"\u2581\u0448\u0442\u0430\", \"headquarters\"],\n[\"\u0431\", null],\n[\"-\", null],\n[\"\u043a\", null],\n[\"\u0432\u0430\u0440\", null],\n[\"\u0442\u0438\", null],\n[\"\u0440\u044b\", null],\n[\"\u2581\u041d\", \"NASA\"],\n[\"\u0410\", null],\n[\"\u0421\", null],\n[\"\u0410\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u0412 2012 \u0433. \u043c\u043e\u043d\u0443\u043c\u0435\u043d\u0442 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043b\u0438 \u0432 \u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0441\u043a\u043e\u043c \u0425\u044c\u044e\u0441\u0442\u043e\u043d\u0435 \u0432\u043e\u0437\u043b\u0435 \u0431\u044b\u0432\u0448\u0435\u0439 \u0448\u0442\u0430\u0431-\u043a\u0432\u0430\u0440\u0442\u0438\u0440\u044b \u041d\u0410\u0421\u0410.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0412', '\u2581', '2', '0', '1', '2', '\u2581\u0433', '.', '\u2581\u043c\u043e', '\u043d\u0443', '\u043c\u0435\u043d\u0442', '\u2581\u0443', '\u0441\u0442\u0430', '\u043d\u043e\u0432\u0438', '\u043b\u0438', '\u2581\u0432', '\u2581\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d', '\u0441\u043a\u043e\u043c', '\u2581\u0425', '\u044c\u044e', '\u0441\u0442\u043e', '\u043d\u0435', '\u2581\u0432\u043e\u0437', '\u043b\u0435', '\u2581\u0431\u044b', '\u0432', '\u0448\u0435\u0439', '\u2581\u0448\u0442\u0430', '\u0431', '-', '\u043a', '\u0432\u0430\u0440', '\u0442\u0438', '\u0440\u044b', '\u2581\u041d', '\u0410', '\u0421', '\u0410', '.']"}], "[\n[\"\u2581\u0412\", null],\n[\"\u2581\", null],\n[\"2\", null],\n[\"0\", null],\n[\"1\", null],\n[\"2\", null],\n[\"\u2581\u0433\", \"\u5e74\"],\n[\".\", null],\n[\"\u2581\u043c\u043e\", \"\u7eaa\u5ff5\u7891\"],\n[\"\u043d\u0443\", null],\n[\"\u043c\u0435\u043d\u0442\", null],\n[\"\u2581\u0443\", \"\u5b89\u88c5\"],\n[\"\u0441\u0442\u0430\", null],\n[\"\u043d\u043e\u0432\u0438\", null],\n[\"\u043b\u0438\", null],\n[\"\u2581\u0432\", \"\u5728\"],\n[\"\u2581\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\", \"\u7f8e\u56fd\"],\n[\"\u0441\u043a\u043e\u043c\", null],\n[\"\u2581\u0425\", \"\u4f11\u65af\u987f\"],\n[\"\u044c\u044e\", null],\n[\"\u0441\u0442\u043e\", null],\n[\"\u043d\u0435\", null],\n[\"\u2581\u0432\u043e\u0437\", \"\u9644\u8fd1\"],\n[\"\u043b\u0435\", null],\n[\"\u2581\u0431\u044b\", \"\u524d\"],\n[\"\u0432\", null],\n[\"\u0448\u0435\u0439\", null],\n[\"\u2581\u0448\u0442\u0430\", \"\u603b\u90e8\"],\n[\"\u0431\", null],\n[\"-\", null],\n[\"\u043a\", null],\n[\"\u0432\u0430\u0440\", null],\n[\"\u0442\u0438\", null],\n[\"\u0440\u044b\", null],\n[\"\u2581\u041d\", \"\u7f8e\u56fd\u56fd\u5bb6\u822a\u7a7a\u822a\u5929\u5c40\"],\n[\"\u0410\", null],\n[\"\u0421\", null],\n[\"\u0410\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u0412 2012 \u0433. \u043c\u043e\u043d\u0443\u043c\u0435\u043d\u0442 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043b\u0438 \u0432 \u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0441\u043a\u043e\u043c \u0425\u044c\u044e\u0441\u0442\u043e\u043d\u0435 \u0432\u043e\u0437\u043b\u0435 \u0431\u044b\u0432\u0448\u0435\u0439 \u0448\u0442\u0430\u0431-\u043a\u0432\u0430\u0440\u0442\u0438\u0440\u044b \u041d\u0410\u0421\u0410.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0412', '\u2581', '2', '0', '1', '2', '\u2581\u0433', '.', '\u2581\u043c\u043e', '\u043d\u0443', '\u043c\u0435\u043d\u0442', '\u2581\u0443', '\u0441\u0442\u0430', '\u043d\u043e\u0432\u0438', '\u043b\u0438', '\u2581\u0432', '\u2581\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d', '\u0441\u043a\u043e\u043c', '\u2581\u0425', '\u044c\u044e', '\u0441\u0442\u043e', '\u043d\u0435', '\u2581\u0432\u043e\u0437', '\u043b\u0435', '\u2581\u0431\u044b', '\u0432', '\u0448\u0435\u0439', '\u2581\u0448\u0442\u0430', '\u0431', '-', '\u043a', '\u0432\u0430\u0440', '\u0442\u0438', '\u0440\u044b', '\u2581\u041d', '\u0410', '\u0421', '\u0410', '.']"}], "[\n[\"\u2581\u0412\", null],\n[\"\u2581\", null],\n[\"2\", null],\n[\"0\", null],\n[\"1\", null],\n[\"2\", null],\n[\"\u2581\u0433\", null],\n[\".\", null],\n[\"\u2581\u043c\u043e\", \"\u0435\u0441\u043a\u0435\u0440\u0442\u043a\u0456\u0448\"],\n[\"\u043d\u0443\", null],\n[\"\u043c\u0435\u043d\u0442\", null],\n[\"\u2581\u0443\", \"\u043e\u0440\u043d\u0430\u0442\u044b\u043b\u0434\u044b\"],\n[\"\u0441\u0442\u0430\", null],\n[\"\u043d\u043e\u0432\u0438\", null],\n[\"\u043b\u0438\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\", \"\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043b\u044b\u049b\"],\n[\"\u0441\u043a\u043e\u043c\", null],\n[\"\u2581\u0425\", \"\u0425\u044c\u044e\u0441\u0442\u043e\u043d\"],\n[\"\u044c\u044e\", null],\n[\"\u0441\u0442\u043e\", null],\n[\"\u043d\u0435\", null],\n[\"\u2581\u0432\u043e\u0437\", \"\u0436\u0430\u043d\u044b\u043d\u0434\u0430\"],\n[\"\u043b\u0435\", null],\n[\"\u2581\u0431\u044b\", \"\u0431\u04b1\u0440\u044b\u043d\u0493\u044b\"],\n[\"\u0432\", null],\n[\"\u0448\u0435\u0439\", null],\n[\"\u2581\u0448\u0442\u0430\", \"\u0448\u0442\u0430\u0431\"],\n[\"\u0431\", null],\n[\"-\", null],\n[\"\u043a\", \"\u043f\u04d9\u0442\u0435\u0440\u0456\"],\n[\"\u0432\u0430\u0440\", null],\n[\"\u0442\u0438\", null],\n[\"\u0440\u044b\", null],\n[\"\u2581\u041d\", \"\u041d\u0410\u0421\u0410\"],\n[\"\u0410\", null],\n[\"\u0421\", null],\n[\"\u0410\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f \u043f\u043e \u043f\u0435\u0440\u0435\u0434\u0432\u0438\u0436\u0435\u043d\u0438\u044e \u043e\u0431\u044b\u0447\u043d\u043e \u0432\u044b\u043f\u0443\u0441\u043a\u0430\u044e\u0442\u0441\u044f \u0413\u043e\u0441\u0443\u0434\u0430\u0440\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u043c \u0434\u0435\u043f\u0430\u0440\u0442\u0430\u043c\u0435\u043d\u0442\u043e\u043c \u043f\u0440\u0438\u043c\u0435\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043a \u0434\u0440\u0443\u0433\u0438\u043c \u0441\u0442\u0440\u0430\u043d\u0430\u043c, \u043d\u043e \u0432 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435 \u0432\u0440\u0435\u043c\u044f \u043e\u0431\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0435 \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u0440\u0438\u0431\u0435\u0433\u043b\u0438 \u043a \u044d\u0442\u043e\u0439 \u043c\u0435\u0440\u0435 \u0432 \u043e\u0442\u0432\u0435\u0442 \u043d\u0430 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0451\u043d\u043d\u044b\u0435 \u0437\u0430\u043a\u043e\u043d\u044b \u0438 \u0442\u0435\u043d\u0434\u0435\u043d\u0446\u0438\u0438 \u0432 \u043f\u0440\u0435\u0434\u0435\u043b\u0430\u0445 \u0421\u0428\u0410.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041f\u0440\u0435', '\u0434\u0443', '\u043f\u0440\u0435', '\u0436\u0434\u0435\u043d\u0438\u044f', '\u2581\u043f\u043e', '\u2581\u043f\u0435\u0440\u0435\u0434', '\u0432\u0438', '\u0436\u0435', '\u043d\u0438\u044e', '\u2581\u043e\u0431', '\u044b', '\u0447\u043d\u043e', '\u2581\u0432\u044b\u043f\u0443', '\u0441\u043a\u0430', '\u044e\u0442\u0441\u044f', '\u2581\u0413\u043e', '\u0441\u0443\u0434\u0430\u0440', '\u0441\u0442\u0432\u0435\u043d', '\u043d\u044b\u043c', '\u2581\u0434\u0435', '\u043f\u0430\u0440', '\u0442\u0430', '\u043c\u0435\u043d', '\u0442\u043e\u043c', '\u2581\u043f\u0440\u0438', '\u043c\u0435\u043d\u0438', '\u0442\u0435\u043b\u044c\u043d\u043e', '\u2581\u043a', '\u2581\u0434\u0440\u0443\u0433\u0438', '\u043c', '\u2581\u0441\u0442\u0440\u0430', '\u043d\u0430', '\u043c', ',', '\u2581\u043d\u043e', '\u2581\u0432', '\u2581\u043f\u043e\u0441\u043b\u0435\u0434', '\u043d\u0435\u0435', '\u2581\u0432\u0440\u0435\u043c\u044f', '\u2581\u043e\u0431\u0449\u0435', '\u0441\u0442\u0432\u0435\u043d', '\u043d\u044b\u0435', '\u2581\u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430', '\u0446\u0438\u0438', '\u2581\u043f\u0440\u0438', '\u0431\u0435', '\u0433', '\u043b\u0438', '\u2581\u043a', '\u2581\u044d\u0442\u043e', '\u0439', '\u2581\u043c', '\u0435\u0440\u0435', '\u2581\u0432', '\u2581\u043e\u0442', '\u0432\u0435\u0442', '\u2581\u043d\u0430', '\u2581\u043e', '\u043f\u0440\u0435\u0434\u0435', '\u043b\u0451\u043d', '\u043d\u044b\u0435', '\u2581\u0437\u0430', '\u043a\u043e', '\u043d\u044b', '\u2581\u0438', '\u2581\u0442', '\u0435\u043d', '\u0434\u0435\u043d', '\u0446\u0438\u0438', '\u2581\u0432', '\u2581\u043f\u0440\u0435', '\u0434\u0435\u043b\u0430', '\u0445', '\u2581\u0421\u0428\u0410', '.']"}], "[\n[\"\u2581\u041f\u0440\u0435\", \"Warnings\"],\n[\"\u0434\u0443\", null],\n[\"\u043f\u0440\u0435\", null],\n[\"\u0436\u0434\u0435\u043d\u0438\u044f\", null],\n[\"\u2581\u043f\u043e\", null],\n[\"\u2581\u043f\u0435\u0440\u0435\u0434\", \"movement\"],\n[\"\u0432\u0438\", null],\n[\"\u0436\u0435\", null],\n[\"\u043d\u0438\u044e\", null],\n[\"\u2581\u043e\u0431\", \"usually\"],\n[\"\u044b\", null],\n[\"\u0447\u043d\u043e\", null],\n[\"\u2581\u0432\u044b\u043f\u0443\", \"issued\"],\n[\"\u0441\u043a\u0430\", null],\n[\"\u044e\u0442\u0441\u044f\", null],\n[\"\u2581\u0413\u043e\", \"State\"],\n[\"\u0441\u0443\u0434\u0430\u0440\", null],\n[\"\u0441\u0442\u0432\u0435\u043d\", null],\n[\"\u043d\u044b\u043c\", null],\n[\"\u2581\u0434\u0435\", \"Department\"],\n[\"\u043f\u0430\u0440\", null],\n[\"\u0442\u0430\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u0442\u043e\u043c\", null],\n[\"\u2581\u043f\u0440\u0438\", \"regarding\"],\n[\"\u043c\u0435\u043d\u0438\", null],\n[\"\u0442\u0435\u043b\u044c\u043d\u043e\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u0434\u0440\u0443\u0433\u0438\", \"other\"],\n[\"\u043c\", null],\n[\"\u2581\u0441\u0442\u0440\u0430\", \"countries\"],\n[\"\u043d\u0430\", null],\n[\"\u043c\", null],\n[\",\", null],\n[\"\u2581\u043d\u043e\", \"but\"],\n[\"\u2581\u0432\", null],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\u0434\", \"recently\"],\n[\"\u043d\u0435\u0435\", null],\n[\"\u2581\u0432\u0440\u0435\u043c\u044f\", \"time\"],\n[\"\u2581\u043e\u0431\u0449\u0435\", \"public\"],\n[\"\u0441\u0442\u0432\u0435\u043d\", null],\n[\"\u043d\u044b\u0435\", null],\n[\"\u2581\u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\", \"organizations\"],\n[\"\u0446\u0438\u0438\", null],\n[\"\u2581\u043f\u0440\u0438\", \"resorted\"],\n[\"\u0431\u0435\", null],\n[\"\u0433\", null],\n[\"\u043b\u0438\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u044d\u0442\u043e\", \"this\"],\n[\"\u0439\", null],\n[\"\u2581\u043c\", \"measure\"],\n[\"\u0435\u0440\u0435\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u043e\u0442\", \"response\"],\n[\"\u0432\u0435\u0442\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u043e\", \"certain\"],\n[\"\u043f\u0440\u0435\u0434\u0435\", null],\n[\"\u043b\u0451\u043d\", null],\n[\"\u043d\u044b\u0435\", null],\n[\"\u2581\u0437\u0430\", \"laws\"],\n[\"\u043a\u043e\", null],\n[\"\u043d\u044b\", null],\n[\"\u2581\u0438\", \"and\"],\n[\"\u2581\u0442\", \"trends\"],\n[\"\u0435\u043d\", null],\n[\"\u0434\u0435\u043d\", null],\n[\"\u0446\u0438\u0438\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u043f\u0440\u0435\", \"within\"],\n[\"\u0434\u0435\u043b\u0430\", null],\n[\"\u0445\", null],\n[\"\u2581\u0421\u0428\u0410\", \"USA\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u041f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f \u043f\u043e \u043f\u0435\u0440\u0435\u0434\u0432\u0438\u0436\u0435\u043d\u0438\u044e \u043e\u0431\u044b\u0447\u043d\u043e \u0432\u044b\u043f\u0443\u0441\u043a\u0430\u044e\u0442\u0441\u044f \u0413\u043e\u0441\u0443\u0434\u0430\u0440\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u043c \u0434\u0435\u043f\u0430\u0440\u0442\u0430\u043c\u0435\u043d\u0442\u043e\u043c \u043f\u0440\u0438\u043c\u0435\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043a \u0434\u0440\u0443\u0433\u0438\u043c \u0441\u0442\u0440\u0430\u043d\u0430\u043c, \u043d\u043e \u0432 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435 \u0432\u0440\u0435\u043c\u044f \u043e\u0431\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0435 \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u0440\u0438\u0431\u0435\u0433\u043b\u0438 \u043a \u044d\u0442\u043e\u0439 \u043c\u0435\u0440\u0435 \u0432 \u043e\u0442\u0432\u0435\u0442 \u043d\u0430 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0451\u043d\u043d\u044b\u0435 \u0437\u0430\u043a\u043e\u043d\u044b \u0438 \u0442\u0435\u043d\u0434\u0435\u043d\u0446\u0438\u0438 \u0432 \u043f\u0440\u0435\u0434\u0435\u043b\u0430\u0445 \u0421\u0428\u0410.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041f\u0440\u0435', '\u0434\u0443', '\u043f\u0440\u0435', '\u0436\u0434\u0435\u043d\u0438\u044f', '\u2581\u043f\u043e', '\u2581\u043f\u0435\u0440\u0435\u0434', '\u0432\u0438', '\u0436\u0435', '\u043d\u0438\u044e', '\u2581\u043e\u0431', '\u044b', '\u0447\u043d\u043e', '\u2581\u0432\u044b\u043f\u0443', '\u0441\u043a\u0430', '\u044e\u0442\u0441\u044f', '\u2581\u0413\u043e', '\u0441\u0443\u0434\u0430\u0440', '\u0441\u0442\u0432\u0435\u043d', '\u043d\u044b\u043c', '\u2581\u0434\u0435', '\u043f\u0430\u0440', '\u0442\u0430', '\u043c\u0435\u043d', '\u0442\u043e\u043c', '\u2581\u043f\u0440\u0438', '\u043c\u0435\u043d\u0438', '\u0442\u0435\u043b\u044c\u043d\u043e', '\u2581\u043a', '\u2581\u0434\u0440\u0443\u0433\u0438', '\u043c', '\u2581\u0441\u0442\u0440\u0430', '\u043d\u0430', '\u043c', ',', '\u2581\u043d\u043e', '\u2581\u0432', '\u2581\u043f\u043e\u0441\u043b\u0435\u0434', '\u043d\u0435\u0435', '\u2581\u0432\u0440\u0435\u043c\u044f', '\u2581\u043e\u0431\u0449\u0435', '\u0441\u0442\u0432\u0435\u043d', '\u043d\u044b\u0435', '\u2581\u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430', '\u0446\u0438\u0438', '\u2581\u043f\u0440\u0438', '\u0431\u0435', '\u0433', '\u043b\u0438', '\u2581\u043a', '\u2581\u044d\u0442\u043e', '\u0439', '\u2581\u043c', '\u0435\u0440\u0435', '\u2581\u0432', '\u2581\u043e\u0442', '\u0432\u0435\u0442', '\u2581\u043d\u0430', '\u2581\u043e', '\u043f\u0440\u0435\u0434\u0435', '\u043b\u0451\u043d', '\u043d\u044b\u0435', '\u2581\u0437\u0430', '\u043a\u043e', '\u043d\u044b', '\u2581\u0438', '\u2581\u0442', '\u0435\u043d', '\u0434\u0435\u043d', '\u0446\u0438\u0438', '\u2581\u0432', '\u2581\u043f\u0440\u0435', '\u0434\u0435\u043b\u0430', '\u0445', '\u2581\u0421\u0428\u0410', '.']"}], "[\n[\"\u2581\u041f\u0440\u0435\", null],\n[\"\u0434\u0443\", null],\n[\"\u043f\u0440\u0435\", null],\n[\"\u0436\u0434\u0435\u043d\u0438\u044f\", \"\u8b66\u544a\"],\n[\"\u2581\u043f\u043e\", null],\n[\"\u2581\u043f\u0435\u0440\u0435\u0434\", \"\u79fb\u52a8\"],\n[\"\u0432\u0438\", null],\n[\"\u0436\u0435\", null],\n[\"\u043d\u0438\u044e\", null],\n[\"\u2581\u043e\u0431\", null],\n[\"\u044b\", null],\n[\"\u0447\u043d\u043e\", null],\n[\"\u2581\u0432\u044b\u043f\u0443\", \"\u53d1\u5e03\"],\n[\"\u0441\u043a\u0430\", null],\n[\"\u044e\u0442\u0441\u044f\", null],\n[\"\u2581\u0413\u043e\", \"\u56fd\u52a1\u9662\"],\n[\"\u0441\u0443\u0434\u0430\u0440\", null],\n[\"\u0441\u0442\u0432\u0435\u043d\", null],\n[\"\u043d\u044b\u043c\", null],\n[\"\u2581\u0434\u0435\", null],\n[\"\u043f\u0430\u0440\", null],\n[\"\u0442\u0430\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u0442\u043e\u043c\", null],\n[\"\u2581\u043f\u0440\u0438\", \"\u5173\u4e8e\"],\n[\"\u043c\u0435\u043d\u0438\", null],\n[\"\u0442\u0435\u043b\u044c\u043d\u043e\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u0434\u0440\u0443\u0433\u0438\", \"\u5176\u4ed6\"],\n[\"\u043c\", null],\n[\"\u2581\u0441\u0442\u0440\u0430\", \"\u56fd\u5bb6\"],\n[\"\u043d\u0430\", null],\n[\"\u043c\", null],\n[\",\", null],\n[\"\u2581\u043d\u043e\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\u0434\", \"\u6700\u8fd1\"],\n[\"\u043d\u0435\u0435\", null],\n[\"\u2581\u0432\u0440\u0435\u043c\u044f\", \"\u65f6\u95f4\"],\n[\"\u2581\u043e\u0431\u0449\u0435\", \"\u516c\u5171\"],\n[\"\u0441\u0442\u0432\u0435\u043d\", null],\n[\"\u043d\u044b\u0435\", null],\n[\"\u2581\u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\", \"\u7ec4\u7ec7\"],\n[\"\u0446\u0438\u0438\", null],\n[\"\u2581\u043f\u0440\u0438\", \"\u91c7\u53d6\"],\n[\"\u0431\u0435\", null],\n[\"\u0433\", null],\n[\"\u043b\u0438\", null],\n[\"\u2581\u043a\", null],\n[\"\u2581\u044d\u0442\u043e\", \"\u8fd9\"],\n[\"\u0439\", null],\n[\"\u2581\u043c\", \"\u63aa\u65bd\"],\n[\"\u0435\u0440\u0435\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u043e\u0442\", \"\u56de\u5e94\"],\n[\"\u0432\u0435\u0442\", null],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u043e\", \"\u67d0\u4e9b\"],\n[\"\u043f\u0440\u0435\u0434\u0435\", null],\n[\"\u043b\u0451\u043d\", null],\n[\"\u043d\u044b\u0435\", null],\n[\"\u2581\u0437\u0430\", \"\u6cd5\u5f8b\"],\n[\"\u043a\u043e\", null],\n[\"\u043d\u044b\", null],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0442\", \"\u8d8b\u52bf\"],\n[\"\u0435\u043d\", null],\n[\"\u0434\u0435\u043d\", null],\n[\"\u0446\u0438\u0438\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u043f\u0440\u0435\", \"\u7f8e\u56fd\"],\n[\"\u0434\u0435\u043b\u0430\", null],\n[\"\u0445\", null],\n[\"\u2581\u0421\u0428\u0410\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u041f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f \u043f\u043e \u043f\u0435\u0440\u0435\u0434\u0432\u0438\u0436\u0435\u043d\u0438\u044e \u043e\u0431\u044b\u0447\u043d\u043e \u0432\u044b\u043f\u0443\u0441\u043a\u0430\u044e\u0442\u0441\u044f \u0413\u043e\u0441\u0443\u0434\u0430\u0440\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u043c \u0434\u0435\u043f\u0430\u0440\u0442\u0430\u043c\u0435\u043d\u0442\u043e\u043c \u043f\u0440\u0438\u043c\u0435\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043a \u0434\u0440\u0443\u0433\u0438\u043c \u0441\u0442\u0440\u0430\u043d\u0430\u043c, \u043d\u043e \u0432 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435 \u0432\u0440\u0435\u043c\u044f \u043e\u0431\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0435 \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u0440\u0438\u0431\u0435\u0433\u043b\u0438 \u043a \u044d\u0442\u043e\u0439 \u043c\u0435\u0440\u0435 \u0432 \u043e\u0442\u0432\u0435\u0442 \u043d\u0430 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0451\u043d\u043d\u044b\u0435 \u0437\u0430\u043a\u043e\u043d\u044b \u0438 \u0442\u0435\u043d\u0434\u0435\u043d\u0446\u0438\u0438 \u0432 \u043f\u0440\u0435\u0434\u0435\u043b\u0430\u0445 \u0421\u0428\u0410.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041f\u0440\u0435', '\u0434\u0443', '\u043f\u0440\u0435', '\u0436\u0434\u0435\u043d\u0438\u044f', '\u2581\u043f\u043e', '\u2581\u043f\u0435\u0440\u0435\u0434', '\u0432\u0438', '\u0436\u0435', '\u043d\u0438\u044e', '\u2581\u043e\u0431', '\u044b', '\u0447\u043d\u043e', '\u2581\u0432\u044b\u043f\u0443', '\u0441\u043a\u0430', '\u044e\u0442\u0441\u044f', '\u2581\u0413\u043e', '\u0441\u0443\u0434\u0430\u0440', '\u0441\u0442\u0432\u0435\u043d', '\u043d\u044b\u043c', '\u2581\u0434\u0435', '\u043f\u0430\u0440', '\u0442\u0430', '\u043c\u0435\u043d', '\u0442\u043e\u043c', '\u2581\u043f\u0440\u0438', '\u043c\u0435\u043d\u0438', '\u0442\u0435\u043b\u044c\u043d\u043e', '\u2581\u043a', '\u2581\u0434\u0440\u0443\u0433\u0438', '\u043c', '\u2581\u0441\u0442\u0440\u0430', '\u043d\u0430', '\u043c', ',', '\u2581\u043d\u043e', '\u2581\u0432', '\u2581\u043f\u043e\u0441\u043b\u0435\u0434', '\u043d\u0435\u0435', '\u2581\u0432\u0440\u0435\u043c\u044f', '\u2581\u043e\u0431\u0449\u0435', '\u0441\u0442\u0432\u0435\u043d', '\u043d\u044b\u0435', '\u2581\u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430', '\u0446\u0438\u0438', '\u2581\u043f\u0440\u0438', '\u0431\u0435', '\u0433', '\u043b\u0438', '\u2581\u043a', '\u2581\u044d\u0442\u043e', '\u0439', '\u2581\u043c', '\u0435\u0440\u0435', '\u2581\u0432', '\u2581\u043e\u0442', '\u0432\u0435\u0442', '\u2581\u043d\u0430', '\u2581\u043e', '\u043f\u0440\u0435\u0434\u0435', '\u043b\u0451\u043d', '\u043d\u044b\u0435', '\u2581\u0437\u0430', '\u043a\u043e', '\u043d\u044b', '\u2581\u0438', '\u2581\u0442', '\u0435\u043d', '\u0434\u0435\u043d', '\u0446\u0438\u0438', '\u2581\u0432', '\u2581\u043f\u0440\u0435', '\u0434\u0435\u043b\u0430', '\u0445', '\u2581\u0421\u0428\u0410', '.']"}], "[\n[\"\u2581\u041f\u0440\u0435\", null],\n[\"\u0434\u0443\", null],\n[\"\u043f\u0440\u0435\", null],\n[\"\u0436\u0434\u0435\u043d\u0438\u044f\", \"\u0415\u0441\u043a\u0435\u0440\u0442\u0443\u043b\u0435\u0440\"],\n[\"\u2581\u043f\u043e\", null],\n[\"\u2581\u043f\u0435\u0440\u0435\u0434\", null],\n[\"\u0432\u0438\", null],\n[\"\u0436\u0435\", null],\n[\"\u043d\u0438\u044e\", \"\u049b\u043e\u0437\u0493\u0430\u043b\u044b\u0441\"],\n[\"\u2581\u043e\u0431\", null],\n[\"\u044b\", null],\n[\"\u0447\u043d\u043e\", \"\u04d9\u0434\u0435\u0442\u0442\u0435\"],\n[\"\u2581\u0432\u044b\u043f\u0443\", null],\n[\"\u0441\u043a\u0430\", null],\n[\"\u044e\u0442\u0441\u044f\", \"\u0448\u044b\u0493\u0430\u0440\u044b\u043b\u0430\u0434\u044b\"],\n[\"\u2581\u0413\u043e\", null],\n[\"\u0441\u0443\u0434\u0430\u0440\", null],\n[\"\u0441\u0442\u0432\u0435\u043d\", null],\n[\"\u043d\u044b\u043c\", \"\u043c\u0435\u043c\u043b\u0435\u043a\u0435\u0442\u0442\u0456\u043a\"],\n[\"\u2581\u0434\u0435\", null],\n[\"\u043f\u0430\u0440\", null],\n[\"\u0442\u0430\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u0442\u043e\u043c\", \"\u0434\u0435\u043f\u0430\u0440\u0442\u0430\u043c\u0435\u043d\u0442\"],\n[\"\u2581\u043f\u0440\u0438\", null],\n[\"\u043c\u0435\u043d\u0438\", null],\n[\"\u0442\u0435\u043b\u044c\u043d\u043e\", \"\u049b\u0430\u0442\u044b\u0441\u0442\u044b\"],\n[\"\u2581\u043a\", null],\n[\"\u2581\u0434\u0440\u0443\u0433\u0438\", null],\n[\"\u043c\", \"\u0431\u0430\u0441\u049b\u0430\"],\n[\"\u2581\u0441\u0442\u0440\u0430\", null],\n[\"\u043d\u0430\", null],\n[\"\u043c\", \"\u0435\u043b\u0434\u0435\u0440\"],\n[\",\", null],\n[\"\u2581\u043d\u043e\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\u0434\", null],\n[\"\u043d\u0435\u0435\", \"\u0441\u043e\u04a3\u0493\u044b\"],\n[\"\u2581\u0432\u0440\u0435\u043c\u044f\", \"\u0443\u0430\u049b\u044b\u0442\u0442\u0430\"],\n[\"\u2581\u043e\u0431\u0449\u0435\", null],\n[\"\u0441\u0442\u0432\u0435\u043d\", null],\n[\"\u043d\u044b\u0435\", \"\u049b\u043e\u0493\u0430\u043c\u0434\u044b\u049b\"],\n[\"\u2581\u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\", null],\n[\"\u0446\u0438\u0438\", \"\u04b1\u0439\u044b\u043c\u0434\u0430\u0440\"],\n[\"\u2581\u043f\u0440\u0438\", null],\n[\"\u0431\u0435\", null],\n[\"\u0433\", null],\n[\"\u043b\u0438\", \"\u0436\u04af\u0433\u0456\u043d\u0434\u0456\"],\n[\"\u2581\u043a\", null],\n[\"\u2581\u044d\u0442\u043e\", null],\n[\"\u0439\", \"\u0431\u04b1\u043b\"],\n[\"\u2581\u043c\", null],\n[\"\u0435\u0440\u0435\", \"\u0448\u0430\u0440\u0430\"],\n[\"\u2581\u0432\", null],\n[\"\u2581\u043e\u0442\", null],\n[\"\u0432\u0435\u0442\", \"\u0436\u0430\u0443\u0430\u043f\"],\n[\"\u2581\u043d\u0430\", null],\n[\"\u2581\u043e\", null],\n[\"\u043f\u0440\u0435\u0434\u0435\", null],\n[\"\u043b\u0451\u043d\", null],\n[\"\u043d\u044b\u0435\", \"\u0431\u0435\u043b\u0433\u0456\u043b\u0456\"],\n[\"\u2581\u0437\u0430\", null],\n[\"\u043a\u043e\", null],\n[\"\u043d\u044b\", \"\u0437\u0430\u04a3\u0434\u0430\u0440\"],\n[\"\u2581\u0438\", null],\n[\"\u2581\u0442\", null],\n[\"\u0435\u043d\", null],\n[\"\u0434\u0435\u043d\", null],\n[\"\u0446\u0438\u0438\", \"\u04af\u0440\u0434\u0456\u0441\u0442\u0435\u0440\"],\n[\"\u2581\u0432\", null],\n[\"\u2581\u043f\u0440\u0435\", null],\n[\"\u0434\u0435\u043b\u0430\", null],\n[\"\u0445\", \"\u0448\u0435\u0433\u0456\u043d\u0434\u0435\"],\n[\"\u2581\u0421\u0428\u0410\", \"\u0410\u049a\u0428\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u041a\u0430\u043a\u0438\u0435 \u0431\u044b \u0442\u043e \u043d\u0438 \u0431\u044b\u043b\u043e \u043f\u043e\u0441\u043b\u0435\u0434\u0441\u0442\u0432\u0438\u044f \u043c\u043e\u0433\u0443\u0442 \u043d\u0430\u0441\u0442\u0443\u043f\u0438\u0442\u044c \u0434\u0430\u043b\u0435\u0435 \u043f\u043e \u0446\u0435\u043f\u043e\u0447\u043a\u0435, \u0434\u043b\u044f \u00ab\u041f\u0430\u0440\u0438 \u0421\u0435\u043d-\u0416\u0435\u0440\u043c\u0435\u043d\u00bb \u0441\u043e \u0441\u0442\u043e\u0440\u043e\u043d\u044b \u0423\u0415\u0424\u0410, \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u044f\u0449\u0435\u0433\u043e \u043e\u0440\u0433\u0430\u043d\u0430, \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u0443\u044e\u0449\u0435\u0433\u043e \u0435\u0432\u0440\u043e\u043f\u0435\u0439\u0441\u043a\u0438\u0439 \u0444\u0443\u0442\u0431\u043e\u043b, \u0432 \u0447\u044c\u0435\u0439 \u0432\u043b\u0430\u0441\u0442\u0438 \u0437\u0430\u043f\u0440\u0435\u0449\u0430\u0442\u044c \u043a\u043e\u043c\u0430\u043d\u0434\u0430\u043c \u0443\u0447\u0430\u0441\u0442\u0438\u0435 \u0432 \u043f\u0440\u0435\u0441\u0442\u0438\u0436\u043d\u043e\u043c \u0442\u0443\u0440\u043d\u0438\u0440\u0435 \u00ab\u041b\u0438\u0433\u0430 \u0447\u0435\u043c\u043f\u0438\u043e\u043d\u043e\u0432\u00bb.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041a\u0430', '\u043a\u0438', '\u0435', '\u2581\u0431\u044b', '\u2581\u0442\u043e', '\u2581\u043d\u0438', '\u2581\u0431\u044b\u043b\u043e', '\u2581\u043f\u043e\u0441\u043b\u0435\u0434', '\u0441\u0442\u0432\u0438', '\u044f', '\u2581\u043c\u043e', '\u0433\u0443', '\u0442', '\u2581\u043d\u0430', '\u0441\u0442\u0443\u043f\u0438', '\u0442\u044c', '\u2581\u0434\u0430', '\u043b\u0435\u0435', '\u2581\u043f\u043e', '\u2581\u0446\u0435', '\u043f\u043e', '\u0447', '\u043a\u0435', ',', '\u2581\u0434\u043b\u044f', '\u2581\u00ab', '\u041f', '\u0430', '\u0440\u0438', '\u2581\u0421', '\u0435\u043d', '-', '\u0416', '\u0435\u0440', '\u043c\u0435\u043d', '\u00bb', '\u2581\u0441\u043e', '\u2581\u0441\u0442\u043e\u0440\u043e', '\u043d\u044b', '\u2581\u0423', '\u0415', '\u0424', '\u0410', ',', '\u2581\u0440\u0443\u043a\u043e', '\u0432\u043e\u0434', '\u044f', '\u0449\u0435', '\u0433\u043e', '\u2581\u043e\u0440\u0433\u0430', '\u043d\u0430', ',', '\u2581\u043a\u043e\u043d', '\u0442\u0440\u043e', '\u043b\u0438', '\u0440\u0443', '\u044e\u0449\u0435', '\u0433\u043e', '\u2581\u0435', '\u0432\u0440\u043e', '\u043f\u0435\u0439', '\u0441\u043a\u0438\u0439', '\u2581\u0444\u0443\u0442\u0431\u043e', '\u043b', ',', '\u2581\u0432', '\u2581\u0447', '\u044c', '\u0435\u0439', '\u2581\u0432\u043b\u0430', '\u0441\u0442\u0438', '\u2581\u0437\u0430', '\u043f\u0440\u0435', '\u0449\u0430', '\u0442\u044c', '\u2581\u043a\u043e\u043c\u0430\u043d', '\u0434\u0430', '\u043c', '\u2581\u0443\u0447\u0430\u0441\u0442\u0438', '\u0435', '\u2581\u0432', '\u2581\u043f\u0440\u0435', '\u0441\u0442\u0438', '\u0436', '\u043d\u043e\u043c', '\u2581\u0442\u0443\u0440', '\u043d\u0438', '\u0440\u0435', '\u2581\u00ab', '\u041b', '\u0438', '\u0433\u0430', '\u2581\u0447\u0435\u043c\u043f\u0438', '\u043e', '\u043d\u043e\u0432', '\u00bb.']"}], "[\n[\"\u2581\u041a\u0430\", \"Whatever\"],\n[\"\u043a\u0438\", null],\n[\"\u0435\", null],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u0442\u043e\", null],\n[\"\u2581\u043d\u0438\", null],\n[\"\u2581\u0431\u044b\u043b\u043e\", null],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\u0434\", \"consequences\"],\n[\"\u0441\u0442\u0432\u0438\", null],\n[\"\u044f\", null],\n[\"\u2581\u043c\u043e\", \"may\"],\n[\"\u0433\u0443\", null],\n[\"\u0442\", null],\n[\"\u2581\u043d\u0430\", \"occur\"],\n[\"\u0441\u0442\u0443\u043f\u0438\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0434\u0430\", \"further\"],\n[\"\u043b\u0435\u0435\", null],\n[\"\u2581\u043f\u043e\", \"down\"],\n[\"\u2581\u0446\u0435\", \"chain\"],\n[\"\u043f\u043e\", null],\n[\"\u0447\", null],\n[\"\u043a\u0435\", null],\n[\",\", null],\n[\"\u2581\u0434\u043b\u044f\", \"for\"],\n[\"\u2581\u00ab\", null],\n[\"\u041f\", \"Paris\"],\n[\"\u0430\", null],\n[\"\u0440\u0438\", null],\n[\"\u2581\u0421\", \"Saint\"],\n[\"\u0435\u043d\", null],\n[\"-\", null],\n[\"\u0416\", \"Germain\"],\n[\"\u0435\u0440\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u00bb\", null],\n[\"\u2581\u0441\u043e\", \"from\"],\n[\"\u2581\u0441\u0442\u043e\u0440\u043e\", \"side\"],\n[\"\u043d\u044b\", null],\n[\"\u2581\u0423\", \"UEFA\"],\n[\"\u0415\", null],\n[\"\u0424\", null],\n[\"\u0410\", null],\n[\",\", null],\n[\"\u2581\u0440\u0443\u043a\u043e\", \"governing\"],\n[\"\u0432\u043e\u0434\", null],\n[\"\u044f\", null],\n[\"\u0449\u0435\", null],\n[\"\u0433\u043e\", null],\n[\"\u2581\u043e\u0440\u0433\u0430\", \"body\"],\n[\"\u043d\u0430\", null],\n[\",\", null],\n[\"\u2581\u043a\u043e\u043d\", \"controlling\"],\n[\"\u0442\u0440\u043e\", null],\n[\"\u043b\u0438\", null],\n[\"\u0440\u0443\", null],\n[\"\u044e\u0449\u0435\", null],\n[\"\u0433\u043e\", null],\n[\"\u2581\u0435\", \"European\"],\n[\"\u0432\u0440\u043e\", null],\n[\"\u043f\u0435\u0439\", null],\n[\"\u0441\u043a\u0438\u0439\", null],\n[\"\u2581\u0444\u0443\u0442\u0431\u043e\", \"football\"],\n[\"\u043b\", null],\n[\",\", null],\n[\"\u2581\u0432\", \"in\"],\n[\"\u2581\u0447\", \"whose\"],\n[\"\u044c\", null],\n[\"\u0435\u0439\", null],\n[\"\u2581\u0432\u043b\u0430\", \"power\"],\n[\"\u0441\u0442\u0438\", null],\n[\"\u2581\u0437\u0430\", \"prohibit\"],\n[\"\u043f\u0440\u0435\", null],\n[\"\u0449\u0430\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043a\u043e\u043c\u0430\u043d\", \"teams\"],\n[\"\u0434\u0430\", null],\n[\"\u043c\", null],\n[\"\u2581\u0443\u0447\u0430\u0441\u0442\u0438\", \"participation\"],\n[\"\u0435\", null],\n[\"\u2581\u0432\", \"in\"],\n[\"\u2581\u043f\u0440\u0435\", \"prestigious\"],\n[\"\u0441\u0442\u0438\", null],\n[\"\u0436\", null],\n[\"\u043d\u043e\u043c\", null],\n[\"\u2581\u0442\u0443\u0440\", \"tournament\"],\n[\"\u043d\u0438\", null],\n[\"\u0440\u0435\", null],\n[\"\u2581\u00ab\", null],\n[\"\u041b\", \"Champions\"],\n[\"\u0438\", null],\n[\"\u0433\u0430\", null],\n[\"\u2581\u0447\u0435\u043c\u043f\u0438\", \"League\"],\n[\"\u043e\", null],\n[\"\u043d\u043e\u0432\", null],\n[\"\u00bb\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u041a\u0430\u043a\u0438\u0435 \u0431\u044b \u0442\u043e \u043d\u0438 \u0431\u044b\u043b\u043e \u043f\u043e\u0441\u043b\u0435\u0434\u0441\u0442\u0432\u0438\u044f \u043c\u043e\u0433\u0443\u0442 \u043d\u0430\u0441\u0442\u0443\u043f\u0438\u0442\u044c \u0434\u0430\u043b\u0435\u0435 \u043f\u043e \u0446\u0435\u043f\u043e\u0447\u043a\u0435, \u0434\u043b\u044f \u00ab\u041f\u0430\u0440\u0438 \u0421\u0435\u043d-\u0416\u0435\u0440\u043c\u0435\u043d\u00bb \u0441\u043e \u0441\u0442\u043e\u0440\u043e\u043d\u044b \u0423\u0415\u0424\u0410, \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u044f\u0449\u0435\u0433\u043e \u043e\u0440\u0433\u0430\u043d\u0430, \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u0443\u044e\u0449\u0435\u0433\u043e \u0435\u0432\u0440\u043e\u043f\u0435\u0439\u0441\u043a\u0438\u0439 \u0444\u0443\u0442\u0431\u043e\u043b, \u0432 \u0447\u044c\u0435\u0439 \u0432\u043b\u0430\u0441\u0442\u0438 \u0437\u0430\u043f\u0440\u0435\u0449\u0430\u0442\u044c \u043a\u043e\u043c\u0430\u043d\u0434\u0430\u043c \u0443\u0447\u0430\u0441\u0442\u0438\u0435 \u0432 \u043f\u0440\u0435\u0441\u0442\u0438\u0436\u043d\u043e\u043c \u0442\u0443\u0440\u043d\u0438\u0440\u0435 \u00ab\u041b\u0438\u0433\u0430 \u0447\u0435\u043c\u043f\u0438\u043e\u043d\u043e\u0432\u00bb.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041a\u0430', '\u043a\u0438', '\u0435', '\u2581\u0431\u044b', '\u2581\u0442\u043e', '\u2581\u043d\u0438', '\u2581\u0431\u044b\u043b\u043e', '\u2581\u043f\u043e\u0441\u043b\u0435\u0434', '\u0441\u0442\u0432\u0438', '\u044f', '\u2581\u043c\u043e', '\u0433\u0443', '\u0442', '\u2581\u043d\u0430', '\u0441\u0442\u0443\u043f\u0438', '\u0442\u044c', '\u2581\u0434\u0430', '\u043b\u0435\u0435', '\u2581\u043f\u043e', '\u2581\u0446\u0435', '\u043f\u043e', '\u0447', '\u043a\u0435', ',', '\u2581\u0434\u043b\u044f', '\u2581\u00ab', '\u041f', '\u0430', '\u0440\u0438', '\u2581\u0421', '\u0435\u043d', '-', '\u0416', '\u0435\u0440', '\u043c\u0435\u043d', '\u00bb', '\u2581\u0441\u043e', '\u2581\u0441\u0442\u043e\u0440\u043e', '\u043d\u044b', '\u2581\u0423', '\u0415', '\u0424', '\u0410', ',', '\u2581\u0440\u0443\u043a\u043e', '\u0432\u043e\u0434', '\u044f', '\u0449\u0435', '\u0433\u043e', '\u2581\u043e\u0440\u0433\u0430', '\u043d\u0430', ',', '\u2581\u043a\u043e\u043d', '\u0442\u0440\u043e', '\u043b\u0438', '\u0440\u0443', '\u044e\u0449\u0435', '\u0433\u043e', '\u2581\u0435', '\u0432\u0440\u043e', '\u043f\u0435\u0439', '\u0441\u043a\u0438\u0439', '\u2581\u0444\u0443\u0442\u0431\u043e', '\u043b', ',', '\u2581\u0432', '\u2581\u0447', '\u044c', '\u0435\u0439', '\u2581\u0432\u043b\u0430', '\u0441\u0442\u0438', '\u2581\u0437\u0430', '\u043f\u0440\u0435', '\u0449\u0430', '\u0442\u044c', '\u2581\u043a\u043e\u043c\u0430\u043d', '\u0434\u0430', '\u043c', '\u2581\u0443\u0447\u0430\u0441\u0442\u0438', '\u0435', '\u2581\u0432', '\u2581\u043f\u0440\u0435', '\u0441\u0442\u0438', '\u0436', '\u043d\u043e\u043c', '\u2581\u0442\u0443\u0440', '\u043d\u0438', '\u0440\u0435', '\u2581\u00ab', '\u041b', '\u0438', '\u0433\u0430', '\u2581\u0447\u0435\u043c\u043f\u0438', '\u043e', '\u043d\u043e\u0432', '\u00bb.']"}], "[\n[\"\u2581\u041a\u0430\", \"\u4efb\u4f55\"],\n[\"\u043a\u0438\", null],\n[\"\u0435\", null],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u0442\u043e\", null],\n[\"\u2581\u043d\u0438\", null],\n[\"\u2581\u0431\u044b\u043b\u043e\", null],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\u0434\", \"\u540e\u679c\"],\n[\"\u0441\u0442\u0432\u0438\", null],\n[\"\u044f\", null],\n[\"\u2581\u043c\u043e\", \"\u53ef\u80fd\"],\n[\"\u0433\u0443\", null],\n[\"\u0442\", null],\n[\"\u2581\u043d\u0430\", \"\u53d1\u751f\"],\n[\"\u0441\u0442\u0443\u043f\u0438\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0434\u0430\", \"\u63a5\u4e0b\u6765\"],\n[\"\u043b\u0435\u0435\", null],\n[\"\u2581\u043f\u043e\", \"\u6cbf\u7740\"],\n[\"\u2581\u0446\u0435\", \"\u94fe\u6761\"],\n[\"\u043f\u043e\", null],\n[\"\u0447\", null],\n[\"\u043a\u0435\", null],\n[\",\", null],\n[\"\u2581\u0434\u043b\u044f\", \"\u5bf9\u4e8e\"],\n[\"\u2581\u00ab\", null],\n[\"\u041f\", \"\u5df4\u9ece\"],\n[\"\u0430\", null],\n[\"\u0440\u0438\", null],\n[\"\u2581\u0421\", \"\u5723\u65e5\u8033\u66fc\"],\n[\"\u0435\u043d\", null],\n[\"-\", null],\n[\"\u0416\", null],\n[\"\u0435\u0440\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u00bb\", null],\n[\"\u2581\u0441\u043e\", \"\u6765\u81ea\"],\n[\"\u2581\u0441\u0442\u043e\u0440\u043e\", \"\u65b9\u9762\"],\n[\"\u043d\u044b\", null],\n[\"\u2581\u0423\", \"\u6b27\u8db3\u8054\"],\n[\"\u0415\", null],\n[\"\u0424\", null],\n[\"\u0410\", null],\n[\",\", null],\n[\"\u2581\u0440\u0443\u043a\u043e\", \"\u7ba1\u7406\"],\n[\"\u0432\u043e\u0434\", null],\n[\"\u044f\", null],\n[\"\u0449\u0435\", null],\n[\"\u0433\u043e\", null],\n[\"\u2581\u043e\u0440\u0433\u0430\", \"\u673a\u6784\"],\n[\"\u043d\u0430\", null],\n[\",\", null],\n[\"\u2581\u043a\u043e\u043d\", \"\u63a7\u5236\"],\n[\"\u0442\u0440\u043e\", null],\n[\"\u043b\u0438\", null],\n[\"\u0440\u0443\", null],\n[\"\u044e\u0449\u0435\", null],\n[\"\u0433\u043e\", null],\n[\"\u2581\u0435\", \"\u6b27\u6d32\"],\n[\"\u0432\u0440\u043e\", null],\n[\"\u043f\u0435\u0439\", null],\n[\"\u0441\u043a\u0438\u0439\", null],\n[\"\u2581\u0444\u0443\u0442\u0431\u043e\", \"\u8db3\u7403\"],\n[\"\u043b\", null],\n[\",\", null],\n[\"\u2581\u0432\", \"\u5728\"],\n[\"\u2581\u0447\", \"\u5176\"],\n[\"\u044c\", null],\n[\"\u0435\u0439\", null],\n[\"\u2581\u0432\u043b\u0430\", \"\u6743\u529b\"],\n[\"\u0441\u0442\u0438\", null],\n[\"\u2581\u0437\u0430\", \"\u7981\u6b62\"],\n[\"\u043f\u0440\u0435\", null],\n[\"\u0449\u0430\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043a\u043e\u043c\u0430\u043d\", \"\u7403\u961f\"],\n[\"\u0434\u0430\", null],\n[\"\u043c\", null],\n[\"\u2581\u0443\u0447\u0430\u0441\u0442\u0438\", \"\u53c2\u52a0\"],\n[\"\u0435\", null],\n[\"\u2581\u0432\", \"\u5728\"],\n[\"\u2581\u043f\u0440\u0435\", \"prestigio\"],\n[\"\u0441\u0442\u0438\", null],\n[\"\u0436\", null],\n[\"\u043d\u043e\u043c\", null],\n[\"\u2581\u0442\u0443\u0440\", \"\u6bd4\u8d5b\"],\n[\"\u043d\u0438\", null],\n[\"\u0440\u0435\", null],\n[\"\u2581\u00ab\", null],\n[\"\u041b\", \"\u51a0\u519b\"],\n[\"\u0438\", null],\n[\"\u0433\u0430\", null],\n[\"\u2581\u0447\u0435\u043c\u043f\u0438\", \"\u8054\u8d5b\"],\n[\"\u043e\", null],\n[\"\u043d\u043e\u0432\", null],\n[\"\u00bb\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u041a\u0430\u043a\u0438\u0435 \u0431\u044b \u0442\u043e \u043d\u0438 \u0431\u044b\u043b\u043e \u043f\u043e\u0441\u043b\u0435\u0434\u0441\u0442\u0432\u0438\u044f \u043c\u043e\u0433\u0443\u0442 \u043d\u0430\u0441\u0442\u0443\u043f\u0438\u0442\u044c \u0434\u0430\u043b\u0435\u0435 \u043f\u043e \u0446\u0435\u043f\u043e\u0447\u043a\u0435, \u0434\u043b\u044f \u00ab\u041f\u0430\u0440\u0438 \u0421\u0435\u043d-\u0416\u0435\u0440\u043c\u0435\u043d\u00bb \u0441\u043e \u0441\u0442\u043e\u0440\u043e\u043d\u044b \u0423\u0415\u0424\u0410, \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u044f\u0449\u0435\u0433\u043e \u043e\u0440\u0433\u0430\u043d\u0430, \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u0443\u044e\u0449\u0435\u0433\u043e \u0435\u0432\u0440\u043e\u043f\u0435\u0439\u0441\u043a\u0438\u0439 \u0444\u0443\u0442\u0431\u043e\u043b, \u0432 \u0447\u044c\u0435\u0439 \u0432\u043b\u0430\u0441\u0442\u0438 \u0437\u0430\u043f\u0440\u0435\u0449\u0430\u0442\u044c \u043a\u043e\u043c\u0430\u043d\u0434\u0430\u043c \u0443\u0447\u0430\u0441\u0442\u0438\u0435 \u0432 \u043f\u0440\u0435\u0441\u0442\u0438\u0436\u043d\u043e\u043c \u0442\u0443\u0440\u043d\u0438\u0440\u0435 \u00ab\u041b\u0438\u0433\u0430 \u0447\u0435\u043c\u043f\u0438\u043e\u043d\u043e\u0432\u00bb.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u041a\u0430', '\u043a\u0438', '\u0435', '\u2581\u0431\u044b', '\u2581\u0442\u043e', '\u2581\u043d\u0438', '\u2581\u0431\u044b\u043b\u043e', '\u2581\u043f\u043e\u0441\u043b\u0435\u0434', '\u0441\u0442\u0432\u0438', '\u044f', '\u2581\u043c\u043e', '\u0433\u0443', '\u0442', '\u2581\u043d\u0430', '\u0441\u0442\u0443\u043f\u0438', '\u0442\u044c', '\u2581\u0434\u0430', '\u043b\u0435\u0435', '\u2581\u043f\u043e', '\u2581\u0446\u0435', '\u043f\u043e', '\u0447', '\u043a\u0435', ',', '\u2581\u0434\u043b\u044f', '\u2581\u00ab', '\u041f', '\u0430', '\u0440\u0438', '\u2581\u0421', '\u0435\u043d', '-', '\u0416', '\u0435\u0440', '\u043c\u0435\u043d', '\u00bb', '\u2581\u0441\u043e', '\u2581\u0441\u0442\u043e\u0440\u043e', '\u043d\u044b', '\u2581\u0423', '\u0415', '\u0424', '\u0410', ',', '\u2581\u0440\u0443\u043a\u043e', '\u0432\u043e\u0434', '\u044f', '\u0449\u0435', '\u0433\u043e', '\u2581\u043e\u0440\u0433\u0430', '\u043d\u0430', ',', '\u2581\u043a\u043e\u043d', '\u0442\u0440\u043e', '\u043b\u0438', '\u0440\u0443', '\u044e\u0449\u0435', '\u0433\u043e', '\u2581\u0435', '\u0432\u0440\u043e', '\u043f\u0435\u0439', '\u0441\u043a\u0438\u0439', '\u2581\u0444\u0443\u0442\u0431\u043e', '\u043b', ',', '\u2581\u0432', '\u2581\u0447', '\u044c', '\u0435\u0439', '\u2581\u0432\u043b\u0430', '\u0441\u0442\u0438', '\u2581\u0437\u0430', '\u043f\u0440\u0435', '\u0449\u0430', '\u0442\u044c', '\u2581\u043a\u043e\u043c\u0430\u043d', '\u0434\u0430', '\u043c', '\u2581\u0443\u0447\u0430\u0441\u0442\u0438', '\u0435', '\u2581\u0432', '\u2581\u043f\u0440\u0435', '\u0441\u0442\u0438', '\u0436', '\u043d\u043e\u043c', '\u2581\u0442\u0443\u0440', '\u043d\u0438', '\u0440\u0435', '\u2581\u00ab', '\u041b', '\u0438', '\u0433\u0430', '\u2581\u0447\u0435\u043c\u043f\u0438', '\u043e', '\u043d\u043e\u0432', '\u00bb.']"}], "[\n[\"\u2581\u041a\u0430\", \"\u049a\u0430\u043d\u0434\u0430\u0439\"],\n[\"\u043a\u0438\", null],\n[\"\u0435\", null],\n[\"\u2581\u0431\u044b\", null],\n[\"\u2581\u0442\u043e\", null],\n[\"\u2581\u043d\u0438\", null],\n[\"\u2581\u0431\u044b\u043b\u043e\", \"\u0431\u043e\u043b\u0441\u0430\"],\n[\"\u2581\u043f\u043e\u0441\u043b\u0435\u0434\", \"\u0441\u0430\u043b\u0434\u0430\u0440\"],\n[\"\u0441\u0442\u0432\u0438\", null],\n[\"\u044f\", null],\n[\"\u2581\u043c\u043e\", \"\u043c\u04af\u043c\u043a\u0456\u043d\"],\n[\"\u0433\u0443\", null],\n[\"\u0442\", null],\n[\"\u2581\u043d\u0430\", \"\u0431\u0430\u0441\u0442\u0430\u043b\u0443\u044b\"],\n[\"\u0441\u0442\u0443\u043f\u0438\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u0434\u0430\", \"\u04d9\u0440\u0456\"],\n[\"\u043b\u0435\u0435\", null],\n[\"\u2581\u043f\u043e\", \"\u0431\u043e\u0439\u044b\u043d\u0448\u0430\"],\n[\"\u2581\u0446\u0435\", \"\u0442\u0456\u0437\u0431\u0435\u043a\"],\n[\"\u043f\u043e\", null],\n[\"\u0447\", null],\n[\"\u043a\u0435\", null],\n[\",\", null],\n[\"\u2581\u0434\u043b\u044f\", \"\u04af\u0448\u0456\u043d\"],\n[\"\u2581\u00ab\", null],\n[\"\u041f\", \"\u041f\u0430\u0440\u0438\"],\n[\"\u0430\", null],\n[\"\u0440\u0438\", null],\n[\"\u2581\u0421\", \"\u0421\u0435\u043d\"],\n[\"\u0435\u043d\", null],\n[\"-\", null],\n[\"\u0416\", \"\u0416\u0435\u0440\u043c\u0435\u043d\"],\n[\"\u0435\u0440\", null],\n[\"\u043c\u0435\u043d\", null],\n[\"\u00bb\", null],\n[\"\u2581\u0441\u043e\", null],\n[\"\u2581\u0441\u0442\u043e\u0440\u043e\", \"\u0442\u0430\u0440\u0430\u043f\u044b\u043d\u0430\u043d\"],\n[\"\u043d\u044b\", null],\n[\"\u2581\u0423\", \"\u0423\u0415\u0424\u0410\"],\n[\"\u0415\", null],\n[\"\u0424\", null],\n[\"\u0410\", null],\n[\",\", null],\n[\"\u2581\u0440\u0443\u043a\u043e\", \"\u0431\u0430\u0441\u049b\u0430\u0440\u0443\u0448\u044b\"],\n[\"\u0432\u043e\u0434\", null],\n[\"\u044f\", null],\n[\"\u0449\u0435\", null],\n[\"\u0433\u043e\", null],\n[\"\u2581\u043e\u0440\u0433\u0430\", \"\u043e\u0440\u0433\u0430\u043d\"],\n[\"\u043d\u0430\", null],\n[\",\", null],\n[\"\u2581\u043a\u043e\u043d\", \"\u0431\u0430\u049b\u044b\u043b\u0430\u0439\u0442\u044b\u043d\"],\n[\"\u0442\u0440\u043e\", null],\n[\"\u043b\u0438\", null],\n[\"\u0440\u0443\", null],\n[\"\u044e\u0449\u0435\", null],\n[\"\u0433\u043e\", null],\n[\"\u2581\u0435\", \"\u0435\u0443\u0440\u043e\u043f\u0430\u043b\u044b\u049b\"],\n[\"\u0432\u0440\u043e\", null],\n[\"\u043f\u0435\u0439\", null],\n[\"\u0441\u043a\u0438\u0439\", null],\n[\"\u2581\u0444\u0443\u0442\u0431\u043e\", \"\u0444\u0443\u0442\u0431\u043e\u043b\"],\n[\"\u043b\", null],\n[\",\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0447\", \"\u04e9\u043a\u0456\u043b\u0435\u0442\u0442\u0456\u0433\u0456\u043d\u0434\u0435\"],\n[\"\u044c\", null],\n[\"\u0435\u0439\", null],\n[\"\u2581\u0432\u043b\u0430\", \"\u049b\u04b1\u0437\u044b\u0440\u044b\u043d\u0434\u0430\"],\n[\"\u0441\u0442\u0438\", null],\n[\"\u2581\u0437\u0430\", \"\u0442\u044b\u0439\u044b\u043c\"],\n[\"\u043f\u0440\u0435\", null],\n[\"\u0449\u0430\", null],\n[\"\u0442\u044c\", null],\n[\"\u2581\u043a\u043e\u043c\u0430\u043d\", \"\u043a\u043e\u043c\u0430\u043d\u0434\u0430\u043b\u0430\u0440\u0493\u0430\"],\n[\"\u0434\u0430\", null],\n[\"\u043c\", null],\n[\"\u2581\u0443\u0447\u0430\u0441\u0442\u0438\", \"\u049b\u0430\u0442\u044b\u0441\u0443\u0493\u0430\"],\n[\"\u0435\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u043f\u0440\u0435\", \"\u0431\u0435\u0434\u0435\u043b\u0434\u0456\"],\n[\"\u0441\u0442\u0438\", null],\n[\"\u0436\", null],\n[\"\u043d\u043e\u043c\", null],\n[\"\u2581\u0442\u0443\u0440\", \"\u0442\u0443\u0440\u043d\u0438\u0440\"],\n[\"\u043d\u0438\", null],\n[\"\u0440\u0435\", null],\n[\"\u2581\u00ab\", null],\n[\"\u041b\", \"\u0427\u0435\u043c\u043f\u0438\u043e\u043d\u0434\u0430\u0440\"],\n[\"\u0438\", null],\n[\"\u0433\u0430\", null],\n[\"\u2581\u0447\u0435\u043c\u043f\u0438\", \"\u041b\u0438\u0433\u0430\u0441\u044b\"],\n[\"\u043e\", null],\n[\"\u043d\u043e\u0432\", null],\n[\"\u00bb\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0412 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0435 \u0432 \u00ab\u0427\u044c\u0438 \u0443\u043b\u0438\u0446\u044b?\u00bb \u043f\u043e\u043a\u0430\u0437\u0430\u043d\u044b \u043d\u0435\u043e\u0436\u0438\u0434\u0430\u043d\u043d\u043e \u043b\u0438\u0447\u043d\u044b\u0435 \u0430\u0441\u043f\u0435\u043a\u0442\u044b \u0436\u0438\u0437\u043d\u0438 \u0424\u0435\u0440\u0440\u0435\u043b, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0442\u043e, \u043a\u0430\u043a \u043e\u043d\u0430 \u0432\u043b\u044e\u0431\u0438\u043b\u0430\u0441\u044c \u0438 \u0437\u0430\u043a\u043b\u044e\u0447\u0438\u043b\u0430 \u0431\u0440\u0430\u043a \u0441\u043e \u0441\u0432\u043e\u0435\u0439 \u0436\u0435\u043d\u043e\u0439.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0412', '\u2581\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430', '\u0442\u0435', '\u2581\u0432', '\u2581\u00ab', '\u0427', '\u044c', '\u0438', '\u2581\u0443\u043b\u0438', '\u0446\u044b', '?', '\u00bb', '\u2581\u043f\u043e', '\u043a\u0430\u0437\u0430', '\u043d\u044b', '\u2581\u043d\u0435', '\u043e', '\u0436\u0438', '\u0434\u0430\u043d', '\u043d\u043e', '\u2581\u043b\u0438', '\u0447', '\u043d\u044b\u0435', '\u2581\u0430', '\u0441\u043f\u0435', '\u043a', '\u0442\u044b', '\u2581\u0436\u0438', '\u0437\u043d\u0438', '\u2581\u0424', '\u0435\u0440', '\u0440\u0435', '\u043b', ',', '\u2581\u043d\u0430', '\u043f\u0440\u0438', '\u043c\u0435\u0440', ',', '\u2581\u0442\u043e', ',', '\u2581\u043a\u0430\u043a', '\u2581\u043e\u043d\u0430', '\u2581\u0432', '\u043b\u044e', '\u0431\u0438', '\u043b\u0430\u0441\u044c', '\u2581\u0438', '\u2581\u0437\u0430', '\u043a\u043b\u044e', '\u0447\u0438', '\u043b\u0430', '\u2581\u0431', '\u0440\u0430\u043a', '\u2581\u0441\u043e', '\u2581\u0441\u0432\u043e', '\u0435\u0439', '\u2581\u0436\u0435', '\u043d\u043e\u0439', '.']"}], "[\n[\"\u2581\u0412\", \"As\"],\n[\"\u2581\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\", \"result\"],\n[\"\u0442\u0435\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u00ab\", null],\n[\"\u0427\", \"Whose\"],\n[\"\u044c\", null],\n[\"\u0438\", null],\n[\"\u2581\u0443\u043b\u0438\", \"streets\"],\n[\"\u0446\u044b\", null],\n[\"?\", null],\n[\"\u00bb\", null],\n[\"\u2581\u043f\u043e\", \"shown\"],\n[\"\u043a\u0430\u0437\u0430\", null],\n[\"\u043d\u044b\", null],\n[\"\u2581\u043d\u0435\", \"unexpectedly\"],\n[\"\u043e\", null],\n[\"\u0436\u0438\", null],\n[\"\u0434\u0430\u043d\", null],\n[\"\u043d\u043e\", null],\n[\"\u2581\u043b\u0438\", \"personal\"],\n[\"\u0447\", null],\n[\"\u043d\u044b\u0435\", null],\n[\"\u2581\u0430\", \"aspects\"],\n[\"\u0441\u043f\u0435\", null],\n[\"\u043a\", null],\n[\"\u0442\u044b\", null],\n[\"\u2581\u0436\u0438\", \"life\"],\n[\"\u0437\u043d\u0438\", null],\n[\"\u2581\u0424\", \"Ferrell\"],\n[\"\u0435\u0440\", null],\n[\"\u0440\u0435\", null],\n[\"\u043b\", null],\n[\",\", null],\n[\"\u2581\u043d\u0430\", \"for\"],\n[\"\u043f\u0440\u0438\", null],\n[\"\u043c\u0435\u0440\", \"example\"],\n[\",\", null],\n[\"\u2581\u0442\u043e\", \"how\"],\n[\",\", null],\n[\"\u2581\u043a\u0430\u043a\", null],\n[\"\u2581\u043e\u043d\u0430\", \"she\"],\n[\"\u2581\u0432\", \"fell\"],\n[\"\u043b\u044e\", null],\n[\"\u0431\u0438\", null],\n[\"\u043b\u0430\u0441\u044c\", null],\n[\"\u2581\u0438\", \"and\"],\n[\"\u2581\u0437\u0430\", \"got\"],\n[\"\u043a\u043b\u044e\", null],\n[\"\u0447\u0438\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\u0431\", \"married\"],\n[\"\u0440\u0430\u043a\", null],\n[\"\u2581\u0441\u043e\", \"to\"],\n[\"\u2581\u0441\u0432\u043e\", \"her\"],\n[\"\u0435\u0439\", null],\n[\"\u2581\u0436\u0435\", \"wife\"],\n[\"\u043d\u043e\u0439\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u0412 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0435 \u0432 \u00ab\u0427\u044c\u0438 \u0443\u043b\u0438\u0446\u044b?\u00bb \u043f\u043e\u043a\u0430\u0437\u0430\u043d\u044b \u043d\u0435\u043e\u0436\u0438\u0434\u0430\u043d\u043d\u043e \u043b\u0438\u0447\u043d\u044b\u0435 \u0430\u0441\u043f\u0435\u043a\u0442\u044b \u0436\u0438\u0437\u043d\u0438 \u0424\u0435\u0440\u0440\u0435\u043b, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0442\u043e, \u043a\u0430\u043a \u043e\u043d\u0430 \u0432\u043b\u044e\u0431\u0438\u043b\u0430\u0441\u044c \u0438 \u0437\u0430\u043a\u043b\u044e\u0447\u0438\u043b\u0430 \u0431\u0440\u0430\u043a \u0441\u043e \u0441\u0432\u043e\u0435\u0439 \u0436\u0435\u043d\u043e\u0439.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0412', '\u2581\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430', '\u0442\u0435', '\u2581\u0432', '\u2581\u00ab', '\u0427', '\u044c', '\u0438', '\u2581\u0443\u043b\u0438', '\u0446\u044b', '?', '\u00bb', '\u2581\u043f\u043e', '\u043a\u0430\u0437\u0430', '\u043d\u044b', '\u2581\u043d\u0435', '\u043e', '\u0436\u0438', '\u0434\u0430\u043d', '\u043d\u043e', '\u2581\u043b\u0438', '\u0447', '\u043d\u044b\u0435', '\u2581\u0430', '\u0441\u043f\u0435', '\u043a', '\u0442\u044b', '\u2581\u0436\u0438', '\u0437\u043d\u0438', '\u2581\u0424', '\u0435\u0440', '\u0440\u0435', '\u043b', ',', '\u2581\u043d\u0430', '\u043f\u0440\u0438', '\u043c\u0435\u0440', ',', '\u2581\u0442\u043e', ',', '\u2581\u043a\u0430\u043a', '\u2581\u043e\u043d\u0430', '\u2581\u0432', '\u043b\u044e', '\u0431\u0438', '\u043b\u0430\u0441\u044c', '\u2581\u0438', '\u2581\u0437\u0430', '\u043a\u043b\u044e', '\u0447\u0438', '\u043b\u0430', '\u2581\u0431', '\u0440\u0430\u043a', '\u2581\u0441\u043e', '\u2581\u0441\u0432\u043e', '\u0435\u0439', '\u2581\u0436\u0435', '\u043d\u043e\u0439', '.']"}], "[\n  [\"\u2581\u0412\", \"\u7ed3\u679c\"],\n  [\"\u2581\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\", null],\n  [\"\u0442\u0435\", null],\n  [\"\u2581\u0432\", null],\n  [\"\u2581\u00ab\", null],\n  [\"\u0427\", \"\u8c01\u7684\"],\n  [\"\u044c\", null],\n  [\"\u0438\", null],\n  [\"\u2581\u0443\u043b\u0438\", null],\n  [\"\u0446\u044b\", \"\u8857\u9053\"],\n  [\"?\", null],\n  [\"\u00bb\", null],\n  [\"\u2581\u043f\u043e\", \"\u5c55\u793a\"],\n  [\"\u043a\u0430\u0437\u0430\", null],\n  [\"\u043d\u044b\", null],\n  [\"\u2581\u043d\u0435\", \"\u610f\u5916\"],\n  [\"\u043e\", null],\n  [\"\u0436\u0438\", null],\n  [\"\u0434\u0430\u043d\", null],\n  [\"\u043d\u043e\", null],\n  [\"\u2581\u043b\u0438\", \"\u4e2a\u4eba\"],\n  [\"\u0447\", null],\n  [\"\u043d\u044b\u0435\", null],\n  [\"\u2581\u0430\", \"\u65b9\u9762\"],\n  [\"\u0441\u043f\u0435\", null],\n  [\"\u043a\", null],\n  [\"\u0442\u044b\", null],\n  [\"\u2581\u0436\u0438\", \"\u751f\u6d3b\"],\n  [\"\u0437\u043d\u0438\", null],\n  [\"\u2581\u0424\", \"\u8d39\u96f7\u5c14\"],\n  [\"\u0435\u0440\", null],\n  [\"\u0440\u0435\", null],\n  [\"\u043b\", null],\n  [\",\", null],\n  [\"\u2581\u043d\u0430\", \"\u4f8b\u5982\"],\n  [\"\u043f\u0440\u0438\", null],\n  [\"\u043c\u0435\u0440\", null],\n  [\",\", null],\n  [\"\u2581\u0442\u043e\", \"\u5979\"],\n  [\",\", null],\n  [\"\u2581\u043a\u0430\u043a\", \"\u5982\u4f55\"],\n  [\"\u2581\u043e\u043d\u0430\", null],\n  [\"\u2581\u0432\", \"\u7231\u4e0a\"],\n  [\"\u043b\u044e\", null],\n  [\"\u0431\u0438\", null],\n  [\"\u043b\u0430\u0441\u044c\", null],\n  [\"\u2581\u0438\", \"\u5e76\"],\n  [\"\u2581\u0437\u0430\", \"\u7ed3\u5a5a\"],\n  [\"\u043a\u043b\u044e\", null],\n  [\"\u0447\u0438\", null],\n  [\"\u043b\u0430\", null],\n  [\"\u2581\u0431\", null],\n  [\"\u0440\u0430\u043a\", null],\n  [\"\u2581\u0441\u043e\", \"\u4e0e\"],\n  [\"\u2581\u0441\u0432\u043e\", \"\u5979\u7684\"],\n  [\"\u0435\u0439\", null],\n  [\"\u2581\u0436\u0435\", \"\u59bb\u5b50\"],\n  [\"\u043d\u043e\u0439\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u0412 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0435 \u0432 \u00ab\u0427\u044c\u0438 \u0443\u043b\u0438\u0446\u044b?\u00bb \u043f\u043e\u043a\u0430\u0437\u0430\u043d\u044b \u043d\u0435\u043e\u0436\u0438\u0434\u0430\u043d\u043d\u043e \u043b\u0438\u0447\u043d\u044b\u0435 \u0430\u0441\u043f\u0435\u043a\u0442\u044b \u0436\u0438\u0437\u043d\u0438 \u0424\u0435\u0440\u0440\u0435\u043b, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0442\u043e, \u043a\u0430\u043a \u043e\u043d\u0430 \u0432\u043b\u044e\u0431\u0438\u043b\u0430\u0441\u044c \u0438 \u0437\u0430\u043a\u043b\u044e\u0447\u0438\u043b\u0430 \u0431\u0440\u0430\u043a \u0441\u043e \u0441\u0432\u043e\u0435\u0439 \u0436\u0435\u043d\u043e\u0439.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0412', '\u2581\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430', '\u0442\u0435', '\u2581\u0432', '\u2581\u00ab', '\u0427', '\u044c', '\u0438', '\u2581\u0443\u043b\u0438', '\u0446\u044b', '?', '\u00bb', '\u2581\u043f\u043e', '\u043a\u0430\u0437\u0430', '\u043d\u044b', '\u2581\u043d\u0435', '\u043e', '\u0436\u0438', '\u0434\u0430\u043d', '\u043d\u043e', '\u2581\u043b\u0438', '\u0447', '\u043d\u044b\u0435', '\u2581\u0430', '\u0441\u043f\u0435', '\u043a', '\u0442\u044b', '\u2581\u0436\u0438', '\u0437\u043d\u0438', '\u2581\u0424', '\u0435\u0440', '\u0440\u0435', '\u043b', ',', '\u2581\u043d\u0430', '\u043f\u0440\u0438', '\u043c\u0435\u0440', ',', '\u2581\u0442\u043e', ',', '\u2581\u043a\u0430\u043a', '\u2581\u043e\u043d\u0430', '\u2581\u0432', '\u043b\u044e', '\u0431\u0438', '\u043b\u0430\u0441\u044c', '\u2581\u0438', '\u2581\u0437\u0430', '\u043a\u043b\u044e', '\u0447\u0438', '\u043b\u0430', '\u2581\u0431', '\u0440\u0430\u043a', '\u2581\u0441\u043e', '\u2581\u0441\u0432\u043e', '\u0435\u0439', '\u2581\u0436\u0435', '\u043d\u043e\u0439', '.']"}], "[\n[\"\u2581\u0412\", \"\u041d\u04d9\u0442\u0438\u0436\u0435\u0441\u0456\u043d\u0434\u0435\"],\n[\"\u2581\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\", null],\n[\"\u0442\u0435\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u00ab\", null],\n[\"\u0427\", \"\u041a\u04e9\u0448\u0435\u043b\u0435\u0440\"],\n[\"\u044c\", null],\n[\"\u0438\", null],\n[\"\u2581\u0443\u043b\u0438\", null],\n[\"\u0446\u044b\", null],\n[\"?\", null],\n[\"\u00bb\", null],\n[\"\u2581\u043f\u043e\", \"\u043a\u04e9\u0440\u0441\u0435\u0442\u0456\u043b\u0433\u0435\u043d\"],\n[\"\u043a\u0430\u0437\u0430\", null],\n[\"\u043d\u044b\", null],\n[\"\u2581\u043d\u0435\", \"\u043a\u04af\u0442\u043f\u0435\u0433\u0435\u043d\"],\n[\"\u043e\", null],\n[\"\u0436\u0438\", null],\n[\"\u0434\u0430\u043d\", null],\n[\"\u043d\u043e\", null],\n[\"\u2581\u043b\u0438\", \"\u0436\u0435\u043a\u0435\"],\n[\"\u0447\", null],\n[\"\u043d\u044b\u0435\", null],\n[\"\u2581\u0430\", \"\u0430\u0441\u043f\u0435\u043a\u0442\u0456\u043b\u0435\u0440\u0456\"],\n[\"\u0441\u043f\u0435\", null],\n[\"\u043a\", null],\n[\"\u0442\u044b\", null],\n[\"\u2581\u0436\u0438\", \"\u04e9\u043c\u0456\u0440\u0456\u043d\u0456\u04a3\"],\n[\"\u0437\u043d\u0438\", null],\n[\"\u2581\u0424\", \"\u0424\u0435\u0440\u0440\u0435\u043b\u0434\u0456\u04a3\"],\n[\"\u0435\u0440\", null],\n[\"\u0440\u0435\", null],\n[\"\u043b\", null],\n[\",\", null],\n[\"\u2581\u043d\u0430\", \"\u043c\u044b\u0441\u0430\u043b\u044b\"],\n[\"\u043f\u0440\u0438\", null],\n[\"\u043c\u0435\u0440\", null],\n[\",\", null],\n[\"\u2581\u0442\u043e\", null],\n[\",\", null],\n[\"\u2581\u043a\u0430\u043a\", \"\u049b\u0430\u043b\u0430\u0439\"],\n[\"\u2581\u043e\u043d\u0430\", \"\u043e\u043b\"],\n[\"\u2581\u0432\", \"\u0493\u0430\u0448\u044b\u049b\"],\n[\"\u043b\u044e\", null],\n[\"\u0431\u0438\", null],\n[\"\u043b\u0430\u0441\u044c\", null],\n[\"\u2581\u0438\", \"\u0436\u04d9\u043d\u0435\"],\n[\"\u2581\u0437\u0430\", \"\u043d\u0435\u043a\u0435\u0433\u0435\"],\n[\"\u043a\u043b\u044e\", null],\n[\"\u0447\u0438\", null],\n[\"\u043b\u0430\", null],\n[\"\u2581\u0431\", null],\n[\"\u0440\u0430\u043a\", null],\n[\"\u2581\u0441\u043e\", null],\n[\"\u2581\u0441\u0432\u043e\", \"\u04e9\u0437\u0456\u043d\u0456\u04a3\"],\n[\"\u0435\u0439\", null],\n[\"\u2581\u0436\u0435\", \"\u04d9\u0439\u0435\u043b\u0456\u043c\u0435\u043d\"],\n[\"\u043d\u043e\u0439\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0420\u044d\u043f\u0435\u0440 \u0438\u0437 \u0411\u043e\u043b\u044c\u0448\u043e\u0433\u043e \u0421\u0435\u043d\u0442-\u041b\u0443\u0438\u0441\u0430 \u0432 \u0440\u0435\u0436\u0438\u043c\u0435 \u0440\u0435\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u043e\u0441\u0432\u0435\u0449\u0430\u043b \u0432 \u0442\u0432\u0438\u0442\u0442\u0435\u0440\u0435 \u0441\u043c\u0435\u0440\u0442\u044c \u041c\u0430\u0439\u043a\u043b\u0430 \u0411\u0440\u0430\u0443\u043d\u0430, \u0431\u0435\u0437\u043e\u0440\u0443\u0436\u043d\u043e\u0433\u043e \u0447\u0435\u0440\u043d\u043e\u043a\u043e\u0436\u0435\u0433\u043e \u0432\u043e\u0441\u0435\u043c\u043d\u0430\u0434\u0446\u0430\u0442\u0438\u043b\u0435\u0442\u043d\u0435\u0433\u043e \u043f\u043e\u0434\u0440\u043e\u0441\u0442\u0430, \u0443\u0431\u0438\u0442\u043e\u0433\u043e \u0431\u0435\u043b\u044b\u043c \u043e\u0444\u0438\u0446\u0435\u0440\u043e\u043c \u043f\u043e\u043b\u0438\u0446\u0438\u0438 \u0432 \u0424\u0435\u0440\u0433\u044e\u0441\u043e\u043d\u0435, \u0448\u0442\u0430\u0442 \u041c\u0438\u0441\u0441\u0443\u0440\u0438.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0420', '\u044d', '\u043f\u0435\u0440', '\u2581\u0438\u0437', '\u2581\u0411\u043e', '\u043b\u044c', '\u0448\u043e', '\u0433\u043e', '\u2581\u0421', '\u0435\u043d\u0442', '-', '\u041b', '\u0443', '\u0438', '\u0441\u0430', '\u2581\u0432', '\u2581\u0440\u0435\u0436\u0438', '\u043c\u0435', '\u2581\u0440\u0435', '\u0430\u043b\u044c', '\u043d\u043e\u0433\u043e', '\u2581\u0432\u0440\u0435', '\u043c\u0435\u043d\u0438', '\u2581\u043e\u0441', '\u0432\u0435', '\u0449\u0430', '\u043b', '\u2581\u0432', '\u2581\u0442', '\u0432\u0438', '\u0442', '\u0442\u0435', '\u0440\u0435', '\u2581\u0441\u043c\u0435\u0440', '\u0442\u044c', '\u2581\u041c\u0430', '\u0439', '\u043a\u043b\u0430', '\u2581\u0411\u0440\u0430', '\u0443', '\u043d\u0430', ',', '\u2581\u0431\u0435', '\u0437\u043e', '\u0440\u0443', '\u0436', '\u043d\u043e\u0433\u043e', '\u2581\u0447\u0435\u0440', '\u043d\u043e', '\u043a\u043e', '\u0436\u0435', '\u0433\u043e', '\u2581\u0432\u043e', '\u0441\u0435', '\u043c', '\u043d\u0430\u0434', '\u0446\u0430', '\u0442\u0438', '\u043b\u0435\u0442', '\u043d\u0435\u0433\u043e', '\u2581\u043f\u043e\u0434', '\u0440\u043e', '\u0441\u0442\u0430', ',', '\u2581\u0443\u0431\u0438', '\u0442\u043e\u0433\u043e', '\u2581\u0431\u0435', '\u043b\u044b', '\u043c', '\u2581\u043e\u0444\u0438', '\u0446\u0435', '\u0440\u043e\u043c', '\u2581\u043f\u043e\u043b\u0438', '\u0446\u0438\u0438', '\u2581\u0432', '\u2581\u0424', '\u0435\u0440', '\u0433', '\u044e', '\u0441\u043e', '\u043d\u0435', ',', '\u2581\u0448\u0442\u0430', '\u0442', '\u2581\u041c\u0438', '\u0441', '\u0441\u0443', '\u0440\u0438', '.']"}], "[\n[\"\u2581\u0420\", \"Rapper\"],\n[\"\u044d\", null],\n[\"\u043f\u0435\u0440\", null],\n[\"\u2581\u0438\u0437\", null],\n[\"\u2581\u0411\u043e\", \"Greater\"],\n[\"\u043b\u044c\", null],\n[\"\u0448\u043e\", null],\n[\"\u0433\u043e\", null],\n[\"\u2581\u0421\", \"Saint\"],\n[\"\u0435\u043d\u0442\", null],\n[\"-\", null],\n[\"\u041b\", \"Louis\"],\n[\"\u0443\", null],\n[\"\u0438\", null],\n[\"\u0441\u0430\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0440\u0435\u0436\u0438\", \"real\"],\n[\"\u043c\u0435\", null],\n[\"\u2581\u0440\u0435\", \"time\"],\n[\"\u0430\u043b\u044c\", null],\n[\"\u043d\u043e\u0433\u043e\", null],\n[\"\u2581\u0432\u0440\u0435\", null],\n[\"\u043c\u0435\u043d\u0438\", null],\n[\"\u2581\u043e\u0441\", \"covered\"],\n[\"\u0432\u0435\", null],\n[\"\u0449\u0430\", null],\n[\"\u043b\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0442\", \"Twitter\"],\n[\"\u0432\u0438\", null],\n[\"\u0442\", null],\n[\"\u0442\u0435\", null],\n[\"\u0440\u0435\", null],\n[\"\u2581\u0441\u043c\u0435\u0440\", \"death\"],\n[\"\u0442\u044c\", null],\n[\"\u2581\u041c\u0430\", \"Michael\"],\n[\"\u0439\", null],\n[\"\u043a\u043b\u0430\", null],\n[\"\u2581\u0411\u0440\u0430\", \"Brown\"],\n[\"\u0443\", null],\n[\"\u043d\u0430\", null],\n[\",\", null],\n[\"\u2581\u0431\u0435\", \"unarmed\"],\n[\"\u0437\u043e\", null],\n[\"\u0440\u0443\", null],\n[\"\u0436\", null],\n[\"\u043d\u043e\u0433\u043e\", null],\n[\"\u2581\u0447\u0435\u0440\", \"black\"],\n[\"\u043d\u043e\", null],\n[\"\u043a\u043e\", null],\n[\"\u0436\u0435\", null],\n[\"\u0433\u043e\", null],\n[\"\u2581\u0432\u043e\", \"eighteen\"],\n[\"\u0441\u0435\", null],\n[\"\u043c\", null],\n[\"\u043d\u0430\u0434\", null],\n[\"\u0446\u0430\", null],\n[\"\u0442\u0438\", null],\n[\"\u043b\u0435\u0442\", null],\n[\"\u043d\u0435\u0433\u043e\", null],\n[\"\u2581\u043f\u043e\u0434\", \"teenager\"],\n[\"\u0440\u043e\", null],\n[\"\u0441\u0442\u0430\", null],\n[\",\", null],\n[\"\u2581\u0443\u0431\u0438\", \"killed\"],\n[\"\u0442\u043e\u0433\u043e\", null],\n[\"\u2581\u0431\u0435\", \"white\"],\n[\"\u043b\u044b\", null],\n[\"\u043c\", null],\n[\"\u2581\u043e\u0444\u0438\", \"police\"],\n[\"\u0446\u0435\", null],\n[\"\u0440\u043e\u043c\", null],\n[\"\u2581\u043f\u043e\u043b\u0438\", \"officer\"],\n[\"\u0446\u0438\u0438\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0424\", \"Ferguson\"],\n[\"\u0435\u0440\", null],\n[\"\u0433\", null],\n[\"\u044e\", null],\n[\"\u0441\u043e\", null],\n[\"\u043d\u0435\", null],\n[\",\", null],\n[\"\u2581\u0448\u0442\u0430\", \"state\"],\n[\"\u0442\", null],\n[\"\u2581\u041c\u0438\", \"Missouri\"],\n[\"\u0441\", null],\n[\"\u0441\u0443\", null],\n[\"\u0440\u0438\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u0420\u044d\u043f\u0435\u0440 \u0438\u0437 \u0411\u043e\u043b\u044c\u0448\u043e\u0433\u043e \u0421\u0435\u043d\u0442-\u041b\u0443\u0438\u0441\u0430 \u0432 \u0440\u0435\u0436\u0438\u043c\u0435 \u0440\u0435\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u043e\u0441\u0432\u0435\u0449\u0430\u043b \u0432 \u0442\u0432\u0438\u0442\u0442\u0435\u0440\u0435 \u0441\u043c\u0435\u0440\u0442\u044c \u041c\u0430\u0439\u043a\u043b\u0430 \u0411\u0440\u0430\u0443\u043d\u0430, \u0431\u0435\u0437\u043e\u0440\u0443\u0436\u043d\u043e\u0433\u043e \u0447\u0435\u0440\u043d\u043e\u043a\u043e\u0436\u0435\u0433\u043e \u0432\u043e\u0441\u0435\u043c\u043d\u0430\u0434\u0446\u0430\u0442\u0438\u043b\u0435\u0442\u043d\u0435\u0433\u043e \u043f\u043e\u0434\u0440\u043e\u0441\u0442\u0430, \u0443\u0431\u0438\u0442\u043e\u0433\u043e \u0431\u0435\u043b\u044b\u043c \u043e\u0444\u0438\u0446\u0435\u0440\u043e\u043c \u043f\u043e\u043b\u0438\u0446\u0438\u0438 \u0432 \u0424\u0435\u0440\u0433\u044e\u0441\u043e\u043d\u0435, \u0448\u0442\u0430\u0442 \u041c\u0438\u0441\u0441\u0443\u0440\u0438.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0420', '\u044d', '\u043f\u0435\u0440', '\u2581\u0438\u0437', '\u2581\u0411\u043e', '\u043b\u044c', '\u0448\u043e', '\u0433\u043e', '\u2581\u0421', '\u0435\u043d\u0442', '-', '\u041b', '\u0443', '\u0438', '\u0441\u0430', '\u2581\u0432', '\u2581\u0440\u0435\u0436\u0438', '\u043c\u0435', '\u2581\u0440\u0435', '\u0430\u043b\u044c', '\u043d\u043e\u0433\u043e', '\u2581\u0432\u0440\u0435', '\u043c\u0435\u043d\u0438', '\u2581\u043e\u0441', '\u0432\u0435', '\u0449\u0430', '\u043b', '\u2581\u0432', '\u2581\u0442', '\u0432\u0438', '\u0442', '\u0442\u0435', '\u0440\u0435', '\u2581\u0441\u043c\u0435\u0440', '\u0442\u044c', '\u2581\u041c\u0430', '\u0439', '\u043a\u043b\u0430', '\u2581\u0411\u0440\u0430', '\u0443', '\u043d\u0430', ',', '\u2581\u0431\u0435', '\u0437\u043e', '\u0440\u0443', '\u0436', '\u043d\u043e\u0433\u043e', '\u2581\u0447\u0435\u0440', '\u043d\u043e', '\u043a\u043e', '\u0436\u0435', '\u0433\u043e', '\u2581\u0432\u043e', '\u0441\u0435', '\u043c', '\u043d\u0430\u0434', '\u0446\u0430', '\u0442\u0438', '\u043b\u0435\u0442', '\u043d\u0435\u0433\u043e', '\u2581\u043f\u043e\u0434', '\u0440\u043e', '\u0441\u0442\u0430', ',', '\u2581\u0443\u0431\u0438', '\u0442\u043e\u0433\u043e', '\u2581\u0431\u0435', '\u043b\u044b', '\u043c', '\u2581\u043e\u0444\u0438', '\u0446\u0435', '\u0440\u043e\u043c', '\u2581\u043f\u043e\u043b\u0438', '\u0446\u0438\u0438', '\u2581\u0432', '\u2581\u0424', '\u0435\u0440', '\u0433', '\u044e', '\u0441\u043e', '\u043d\u0435', ',', '\u2581\u0448\u0442\u0430', '\u0442', '\u2581\u041c\u0438', '\u0441', '\u0441\u0443', '\u0440\u0438', '.']"}], "[\n[\"\u2581\u0420\", \"\u8bf4\u5531\u6b4c\u624b\"],\n[\"\u044d\", null],\n[\"\u043f\u0435\u0440\", null],\n[\"\u2581\u0438\u0437\", null],\n[\"\u2581\u0411\u043e\", \"\u5927\"],\n[\"\u043b\u044c\", null],\n[\"\u0448\u043e\", null],\n[\"\u0433\u043e\", null],\n[\"\u2581\u0421\", \"\u5723\"],\n[\"\u0435\u043d\u0442\", null],\n[\"-\", null],\n[\"\u041b\", \"\u8def\"],\n[\"\u0443\", null],\n[\"\u0438\", null],\n[\"\u0441\u0430\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0440\u0435\u0436\u0438\", \"\u5b9e\u65f6\"],\n[\"\u043c\u0435\", null],\n[\"\u2581\u0440\u0435\", null],\n[\"\u0430\u043b\u044c\", null],\n[\"\u043d\u043e\u0433\u043e\", null],\n[\"\u2581\u0432\u0440\u0435\", \"\u65f6\u95f4\"],\n[\"\u043c\u0435\u043d\u0438\", null],\n[\"\u2581\u043e\u0441\", \"\u62a5\u9053\"],\n[\"\u0432\u0435\", null],\n[\"\u0449\u0430\", null],\n[\"\u043b\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0442\", \"\u63a8\u7279\"],\n[\"\u0432\u0438\", null],\n[\"\u0442\", null],\n[\"\u0442\u0435\", null],\n[\"\u0440\u0435\", null],\n[\"\u2581\u0441\u043c\u0435\u0440\", \"\u6b7b\u4ea1\"],\n[\"\u0442\u044c\", null],\n[\"\u2581\u041c\u0430\", \"\u8fc8\u514b\u5c14\"],\n[\"\u0439\", null],\n[\"\u043a\u043b\u0430\", null],\n[\"\u2581\u0411\u0440\u0430\", \"\u5e03\u6717\"],\n[\"\u0443\", null],\n[\"\u043d\u0430\", null],\n[\",\", null],\n[\"\u2581\u0431\u0435\", \"\u624b\u65e0\u5bf8\u94c1\"],\n[\"\u0437\u043e\", null],\n[\"\u0440\u0443\", null],\n[\"\u0436\", null],\n[\"\u043d\u043e\u0433\u043e\", null],\n[\"\u2581\u0447\u0435\u0440\", \"\u9ed1\u4eba\"],\n[\"\u043d\u043e\", null],\n[\"\u043a\u043e\", null],\n[\"\u0436\u0435\", null],\n[\"\u0433\u043e\", null],\n[\"\u2581\u0432\u043e\", \"\u5341\u516b\u5c81\"],\n[\"\u0441\u0435\", null],\n[\"\u043c\", null],\n[\"\u043d\u0430\u0434\", null],\n[\"\u0446\u0430\", null],\n[\"\u0442\u0438\", null],\n[\"\u043b\u0435\u0442\", null],\n[\"\u043d\u0435\u0433\u043e\", null],\n[\"\u2581\u043f\u043e\u0434\", \"\u9752\u5c11\u5e74\"],\n[\"\u0440\u043e\", null],\n[\"\u0441\u0442\u0430\", null],\n[\",\", null],\n[\"\u2581\u0443\u0431\u0438\", \"\u88ab\u6740\"],\n[\"\u0442\u043e\u0433\u043e\", null],\n[\"\u2581\u0431\u0435\", \"\u767d\u4eba\"],\n[\"\u043b\u044b\", null],\n[\"\u043c\", null],\n[\"\u2581\u043e\u0444\u0438\", \"\u8b66\u5b98\"],\n[\"\u0446\u0435\", null],\n[\"\u0440\u043e\u043c\", null],\n[\"\u2581\u043f\u043e\u043b\u0438\", \"\u8b66\u5bdf\"],\n[\"\u0446\u0438\u0438\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0424\", \"\u5f17\u683c\u68ee\"],\n[\"\u0435\u0440\", null],\n[\"\u0433\", null],\n[\"\u044e\", null],\n[\"\u0441\u043e\", null],\n[\"\u043d\u0435\", null],\n[\",\", null],\n[\"\u2581\u0448\u0442\u0430\", \"\u5bc6\u82cf\u91cc\u5dde\"],\n[\"\u0442\", null],\n[\"\u2581\u041c\u0438\", null],\n[\"\u0441\", null],\n[\"\u0441\u0443\", null],\n[\"\u0440\u0438\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u0420\u044d\u043f\u0435\u0440 \u0438\u0437 \u0411\u043e\u043b\u044c\u0448\u043e\u0433\u043e \u0421\u0435\u043d\u0442-\u041b\u0443\u0438\u0441\u0430 \u0432 \u0440\u0435\u0436\u0438\u043c\u0435 \u0440\u0435\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u043e\u0441\u0432\u0435\u0449\u0430\u043b \u0432 \u0442\u0432\u0438\u0442\u0442\u0435\u0440\u0435 \u0441\u043c\u0435\u0440\u0442\u044c \u041c\u0430\u0439\u043a\u043b\u0430 \u0411\u0440\u0430\u0443\u043d\u0430, \u0431\u0435\u0437\u043e\u0440\u0443\u0436\u043d\u043e\u0433\u043e \u0447\u0435\u0440\u043d\u043e\u043a\u043e\u0436\u0435\u0433\u043e \u0432\u043e\u0441\u0435\u043c\u043d\u0430\u0434\u0446\u0430\u0442\u0438\u043b\u0435\u0442\u043d\u0435\u0433\u043e \u043f\u043e\u0434\u0440\u043e\u0441\u0442\u0430, \u0443\u0431\u0438\u0442\u043e\u0433\u043e \u0431\u0435\u043b\u044b\u043c \u043e\u0444\u0438\u0446\u0435\u0440\u043e\u043c \u043f\u043e\u043b\u0438\u0446\u0438\u0438 \u0432 \u0424\u0435\u0440\u0433\u044e\u0441\u043e\u043d\u0435, \u0448\u0442\u0430\u0442 \u041c\u0438\u0441\u0441\u0443\u0440\u0438.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u2581\u0420', '\u044d', '\u043f\u0435\u0440', '\u2581\u0438\u0437', '\u2581\u0411\u043e', '\u043b\u044c', '\u0448\u043e', '\u0433\u043e', '\u2581\u0421', '\u0435\u043d\u0442', '-', '\u041b', '\u0443', '\u0438', '\u0441\u0430', '\u2581\u0432', '\u2581\u0440\u0435\u0436\u0438', '\u043c\u0435', '\u2581\u0440\u0435', '\u0430\u043b\u044c', '\u043d\u043e\u0433\u043e', '\u2581\u0432\u0440\u0435', '\u043c\u0435\u043d\u0438', '\u2581\u043e\u0441', '\u0432\u0435', '\u0449\u0430', '\u043b', '\u2581\u0432', '\u2581\u0442', '\u0432\u0438', '\u0442', '\u0442\u0435', '\u0440\u0435', '\u2581\u0441\u043c\u0435\u0440', '\u0442\u044c', '\u2581\u041c\u0430', '\u0439', '\u043a\u043b\u0430', '\u2581\u0411\u0440\u0430', '\u0443', '\u043d\u0430', ',', '\u2581\u0431\u0435', '\u0437\u043e', '\u0440\u0443', '\u0436', '\u043d\u043e\u0433\u043e', '\u2581\u0447\u0435\u0440', '\u043d\u043e', '\u043a\u043e', '\u0436\u0435', '\u0433\u043e', '\u2581\u0432\u043e', '\u0441\u0435', '\u043c', '\u043d\u0430\u0434', '\u0446\u0430', '\u0442\u0438', '\u043b\u0435\u0442', '\u043d\u0435\u0433\u043e', '\u2581\u043f\u043e\u0434', '\u0440\u043e', '\u0441\u0442\u0430', ',', '\u2581\u0443\u0431\u0438', '\u0442\u043e\u0433\u043e', '\u2581\u0431\u0435', '\u043b\u044b', '\u043c', '\u2581\u043e\u0444\u0438', '\u0446\u0435', '\u0440\u043e\u043c', '\u2581\u043f\u043e\u043b\u0438', '\u0446\u0438\u0438', '\u2581\u0432', '\u2581\u0424', '\u0435\u0440', '\u0433', '\u044e', '\u0441\u043e', '\u043d\u0435', ',', '\u2581\u0448\u0442\u0430', '\u0442', '\u2581\u041c\u0438', '\u0441', '\u0441\u0443', '\u0440\u0438', '.']"}], "[\n[\"\u2581\u0420\", \"\u0420\u044d\u043f\u0435\u0440\"],\n[\"\u044d\", null],\n[\"\u043f\u0435\u0440\", null],\n[\"\u2581\u0438\u0437\", null],\n[\"\u2581\u0411\u043e\", \"\u04ae\u043b\u043a\u0435\u043d\"],\n[\"\u043b\u044c\", null],\n[\"\u0448\u043e\", null],\n[\"\u0433\u043e\", null],\n[\"\u2581\u0421\", \"\u0421\u0435\u043d\u0442\"],\n[\"\u0435\u043d\u0442\", null],\n[\"-\", null],\n[\"\u041b\", \"\u041b\u0443\u0438\u0441\"],\n[\"\u0443\", null],\n[\"\u0438\", null],\n[\"\u0441\u0430\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0440\u0435\u0436\u0438\", \"\u0440\u0435\u0436\u0438\u043c\u0456\u043d\u0434\u0435\"],\n[\"\u043c\u0435\", null],\n[\"\u2581\u0440\u0435\", \"\u043d\u0430\u049b\u0442\u044b\"],\n[\"\u0430\u043b\u044c\", null],\n[\"\u043d\u043e\u0433\u043e\", null],\n[\"\u2581\u0432\u0440\u0435\", \"\u0443\u0430\u049b\u044b\u0442\"],\n[\"\u043c\u0435\u043d\u0438\", null],\n[\"\u2581\u043e\u0441\", \"\u0436\u0430\u0440\u0438\u044f\u043b\u0430\u0434\u044b\"],\n[\"\u0432\u0435\", null],\n[\"\u0449\u0430\", null],\n[\"\u043b\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0442\", \"Twitter\"],\n[\"\u0432\u0438\", null],\n[\"\u0442\", null],\n[\"\u0442\u0435\", null],\n[\"\u0440\u0435\", null],\n[\"\u2581\u0441\u043c\u0435\u0440\", \"\u04e9\u043b\u0456\u043c\u0456\"],\n[\"\u0442\u044c\", null],\n[\"\u2581\u041c\u0430\", \"\u041c\u0430\u0439\u043a\u043b\"],\n[\"\u0439\", null],\n[\"\u043a\u043b\u0430\", null],\n[\"\u2581\u0411\u0440\u0430\", \"\u0411\u0440\u0430\u0443\u043d\"],\n[\"\u0443\", null],\n[\"\u043d\u0430\", null],\n[\",\", null],\n[\"\u2581\u0431\u0435\", \"\u049b\u0430\u0440\u0443\u0441\u044b\u0437\"],\n[\"\u0437\u043e\", null],\n[\"\u0440\u0443\", null],\n[\"\u0436\", null],\n[\"\u043d\u043e\u0433\u043e\", null],\n[\"\u2581\u0447\u0435\u0440\", \"\u049b\u0430\u0440\u0430\"],\n[\"\u043d\u043e\", null],\n[\"\u043a\u043e\", null],\n[\"\u0436\u0435\", null],\n[\"\u0433\u043e\", null],\n[\"\u2581\u0432\u043e\", \"\u043e\u043d\"],\n[\"\u0441\u0435\", null],\n[\"\u043c\", null],\n[\"\u043d\u0430\u0434\", null],\n[\"\u0446\u0430\", null],\n[\"\u0442\u0438\", null],\n[\"\u043b\u0435\u0442\", null],\n[\"\u043d\u0435\u0433\u043e\", null],\n[\"\u2581\u043f\u043e\u0434\", \"\u0436\u0430\u0441\u04e9\u0441\u043f\u0456\u0440\u0456\u043c\"],\n[\"\u0440\u043e\", null],\n[\"\u0441\u0442\u0430\", null],\n[\",\", null],\n[\"\u2581\u0443\u0431\u0438\", \"\u04e9\u043b\u0442\u0456\u0440\u0456\u043b\u0433\u0435\u043d\"],\n[\"\u0442\u043e\u0433\u043e\", null],\n[\"\u2581\u0431\u0435\", \"\u0430\u049b\"],\n[\"\u043b\u044b\", null],\n[\"\u043c\", null],\n[\"\u2581\u043e\u0444\u0438\", \"\u043f\u043e\u043b\u0438\u0446\u0438\u044f\"],\n[\"\u0446\u0435\", null],\n[\"\u0440\u043e\u043c\", null],\n[\"\u2581\u043f\u043e\u043b\u0438\", \"\u043e\u0444\u0438\u0446\u0435\u0440\u0456\"],\n[\"\u0446\u0438\u0438\", null],\n[\"\u2581\u0432\", null],\n[\"\u2581\u0424\", \"\u0424\u0435\u0440\u0433\u044e\u0441\u043e\u043d\"],\n[\"\u0435\u0440\", null],\n[\"\u0433\", null],\n[\"\u044e\", null],\n[\"\u0441\u043e\", null],\n[\"\u043d\u0435\", null],\n[\",\", null],\n[\"\u2581\u0448\u0442\u0430\", \"\u0448\u0442\u0430\u0442\u044b\"],\n[\"\u0442\", null],\n[\"\u2581\u041c\u0438\", \"\u041c\u0438\u0441\u0441\u0443\u0440\u0438\"],\n[\"\u0441\", null],\n[\"\u0441\u0443\", null],\n[\"\u0440\u0438\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nManchester United have signed a \"genius\" in Nemanja Matic, believes Jose Mourinho, after the midfielder made his debut against Sampdoria on Wednesday evening.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Manchester', ' United', ' have', ' signed', ' a', ' \"', 'gen', 'ius', '\"', ' in', ' N', 'eman', 'ja', ' M', 'atic', ',', ' believes', ' Jose', ' Mourinho', ',', ' after', ' the', ' midfielder', ' made', ' his', ' debut', ' against', ' Samp', 'd', 'oria', ' on', ' Wednesday', ' evening', '.']"}], "[\n[\"Manchester\", \"\u66fc\u5f7b\u65af\u7279\"],\n[\"United\", \"\u8054\u961f\"],\n[\" \", null],\n[\"have\", null],\n[\" \", null],\n[\"signed\", \"\u7b7e\u4e0b\"],\n[\" \", null],\n[\"a\", null],\n[\" \", null],\n[\"\\\"\", null],\n[\"gen\", \"\u5929\u624d\"],\n[\"ius\", null],\n[\"\\\"\", null],\n[\" \", null],\n[\"in\", null],\n[\" \", null],\n[\"N\", \"\u5185\u9a6c\u5c3c\u4e9a\"],\n[\"eman\", null],\n[\"ja\", null],\n[\" \", null],\n[\"M\", \"\u9a6c\u8482\u5947\"],\n[\"atic\", null],\n[\",\", null],\n[\" \", null],\n[\"believes\", \"\u8ba4\u4e3a\"],\n[\" \", null],\n[\"Jose\", \"\u4f55\u585e\"],\n[\" \", null],\n[\"Mourinho\", \"\u7a46\u91cc\u5c3c\u5965\"],\n[\",\", null],\n[\" \", null],\n[\"after\", null],\n[\" \", null],\n[\"the\", null],\n[\" \", null],\n[\"midfielder\", \"\u4e2d\u573a\u7403\u5458\"],\n[\" \", null],\n[\"made\", \"\u5b8c\u6210\"],\n[\" \", null],\n[\"his\", null],\n[\" \", null],\n[\"debut\", \"\u9996\u79c0\"],\n[\" \", null],\n[\"against\", \"\u5bf9\u9635\"],\n[\" \", null],\n[\"Samp\", \"\u6851\u666e\u591a\u5229\u4e9a\"],\n[\"d\", null],\n[\"oria\", null],\n[\" \", null],\n[\"on\", null],\n[\" \", null],\n[\"Wednesday\", \"\u661f\u671f\u4e09\"],\n[\" \", null],\n[\"evening\", \"\u665a\u4e0a\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nManchester United have signed a \"genius\" in Nemanja Matic, believes Jose Mourinho, after the midfielder made his debut against Sampdoria on Wednesday evening.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Manchester', ' United', ' have', ' signed', ' a', ' \"', 'gen', 'ius', '\"', ' in', ' N', 'eman', 'ja', ' M', 'atic', ',', ' believes', ' Jose', ' Mourinho', ',', ' after', ' the', ' midfielder', ' made', ' his', ' debut', ' against', ' Samp', 'd', 'oria', ' on', ' Wednesday', ' evening', '.']"}], "[\n[\"Manchester\", \"\u041c\u0430\u043d\u0447\u0435\u0441\u0442\u0435\u0440\"],\n[\"United\", \"\u042e\u043d\u0430\u0439\u0442\u0435\u0434\"],\n[\" \", null],\n[\"have\", null],\n[\" \", null],\n[\"signed\", \"\u049b\u043e\u043b \u049b\u043e\u0439\u0434\u044b\"],\n[\" \", null],\n[\"a\", null],\n[\" \", null],\n[\"\\\"\", null],\n[\"gen\", \"\u0434\u0430\u043d\u044b\u0448\u043f\u0430\u043d\"],\n[\"ius\", null],\n[\"\\\"\", null],\n[\" \", null],\n[\"in\", null],\n[\" \", null],\n[\"N\", \"\u041d\u0435\u043c\u0430\u043d\u044c\u044f\"],\n[\"eman\", null],\n[\"ja\", null],\n[\" \", null],\n[\"M\", \"\u041c\u0430\u0442\u0438\u0447\"],\n[\"atic\", null],\n[\",\", null],\n[\" \", null],\n[\"believes\", \"\u0441\u0430\u043d\u0430\u0439\u0434\u044b\"],\n[\" \", null],\n[\"Jose\", \"\u0416\u043e\u0437\u0435\"],\n[\" \", null],\n[\"Mourinho\", \"\u041c\u043e\u0443\u0440\u0438\u043d\u044c\u044e\"],\n[\",\", null],\n[\" \", null],\n[\"after\", null],\n[\" \", null],\n[\"the\", null],\n[\" \", null],\n[\"midfielder\", \"\u0436\u0430\u0440\u0442\u044b\u043b\u0430\u0439 \u049b\u043e\u0440\u0493\u0430\u0443\u0448\u044b\"],\n[\" \", null],\n[\"made\", \"\u0436\u0430\u0441\u0430\u0434\u044b\"],\n[\" \", null],\n[\"his\", null],\n[\" \", null],\n[\"debut\", \"\u0434\u0435\u0431\u044e\u0442\"],\n[\" \", null],\n[\"against\", null],\n[\" \", null],\n[\"Samp\", \"\u0421\u0430\u043c\u043f\u0434\u043e\u0440\u0438\u044f\"],\n[\"d\", null],\n[\"oria\", null],\n[\" \", null],\n[\"on\", null],\n[\" \", null],\n[\"Wednesday\", \"\u0441\u04d9\u0440\u0441\u0435\u043d\u0431\u0456\"],\n[\" \", null],\n[\"evening\", \"\u043a\u0435\u0448\u0456\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nManchester United have signed a \"genius\" in Nemanja Matic, believes Jose Mourinho, after the midfielder made his debut against Sampdoria on Wednesday evening.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Manchester', ' United', ' have', ' signed', ' a', ' \"', 'gen', 'ius', '\"', ' in', ' N', 'eman', 'ja', ' M', 'atic', ',', ' believes', ' Jose', ' Mourinho', ',', ' after', ' the', ' midfielder', ' made', ' his', ' debut', ' against', ' Samp', 'd', 'oria', ' on', ' Wednesday', ' evening', '.']"}], "[\n[\"Manchester\", \"\u041c\u0430\u043d\u0447\u0435\u0441\u0442\u0435\u0440\"],\n[\"United\", \"\u042e\u043d\u0430\u0439\u0442\u0435\u0434\"],\n[\" \", null],\n[\"have\", null],\n[\" \", null],\n[\"signed\", \"\u043f\u043e\u0434\u043f\u0438\u0441\u0430\u043b\u0438\"],\n[\" \", null],\n[\"a\", null],\n[\" \", null],\n[\"\\\"\", null],\n[\"gen\", \"\u0433\u0435\u043d\u0438\u0439\"],\n[\"ius\", null],\n[\"\\\"\", null],\n[\" \", null],\n[\"in\", null],\n[\" \", null],\n[\"N\", \"\u041d\u0435\u043c\u0430\u043d\u044c\u044f\"],\n[\"eman\", null],\n[\"ja\", null],\n[\" \", null],\n[\"M\", \"\u041c\u0430\u0442\u0438\u0447\"],\n[\"atic\", null],\n[\",\", null],\n[\" \", null],\n[\"believes\", \"\u0441\u0447\u0438\u0442\u0430\u0435\u0442\"],\n[\" \", null],\n[\"Jose\", \"\u0416\u043e\u0437\u0435\"],\n[\" \", null],\n[\"Mourinho\", \"\u041c\u043e\u0443\u0440\u0438\u043d\u044c\u044e\"],\n[\",\", null],\n[\" \", null],\n[\"after\", \"\u043f\u043e\u0441\u043b\u0435\"],\n[\" \", null],\n[\"the\", null],\n[\" \", null],\n[\"midfielder\", \"\u043f\u043e\u043b\u0443\u0437\u0430\u0449\u0438\u0442\u043d\u0438\u043a\"],\n[\" \", null],\n[\"made\", \"\u043f\u0440\u043e\u0432\u0435\u043b\"],\n[\" \", null],\n[\"his\", null],\n[\" \", null],\n[\"debut\", \"\u0434\u0435\u0431\u044e\u0442\"],\n[\" \", null],\n[\"against\", \"\u043f\u0440\u043e\u0442\u0438\u0432\"],\n[\" \", null],\n[\"Samp\", \"\u0421\u0430\u043c\u043f\u0434\u043e\u0440\u0438\u044f\"],\n[\"d\", null],\n[\"oria\", null],\n[\" \", null],\n[\"on\", null],\n[\" \", null],\n[\"Wednesday\", \"\u0441\u0440\u0435\u0434\u0443\"],\n[\" \", null],\n[\"evening\", \"\u0432\u0435\u0447\u0435\u0440\u043e\u043c\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\n\"The idea is to showcase this week things that are made in America,\" Spicer said.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\"The', ' idea', ' is', ' to', ' showcase', ' this', ' week', ' things', ' that', ' are', ' made', ' in', ' America', ',\"', ' Spicer', ' said', '.']"}], "[\n  [\"\\\"The\", null],\n  [\" idea\", \"\u60f3\u6cd5\"],\n  [\" is\", null],\n  [\" to\", null],\n  [\" showcase\", \"\u5c55\u793a\"],\n  [\" this\", null],\n  [\" week\", \"\u5468\"],\n  [\" things\", \"\u4e8b\u7269\"],\n  [\" that\", null],\n  [\" are\", null],\n  [\" made\", \"\u5236\u9020\"],\n  [\" in\", null],\n  [\" America\", \"\u7f8e\u56fd\"],\n  [\",\\\"\", null],\n  [\" Spicer\", \"\u65af\u6d3e\u585e\"],\n  [\" said\", \"\u8bf4\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\n\"The idea is to showcase this week things that are made in America,\" Spicer said.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\"The', ' idea', ' is', ' to', ' showcase', ' this', ' week', ' things', ' that', ' are', ' made', ' in', ' America', ',\"', ' Spicer', ' said', '.']"}], "[\n  ['\"The', null],\n  [\" idea\", \"\u0438\u0434\u0435\u044f\"],\n  [\" is\", null],\n  [\" to\", null],\n  [\" showcase\", \"\u043a\u04e9\u0440\u0441\u0435\u0442\u0443\"],\n  [\" this\", null],\n  [\" week\", \"\u0430\u043f\u0442\u0430\"],\n  [\" things\", \"\u0437\u0430\u0442\u0442\u0430\u0440\"],\n  [\" that\", null],\n  [\" are\", null],\n  [\" made\", \"\u0436\u0430\u0441\u0430\u043b\u0493\u0430\u043d\"],\n  [\" in\", null],\n  [\" America\", \"\u0410\u043c\u0435\u0440\u0438\u043a\u0430\"],\n  [',\"', null],\n  [\" Spicer\", \"\u0421\u043f\u0430\u0439\u0441\u0435\u0440\"],\n  [\" said\", \"\u0434\u0435\u0434\u0456\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\n\"The idea is to showcase this week things that are made in America,\" Spicer said.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\"The', ' idea', ' is', ' to', ' showcase', ' this', ' week', ' things', ' that', ' are', ' made', ' in', ' America', ',\"', ' Spicer', ' said', '.']"}], "[\n  [\"\\\"The\", null],\n  [\" idea\", \"\u0438\u0434\u0435\u044f\"],\n  [\" is\", null],\n  [\" to\", null],\n  [\" showcase\", \"\u043f\u0440\u043e\u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c\"],\n  [\" this\", null],\n  [\" week\", \"\u043d\u0435\u0434\u0435\u043b\u044f\"],\n  [\" things\", \"\u0432\u0435\u0449\u0438\"],\n  [\" that\", null],\n  [\" are\", null],\n  [\" made\", \"\u0441\u0434\u0435\u043b\u0430\u043d\u044b\"],\n  [\" in\", null],\n  [\" America\", \"\u0410\u043c\u0435\u0440\u0438\u043a\u0430\"],\n  [\",\\\"\", null],\n  [\" Spicer\", \"\u0421\u043f\u0430\u0439\u0441\u0435\u0440\"],\n  [\" said\", \"\u0441\u043a\u0430\u0437\u0430\u043b\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nThe method of medical age assessment, which consists of taking X-rays of wisdom teeth and MRI scans of knee joints, then having dentists and radiologists analyse them, has also been criticised in some quarters.\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', ' method', ' of', ' medical', ' age', ' assessment', ',', ' which', ' consists', ' of', ' taking', ' X', '-rays', ' of', ' wisdom', ' teeth', ' and', ' MRI', ' scans', ' of', ' knee', ' joints', ',', ' then', ' having', ' dent', 'ists', ' and', ' radi', 'ologists', ' analyse', ' them', ',', ' has', ' also', ' been', ' criticised', ' in', ' some', ' quarters', '.']"}], "[\n[\"The\", null],\n[\"method\", \"\u65b9\u6cd5\"],\n[\"of\", null],\n[\"medical\", \"\u533b\u5b66\"],\n[\"age\", \"\u5e74\u9f84\"],\n[\"assessment\", \"\u8bc4\u4f30\"],\n[\",\", null],\n[\"which\", null],\n[\"consists\", \"\u5305\u62ec\"],\n[\"of\", null],\n[\"taking\", \"\u62cd\u6444\"],\n[\"X\", \"X\"],\n[\"-rays\", null],\n[\"of\", null],\n[\"wisdom\", \"\u667a\u9f7f\"],\n[\"teeth\", null],\n[\"and\", null],\n[\"MRI\", \"\u6838\u78c1\u5171\u632f\"],\n[\"scans\", \"\u626b\u63cf\"],\n[\"of\", null],\n[\"knee\", \"\u819d\u76d6\"],\n[\"joints\", \"\u5173\u8282\"],\n[\",\", null],\n[\"then\", null],\n[\"having\", null],\n[\"dent\", \"\u7259\u79d1\u533b\u751f\"],\n[\"ists\", null],\n[\"and\", null],\n[\"radi\", \"\u653e\u5c04\u79d1\u533b\u751f\"],\n[\"ologists\", null],\n[\"analyse\", \"\u5206\u6790\"],\n[\"them\", null],\n[\",\", null],\n[\"has\", null],\n[\"also\", null],\n[\"been\", null],\n[\"criticised\", \"\u53d7\u5230\u6279\u8bc4\"],\n[\"in\", null],\n[\"some\", null],\n[\"quarters\", \"\u4e00\u4e9b\u65b9\u9762\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nThe method of medical age assessment, which consists of taking X-rays of wisdom teeth and MRI scans of knee joints, then having dentists and radiologists analyse them, has also been criticised in some quarters.\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', ' method', ' of', ' medical', ' age', ' assessment', ',', ' which', ' consists', ' of', ' taking', ' X', '-rays', ' of', ' wisdom', ' teeth', ' and', ' MRI', ' scans', ' of', ' knee', ' joints', ',', ' then', ' having', ' dent', 'ists', ' and', ' radi', 'ologists', ' analyse', ' them', ',', ' has', ' also', ' been', ' criticised', ' in', ' some', ' quarters', '.']"}], "[\n  [\"The\", null],\n  [\" method\", \"\u04d9\u0434\u0456\u0441\"],\n  [\" of\", null],\n  [\" medical\", \"\u043c\u0435\u0434\u0438\u0446\u0438\u043d\u0430\u043b\u044b\u049b\"],\n  [\" age\", \"\u0436\u0430\u0441\"],\n  [\" assessment\", \"\u0431\u0430\u0493\u0430\u043b\u0430\u0443\"],\n  [\",\", null],\n  [\" which\", null],\n  [\" consists\", \"\u0442\u04b1\u0440\u0430\u0434\u044b\"],\n  [\" of\", null],\n  [\" taking\", \"\u0442\u04af\u0441\u0456\u0440\u0443\"],\n  [\" X\", \"\u0440\u0435\u043d\u0442\u0433\u0435\u043d\"],\n  [\"-rays\", null],\n  [\" of\", null],\n  [\" wisdom\", \"\u0430\u049b\u044b\u043b\"],\n  [\" teeth\", \"\u0442\u0456\u0441\u0442\u0435\u0440\u0456\"],\n  [\" and\", null],\n  [\" MRI\", \"\u041c\u0420\u0422\"],\n  [\" scans\", \"\u0441\u043a\u0430\u043d\u0435\u0440\u043b\u0435\u0443\"],\n  [\" of\", null],\n  [\" knee\", \"\u0442\u0456\u0437\u0435\"],\n  [\" joints\", \"\u0431\u0443\u044b\u043d\u0434\u0430\u0440\u044b\"],\n  [\",\", null],\n  [\" then\", null],\n  [\" having\", null],\n  [\" dent\", \"\u0442\u0456\u0441\"],\n  [\"ists\", null],\n  [\" and\", null],\n  [\" radi\", \"\u0440\u0430\u0434\u0438\u043e\"],\n  [\"ologists\", null],\n  [\" analyse\", \"\u0442\u0430\u043b\u0434\u0430\u0443\"],\n  [\" them\", null],\n  [\",\", null],\n  [\" has\", null],\n  [\" also\", null],\n  [\" been\", null],\n  [\" criticised\", \"\u0441\u044b\u043d\u0493\u0430 \u0430\u043b\u044b\u043d\u0434\u044b\"],\n  [\" in\", null],\n  [\" some\", \"\u043a\u0435\u0439\u0431\u0456\u0440\"],\n  [\" quarters\", \"\u043e\u0440\u0442\u0430\u043b\u0430\u0440\u0434\u0430\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThe method of medical age assessment, which consists of taking X-rays of wisdom teeth and MRI scans of knee joints, then having dentists and radiologists analyse them, has also been criticised in some quarters.\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', ' method', ' of', ' medical', ' age', ' assessment', ',', ' which', ' consists', ' of', ' taking', ' X', '-rays', ' of', ' wisdom', ' teeth', ' and', ' MRI', ' scans', ' of', ' knee', ' joints', ',', ' then', ' having', ' dent', 'ists', ' and', ' radi', 'ologists', ' analyse', ' them', ',', ' has', ' also', ' been', ' criticised', ' in', ' some', ' quarters', '.']"}], "[\n[\"The\", null],\n[\" method\", \"\u043c\u0435\u0442\u043e\u0434\"],\n[\" of\", null],\n[\" medical\", \"\u043c\u0435\u0434\u0438\u0446\u0438\u043d\u0441\u043a\u043e\u0439\"],\n[\" age\", \"\u0432\u043e\u0437\u0440\u0430\u0441\u0442\u043d\u043e\u0439\"],\n[\" assessment\", \"\u043e\u0446\u0435\u043d\u043a\u0438\"],\n[\",\", null],\n[\" which\", null],\n[\" consists\", \"\u0441\u043e\u0441\u0442\u043e\u0438\u0442\"],\n[\" of\", null],\n[\" taking\", \"\u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u0438\"],\n[\" X\", \"\u0440\u0435\u043d\u0442\u0433\u0435\u043d\u043e\u0432\u0441\u043a\u0438\u0445\"],\n[\"-rays\", null],\n[\" of\", null],\n[\" wisdom\", \"\u0437\u0443\u0431\u043e\u0432\"],\n[\" teeth\", null],\n[\" and\", null],\n[\" MRI\", \"\u041c\u0420\u0422\"],\n[\" scans\", \"\u0441\u043a\u0430\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0439\"],\n[\" of\", null],\n[\" knee\", \"\u043a\u043e\u043b\u0435\u043d\u043d\u044b\u0445\"],\n[\" joints\", \"\u0441\u0443\u0441\u0442\u0430\u0432\u043e\u0432\"],\n[\",\", null],\n[\" then\", null],\n[\" having\", null],\n[\" dent\", \"\u0441\u0442\u043e\u043c\u0430\u0442\u043e\u043b\u043e\u0433\u0438\"],\n[\"ists\", null],\n[\" and\", null],\n[\" radi\", \"\u0440\u0430\u0434\u0438\u043e\u043b\u043e\u0433\u0438\"],\n[\"ologists\", null],\n[\" analyse\", \"\u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u044e\u0442\"],\n[\" them\", null],\n[\",\", null],\n[\" has\", null],\n[\" also\", null],\n[\" been\", null],\n[\" criticised\", \"\u043f\u043e\u0434\u0432\u0435\u0440\u0433\u0430\u043b\u0430\u0441\u044c \u043a\u0440\u0438\u0442\u0438\u043a\u0435\"],\n[\" in\", null],\n[\" some\", null],\n[\" quarters\", \"\u043a\u0440\u0443\u0433\u0430\u0445\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nAfter Entering a new era, to carry out the new guidelines and start a new journey, we must give our firm support\n\n\n---=== TOKENIZED TEXT ===---\n\n['After', ' Ent', 'ering', ' a', ' new', ' era', ',', ' to', ' carry', ' out', ' the', ' new', ' guidelines', ' and', ' start', ' a', ' new', ' journey', ',', ' we', ' must', ' give', ' our', ' firm', ' support']"}], "[\n  [\"After\", \"\u5728\"],\n  [\" Ent\", \"\u8fdb\u5165\"],\n  [\"ering\", null],\n  [\" a\", null],\n  [\" new\", \"\u65b0\"],\n  [\" era\", \"\u65f6\u4ee3\"],\n  [\",\", null],\n  [\" to\", null],\n  [\" carry\", \"\u8d2f\u5f7b\"],\n  [\" out\", null],\n  [\" the\", null],\n  [\" new\", \"\u65b0\"],\n  [\" guidelines\", \"\u65b9\u9488\"],\n  [\" and\", null],\n  [\" start\", \"\u5f00\u542f\"],\n  [\" a\", null],\n  [\" new\", \"\u65b0\"],\n  [\" journey\", \"\u5f81\u7a0b\"],\n  [\",\", null],\n  [\" we\", \"\u6211\u4eec\"],\n  [\" must\", \"\u5fc5\u987b\"],\n  [\" give\", \"\u7ed9\u4e88\"],\n  [\" our\", \"\u6211\u4eec\u7684\"],\n  [\" firm\", \"\u575a\u5b9a\"],\n  [\" support\", \"\u652f\u6301\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nAfter Entering a new era, to carry out the new guidelines and start a new journey, we must give our firm support\n\n\n---=== TOKENIZED TEXT ===---\n\n['After', ' Ent', 'ering', ' a', ' new', ' era', ',', ' to', ' carry', ' out', ' the', ' new', ' guidelines', ' and', ' start', ' a', ' new', ' journey', ',', ' we', ' must', ' give', ' our', ' firm', ' support']"}], "[\n  [\"After\", \"\u041a\u0435\u0439\u0456\u043d\"],\n  [\" Ent\", null],\n  [\"ering\", null],\n  [\" a\", null],\n  [\" new\", \"\u0436\u0430\u04a3\u0430\"],\n  [\" era\", \"\u0434\u04d9\u0443\u0456\u0440\"],\n  [\",\", null],\n  [\" to\", null],\n  [\" carry\", \"\u0436\u04af\u0437\u0435\u0433\u0435 \u0430\u0441\u044b\u0440\u0443\"],\n  [\" out\", null],\n  [\" the\", null],\n  [\" new\", \"\u0436\u0430\u04a3\u0430\"],\n  [\" guidelines\", \"\u043d\u04b1\u0441\u049b\u0430\u0443\u043b\u0430\u0440\"],\n  [\" and\", null],\n  [\" start\", \"\u0431\u0430\u0441\u0442\u0430\u0443\"],\n  [\" a\", null],\n  [\" new\", \"\u0436\u0430\u04a3\u0430\"],\n  [\" journey\", \"\u0441\u0430\u043f\u0430\u0440\"],\n  [\",\", null],\n  [\" we\", \"\u0431\u0456\u0437\"],\n  [\" must\", null],\n  [\" give\", \"\u0431\u0435\u0440\u0443\"],\n  [\" our\", null],\n  [\" firm\", \"\u0431\u0435\u0440\u0456\u043a\"],\n  [\" support\", \"\u049b\u043e\u043b\u0434\u0430\u0443\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nAfter Entering a new era, to carry out the new guidelines and start a new journey, we must give our firm support\n\n\n---=== TOKENIZED TEXT ===---\n\n['After', ' Ent', 'ering', ' a', ' new', ' era', ',', ' to', ' carry', ' out', ' the', ' new', ' guidelines', ' and', ' start', ' a', ' new', ' journey', ',', ' we', ' must', ' give', ' our', ' firm', ' support']"}], "[\n  [\"After\", \"\u041f\u043e\u0441\u043b\u0435\"],\n  [\" Ent\", null],\n  [\"ering\", null],\n  [\" a\", null],\n  [\" new\", \"\u043d\u043e\u0432\u0443\u044e\"],\n  [\" era\", \"\u044d\u043f\u043e\u0445\u0443\"],\n  [\",\", null],\n  [\" to\", null],\n  [\" carry\", \"\u043e\u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043b\u044f\u0442\u044c\"],\n  [\" out\", null],\n  [\" the\", null],\n  [\" new\", \"\u043d\u043e\u0432\u044b\u0435\"],\n  [\" guidelines\", \"\u0443\u043a\u0430\u0437\u0430\u043d\u0438\u044f\"],\n  [\" and\", null],\n  [\" start\", \"\u043d\u0430\u0447\u0430\u0442\u044c\"],\n  [\" a\", null],\n  [\" new\", \"\u043d\u043e\u0432\u044b\u0439\"],\n  [\" journey\", \"\u043f\u0443\u0442\u044c\"],\n  [\",\", null],\n  [\" we\", \"\u043c\u044b\"],\n  [\" must\", \"\u0434\u043e\u043b\u0436\u043d\u044b\"],\n  [\" give\", \"\u043e\u043a\u0430\u0437\u0430\u0442\u044c\"],\n  [\" our\", \"\u043d\u0430\u0448\u0443\"],\n  [\" firm\", \"\u0442\u0432\u0435\u0440\u0434\u0443\u044e\"],\n  [\" support\", \"\u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0443\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nMeanwhile, Apple emphasized in its statement that, \u201cApple has a strong data privacy and security mechanism and will not create a backdoor in any of our systems.\u201d\n\n\n---=== TOKENIZED TEXT ===---\n\n['Meanwhile', ',', ' Apple', ' emphasized', ' in', ' its', ' statement', ' that', ',', ' \u201c', 'Apple', ' has', ' a', ' strong', ' data', ' privacy', ' and', ' security', ' mechanism', ' and', ' will', ' not', ' create', ' a', ' back', 'door', ' in', ' any', ' of', ' our', ' systems', '.\u201d']"}], "[\n  [\"Meanwhile\", \"\u540c\u65f6\"],\n  [\",\", null],\n  [\" Apple\", \"\u82f9\u679c\u516c\u53f8\"],\n  [\" emphasized\", \"\u5f3a\u8c03\"],\n  [\" in\", null],\n  [\" its\", null],\n  [\" statement\", \"\u58f0\u660e\"],\n  [\" that\", null],\n  [\",\", null],\n  [\" \\\"\", null],\n  [\"Apple\", \"\u82f9\u679c\u516c\u53f8\"],\n  [\" has\", \"\u6709\"],\n  [\" a\", null],\n  [\" strong\", \"\u5f3a\u5927\u7684\"],\n  [\" data\", \"\u6570\u636e\"],\n  [\" privacy\", \"\u9690\u79c1\"],\n  [\" and\", null],\n  [\" security\", \"\u5b89\u5168\"],\n  [\" mechanism\", \"\u673a\u5236\"],\n  [\" and\", null],\n  [\" will\", null],\n  [\" not\", \"\u4e0d\u4f1a\"],\n  [\" create\", \"\u521b\u5efa\"],\n  [\" a\", null],\n  [\" back\", \"\u540e\u95e8\"],\n  [\"door\", null],\n  [\" in\", null],\n  [\" any\", \"\u4efb\u4f55\"],\n  [\" of\", null],\n  [\" our\", null],\n  [\" systems\", \"\u7cfb\u7edf\"],\n  [\".\\\"\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nMeanwhile, Apple emphasized in its statement that, \u201cApple has a strong data privacy and security mechanism and will not create a backdoor in any of our systems.\u201d\n\n\n---=== TOKENIZED TEXT ===---\n\n['Meanwhile', ',', ' Apple', ' emphasized', ' in', ' its', ' statement', ' that', ',', ' \u201c', 'Apple', ' has', ' a', ' strong', ' data', ' privacy', ' and', ' security', ' mechanism', ' and', ' will', ' not', ' create', ' a', ' back', 'door', ' in', ' any', ' of', ' our', ' systems', '.\u201d']"}], "[\n  [\"Meanwhile\", \"\u041e\u0441\u044b \u0430\u0440\u0430\u0434\u0430\"],\n  [\",\", null],\n  [\" Apple\", \"Apple\"],\n  [\" emphasized\", \"\u0431\u0430\u0441\u0430 \u0430\u0439\u0442\u0442\u044b\"],\n  [\" in\", null],\n  [\" its\", null],\n  [\" statement\", \"\u043c\u04d9\u043b\u0456\u043c\u0434\u0435\u043c\u0435\u0441\u0456\u043d\u0434\u0435\"],\n  [\" that\", null],\n  [\",\", null],\n  [\" \\\"\", null],\n  [\"Apple\", \"Apple\"],\n  [\" has\", null],\n  [\" a\", null],\n  [\" strong\", \"\u043a\u04af\u0448\u0442\u0456\"],\n  [\" data\", \"\u0434\u0435\u0440\u0435\u043a\u0442\u0435\u0440\"],\n  [\" privacy\", \"\u049b\u04b1\u043f\u0438\u044f\u043b\u044b\u043b\u044b\u0493\u044b\"],\n  [\" and\", null],\n  [\" security\", \"\u049b\u0430\u0443\u0456\u043f\u0441\u0456\u0437\u0434\u0456\u043a\"],\n  [\" mechanism\", \"\u043c\u0435\u0445\u0430\u043d\u0438\u0437\u043c\u0456\"],\n  [\" and\", null],\n  [\" will\", null],\n  [\" not\", null],\n  [\" create\", \"\u0436\u0430\u0441\u0430\u043c\u0430\u0439\u0434\u044b\"],\n  [\" a\", null],\n  [\" back\", \"\u0430\u0440\u0442\u049b\u044b\"],\n  [\"door\", null],\n  [\" in\", null],\n  [\" any\", null],\n  [\" of\", null],\n  [\" our\", null],\n  [\" systems\", \"\u0436\u04af\u0439\u0435\u043b\u0435\u0440\u0456\u043c\u0456\u0437\u0434\u0435\"],\n  [\".\\\"\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nMeanwhile, Apple emphasized in its statement that, \u201cApple has a strong data privacy and security mechanism and will not create a backdoor in any of our systems.\u201d\n\n\n---=== TOKENIZED TEXT ===---\n\n['Meanwhile', ',', ' Apple', ' emphasized', ' in', ' its', ' statement', ' that', ',', ' \u201c', 'Apple', ' has', ' a', ' strong', ' data', ' privacy', ' and', ' security', ' mechanism', ' and', ' will', ' not', ' create', ' a', ' back', 'door', ' in', ' any', ' of', ' our', ' systems', '.\u201d']"}], "[\n    [\"Meanwhile\", \"\u041c\u0435\u0436\u0434\u0443 \u0442\u0435\u043c\"],\n    [\",\", null],\n    [\" Apple\", \"Apple\"],\n    [\" emphasized\", \"\u043f\u043e\u0434\u0447\u0435\u0440\u043a\u043d\u0443\u043b\u0430\"],\n    [\" in\", null],\n    [\" its\", null],\n    [\" statement\", \"\u0437\u0430\u044f\u0432\u043b\u0435\u043d\u0438\u0438\"],\n    [\" that\", null],\n    [\",\", null],\n    [\" \\\"\", null],\n    [\"Apple\", \"Apple\"],\n    [\" has\", null],\n    [\" a\", null],\n    [\" strong\", \"\u0441\u0438\u043b\u044c\u043d\u044b\u0439\"],\n    [\" data\", \"\u0434\u0430\u043d\u043d\u044b\u0445\"],\n    [\" privacy\", \"\u043a\u043e\u043d\u0444\u0438\u0434\u0435\u043d\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438\"],\n    [\" and\", null],\n    [\" security\", \"\u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u0438\"],\n    [\" mechanism\", \"\u043c\u0435\u0445\u0430\u043d\u0438\u0437\u043c\"],\n    [\" and\", null],\n    [\" will\", null],\n    [\" not\", null],\n    [\" create\", \"\u0441\u043e\u0437\u0434\u0430\u0432\u0430\u0442\u044c\"],\n    [\" a\", null],\n    [\" back\", \"\u0447\u0435\u0440\u043d\u044b\u0439\"],\n    [\"door\", null],\n    [\" in\", null],\n    [\" any\", null],\n    [\" of\", null],\n    [\" our\", null],\n    [\" systems\", \"\u0441\u0438\u0441\u0442\u0435\u043c\"],\n    [\".\\\"\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nLocal Women's Federation invited teachers from party schools to explain to local villagers the contents they were particularly concerned about in the 19th CPC National Congress report in a plain manner.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Local', ' Women', \"'s\", ' Federation', ' invited', ' teachers', ' from', ' party', ' schools', ' to', ' explain', ' to', ' local', ' villagers', ' the', ' contents', ' they', ' were', ' particularly', ' concerned', ' about', ' in', ' the', ' ', '1', '9', 'th', ' CPC', ' National', ' Congress', ' report', ' in', ' a', ' plain', ' manner', '.']"}], "[\n    [\"Local\", \"\u5f53\u5730\"],\n    [\" Women\", \"\u5987\u5973\"],\n    [\"'s\", null],\n    [\" Federation\", \"\u8054\u5408\u4f1a\"],\n    [\" invited\", \"\u9080\u8bf7\"],\n    [\" teachers\", \"\u6559\u5e08\"],\n    [\" from\", null],\n    [\" party\", \"\u515a\"],\n    [\" schools\", \"\u6821\"],\n    [\" to\", null],\n    [\" explain\", \"\u89e3\u91ca\"],\n    [\" to\", null],\n    [\" local\", \"\u5f53\u5730\"],\n    [\" villagers\", \"\u6751\u6c11\"],\n    [\" the\", null],\n    [\" contents\", \"\u5185\u5bb9\"],\n    [\" they\", \"\u4ed6\u4eec\"],\n    [\" were\", null],\n    [\" particularly\", \"\u7279\u522b\"],\n    [\" concerned\", \"\u5173\u6ce8\"],\n    [\" about\", null],\n    [\" in\", null],\n    [\" the\", null],\n    [\" \", null],\n    [\"1\", null],\n    [\"9\", null],\n    [\"th\", null],\n    [\" CPC\", \"\u4e2d\u5171\"],\n    [\" National\", \"\u5168\u56fd\"],\n    [\" Congress\", \"\u4ee3\u8868\u5927\u4f1a\"],\n    [\" report\", \"\u62a5\u544a\"],\n    [\" in\", null],\n    [\" a\", null],\n    [\" plain\", \"\u901a\u4fd7\"],\n    [\" manner\", \"\u65b9\u5f0f\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nLocal Women's Federation invited teachers from party schools to explain to local villagers the contents they were particularly concerned about in the 19th CPC National Congress report in a plain manner.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Local', ' Women', \"'s\", ' Federation', ' invited', ' teachers', ' from', ' party', ' schools', ' to', ' explain', ' to', ' local', ' villagers', ' the', ' contents', ' they', ' were', ' particularly', ' concerned', ' about', ' in', ' the', ' ', '1', '9', 'th', ' CPC', ' National', ' Congress', ' report', ' in', ' a', ' plain', ' manner', '.']"}], "[\n[\"Local\", \"\u0416\u0435\u0440\u0433\u0456\u043b\u0456\u043a\u0442\u0456\"],\n[\" Women\", \"\u04d8\u0439\u0435\u043b\u0434\u0435\u0440\"],\n[\"'s\", null],\n[\" Federation\", \"\u0424\u0435\u0434\u0435\u0440\u0430\u0446\u0438\u044f\u0441\u044b\"],\n[\" invited\", \"\u0448\u0430\u049b\u044b\u0440\u0434\u044b\"],\n[\" teachers\", \"\u043c\u04b1\u0493\u0430\u043b\u0456\u043c\u0434\u0435\u0440\u0434\u0456\"],\n[\" from\", null],\n[\" party\", \"\u043f\u0430\u0440\u0442\u0438\u044f\"],\n[\" schools\", \"\u043c\u0435\u043a\u0442\u0435\u043f\u0442\u0435\u0440\u0456\u043d\u0435\u043d\"],\n[\" to\", null],\n[\" explain\", \"\u0442\u04af\u0441\u0456\u043d\u0434\u0456\u0440\u0443\"],\n[\" to\", null],\n[\" local\", \"\u0436\u0435\u0440\u0433\u0456\u043b\u0456\u043a\u0442\u0456\"],\n[\" villagers\", \"\u0430\u0443\u044b\u043b \u0442\u04b1\u0440\u0493\u044b\u043d\u0434\u0430\u0440\u044b\u043d\u0430\"],\n[\" the\", null],\n[\" contents\", \"\u043c\u0430\u0437\u043c\u04b1\u043d\u044b\u043d\"],\n[\" they\", \"\u043e\u043b\u0430\u0440\"],\n[\" were\", null],\n[\" particularly\", \"\u0435\u0440\u0435\u043a\u0448\u0435\"],\n[\" concerned\", \"\u0430\u043b\u0430\u04a3\u0434\u0430\u0493\u0430\u043d\"],\n[\" about\", null],\n[\" in\", null],\n[\" the\", null],\n[\" \", null],\n[\"1\", null],\n[\"9\", null],\n[\"th\", null],\n[\" CPC\", \"\u049a\u041a\u041f\"],\n[\" National\", \"\u04b0\u043b\u0442\u0442\u044b\u049b\"],\n[\" Congress\", \"\u041a\u043e\u043d\u0433\u0440\u0435\u0441\u0456\"],\n[\" report\", \"\u0431\u0430\u044f\u043d\u0434\u0430\u043c\u0430\u0441\u044b\u043d\u044b\u04a3\"],\n[\" in\", null],\n[\" a\", null],\n[\" plain\", \"\u049b\u0430\u0440\u0430\u043f\u0430\u0439\u044b\u043c\"],\n[\" manner\", \"\u0442\u04af\u0440\u0434\u0435\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nLocal Women's Federation invited teachers from party schools to explain to local villagers the contents they were particularly concerned about in the 19th CPC National Congress report in a plain manner.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Local', ' Women', \"'s\", ' Federation', ' invited', ' teachers', ' from', ' party', ' schools', ' to', ' explain', ' to', ' local', ' villagers', ' the', ' contents', ' they', ' were', ' particularly', ' concerned', ' about', ' in', ' the', ' ', '1', '9', 'th', ' CPC', ' National', ' Congress', ' report', ' in', ' a', ' plain', ' manner', '.']"}], "[\n[\"Local\", \"\u041c\u0435\u0441\u0442\u043d\u0430\u044f\"],\n[\" Women\", \"\u0416\u0435\u043d\u0441\u043a\u0430\u044f\"],\n[\"'s\", null],\n[\" Federation\", \"\u0424\u0435\u0434\u0435\u0440\u0430\u0446\u0438\u044f\"],\n[\" invited\", \"\u043f\u0440\u0438\u0433\u043b\u0430\u0441\u0438\u043b\u0430\"],\n[\" teachers\", \"\u043f\u0440\u0435\u043f\u043e\u0434\u0430\u0432\u0430\u0442\u0435\u043b\u0435\u0439\"],\n[\" from\", null],\n[\" party\", \"\u043f\u0430\u0440\u0442\u0438\u0439\u043d\u044b\u0445\"],\n[\" schools\", \"\u0448\u043a\u043e\u043b\"],\n[\" to\", null],\n[\" explain\", \"\u043e\u0431\u044a\u044f\u0441\u043d\u0438\u0442\u044c\"],\n[\" to\", null],\n[\" local\", \"\u043c\u0435\u0441\u0442\u043d\u044b\u043c\"],\n[\" villagers\", \"\u0436\u0438\u0442\u0435\u043b\u044f\u043c\"],\n[\" the\", null],\n[\" contents\", \"\u0441\u043e\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u0435\"],\n[\" they\", null],\n[\" were\", null],\n[\" particularly\", \"\u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\"],\n[\" concerned\", \"\u0431\u0435\u0441\u043f\u043e\u043a\u043e\u0438\u043b\u043e\"],\n[\" about\", null],\n[\" in\", null],\n[\" the\", null],\n[\" \", null],\n[\"1\", null],\n[\"9\", null],\n[\"th\", null],\n[\" CPC\", \"\u041a\u041f\u041a\"],\n[\" National\", \"\u0412\u0441\u0435\u043a\u0438\u0442\u0430\u0439\u0441\u043a\u043e\u0433\u043e\"],\n[\" Congress\", \"\u0441\u044a\u0435\u0437\u0434\u0430\"],\n[\" report\", \"\u0434\u043e\u043a\u043b\u0430\u0434\u0430\"],\n[\" in\", null],\n[\" a\", null],\n[\" plain\", \"\u043f\u0440\u043e\u0441\u0442\u044b\u043c\"],\n[\" manner\", \"\u0441\u043f\u043e\u0441\u043e\u0431\u043e\u043c\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nDeborah Gold, chief executive at NAT (National Aids Trust), said: 'This is a pivotal moment in the fight against HIV.\n\n\n---=== TOKENIZED TEXT ===---\n\n['De', 'borah', ' Gold', ',', ' chief', ' executive', ' at', ' NAT', ' (', 'National', ' A', 'ids', ' Trust', '),', ' said', ':', \" '\", 'This', ' is', ' a', ' pivotal', ' moment', ' in', ' the', ' fight', ' against', ' HIV', '.']"}], "[\n  [\"De\", \"\u5fb7\u535a\u62c9\"],\n  [\"borah\", null],\n  [\" Gold\", null],\n  [\",\", null],\n  [\" chief\", \"\u9996\u5e2d\"],\n  [\" executive\", \"\u6267\u884c\u5b98\"],\n  [\" at\", null],\n  [\" NAT\", \"NAT\"],\n  [\" (\", null],\n  [\"National\", \"\u5168\u56fd\"],\n  [\" A\", \"\u827e\u6ecb\u75c5\"],\n  [\"ids\", null],\n  [\" Trust\", \"\u4fe1\u6258\"],\n  [\")\", null],\n  [\",\", null],\n  [\" said\", \"\u8bf4\"],\n  [\":\", null],\n  [\" '\", null],\n  [\"This\", \"\u8fd9\"],\n  [\" is\", null],\n  [\" a\", null],\n  [\" pivotal\", \"\u5173\u952e\"],\n  [\" moment\", \"\u65f6\u523b\"],\n  [\" in\", null],\n  [\" the\", null],\n  [\" fight\", \"\u6597\u4e89\"],\n  [\" against\", \"\u53cd\u5bf9\"],\n  [\" HIV\", \"\u827e\u6ecb\u75c5\u6bd2\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nDeborah Gold, chief executive at NAT (National Aids Trust), said: 'This is a pivotal moment in the fight against HIV.\n\n\n---=== TOKENIZED TEXT ===---\n\n['De', 'borah', ' Gold', ',', ' chief', ' executive', ' at', ' NAT', ' (', 'National', ' A', 'ids', ' Trust', '),', ' said', ':', \" '\", 'This', ' is', ' a', ' pivotal', ' moment', ' in', ' the', ' fight', ' against', ' HIV', '.']"}], "[\n[\"De\", \"\u0414\u0435\u0431\u043e\u0440\u0430\"],\n[\"borah\", null],\n[\" Gold\", null],\n[\",\", null],\n[\" chief\", \"\u0431\u0430\u0441\"],\n[\" executive\", \"\u0430\u0442\u049b\u0430\u0440\u0443\u0448\u044b\"],\n[\" at\", null],\n[\" NAT\", \"\u04b0\u0410\u0422\"],\n[\" (\", null],\n[\"National\", \"\u04b0\u043b\u0442\u0442\u044b\u049b\"],\n[\" A\", \"\u0416\u0418\u0422\u0421\"],\n[\"ids\", null],\n[\" Trust\", \"\u049a\u043e\u0440\u044b\"],\n[\")\", null],\n[\",\", null],\n[\" said\", \"\u0434\u0435\u0434\u0456\"],\n[\":\", null],\n[\" '\", null],\n[\"This\", \"\u0411\u04b1\u043b\"],\n[\" is\", null],\n[\" a\", null],\n[\" pivotal\", \"\u0448\u0435\u0448\u0443\u0448\u0456\"],\n[\" moment\", \"\u0441\u04d9\u0442\"],\n[\" in\", null],\n[\" the\", null],\n[\" fight\", \"\u043a\u04af\u0440\u0435\u0441\u0442\u0435\"],\n[\" against\", null],\n[\" HIV\", \"\u0410\u0418\u0422\u0412\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nDeborah Gold, chief executive at NAT (National Aids Trust), said: 'This is a pivotal moment in the fight against HIV.\n\n\n---=== TOKENIZED TEXT ===---\n\n['De', 'borah', ' Gold', ',', ' chief', ' executive', ' at', ' NAT', ' (', 'National', ' A', 'ids', ' Trust', '),', ' said', ':', \" '\", 'This', ' is', ' a', ' pivotal', ' moment', ' in', ' the', ' fight', ' against', ' HIV', '.']"}], "[\n  [\"De\", \"\u0414\u0435\u0431\u043e\u0440\u0430\"],\n  [\"borah\", null],\n  [\" Gold\", null],\n  [\",\", null],\n  [\" chief\", \"\u0433\u043b\u0430\u0432\u043d\u044b\u0439\"],\n  [\" executive\", \"\u0438\u0441\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\"],\n  [\" at\", null],\n  [\" NAT\", \"\u041d\u0410\u0422\"],\n  [\" (\", null],\n  [\"National\", \"\u041d\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439\"],\n  [\" A\", \"\u0421\u041f\u0418\u0414\"],\n  [\"ids\", null],\n  [\" Trust\", \"\u0424\u043e\u043d\u0434\"],\n  [\")\", null],\n  [\" said\", \"\u0441\u043a\u0430\u0437\u0430\u043b\u0430\"],\n  [\":\", null],\n  [\" '\", null],\n  [\"This\", \"\u042d\u0442\u043e\"],\n  [\" is\", null],\n  [\" a\", null],\n  [\" pivotal\", \"\u043f\u0435\u0440\u0435\u043b\u043e\u043c\u043d\u044b\u0439\"],\n  [\" moment\", \"\u043c\u043e\u043c\u0435\u043d\u0442\"],\n  [\" in\", null],\n  [\" the\", null],\n  [\" fight\", \"\u0431\u043e\u0440\u044c\u0431\u0435\"],\n  [\" against\", \"\u043f\u0440\u043e\u0442\u0438\u0432\"],\n  [\" HIV\", \"\u0412\u0418\u0427\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nAfter the poverty-stricken counties are lifted out of poverty, relevant policies, supports and measures shall be continued for them.\n\n\n---=== TOKENIZED TEXT ===---\n\n['After', ' the', ' poverty', '-str', 'icken', ' counties', ' are', ' lifted', ' out', ' of', ' poverty', ',', ' relevant', ' policies', ',', ' supports', ' and', ' measures', ' shall', ' be', ' continued', ' for', ' them', '.']"}], "[\n  [\"After\", \"\u5728\"],\n  [\" \", null],\n  [\"the\", null],\n  [\" \", null],\n  [\"poverty\", \"\u8d2b\u56f0\"],\n  [\"-\", null],\n  [\"str\", null],\n  [\"icken\", null],\n  [\" \", null],\n  [\"counties\", \"\u53bf\"],\n  [\" \", null],\n  [\"are\", null],\n  [\" \", null],\n  [\"lifted\", \"\u8131\u8d2b\"],\n  [\" \", null],\n  [\"out\", null],\n  [\" \", null],\n  [\"of\", null],\n  [\" \", null],\n  [\"poverty\", \"\u8d2b\u56f0\"],\n  [\",\", null],\n  [\" \", null],\n  [\"relevant\", \"\u76f8\u5173\"],\n  [\" \", null],\n  [\"policies\", \"\u653f\u7b56\"],\n  [\",\", null],\n  [\" \", null],\n  [\"supports\", \"\u652f\u6301\"],\n  [\" \", null],\n  [\"and\", null],\n  [\" \", null],\n  [\"measures\", \"\u63aa\u65bd\"],\n  [\" \", null],\n  [\"shall\", null],\n  [\" \", null],\n  [\"be\", null],\n  [\" \", null],\n  [\"continued\", \"\u7ee7\u7eed\"],\n  [\" \", null],\n  [\"for\", null],\n  [\" \", null],\n  [\"them\", \"\u5b83\u4eec\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nAfter the poverty-stricken counties are lifted out of poverty, relevant policies, supports and measures shall be continued for them.\n\n\n---=== TOKENIZED TEXT ===---\n\n['After', ' the', ' poverty', '-str', 'icken', ' counties', ' are', ' lifted', ' out', ' of', ' poverty', ',', ' relevant', ' policies', ',', ' supports', ' and', ' measures', ' shall', ' be', ' continued', ' for', ' them', '.']"}], "[\n[\"After\", \"\u041a\u0435\u0439\u0456\u043d\"],\n[\" \", null],\n[\"the\", null],\n[\" \", null],\n[\"poverty\", \"\u043a\u0435\u0434\u0435\u0439\u043b\u0456\u043a\"],\n[\"-\", null],\n[\"str\", null],\n[\"icken\", null],\n[\" \", null],\n[\"counties\", \"\u0430\u0443\u0434\u0430\u043d\u0434\u0430\u0440\"],\n[\" \", null],\n[\"are\", null],\n[\" \", null],\n[\"lifted\", \"\u0448\u044b\u0493\u0430\u0440\u044b\u043b\u0493\u0430\u043d\u043d\u0430\u043d\"],\n[\" \", null],\n[\"out\", null],\n[\" \", null],\n[\"of\", null],\n[\" \", null],\n[\"poverty\", \"\u043a\u0435\u0434\u0435\u0439\u043b\u0456\u043a\u0442\u0435\u043d\"],\n[\",\", null],\n[\" \", null],\n[\"relevant\", \"\u0442\u0438\u0456\u0441\u0442\u0456\"],\n[\" \", null],\n[\"policies\", \"\u0441\u0430\u044f\u0441\u0430\u0442\u0442\u0430\u0440\"],\n[\",\", null],\n[\" \", null],\n[\"supports\", \"\u049b\u043e\u043b\u0434\u0430\u0443\u043b\u0430\u0440\"],\n[\" \", null],\n[\"and\", null],\n[\" \", null],\n[\"measures\", \"\u0448\u0430\u0440\u0430\u043b\u0430\u0440\"],\n[\" \", null],\n[\"shall\", null],\n[\" \", null],\n[\"be\", null],\n[\" \", null],\n[\"continued\", \"\u0436\u0430\u043b\u0493\u0430\u0441\u0442\u044b\u0440\u044b\u043b\u0443\u044b\"],\n[\" \", null],\n[\"for\", null],\n[\" \", null],\n[\"them\", \"\u043e\u043b\u0430\u0440\u0493\u0430\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nAfter the poverty-stricken counties are lifted out of poverty, relevant policies, supports and measures shall be continued for them.\n\n\n---=== TOKENIZED TEXT ===---\n\n['After', ' the', ' poverty', '-str', 'icken', ' counties', ' are', ' lifted', ' out', ' of', ' poverty', ',', ' relevant', ' policies', ',', ' supports', ' and', ' measures', ' shall', ' be', ' continued', ' for', ' them', '.']"}], "[\n[\"After\", \"\u041f\u043e\u0441\u043b\u0435\"],\n[\" \", null],\n[\"the\", null],\n[\" \", null],\n[\"poverty\", \"\u0431\u0435\u0434\u043d\u043e\u0441\u0442\u044c\"],\n[\"-str\", null],\n[\"icken\", null],\n[\" \", null],\n[\"counties\", \"\u043e\u043a\u0440\u0443\u0433\u0430\"],\n[\" \", null],\n[\"are\", null],\n[\" \", null],\n[\"lifted\", \"\u0432\u044b\u0432\u0435\u0434\u0435\u043d\u044b\"],\n[\" \", null],\n[\"out\", null],\n[\" \", null],\n[\"of\", null],\n[\" \", null],\n[\"poverty\", \"\u0431\u0435\u0434\u043d\u043e\u0441\u0442\u0438\"],\n[\",\", null],\n[\" \", null],\n[\"relevant\", \"\u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435\"],\n[\" \", null],\n[\"policies\", \"\u043f\u043e\u043b\u0438\u0442\u0438\u043a\u0438\"],\n[\",\", null],\n[\" \", null],\n[\"supports\", \"\u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0438\"],\n[\" \", null],\n[\"and\", null],\n[\" \", null],\n[\"measures\", \"\u043c\u0435\u0440\u044b\"],\n[\" \", null],\n[\"shall\", null],\n[\" \", null],\n[\"be\", null],\n[\" \", null],\n[\"continued\", \"\u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0435\u043d\u044b\"],\n[\" \", null],\n[\"for\", null],\n[\" \", null],\n[\"them\", \"\u043d\u0438\u0445\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nThe day after the Congress was concluded, Zhang Jian, a delegate of the 19th CPC National Congress and prima ballerina of the National Ballet of China,\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', ' day', ' after', ' the', ' Congress', ' was', ' concluded', ',', ' Zhang', ' Jian', ',', ' a', ' delegate', ' of', ' the', ' ', '1', '9', 'th', ' CPC', ' National', ' Congress', ' and', ' prima', ' ball', 'er', 'ina', ' of', ' the', ' National', ' Ballet', ' of', ' China', ',']"}], "[\n[\"The\", null],\n[\" day\", \"\u5929\"],\n[\" after\", null],\n[\" the\", null],\n[\" Congress\", \"\u5927\u4f1a\"],\n[\" was\", null],\n[\" concluded\", \"\u7ed3\u675f\"],\n[\",\", null],\n[\" Zhang\", \"\u5f20\"],\n[\" Jian\", \"\u5251\"],\n[\",\", null],\n[\" a\", null],\n[\" delegate\", \"\u4ee3\u8868\"],\n[\" of\", null],\n[\" the\", null],\n[\" \", null],\n[\"1\", null],\n[\"9\", null],\n[\"th\", null],\n[\" CPC\", \"\u4e2d\u5171\"],\n[\" National\", \"\u5168\u56fd\"],\n[\" Congress\", \"\u4ee3\u8868\u5927\u4f1a\"],\n[\" and\", null],\n[\" prima\", \"\u9996\u5e2d\"],\n[\" ball\", null],\n[\"er\", null],\n[\"ina\", null],\n[\" of\", null],\n[\" the\", null],\n[\" National\", \"\u56fd\u5bb6\"],\n[\" Ballet\", \"\u82ad\u857e\u821e\u56e2\"],\n[\" of\", null],\n[\" China\", \"\u4e2d\u56fd\"],\n[\",\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nThe day after the Congress was concluded, Zhang Jian, a delegate of the 19th CPC National Congress and prima ballerina of the National Ballet of China,\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', ' day', ' after', ' the', ' Congress', ' was', ' concluded', ',', ' Zhang', ' Jian', ',', ' a', ' delegate', ' of', ' the', ' ', '1', '9', 'th', ' CPC', ' National', ' Congress', ' and', ' prima', ' ball', 'er', 'ina', ' of', ' the', ' National', ' Ballet', ' of', ' China', ',']"}], "[\n    [\"The\", null],\n    [\" day\", \"\u043a\u04af\u043d\"],\n    [\" after\", \"\u043a\u0435\u0439\u0456\u043d\"],\n    [\" the\", null],\n    [\" Congress\", \"\u041a\u043e\u043d\u0433\u0440\u0435\u0441\u0441\"],\n    [\" was\", null],\n    [\" concluded\", \"\u0430\u044f\u049b\u0442\u0430\u043b\u0434\u044b\"],\n    [\",\", null],\n    [\" Zhang\", \"\u0427\u0436\u0430\u043d\"],\n    [\" Jian\", \"\u0426\u0437\u044f\u043d\u044c\"],\n    [\",\", null],\n    [\" a\", null],\n    [\" delegate\", \"\u0434\u0435\u043b\u0435\u0433\u0430\u0442\"],\n    [\" of\", null],\n    [\" the\", null],\n    [\" \", null],\n    [\"1\", null],\n    [\"9\", null],\n    [\"th\", null],\n    [\" CPC\", \"\u049a\u041a\u041f\"],\n    [\" National\", \"\u04b0\u043b\u0442\u0442\u044b\u049b\"],\n    [\" Congress\", \"\u041a\u043e\u043d\u0433\u0440\u0435\u0441\u0441\"],\n    [\" and\", \"\u0436\u04d9\u043d\u0435\"],\n    [\" prima\", \"\u043f\u0440\u0438\u043c\u0430\"],\n    [\" ball\", \"\u0431\u0430\u043b\u0435\u0440\u0438\u043d\u0430\"],\n    [\"er\", null],\n    [\"ina\", null],\n    [\" of\", null],\n    [\" the\", null],\n    [\" National\", \"\u04b0\u043b\u0442\u0442\u044b\u049b\"],\n    [\" Ballet\", \"\u0411\u0430\u043b\u0435\u0442\"],\n    [\" of\", null],\n    [\" China\", \"\u049a\u044b\u0442\u0430\u0439\"],\n    [\",\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThe day after the Congress was concluded, Zhang Jian, a delegate of the 19th CPC National Congress and prima ballerina of the National Ballet of China,\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', ' day', ' after', ' the', ' Congress', ' was', ' concluded', ',', ' Zhang', ' Jian', ',', ' a', ' delegate', ' of', ' the', ' ', '1', '9', 'th', ' CPC', ' National', ' Congress', ' and', ' prima', ' ball', 'er', 'ina', ' of', ' the', ' National', ' Ballet', ' of', ' China', ',']"}], "[\n  [\"The\", null],\n  [\" day\", \"\u0434\u0435\u043d\u044c\"],\n  [\" after\", \"\u043f\u043e\u0441\u043b\u0435\"],\n  [\" the\", null],\n  [\" Congress\", \"\u041a\u043e\u043d\u0433\u0440\u0435\u0441\u0441\"],\n  [\" was\", null],\n  [\" concluded\", \"\u0437\u0430\u0432\u0435\u0440\u0448\u0438\u043b\u0441\u044f\"],\n  [\",\", null],\n  [\" Zhang\", \"\u0427\u0436\u0430\u043d\"],\n  [\" Jian\", \"\u0426\u0437\u044f\u043d\u044c\"],\n  [\",\", null],\n  [\" a\", null],\n  [\" delegate\", \"\u0434\u0435\u043b\u0435\u0433\u0430\u0442\"],\n  [\" of\", null],\n  [\" the\", null],\n  [\" \", null],\n  [\"1\", null],\n  [\"9\", null],\n  [\"th\", null],\n  [\" CPC\", \"\u041a\u041f\u041a\"],\n  [\" National\", \"\u041d\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439\"],\n  [\" Congress\", \"\u041a\u043e\u043d\u0433\u0440\u0435\u0441\u0441\"],\n  [\" and\", \"\u0438\"],\n  [\" prima\", \"\u043f\u0440\u0438\u043c\u0430\"],\n  [\" ball\", \"\u0431\u0430\u043b\u0435\u0440\u0438\u043d\u0430\"],\n  [\"er\", null],\n  [\"ina\", null],\n  [\" of\", null],\n  [\" the\", null],\n  [\" National\", \"\u041d\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439\"],\n  [\" Ballet\", \"\u0431\u0430\u043b\u0435\u0442\"],\n  [\" of\", null],\n  [\" China\", \"\u041a\u0438\u0442\u0430\u044f\"],\n  [\",\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nWe will work closely for cooperation within multilateral frameworks such as the Shanghai Cooperation Organization to help seeking advancement while stabilizing cooperation in the region\n\n\n---=== TOKENIZED TEXT ===---\n\n['We', ' will', ' work', ' closely', ' for', ' cooperation', ' within', ' mult', 'ilateral', ' frameworks', ' such', ' as', ' the', ' Shanghai', ' Cooperation', ' Organization', ' to', ' help', ' seeking', ' advancement', ' while', ' stabil', 'izing', ' cooperation', ' in', ' the', ' region']"}], "[\n  [\"We\", \"\u6211\u4eec\"],\n  [\" will\", null],\n  [\" work\", \"\u5de5\u4f5c\"],\n  [\" closely\", \"\u5bc6\u5207\"],\n  [\" for\", null],\n  [\" cooperation\", \"\u5408\u4f5c\"],\n  [\" within\", \"\u5728\"],\n  [\" mult\", \"\u591a\u8fb9\"],\n  [\"ilateral\", null],\n  [\" frameworks\", \"\u6846\u67b6\"],\n  [\" such\", null],\n  [\" as\", null],\n  [\" the\", null],\n  [\" Shanghai\", \"\u4e0a\u6d77\"],\n  [\" Cooperation\", \"\u5408\u4f5c\"],\n  [\" Organization\", \"\u7ec4\u7ec7\"],\n  [\" to\", null],\n  [\" help\", \"\u5e2e\u52a9\"],\n  [\" seeking\", \"\u5bfb\u6c42\"],\n  [\" advancement\", \"\u8fdb\u6b65\"],\n  [\" while\", \"\u540c\u65f6\"],\n  [\" stabil\", \"\u7a33\u5b9a\"],\n  [\"izing\", null],\n  [\" cooperation\", \"\u5408\u4f5c\"],\n  [\" in\", null],\n  [\" the\", null],\n  [\" region\", \"\u5730\u533a\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nWe will work closely for cooperation within multilateral frameworks such as the Shanghai Cooperation Organization to help seeking advancement while stabilizing cooperation in the region\n\n\n---=== TOKENIZED TEXT ===---\n\n['We', ' will', ' work', ' closely', ' for', ' cooperation', ' within', ' mult', 'ilateral', ' frameworks', ' such', ' as', ' the', ' Shanghai', ' Cooperation', ' Organization', ' to', ' help', ' seeking', ' advancement', ' while', ' stabil', 'izing', ' cooperation', ' in', ' the', ' region']"}], "[\n[\"We\", \"\u0411\u0456\u0437\"],\n[\" will\", null],\n[\" work\", \"\u0436\u04b1\u043c\u044b\u0441 \u0456\u0441\u0442\u0435\u0439\u043c\u0456\u0437\"],\n[\" closely\", \"\u0442\u044b\u0493\u044b\u0437\"],\n[\" for\", null],\n[\" cooperation\", \"\u044b\u043d\u0442\u044b\u043c\u0430\u049b\u0442\u0430\u0441\u0442\u044b\u049b\"],\n[\" within\", \"\u0430\u044f\u0441\u044b\u043d\u0434\u0430\"],\n[\" mult\", \"\u043a\u04e9\u043f\"],\n[\"ilateral\", null],\n[\" frameworks\", \"\u0436\u0430\u049b\u0442\u044b \u049b\u04b1\u0440\u044b\u043b\u044b\u043c\u0434\u0430\u0440\"],\n[\" such\", null],\n[\" as\", null],\n[\" the\", null],\n[\" Shanghai\", \"\u0428\u0430\u043d\u0445\u0430\u0439\"],\n[\" Cooperation\", \"\u042b\u043d\u0442\u044b\u043c\u0430\u049b\u0442\u0430\u0441\u0442\u044b\u049b\"],\n[\" Organization\", \"\u04b0\u0439\u044b\u043c\u044b\"],\n[\" to\", null],\n[\" help\", \"\u043a\u04e9\u043c\u0435\u043a\u0442\u0435\u0441\u0443\"],\n[\" seeking\", \"\u0456\u0437\u0434\u0435\u0443\"],\n[\" advancement\", \"\u0430\u043b\u0493\u0430 \u0431\u0430\u0441\u0443\"],\n[\" while\", null],\n[\" stabil\", \"\u0442\u04b1\u0440\u0430\u049b\u0442\u0430\u043d\u0434\u044b\u0440\"],\n[\"izing\", null],\n[\" cooperation\", \"\u044b\u043d\u0442\u044b\u043c\u0430\u049b\u0442\u0430\u0441\u0442\u044b\u049b\"],\n[\" in\", null],\n[\" the\", null],\n[\" region\", \"\u0430\u0439\u043c\u0430\u049b\u0442\u0430\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nWe will work closely for cooperation within multilateral frameworks such as the Shanghai Cooperation Organization to help seeking advancement while stabilizing cooperation in the region\n\n\n---=== TOKENIZED TEXT ===---\n\n['We', ' will', ' work', ' closely', ' for', ' cooperation', ' within', ' mult', 'ilateral', ' frameworks', ' such', ' as', ' the', ' Shanghai', ' Cooperation', ' Organization', ' to', ' help', ' seeking', ' advancement', ' while', ' stabil', 'izing', ' cooperation', ' in', ' the', ' region']"}], "[\n    [\"We\", \"\u041c\u044b\"],\n    [\" will\", null],\n    [\" work\", \"\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c\"],\n    [\" closely\", \"\u0442\u0435\u0441\u043d\u043e\"],\n    [\" for\", null],\n    [\" cooperation\", \"\u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u0447\u0435\u0441\u0442\u0432\u043e\"],\n    [\" within\", \"\u0432 \u0440\u0430\u043c\u043a\u0430\u0445\"],\n    [\" mult\", \"\u043c\u043d\u043e\u0433\u043e\u0441\u0442\u043e\u0440\u043e\u043d\u043d\u0438\u0439\"],\n    [\"ilateral\", null],\n    [\" frameworks\", \"\u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b\"],\n    [\" such\", null],\n    [\" as\", null],\n    [\" the\", null],\n    [\" Shanghai\", \"\u0428\u0430\u043d\u0445\u0430\u0439\u0441\u043a\u0430\u044f\"],\n    [\" Cooperation\", \"\u041e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u044f\"],\n    [\" Organization\", null],\n    [\" to\", null],\n    [\" help\", \"\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u044c\"],\n    [\" seeking\", \"\u0441\u0442\u0440\u0435\u043c\u0438\u0442\u044c\u0441\u044f\"],\n    [\" advancement\", \"\u043f\u0440\u043e\u0433\u0440\u0435\u0441\u0441\"],\n    [\" while\", \"\u043f\u0440\u0438 \u044d\u0442\u043e\u043c\"],\n    [\" stabil\", \"\u0441\u0442\u0430\u0431\u0438\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c\"],\n    [\"izing\", null],\n    [\" cooperation\", \"\u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u0447\u0435\u0441\u0442\u0432\u043e\"],\n    [\" in\", null],\n    [\" the\", null],\n    [\" region\", \"\u0440\u0435\u0433\u0438\u043e\u043d\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\n\"I just wanted you to make sure you were aware because it's such an unusual high visibility event,\" the commander of HMX-1 emailed his superior, the Marine Deputy Commandant for Aviation, the day before the event.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\"I', ' just', ' wanted', ' you', ' to', ' make', ' sure', ' you', ' were', ' aware', ' because', ' it', \"'s\", ' such', ' an', ' unusual', ' high', ' visibility', ' event', ',\"', ' the', ' commander', ' of', ' H', 'MX', '-', '1', ' emailed', ' his', ' superior', ',', ' the', ' Marine', ' Deputy', ' Command', 'ant', ' for', ' Aviation', ',', ' the', ' day', ' before', ' the', ' event', '.']"}], "[\n['\"I', '\u6211'],\n[' just', null],\n[' wanted', '\u60f3'],\n[' you', '\u4f60'],\n[' to', null],\n[' make', '\u786e\u4fdd'],\n[' sure', null],\n[' you', '\u4f60'],\n[' were', null],\n[' aware', '\u77e5\u9053'],\n[' because', '\u56e0\u4e3a'],\n[' it', '\u5b83'],\n[\"'s\", null],\n[' such', '\u5982\u6b64'],\n[' an', null],\n[' unusual', '\u4e0d\u5bfb\u5e38\u7684'],\n[' high', '\u9ad8'],\n[' visibility', '\u77e5\u540d\u5ea6'],\n[' event', '\u4e8b\u4ef6'],\n[',\"', null],\n[' the', null],\n[' commander', '\u6307\u6325\u5b98'],\n[' of', null],\n[' H', '\u6d77\u519b\u9646\u6218\u961f\u7b2c\u4e00\u76f4\u5347\u673a\u4e2d\u961f'],\n['MX', null],\n['-', null],\n['1', null],\n[' emailed', '\u53d1\u90ae\u4ef6\u7ed9'],\n[' his', '\u4ed6\u7684'],\n[' superior', '\u4e0a\u7ea7'],\n[',', null],\n[' the', null],\n[' Marine', '\u6d77\u519b\u9646\u6218\u961f'],\n[' Deputy', '\u526f'],\n[' Command', '\u53f8\u4ee4'],\n['ant', null],\n[' for', null],\n[' Aviation', '\u822a\u7a7a'],\n[',', null],\n[' the', null],\n[' day', '\u524d\u4e00\u5929'],\n[' before', null],\n[' the', null],\n[' event', '\u6d3b\u52a8'],\n['.', null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\n\"I just wanted you to make sure you were aware because it's such an unusual high visibility event,\" the commander of HMX-1 emailed his superior, the Marine Deputy Commandant for Aviation, the day before the event.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\"I', ' just', ' wanted', ' you', ' to', ' make', ' sure', ' you', ' were', ' aware', ' because', ' it', \"'s\", ' such', ' an', ' unusual', ' high', ' visibility', ' event', ',\"', ' the', ' commander', ' of', ' H', 'MX', '-', '1', ' emailed', ' his', ' superior', ',', ' the', ' Marine', ' Deputy', ' Command', 'ant', ' for', ' Aviation', ',', ' the', ' day', ' before', ' the', ' event', '.']"}], "[\n['\"I', null],\n[' just', \"\u0436\u0430\u0439\"],\n[' wanted', \"\u049b\u0430\u043b\u0430\u0443\"],\n[' you', \"\u0441\u0456\u0437\"],\n[' to', null],\n[' make', \"\u0436\u0430\u0441\u0430\u0443\"],\n[' sure', \"\u0441\u0435\u043d\u0456\u043c\u0434\u0456\"],\n[' you', \"\u0441\u0456\u0437\"],\n[' were', null],\n[' aware', \"\u0445\u0430\u0431\u0430\u0440\u0434\u0430\u0440\"],\n[' because', \"\u0441\u0435\u0431\u0435\u0431\u0456\"],\n[' it', \"\u0431\u04b1\u043b\"],\n[\"'s\", null],\n[' such', \"\u0441\u043e\u043d\u0434\u0430\u0439\"],\n[' an', null],\n[' unusual', \"\u0435\u0440\u0435\u043a\u0448\u0435\"],\n[' high', \"\u0436\u043e\u0493\u0430\u0440\u044b\"],\n[' visibility', \"\u043a\u04e9\u0440\u0456\u043d\u0435\u0442\u0456\u043d\"],\n[' event', \"\u043e\u049b\u0438\u0493\u0430\"],\n[',\"', null],\n[' the', null],\n[' commander', \"\u043a\u043e\u043c\u0430\u043d\u0434\u0438\u0440\"],\n[' of', null],\n[' H', \"HMX-1\"],\n['MX', null],\n['-', null],\n['1', null],\n[' emailed', \"\u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u0434\u044b\u049b \u043f\u043e\u0448\u0442\u0430 \u0436\u0456\u0431\u0435\u0440\u0434\u0456\"],\n[' his', \"\u043e\u043d\u044b\u04a3\"],\n[' superior', \"\u0431\u0430\u0441\u0448\u044b\u0441\u044b\u043d\u0430\"],\n[',', null],\n[' the', null],\n[' Marine', \"\u0422\u0435\u04a3\u0456\u0437 \u0436\u0430\u044f\u0443 \u04d9\u0441\u043a\u0435\u0440\u0456\"],\n[' Deputy', \"\u041e\u0440\u044b\u043d\u0431\u0430\u0441\u0430\u0440\"],\n[' Command', \"\u049a\u043e\u043b\u0431\u0430\u0441\u0448\u044b\u043b\u044b\u049b\"],\n['ant', null],\n[' for', null],\n[' Aviation', \"\u0410\u0432\u0438\u0430\u0446\u0438\u044f\"],\n[',', null],\n[' the', null],\n[' day', \"\u043a\u04af\u043d\"],\n[' before', \"\u0430\u043b\u0434\u044b\u043d\u0434\u0430\"],\n[' the', null],\n[' event', \"\u043e\u049b\u0438\u0493\u0430\"],\n['.', null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\n\"I just wanted you to make sure you were aware because it's such an unusual high visibility event,\" the commander of HMX-1 emailed his superior, the Marine Deputy Commandant for Aviation, the day before the event.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\"I', ' just', ' wanted', ' you', ' to', ' make', ' sure', ' you', ' were', ' aware', ' because', ' it', \"'s\", ' such', ' an', ' unusual', ' high', ' visibility', ' event', ',\"', ' the', ' commander', ' of', ' H', 'MX', '-', '1', ' emailed', ' his', ' superior', ',', ' the', ' Marine', ' Deputy', ' Command', 'ant', ' for', ' Aviation', ',', ' the', ' day', ' before', ' the', ' event', '.']"}], "[\n['\"I', '\"\u042f'],\n[' just', null],\n[' wanted', '\u0445\u043e\u0442\u0435\u043b'],\n[' you', '\u0432\u044b'],\n[' to', null],\n[' make', '\u0443\u0431\u0435\u0434\u0438\u043b\u0438\u0441\u044c'],\n[' sure', null],\n[' you', '\u0432\u044b'],\n[' were', null],\n[' aware', null],\n[' because', '\u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e'],\n[' it', '\u044d\u0442\u043e'],\n[\"'s\", null],\n[' such', '\u0442\u0430\u043a\u043e\u0435'],\n[' an', null],\n[' unusual', '\u043d\u0435\u043e\u0431\u044b\u0447\u043d\u043e\u0435'],\n[' high', '\u0432\u044b\u0441\u043e\u043a\u043e\u0439'],\n[' visibility', '\u0432\u0438\u0434\u0438\u043c\u043e\u0441\u0442\u0438'],\n[' event', '\u043c\u0435\u0440\u043e\u043f\u0440\u0438\u044f\u0442\u0438\u0435'],\n[',\"', null],\n[' the', null],\n[' commander', '\u043a\u043e\u043c\u0430\u043d\u0434\u0438\u0440'],\n[' of', null],\n[' H', 'HMX'],\n['MX', null],\n['-', null],\n['1', null],\n[' emailed', '\u043e\u0442\u043f\u0440\u0430\u0432\u0438\u043b \u043f\u043e \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u043d\u043e\u0439 \u043f\u043e\u0447\u0442\u0435'],\n[' his', '\u0441\u0432\u043e\u0435\u043c\u0443'],\n[' superior', '\u043d\u0430\u0447\u0430\u043b\u044c\u043d\u0438\u043a\u0443'],\n[',', null],\n[' the', null],\n[' Marine', '\u043c\u043e\u0440\u0441\u043a\u043e\u0439'],\n[' Deputy', '\u0437\u0430\u043c\u0435\u0441\u0442\u0438\u0442\u0435\u043b\u044c'],\n[' Command', '\u043a\u043e\u043c\u0430\u043d\u0434\u0443\u044e\u0449\u0435\u0433\u043e'],\n['ant', null],\n[' for', null],\n[' Aviation', '\u0430\u0432\u0438\u0430\u0446\u0438\u0435\u0439'],\n[',', null],\n[' the', null],\n[' day', '\u0434\u0435\u043d\u044c'],\n[' before', '\u043f\u0435\u0440\u0435\u0434'],\n[' the', null],\n[' event', '\u043c\u0435\u0440\u043e\u043f\u0440\u0438\u044f\u0442\u0438\u0435\u043c'],\n['.', null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nHe suggested if the same works in humans, a combination of L. reuteri and a tryptophan-rich diet may foster a more tolerant, less inflammatory gut environment.\n\n\n---=== TOKENIZED TEXT ===---\n\n['He', ' suggested', ' if', ' the', ' same', ' works', ' in', ' humans', ',', ' a', ' combination', ' of', ' L', '.', ' re', 'uter', 'i', ' and', ' a', ' try', 'pt', 'oph', 'an', '-rich', ' diet', ' may', ' foster', ' a', ' more', ' tolerant', ',', ' less', ' inflammatory', ' gut', ' environment', '.']"}], "[\n[\"He\", \"\u4ed6\"],\n[\" suggested\", \"\u5efa\u8bae\"],\n[\" if\", null],\n[\" the\", null],\n[\" same\", \"\u76f8\u540c\"],\n[\" works\", \"\u6709\u6548\"],\n[\" in\", null],\n[\" humans\", \"\u4eba\u7c7b\"],\n[\",\", null],\n[\" a\", null],\n[\" combination\", \"\u7ec4\u5408\"],\n[\" of\", null],\n[\" L\", \"\u4e73\u6746\u83cc\"],\n[\".\", null],\n[\" re\", null],\n[\"uter\", null],\n[\"i\", null],\n[\" and\", null],\n[\" a\", null],\n[\" try\", \"\u8272\u6c28\u9178\"],\n[\"pt\", null],\n[\"oph\", null],\n[\"an\", null],\n[\"-rich\", \"\u5bcc\u542b\"],\n[\" diet\", \"\u996e\u98df\"],\n[\" may\", \"\u53ef\u80fd\"],\n[\" foster\", \"\u4fc3\u8fdb\"],\n[\" a\", null],\n[\" more\", \"\u66f4\"],\n[\" tolerant\", \"\u5bbd\u5bb9\"],\n[\",\", null],\n[\" less\", \"\u8f83\u5c11\"],\n[\" inflammatory\", \"\u708e\u75c7\"],\n[\" gut\", \"\u80a0\u9053\"],\n[\" environment\", \"\u73af\u5883\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nHe suggested if the same works in humans, a combination of L. reuteri and a tryptophan-rich diet may foster a more tolerant, less inflammatory gut environment.\n\n\n---=== TOKENIZED TEXT ===---\n\n['He', ' suggested', ' if', ' the', ' same', ' works', ' in', ' humans', ',', ' a', ' combination', ' of', ' L', '.', ' re', 'uter', 'i', ' and', ' a', ' try', 'pt', 'oph', 'an', '-rich', ' diet', ' may', ' foster', ' a', ' more', ' tolerant', ',', ' less', ' inflammatory', ' gut', ' environment', '.']"}], "[\n[\"He\", \"\u041e\u043b\"],\n[\" suggested\", \"\u04b1\u0441\u044b\u043d\u0434\u044b\"],\n[\" if\", null],\n[\" the\", null],\n[\" same\", \"\u0441\u043e\u043b\"],\n[\" works\", \"\u0436\u04b1\u043c\u044b\u0441 \u0456\u0441\u0442\u0435\u0439\u0434\u0456\"],\n[\" in\", null],\n[\" humans\", \"\u0430\u0434\u0430\u043c\u0434\u0430\u0440\u0434\u0430\"],\n[\",\", null],\n[\" a\", null],\n[\" combination\", \"\u04af\u0439\u043b\u0435\u0441\u0456\u043c\"],\n[\" of\", null],\n[\" L\", \"L\"],\n[\".\", null],\n[\" re\", null],\n[\"uter\", null],\n[\"i\", null],\n[\" and\", null],\n[\" a\", null],\n[\" try\", null],\n[\"pt\", null],\n[\"oph\", null],\n[\"an\", null],\n[\"-rich\", null],\n[\" diet\", \"\u0434\u0438\u0435\u0442\u0430\"],\n[\" may\", null],\n[\" foster\", \"\u044b\u049b\u043f\u0430\u043b \u0435\u0442\u0443\u0456 \u043c\u04af\u043c\u043a\u0456\u043d\"],\n[\" a\", null],\n[\" more\", null],\n[\" tolerant\", \"\u0442\u04e9\u0437\u0456\u043c\u0434\u0456\"],\n[\",\", null],\n[\" less\", null],\n[\" inflammatory\", \"\u049b\u0430\u0431\u044b\u043d\u0443\u0493\u0430 \u049b\u0430\u0440\u0441\u044b\"],\n[\" gut\", \"\u0456\u0448\u0435\u043a\"],\n[\" environment\", \"\u043e\u0440\u0442\u0430\u0441\u044b\u043d\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nHe suggested if the same works in humans, a combination of L. reuteri and a tryptophan-rich diet may foster a more tolerant, less inflammatory gut environment.\n\n\n---=== TOKENIZED TEXT ===---\n\n['He', ' suggested', ' if', ' the', ' same', ' works', ' in', ' humans', ',', ' a', ' combination', ' of', ' L', '.', ' re', 'uter', 'i', ' and', ' a', ' try', 'pt', 'oph', 'an', '-rich', ' diet', ' may', ' foster', ' a', ' more', ' tolerant', ',', ' less', ' inflammatory', ' gut', ' environment', '.']"}], "[\n[\"He\", \"\u041e\u043d\"],\n[\" suggested\", \"\u043f\u0440\u0435\u0434\u043f\u043e\u043b\u043e\u0436\u0438\u043b\"],\n[\" if\", null],\n[\" the\", null],\n[\" same\", \"\u0442\u043e \u0436\u0435 \u0441\u0430\u043c\u043e\u0435\"],\n[\" works\", \"\u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442\"],\n[\" in\", null],\n[\" humans\", \"\u043b\u044e\u0434\u0435\u0439\"],\n[\",\", null],\n[\" a\", null],\n[\" combination\", \"\u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u044f\"],\n[\" of\", null],\n[\" L\", \"\u041b\"],\n[\".\", null],\n[\" re\", null],\n[\"uter\", null],\n[\"i\", null],\n[\" and\", null],\n[\" a\", null],\n[\" try\", null],\n[\"pt\", null],\n[\"oph\", null],\n[\"an\", null],\n[\"-rich\", null],\n[\" diet\", \"\u0434\u0438\u0435\u0442\u0430\"],\n[\" may\", \"\u043c\u043e\u0436\u0435\u0442\"],\n[\" foster\", \"\u0441\u043f\u043e\u0441\u043e\u0431\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c\"],\n[\" a\", null],\n[\" more\", \"\u0431\u043e\u043b\u0435\u0435\"],\n[\" tolerant\", \"\u0442\u043e\u043b\u0435\u0440\u0430\u043d\u0442\u043d\u043e\u0439\"],\n[\",\", null],\n[\" less\", \"\u043c\u0435\u043d\u0435\u0435\"],\n[\" inflammatory\", \"\u0432\u043e\u0441\u043f\u0430\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439\"],\n[\" gut\", \"\u043a\u0438\u0448\u0435\u0447\u043d\u043e\u0439\"],\n[\" environment\", \"\u0441\u0440\u0435\u0434\u0435\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nTo maintain the sound and stable development of Sino-Canadian relations, we must comply with the trend of the times,\n\n\n---=== TOKENIZED TEXT ===---\n\n['To', ' maintain', ' the', ' sound', ' and', ' stable', ' development', ' of', ' S', 'ino', '-', 'Canadian', ' relations', ',', ' we', ' must', ' comply', ' with', ' the', ' trend', ' of', ' the', ' times', ',']"}], "[\n[\"To\", null],\n[\"maintain\", \"\u7ef4\u6301\"],\n[\" \", null],\n[\"the\", null],\n[\" \", null],\n[\"sound\", \"\u826f\u597d\"],\n[\" \", null],\n[\"and\", null],\n[\" \", null],\n[\"stable\", \"\u7a33\u5b9a\"],\n[\" \", null],\n[\"development\", \"\u53d1\u5c55\"],\n[\" \", null],\n[\"of\", null],\n[\" \", null],\n[\"S\", \"\u4e2d\u52a0\"],\n[\"ino\", null],\n[\"-\", null],\n[\"Canadian\", null],\n[\" \", null],\n[\"relations\", \"\u5173\u7cfb\"],\n[\",\", null],\n[\" \", null],\n[\"we\", null],\n[\" \", null],\n[\"must\", null],\n[\" \", null],\n[\"comply\", \"\u9075\u5faa\"],\n[\" \", null],\n[\"with\", null],\n[\" \", null],\n[\"the\", null],\n[\" \", null],\n[\"trend\", \"\u8d8b\u52bf\"],\n[\" \", null],\n[\"of\", null],\n[\" \", null],\n[\"the\", null],\n[\" \", null],\n[\"times\", \"\u65f6\u4ee3\"],\n[\",\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nTo maintain the sound and stable development of Sino-Canadian relations, we must comply with the trend of the times,\n\n\n---=== TOKENIZED TEXT ===---\n\n['To', ' maintain', ' the', ' sound', ' and', ' stable', ' development', ' of', ' S', 'ino', '-', 'Canadian', ' relations', ',', ' we', ' must', ' comply', ' with', ' the', ' trend', ' of', ' the', ' times', ',']"}], "[\n  [\"To\", null],\n  [\"maintain\", \"\u0441\u0430\u049b\u0442\u0430\u0443\"],\n  [\" \", null],\n  [\"the\", null],\n  [\" \", null],\n  [\"sound\", \"\u0434\u04b1\u0440\u044b\u0441\"],\n  [\" \", null],\n  [\"and\", null],\n  [\" \", null],\n  [\"stable\", \"\u0442\u04b1\u0440\u0430\u049b\u0442\u044b\"],\n  [\" \", null],\n  [\"development\", \"\u0434\u0430\u043c\u0443\"],\n  [\" \", null],\n  [\"of\", null],\n  [\" \", null],\n  [\"S\", \"\u049a\u044b\u0442\u0430\u0439\"],\n  [\"ino\", null],\n  [\"-\", null],\n  [\"Canadian\", \"\u041a\u0430\u043d\u0430\u0434\u0430\"],\n  [\" \", null],\n  [\"relations\", \"\u049b\u0430\u0442\u044b\u043d\u0430\u0441\u0442\u0430\u0440\u044b\"],\n  [\",\", null],\n  [\" \", null],\n  [\"we\", null],\n  [\" \", null],\n  [\"must\", null],\n  [\" \", null],\n  [\"comply\", \"\u0441\u04d9\u0439\u043a\u0435\u0441\"],\n  [\" \", null],\n  [\"with\", null],\n  [\" \", null],\n  [\"the\", null],\n  [\" \", null],\n  [\"trend\", \"\u04af\u0440\u0434\u0456\u0441\u043a\u0435\"],\n  [\" \", null],\n  [\"of\", null],\n  [\" \", null],\n  [\"the\", null],\n  [\" \", null],\n  [\"times\", \"\u0437\u0430\u043c\u0430\u043d\"],\n  [\",\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nTo maintain the sound and stable development of Sino-Canadian relations, we must comply with the trend of the times,\n\n\n---=== TOKENIZED TEXT ===---\n\n['To', ' maintain', ' the', ' sound', ' and', ' stable', ' development', ' of', ' S', 'ino', '-', 'Canadian', ' relations', ',', ' we', ' must', ' comply', ' with', ' the', ' trend', ' of', ' the', ' times', ',']"}], "[\n  [\"To\", null],\n  [\"maintain\", \"\u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0442\u044c\"],\n  [\" \", null],\n  [\"the\", null],\n  [\" \", null],\n  [\"sound\", \"\u0437\u0434\u043e\u0440\u043e\u0432\u043e\u0435\"],\n  [\" \", null],\n  [\"and\", null],\n  [\" \", null],\n  [\"stable\", \"\u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u043e\u0435\"],\n  [\" \", null],\n  [\"development\", \"\u0440\u0430\u0437\u0432\u0438\u0442\u0438\u0435\"],\n  [\" \", null],\n  [\"of\", null],\n  [\" \", null],\n  [\"S\", \"\u043a\u0438\u0442\u0430\u0439\u0441\u043a\u043e\"],\n  [\"ino\", null],\n  [\"-\", null],\n  [\"Canadian\", \"\u043a\u0430\u043d\u0430\u0434\u0441\u043a\u0438\u0445\"],\n  [\" \", null],\n  [\"relations\", \"\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0439\"],\n  [\",\", null],\n  [\" \", null],\n  [\"we\", null],\n  [\" \", null],\n  [\"must\", null],\n  [\" \", null],\n  [\"comply\", \"\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u044c\"],\n  [\" \", null],\n  [\"with\", null],\n  [\" \", null],\n  [\"the\", null],\n  [\" \", null],\n  [\"trend\", \"\u0442\u0435\u043d\u0434\u0435\u043d\u0446\u0438\u0438\"],\n  [\" \", null],\n  [\"of\", null],\n  [\" \", null],\n  [\"the\", null],\n  [\" \", null],\n  [\"times\", \"\u0432\u0440\u0435\u043c\u0435\u043d\u0438\"],\n  [\",\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nThey also had an expose on government spending concentrated in the governor's hometown.\n\n\n---=== TOKENIZED TEXT ===---\n\n['They', ' also', ' had', ' an', ' expose', ' on', ' government', ' spending', ' concentrated', ' in', ' the', ' governor', \"'s\", ' hometown', '.']"}], "[\n  [\"They\", \"\u4ed6\u4eec\"],\n  [\" also\", null],\n  [\" had\", \"\u6709\"],\n  [\" an\", null],\n  [\" expose\", \"\u63ed\u9732\"],\n  [\" on\", null],\n  [\" government\", \"\u653f\u5e9c\"],\n  [\" spending\", \"\u652f\u51fa\"],\n  [\" concentrated\", \"\u96c6\u4e2d\"],\n  [\" in\", null],\n  [\" the\", null],\n  [\" governor\", \"\u5dde\u957f\"],\n  [\"'s\", null],\n  [\" hometown\", \"\u5bb6\u4e61\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nThey also had an expose on government spending concentrated in the governor's hometown.\n\n\n---=== TOKENIZED TEXT ===---\n\n['They', ' also', ' had', ' an', ' expose', ' on', ' government', ' spending', ' concentrated', ' in', ' the', ' governor', \"'s\", ' hometown', '.']"}], "[\n  [\"They\", \"\u041e\u043b\u0430\u0440\"],\n  [\" also\", null],\n  [\" had\", \"\u0431\u043e\u043b\u0434\u044b\"],\n  [\" an\", null],\n  [\" expose\", \"\u04d9\u0448\u043a\u0435\u0440\u0435\u043b\u0435\u0443\"],\n  [\" on\", null],\n  [\" government\", \"\u04af\u043a\u0456\u043c\u0435\u0442\"],\n  [\" spending\", \"\u0448\u044b\u0493\u044b\u043d\u0434\u0430\u0440\u044b\"],\n  [\" concentrated\", \"\u0448\u043e\u0493\u044b\u0440\u043b\u0430\u043d\u0493\u0430\u043d\"],\n  [\" in\", null],\n  [\" the\", null],\n  [\" governor\", \"\u04d9\u043a\u0456\u043c\"],\n  [\"'s\", null],\n  [\" hometown\", \"\u0442\u0443\u0493\u0430\u043d \u049b\u0430\u043b\u0430\u0441\u044b\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThey also had an expose on government spending concentrated in the governor's hometown.\n\n\n---=== TOKENIZED TEXT ===---\n\n['They', ' also', ' had', ' an', ' expose', ' on', ' government', ' spending', ' concentrated', ' in', ' the', ' governor', \"'s\", ' hometown', '.']"}], "[\n  [\"They\", \"\u041e\u043d\u0438\"],\n  [\" also\", null],\n  [\" had\", \"\u0438\u043c\u0435\u043b\u0438\"],\n  [\" an\", null],\n  [\" expose\", \"\u0440\u0430\u0437\u043e\u0431\u043b\u0430\u0447\u0435\u043d\u0438\u0435\"],\n  [\" on\", null],\n  [\" government\", \"\u043f\u0440\u0430\u0432\u0438\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0445\"],\n  [\" spending\", \"\u0440\u0430\u0441\u0445\u043e\u0434\u043e\u0432\"],\n  [\" concentrated\", \"\u0441\u043e\u0441\u0440\u0435\u0434\u043e\u0442\u043e\u0447\u0435\u043d\u043d\u044b\u0445\"],\n  [\" in\", null],\n  [\" the\", null],\n  [\" governor\", \"\u0433\u0443\u0431\u0435\u0440\u043d\u0430\u0442\u043e\u0440\u0430\"],\n  [\"'s\", null],\n  [\" hometown\", \"\u0440\u043e\u0434\u043d\u043e\u043c \u0433\u043e\u0440\u043e\u0434\u0435\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nand unexplained explosions have been detected in the vicinity of the watershed, these crew members are very unlikely to have survived.\n\n\n---=== TOKENIZED TEXT ===---\n\n['and', ' un', 'explained', ' explosions', ' have', ' been', ' detected', ' in', ' the', ' vicinity', ' of', ' the', ' watershed', ',', ' these', ' crew', ' members', ' are', ' very', ' unlikely', ' to', ' have', ' survived', '.']"}], "[\n    [\"and\", null],\n    [\" un\", \"\u672a\u89e3\u91ca\"],\n    [\"explained\", null],\n    [\" explosions\", \"\u7206\u70b8\"],\n    [\" have\", null],\n    [\" been\", null],\n    [\" detected\", \"\u88ab\u68c0\u6d4b\u5230\"],\n    [\" in\", null],\n    [\" the\", null],\n    [\" vicinity\", \"\u9644\u8fd1\"],\n    [\" of\", null],\n    [\" the\", null],\n    [\" watershed\", \"\u5206\u6c34\u5cad\"],\n    [\",\", null],\n    [\" these\", null],\n    [\" crew\", \"\u8239\u5458\"],\n    [\" members\", null],\n    [\" are\", null],\n    [\" very\", null],\n    [\" unlikely\", \"\u4e0d\u592a\u53ef\u80fd\"],\n    [\" to\", null],\n    [\" have\", null],\n    [\" survived\", \"\u5e78\u5b58\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nand unexplained explosions have been detected in the vicinity of the watershed, these crew members are very unlikely to have survived.\n\n\n---=== TOKENIZED TEXT ===---\n\n['and', ' un', 'explained', ' explosions', ' have', ' been', ' detected', ' in', ' the', ' vicinity', ' of', ' the', ' watershed', ',', ' these', ' crew', ' members', ' are', ' very', ' unlikely', ' to', ' have', ' survived', '.']"}], "[\n    [\"and\", null],\n    [\" un\", \"\u0442\u04af\u0441\u0456\u043d\u0456\u043a\u0441\u0456\u0437\"],\n    [\"explained\", null],\n    [\" explosions\", \"\u0436\u0430\u0440\u044b\u043b\u044b\u0441\u0442\u0430\u0440\"],\n    [\" have\", null],\n    [\" been\", null],\n    [\" detected\", \"\u0430\u043d\u044b\u049b\u0442\u0430\u043b\u0434\u044b\"],\n    [\" in\", null],\n    [\" the\", null],\n    [\" vicinity\", \"\u043c\u0430\u04a3\u044b\u043d\u0434\u0430\"],\n    [\" of\", null],\n    [\" the\", null],\n    [\" watershed\", \"\u0441\u0443 \u0430\u0439\u0440\u044b\u0493\u044b\"],\n    [\",\", null],\n    [\" these\", null],\n    [\" crew\", \"\u044d\u043a\u0438\u043f\u0430\u0436\"],\n    [\" members\", \"\u043c\u04af\u0448\u0435\u043b\u0435\u0440\u0456\"],\n    [\" are\", null],\n    [\" very\", null],\n    [\" unlikely\", \"\u0435\u043a\u0456\u0442\u0430\u043b\u0430\u0439\"],\n    [\" to\", null],\n    [\" have\", null],\n    [\" survived\", \"\u0430\u043c\u0430\u043d \u049b\u0430\u043b\u0493\u0430\u043d\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nand unexplained explosions have been detected in the vicinity of the watershed, these crew members are very unlikely to have survived.\n\n\n---=== TOKENIZED TEXT ===---\n\n['and', ' un', 'explained', ' explosions', ' have', ' been', ' detected', ' in', ' the', ' vicinity', ' of', ' the', ' watershed', ',', ' these', ' crew', ' members', ' are', ' very', ' unlikely', ' to', ' have', ' survived', '.']"}], "[\n[\"and\", null],\n[\" un\", \"\u043d\u0435\u043e\u0431\u044a\u044f\u0441\u043d\u0438\u043c\u044b\u0435\"],\n[\"explained\", null],\n[\" explosions\", \"\u0432\u0437\u0440\u044b\u0432\u044b\"],\n[\" have\", null],\n[\" been\", null],\n[\" detected\", \"\u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u044b\"],\n[\" in\", null],\n[\" the\", null],\n[\" vicinity\", \"\u043e\u043a\u0440\u0435\u0441\u0442\u043d\u043e\u0441\u0442\u044f\u0445\"],\n[\" of\", null],\n[\" the\", null],\n[\" watershed\", \"\u0432\u043e\u0434\u043e\u0440\u0430\u0437\u0434\u0435\u043b\u0430\"],\n[\",\", null],\n[\" these\", null],\n[\" crew\", \"\u044d\u043a\u0438\u043f\u0430\u0436\u0430\"],\n[\" members\", null],\n[\" are\", null],\n[\" very\", null],\n[\" unlikely\", \"\u043c\u0430\u043b\u043e\u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\"],\n[\" to\", null],\n[\" have\", null],\n[\" survived\", \"\u0432\u044b\u0436\u0438\u043b\u0438\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nEfforts should be made to fight innovative war and make preparation for war, keep up with evolution in war situation and method of war fighting,\n\n\n---=== TOKENIZED TEXT ===---\n\n['Eff', 'orts', ' should', ' be', ' made', ' to', ' fight', ' innovative', ' war', ' and', ' make', ' preparation', ' for', ' war', ',', ' keep', ' up', ' with', ' evolution', ' in', ' war', ' situation', ' and', ' method', ' of', ' war', ' fighting', ',']"}], "[\n[\"Eff\", \"\u52aa\u529b\"],\n[\"orts\", null],\n[\" should\", null],\n[\" be\", null],\n[\" made\", null],\n[\" to\", null],\n[\" fight\", \"\u6253\"],\n[\" innovative\", \"\u521b\u65b0\u7684\"],\n[\" war\", \"\u6218\u4e89\"],\n[\" and\", null],\n[\" make\", \"\u505a\"],\n[\" preparation\", \"\u51c6\u5907\"],\n[\" for\", null],\n[\" war\", \"\u6218\u4e89\"],\n[\",\", null],\n[\" keep\", \"\u4fdd\u6301\"],\n[\" up\", null],\n[\" with\", null],\n[\" evolution\", \"\u6f14\u53d8\"],\n[\" in\", null],\n[\" war\", \"\u6218\u4e89\"],\n[\" situation\", \"\u5c40\u52bf\"],\n[\" and\", null],\n[\" method\", \"\u65b9\u6cd5\"],\n[\" of\", null],\n[\" war\", \"\u6218\u4e89\"],\n[\" fighting\", \"\u4f5c\u6218\"],\n[\",\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nEfforts should be made to fight innovative war and make preparation for war, keep up with evolution in war situation and method of war fighting,\n\n\n---=== TOKENIZED TEXT ===---\n\n['Eff', 'orts', ' should', ' be', ' made', ' to', ' fight', ' innovative', ' war', ' and', ' make', ' preparation', ' for', ' war', ',', ' keep', ' up', ' with', ' evolution', ' in', ' war', ' situation', ' and', ' method', ' of', ' war', ' fighting', ',']"}], "[\n  [\"Eff\", \"\u041a\u04af\u0448-\u0436\u0456\u0433\u0435\u0440\"],\n  [\"orts\", null],\n  [\" should\", null],\n  [\" be\", null],\n  [\" made\", null],\n  [\" to\", null],\n  [\" fight\", \"\u043a\u04af\u0440\u0435\u0441\u0443\"],\n  [\" innovative\", \"\u0438\u043d\u043d\u043e\u0432\u0430\u0446\u0438\u044f\u043b\u044b\u049b\"],\n  [\" war\", \"\u0441\u043e\u0493\u044b\u0441\"],\n  [\" and\", null],\n  [\" make\", \"\u0436\u0430\u0441\u0430\u0443\"],\n  [\" preparation\", \"\u0434\u0430\u0439\u044b\u043d\u0434\u044b\u049b\"],\n  [\" for\", null],\n  [\" war\", \"\u0441\u043e\u0493\u044b\u0441\"],\n  [\",\", null],\n  [\" keep\", \"\u0441\u0430\u049b\u0442\u0430\u0443\"],\n  [\" up\", null],\n  [\" with\", null],\n  [\" evolution\", \"\u044d\u0432\u043e\u043b\u044e\u0446\u0438\u044f\"],\n  [\" in\", null],\n  [\" war\", \"\u0441\u043e\u0493\u044b\u0441\"],\n  [\" situation\", \"\u0436\u0430\u0493\u0434\u0430\u0439\"],\n  [\" and\", null],\n  [\" method\", \"\u04d9\u0434\u0456\u0441\"],\n  [\" of\", null],\n  [\" war\", \"\u0441\u043e\u0493\u044b\u0441\"],\n  [\" fighting\", \"\u0436\u04af\u0440\u0433\u0456\u0437\u0443\"],\n  [\",\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nEfforts should be made to fight innovative war and make preparation for war, keep up with evolution in war situation and method of war fighting,\n\n\n---=== TOKENIZED TEXT ===---\n\n['Eff', 'orts', ' should', ' be', ' made', ' to', ' fight', ' innovative', ' war', ' and', ' make', ' preparation', ' for', ' war', ',', ' keep', ' up', ' with', ' evolution', ' in', ' war', ' situation', ' and', ' method', ' of', ' war', ' fighting', ',']"}], "[\n[\"Eff\", \"\u0423\u0441\u0438\u043b\u0438\u044f\"],\n[\"orts\", null],\n[\" should\", null],\n[\" be\", null],\n[\" made\", null],\n[\" to\", null],\n[\" fight\", \"\u0431\u043e\u0440\u043e\u0442\u044c\u0441\u044f\"],\n[\" innovative\", \"\u0438\u043d\u043d\u043e\u0432\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f\"],\n[\" war\", \"\u0432\u043e\u0439\u043d\u0430\"],\n[\" and\", null],\n[\" make\", \"\u0434\u0435\u043b\u0430\u0442\u044c\"],\n[\" preparation\", \"\u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0443\"],\n[\" for\", null],\n[\" war\", \"\u0432\u043e\u0439\u043d\u0435\"],\n[\",\", null],\n[\" keep\", \"\u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0442\u044c\"],\n[\" up\", null],\n[\" with\", null],\n[\" evolution\", \"\u044d\u0432\u043e\u043b\u044e\u0446\u0438\u044e\"],\n[\" in\", null],\n[\" war\", \"\u0432\u043e\u0435\u043d\u043d\u043e\u0439\"],\n[\" situation\", \"\u043e\u0431\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0435\"],\n[\" and\", null],\n[\" method\", \"\u043c\u0435\u0442\u043e\u0434\u044b\"],\n[\" of\", null],\n[\" war\", \"\u0432\u043e\u0435\u043d\u043d\u043e\u0433\u043e\"],\n[\" fighting\", \"\u0432\u0435\u0434\u0435\u043d\u0438\u044f \u0431\u043e\u044f\"],\n[\",\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\n\u201cMade in Sichuan\u201d renewable energy vehicles rose against the tide. Weakening fiscal subsidies, inadequate charging facilities, and ternary lithium batteries suspended from use in renewable energy passenger vehicles; in the face of prevailing upward pressures in the domestic renewable energy automobile market\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u201c', 'Made', ' in', ' S', 'ich', 'uan', '\u201d', ' renewable', ' energy', ' vehicles', ' rose', ' against', ' the', ' tide', '.', ' We', 'akening', ' fiscal', ' subsidies', ',', ' inadequate', ' charging', ' facilities', ',', ' and', ' tern', 'ary', ' lithium', ' batteries', ' suspended', ' from', ' use', ' in', ' renewable', ' energy', ' passenger', ' vehicles', ';', ' in', ' the', ' face', ' of', ' prevailing', ' upward', ' pressures', ' in', ' the', ' domestic', ' renewable', ' energy', ' automobile', ' market']"}], "[\n  ['\"', null],\n  [\"Made\", \"\u5236\u9020\"],\n  [\" in\", null],\n  [\" S\", \"\u56db\"],\n  [\"ich\", null],\n  [\"uan\", null],\n  ['\"', null],\n  [\" renewable\", \"\u53ef\u518d\u751f\"],\n  [\" energy\", \"\u80fd\u6e90\"],\n  [\" vehicles\", \"\u8f66\u8f86\"],\n  [\" rose\", \"\u9006\u52bf\u4e0a\u626c\"],\n  [\" against\", null],\n  [\" the\", null],\n  [\" tide\", null],\n  [\".\", null],\n  [\" We\", \"\u51cf\"],\n  [\"akening\", null],\n  [\" fiscal\", \"\u8d22\u653f\"],\n  [\" subsidies\", \"\u8865\u8d34\"],\n  [\",\", null],\n  [\" inadequate\", \"\u4e0d\u8db3\u7684\"],\n  [\" charging\", \"\u5145\u7535\"],\n  [\" facilities\", \"\u8bbe\u65bd\"],\n  [\",\", null],\n  [\" and\", null],\n  [\" tern\", \"\u4e09\"],\n  [\"ary\", null],\n  [\" lithium\", \"\u9502\"],\n  [\" batteries\", \"\u7535\u6c60\"],\n  [\" suspended\", \"\u6682\u505c\"],\n  [\" from\", null],\n  [\" use\", \"\u4f7f\u7528\"],\n  [\" in\", null],\n  [\" renewable\", \"\u53ef\u518d\u751f\"],\n  [\" energy\", \"\u80fd\u6e90\"],\n  [\" passenger\", \"\u4e58\u7528\"],\n  [\" vehicles\", \"\u8f66\"],\n  [\";\", null],\n  [\" in\", null],\n  [\" the\", null],\n  [\" face\", \"\u9762\u5bf9\"],\n  [\" of\", null],\n  [\" prevailing\", \"\u666e\u904d\u5b58\u5728\u7684\"],\n  [\" upward\", \"\u4e0a\u884c\"],\n  [\" pressures\", \"\u538b\u529b\"],\n  [\" in\", null],\n  [\" the\", null],\n  [\" domestic\", \"\u56fd\u5185\"],\n  [\" renewable\", \"\u53ef\u518d\u751f\"],\n  [\" energy\", \"\u80fd\u6e90\"],\n  [\" automobile\", \"\u6c7d\u8f66\"],\n  [\" market\", \"\u5e02\u573a\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\n\u201cMade in Sichuan\u201d renewable energy vehicles rose against the tide. Weakening fiscal subsidies, inadequate charging facilities, and ternary lithium batteries suspended from use in renewable energy passenger vehicles; in the face of prevailing upward pressures in the domestic renewable energy automobile market\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u201c', 'Made', ' in', ' S', 'ich', 'uan', '\u201d', ' renewable', ' energy', ' vehicles', ' rose', ' against', ' the', ' tide', '.', ' We', 'akening', ' fiscal', ' subsidies', ',', ' inadequate', ' charging', ' facilities', ',', ' and', ' tern', 'ary', ' lithium', ' batteries', ' suspended', ' from', ' use', ' in', ' renewable', ' energy', ' passenger', ' vehicles', ';', ' in', ' the', ' face', ' of', ' prevailing', ' upward', ' pressures', ' in', ' the', ' domestic', ' renewable', ' energy', ' automobile', ' market']"}], "[\n['\"', null],\n[\"Made\", \"\u0416\u0430\u0441\u0430\u043b\u0493\u0430\u043d\"],\n[\" in\", null],\n[\" S\", \"\u0421\u044b\u0447\u0443\u0430\u043d\u044c\"],\n[\"ich\", null],\n[\"uan\", null],\n['\"', null],\n[\" renewable\", \"\u0436\u0430\u04a3\u0430\u0440\u0442\u044b\u043b\u0430\u0442\u044b\u043d\"],\n[\" energy\", \"\u044d\u043d\u0435\u0440\u0433\u0438\u044f\"],\n[\" vehicles\", \"\u043a\u04e9\u043b\u0456\u043a\u0442\u0435\u0440\"],\n[\" rose\", \"\u043a\u04e9\u0442\u0435\u0440\u0456\u043b\u0434\u0456\"],\n[\" against\", \"\u049b\u0430\u0440\u0441\u044b\"],\n[\" the\", null],\n[\" tide\", \"\u0430\u0493\u044b\u043c\u0493\u0430\"],\n[\".\", null],\n[\" We\", \"\u04d8\u043b\u0441\u0456\"],\n[\"akening\", null],\n[\" fiscal\", \"\u0444\u0438\u0441\u043a\u0430\u043b\u0434\u044b\u049b\"],\n[\" subsidies\", \"\u0441\u0443\u0431\u0441\u0438\u0434\u0438\u044f\u043b\u0430\u0440\"],\n[\",\", null],\n[\" inadequate\", \"\u0436\u0435\u0442\u043a\u0456\u043b\u0456\u043a\u0441\u0456\u0437\"],\n[\" charging\", \"\u0437\u0430\u0440\u044f\u0434\u0442\u0430\u0443\"],\n[\" facilities\", \"\u049b\u04b1\u0440\u044b\u043b\u0493\u044b\u043b\u0430\u0440\u044b\"],\n[\",\", null],\n[\" and\", null],\n[\" tern\", \"\u04af\u0448\u0442\u0456\u043a\"],\n[\"ary\", null],\n[\" lithium\", \"\u043b\u0438\u0442\u0438\u0439\"],\n[\" batteries\", \"\u0431\u0430\u0442\u0430\u0440\u0435\u044f\u043b\u0430\u0440\u044b\"],\n[\" suspended\", \"\u0442\u043e\u049b\u0442\u0430\u0442\u044b\u043b\u0434\u044b\"],\n[\" from\", null],\n[\" use\", \"\u049b\u043e\u043b\u0434\u0430\u043d\u0443\u0434\u0430\u043d\"],\n[\" in\", null],\n[\" renewable\", \"\u0436\u0430\u04a3\u0430\u0440\u0442\u044b\u043b\u0430\u0442\u044b\u043d\"],\n[\" energy\", \"\u044d\u043d\u0435\u0440\u0433\u0438\u044f\"],\n[\" passenger\", \"\u0436\u043e\u043b\u0430\u0443\u0448\u044b\u043b\u0430\u0440\"],\n[\" vehicles\", \"\u043a\u04e9\u043b\u0456\u043a\u0442\u0435\u0440\u0456\"],\n[\";\", null],\n[\" in\", null],\n[\" the\", null],\n[\" face\", \"\u0436\u0430\u0493\u0434\u0430\u0439\u044b\u043d\u0434\u0430\"],\n[\" of\", null],\n[\" prevailing\", \"\u0431\u0430\u0441\u044b\u043c\"],\n[\" upward\", \"\u0436\u043e\u0493\u0430\u0440\u044b\"],\n[\" pressures\", \"\u049b\u044b\u0441\u044b\u043c\u0434\u0430\u0440\"],\n[\" in\", null],\n[\" the\", null],\n[\" domestic\", \"\u043e\u0442\u0430\u043d\u0434\u044b\u049b\"],\n[\" renewable\", \"\u0436\u0430\u04a3\u0430\u0440\u0442\u044b\u043b\u0430\u0442\u044b\u043d\"],\n[\" energy\", \"\u044d\u043d\u0435\u0440\u0433\u0438\u044f\"],\n[\" automobile\", \"\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044c\"],\n[\" market\", \"\u043d\u0430\u0440\u044b\u0493\u044b\u043d\u0434\u0430\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\n\u201cMade in Sichuan\u201d renewable energy vehicles rose against the tide. Weakening fiscal subsidies, inadequate charging facilities, and ternary lithium batteries suspended from use in renewable energy passenger vehicles; in the face of prevailing upward pressures in the domestic renewable energy automobile market\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u201c', 'Made', ' in', ' S', 'ich', 'uan', '\u201d', ' renewable', ' energy', ' vehicles', ' rose', ' against', ' the', ' tide', '.', ' We', 'akening', ' fiscal', ' subsidies', ',', ' inadequate', ' charging', ' facilities', ',', ' and', ' tern', 'ary', ' lithium', ' batteries', ' suspended', ' from', ' use', ' in', ' renewable', ' energy', ' passenger', ' vehicles', ';', ' in', ' the', ' face', ' of', ' prevailing', ' upward', ' pressures', ' in', ' the', ' domestic', ' renewable', ' energy', ' automobile', ' market']"}], "[\n  ['\"', null],\n  [\"Made\", \"\u0421\u0434\u0435\u043b\u0430\u043d\u043e\"],\n  [\" in\", null],\n  [\" S\", \"\u0421\u044b\u0447\u0443\u0430\u043d\u044c\"],\n  [\"ich\", null],\n  [\"uan\", null],\n  ['\"', null],\n  [\" renewable\", \"\u0432\u043e\u0437\u043e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u043c\u044b\u0439\"],\n  [\" energy\", \"\u044d\u043d\u0435\u0440\u0433\u0438\u044f\"],\n  [\" vehicles\", \"\u0442\u0440\u0430\u043d\u0441\u043f\u043e\u0440\u0442\"],\n  [\" rose\", \"\u043f\u043e\u0434\u043d\u044f\u043b\u0441\u044f\"],\n  [\" against\", \"\u043f\u0440\u043e\u0442\u0438\u0432\"],\n  [\" the\", null],\n  [\" tide\", \"\u0442\u0435\u043d\u0434\u0435\u043d\u0446\u0438\u0438\"],\n  [\".\", null],\n  [\" We\", \"\u041e\u0441\u043b\u0430\u0431\u043b\u0435\u043d\u0438\u0435\"],\n  [\"akening\", null],\n  [\" fiscal\", \"\u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u044b\u0445\"],\n  [\" subsidies\", \"\u0441\u0443\u0431\u0441\u0438\u0434\u0438\u0439\"],\n  [\",\", null],\n  [\" inadequate\", \"\u043d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u044b\u0435\"],\n  [\" charging\", \"\u0437\u0430\u0440\u044f\u0434\u043d\u044b\u0435\"],\n  [\" facilities\", \"\u0441\u0442\u0430\u043d\u0446\u0438\u0438\"],\n  [\",\", null],\n  [\" and\", null],\n  [\" tern\", \"\u0442\u0440\u043e\u0439\u043d\u044b\u0435\"],\n  [\"ary\", null],\n  [\" lithium\", \"\u043b\u0438\u0442\u0438\u0435\u0432\u044b\u0435\"],\n  [\" batteries\", \"\u0431\u0430\u0442\u0430\u0440\u0435\u0438\"],\n  [\" suspended\", \"\u043f\u0440\u0438\u043e\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d\u043d\u044b\u0435\"],\n  [\" from\", null],\n  [\" use\", \"\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f\"],\n  [\" in\", null],\n  [\" renewable\", \"\u0432\u043e\u0437\u043e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u043c\u043e\u0439\"],\n  [\" energy\", \"\u044d\u043d\u0435\u0440\u0433\u0438\u0438\"],\n  [\" passenger\", \"\u043f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u0441\u043a\u0438\u0439\"],\n  [\" vehicles\", \"\u0442\u0440\u0430\u043d\u0441\u043f\u043e\u0440\u0442\"],\n  [\";\", null],\n  [\" in\", null],\n  [\" the\", null],\n  [\" face\", \"\u043f\u0435\u0440\u0435\u0434 \u043b\u0438\u0446\u043e\u043c\"],\n  [\" of\", null],\n  [\" prevailing\", \"\u043f\u0440\u0435\u043e\u0431\u043b\u0430\u0434\u0430\u044e\u0449\u0435\u0433\u043e\"],\n  [\" upward\", \"\u0432\u043e\u0441\u0445\u043e\u0434\u044f\u0449\u0435\u0433\u043e\"],\n  [\" pressures\", \"\u0434\u0430\u0432\u043b\u0435\u043d\u0438\u044f\"],\n  [\" in\", null],\n  [\" the\", null],\n  [\" domestic\", \"\u043e\u0442\u0435\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439\"],\n  [\" renewable\", \"\u0432\u043e\u0437\u043e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u043c\u043e\u0439\"],\n  [\" energy\", \"\u044d\u043d\u0435\u0440\u0433\u0438\u0438\"],\n  [\" automobile\", \"\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044c\u043d\u044b\u0439\"],\n  [\" market\", \"\u0440\u044b\u043d\u043e\u043a\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nChronic total occlusion is described as the \u201clast fortress\u201d of interventional therapy in coronary heart disease. The surgery success rate is around 70%.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Ch', 'ronic', ' total', ' oc', 'clusion', ' is', ' described', ' as', ' the', ' \u201c', 'last', ' fortress', '\u201d', ' of', ' inter', 'ventional', ' therapy', ' in', ' coronary', ' heart', ' disease', '.', ' The', ' surgery', ' success', ' rate', ' is', ' around', ' ', '7', '0', '%.']"}], "[\n  [\"Ch\", \"\u6162\"],\n  [\"ronic\", null],\n  [\" \", null],\n  [\"total\", \"\u5b8c\u5168\"],\n  [\" \", null],\n  [\"oc\", \"\u95ed\"],\n  [\"clusion\", null],\n  [\" \", null],\n  [\"is\", null],\n  [\" \", null],\n  [\"described\", \"\u88ab\u63cf\u8ff0\"],\n  [\" \", null],\n  [\"as\", null],\n  [\" \", null],\n  [\"the\", null],\n  [\" \", null],\n  [\"\"\", null],\n  [\"last\", \"\u6700\u540e\"],\n  [\" \", null],\n  [\"fortress\", \"\u5821\u5792\"],\n  [\"\"\", null],\n  [\" \", null],\n  [\"of\", null],\n  [\" \", null],\n  [\"inter\", \"\u4ecb\u5165\"],\n  [\"ventional\", null],\n  [\" \", null],\n  [\"therapy\", \"\u6cbb\u7597\"],\n  [\" \", null],\n  [\"in\", null],\n  [\" \", null],\n  [\"coronary\", \"\u51a0\u72b6\u52a8\u8109\"],\n  [\" \", null],\n  [\"heart\", \"\u5fc3\u810f\"],\n  [\" \", null],\n  [\"disease\", \"\u75be\u75c5\"],\n  [\".\", null],\n  [\" \", null],\n  [\"The\", null],\n  [\" \", null],\n  [\"surgery\", \"\u624b\u672f\"],\n  [\" \", null],\n  [\"success\", \"\u6210\u529f\"],\n  [\" \", null],\n  [\"rate\", \"\u7387\"],\n  [\" \", null],\n  [\"is\", null],\n  [\" \", null],\n  [\"around\", \"\u5927\u7ea6\"],\n  [\" \", null],\n  [\"7\", \"70\"],\n  [\"0\", null],\n  [\"%\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nChronic total occlusion is described as the \u201clast fortress\u201d of interventional therapy in coronary heart disease. The surgery success rate is around 70%.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Ch', 'ronic', ' total', ' oc', 'clusion', ' is', ' described', ' as', ' the', ' \u201c', 'last', ' fortress', '\u201d', ' of', ' inter', 'ventional', ' therapy', ' in', ' coronary', ' heart', ' disease', '.', ' The', ' surgery', ' success', ' rate', ' is', ' around', ' ', '7', '0', '%.']"}], "[\n    [\"Ch\", \"\u0421\u043e\u0437\u044b\u043b\u043c\u0430\u043b\u044b\"],\n    [\"ronic\", null],\n    [\" \", null],\n    [\"total\", \"\u0442\u043e\u043b\u044b\u049b\"],\n    [\" \", null],\n    [\"oc\", \"\u043e\u043a\u043a\u043b\u044e\u0437\u0438\u044f\"],\n    [\"clusion\", null],\n    [\" \", null],\n    [\"is\", null],\n    [\" \", null],\n    [\"described\", \"\u0441\u0438\u043f\u0430\u0442\u0442\u0430\u043b\u0430\u0434\u044b\"],\n    [\" \", null],\n    [\"as\", null],\n    [\" \", null],\n    [\"the\", null],\n    [\" \", null],\n    [\"\\\"\", null],\n    [\"last\", \"\u0441\u043e\u04a3\u0493\u044b\"],\n    [\" \", null],\n    [\"fortress\", \"\u0431\u0435\u043a\u0456\u043d\u0456\u0441\"],\n    [\"\\\"\", null],\n    [\" \", null],\n    [\"of\", null],\n    [\" \", null],\n    [\"inter\", \"\u0438\u043d\u0442\u0435\u0440\u0432\u0435\u043d\u0446\u0438\u044f\u043b\u044b\u049b\"],\n    [\"ventional\", null],\n    [\" \", null],\n    [\"therapy\", \"\u0442\u0435\u0440\u0430\u043f\u0438\u044f\"],\n    [\" \", null],\n    [\"in\", null],\n    [\" \", null],\n    [\"coronary\", \"\u0436\u04af\u0440\u0435\u043a\"],\n    [\" \", null],\n    [\"heart\", \"\u0442\u04d9\u0436\u0434\u0456\"],\n    [\" \", null],\n    [\"disease\", \"\u0430\u0443\u0440\u0443\u044b\"],\n    [\".\", null],\n    [\" \", null],\n    [\"The\", null],\n    [\" \", null],\n    [\"surgery\", \"\u043e\u0442\u0430\"],\n    [\" \", null],\n    [\"success\", \"\u0441\u04d9\u0442\u0442\u0456\u043b\u0456\u043a\"],\n    [\" \", null],\n    [\"rate\", \"\u043a\u04e9\u0440\u0441\u0435\u0442\u043a\u0456\u0448\u0456\"],\n    [\" \", null],\n    [\"is\", null],\n    [\" \", null],\n    [\"around\", \"\u0448\u0430\u043c\u0430\u043c\u0435\u043d\"],\n    [\" \", null],\n    [\"7\", null],\n    [\"0\", null],\n    [\"%\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nChronic total occlusion is described as the \u201clast fortress\u201d of interventional therapy in coronary heart disease. The surgery success rate is around 70%.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Ch', 'ronic', ' total', ' oc', 'clusion', ' is', ' described', ' as', ' the', ' \u201c', 'last', ' fortress', '\u201d', ' of', ' inter', 'ventional', ' therapy', ' in', ' coronary', ' heart', ' disease', '.', ' The', ' surgery', ' success', ' rate', ' is', ' around', ' ', '7', '0', '%.']"}], "[\n  [\"Ch\", \"\u0425\u0440\u043e\u043d\u0438\u0447\u0435\u0441\u043a\u0430\u044f\"],\n  [\"ronic\", null],\n  [\" \", null],\n  [\"total\", \"\u043f\u043e\u043b\u043d\u0430\u044f\"],\n  [\" \", null],\n  [\"oc\", \"\u043e\u043a\u043a\u043b\u044e\u0437\u0438\u044f\"],\n  [\"clusion\", null],\n  [\" \", null],\n  [\"is\", null],\n  [\" \", null],\n  [\"described\", \"\u043e\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u0442\u0441\u044f\"],\n  [\" \", null],\n  [\"as\", null],\n  [\" \", null],\n  [\"the\", null],\n  [\" \", null],\n  [\"\\\"\", null],\n  [\"last\", \"\u043f\u043e\u0441\u043b\u0435\u0434\u043d\u044f\u044f\"],\n  [\" \", null],\n  [\"fortress\", \"\u043a\u0440\u0435\u043f\u043e\u0441\u0442\u044c\"],\n  [\"\\\"\", null],\n  [\" \", null],\n  [\"of\", null],\n  [\" \", null],\n  [\"inter\", \"\u0438\u043d\u0442\u0435\u0440\u0432\u0435\u043d\u0446\u0438\u043e\u043d\u043d\u043e\u0439\"],\n  [\"ventional\", null],\n  [\" \", null],\n  [\"therapy\", \"\u0442\u0435\u0440\u0430\u043f\u0438\u0438\"],\n  [\" \", null],\n  [\"in\", null],\n  [\" \", null],\n  [\"coronary\", \"\u043a\u043e\u0440\u043e\u043d\u0430\u0440\u043d\u043e\u0439\"],\n  [\" \", null],\n  [\"heart\", \"\u0431\u043e\u043b\u0435\u0437\u043d\u0438\"],\n  [\" \", null],\n  [\"disease\", \"\u0441\u0435\u0440\u0434\u0446\u0430\"],\n  [\".\", null],\n  [\" \", null],\n  [\"The\", null],\n  [\" \", null],\n  [\"surgery\", \"\u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438\"],\n  [\" \", null],\n  [\"success\", \"\u0443\u0441\u043f\u0435\u0445\u0430\"],\n  [\" \", null],\n  [\"rate\", \"\u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044c\"],\n  [\" \", null],\n  [\"is\", null],\n  [\" \", null],\n  [\"around\", \"\u043e\u043a\u043e\u043b\u043e\"],\n  [\" \", null],\n  [\"7\", null],\n  [\"0\", null],\n  [\"%\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nNominations for the 90th Academy Awards will be officially announced on January 23, after which Hollywood\u2019s annual most eye-catching \u201cOscar Time\u201d will commence.\n\n\n---=== TOKENIZED TEXT ===---\n\n['N', 'omin', 'ations', ' for', ' the', ' ', '9', '0', 'th', ' Academy', ' Awards', ' will', ' be', ' officially', ' announced', ' on', ' January', ' ', '2', '3', ',', ' after', ' which', ' Hollywood', '\u2019s', ' annual', ' most', ' eye', '-catching', ' \u201c', 'O', 'scar', ' Time', '\u201d', ' will', ' commence', '.']"}], "[\n    [\"N\", \"\u63d0\u540d\"],\n    [\"omin\", null],\n    [\"ations\", null],\n    [\" for\", null],\n    [\" the\", null],\n    [\" \", null],\n    [\"9\", null],\n    [\"0\", null],\n    [\"th\", null],\n    [\" Academy\", \"\u5b66\u9662\"],\n    [\" Awards\", \"\u5956\"],\n    [\" will\", null],\n    [\" be\", null],\n    [\" officially\", \"\u6b63\u5f0f\"],\n    [\" announced\", \"\u5ba3\u5e03\"],\n    [\" on\", null],\n    [\" January\", \"1\u6708\"],\n    [\" \", null],\n    [\"2\", null],\n    [\"3\", null],\n    [\",\", null],\n    [\" after\", null],\n    [\" which\", null],\n    [\" Hollywood\", \"\u597d\u83b1\u575e\"],\n    [\"'s\", null],\n    [\" annual\", \"\u5e74\u5ea6\"],\n    [\" most\", null],\n    [\" eye\", \"\u5f15\u4eba\u6ce8\u76ee\"],\n    [\"-catching\", null],\n    [\" \\\"\", null],\n    [\"O\", \"\u5965\u65af\u5361\"],\n    [\"scar\", null],\n    [\" Time\", \"\u65f6\u95f4\"],\n    [\"\\\"\", null],\n    [\" will\", null],\n    [\" commence\", \"\u5f00\u59cb\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nNominations for the 90th Academy Awards will be officially announced on January 23, after which Hollywood\u2019s annual most eye-catching \u201cOscar Time\u201d will commence.\n\n\n---=== TOKENIZED TEXT ===---\n\n['N', 'omin', 'ations', ' for', ' the', ' ', '9', '0', 'th', ' Academy', ' Awards', ' will', ' be', ' officially', ' announced', ' on', ' January', ' ', '2', '3', ',', ' after', ' which', ' Hollywood', '\u2019s', ' annual', ' most', ' eye', '-catching', ' \u201c', 'O', 'scar', ' Time', '\u201d', ' will', ' commence', '.']"}], "[\n[\"N\", \"\u041d\u043e\u043c\u0438\u043d\u0430\u0446\u0438\u044f\u043b\u0430\u0440\"],\n[\"omin\", null],\n[\"ations\", null],\n[\" for\", null],\n[\" the\", null],\n[\" \", null],\n[\"9\", null],\n[\"0\", null],\n[\"th\", null],\n[\" Academy\", \" \u0410\u043a\u0430\u0434\u0435\u043c\u0438\u044f\"],\n[\" Awards\", \" \u043c\u0430\u0440\u0430\u043f\u0430\u0442\u0442\u0430\u0440\u044b\"],\n[\" will\", null],\n[\" be\", null],\n[\" officially\", \"\u0440\u0435\u0441\u043c\u0438\"],\n[\" announced\", \"\u0436\u0430\u0440\u0438\u044f\u043b\u0430\u043d\u0430\u0434\u044b\"],\n[\" on\", null],\n[\" January\", \"\u049b\u0430\u04a3\u0442\u0430\u0440\"],\n[\" \", null],\n[\"2\", null],\n[\"3\", null],\n[\",\", null],\n[\" after\", null],\n[\" which\", null],\n[\" Hollywood\", \"\u0413\u043e\u043b\u043b\u0438\u0432\u0443\u0434\"],\n[\"'s\", null],\n[\" annual\", \"\u0436\u044b\u043b\u0434\u044b\u049b\"],\n[\" most\", null],\n[\" eye\", \"\u043a\u04e9\u0437\"],\n[\"-catching\", null],\n[\" \", null],\n[\"O\", \"\u041e\u0441\u043a\u0430\u0440\"],\n[\"scar\", null],\n[\" Time\", \" \u0443\u0430\u049b\u044b\u0442\u044b\"],\n[\" \", null],\n[\"will\", null],\n[\" commence\", \"\u0431\u0430\u0441\u0442\u0430\u043b\u0430\u0434\u044b\"],\n[\".\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nNominations for the 90th Academy Awards will be officially announced on January 23, after which Hollywood\u2019s annual most eye-catching \u201cOscar Time\u201d will commence.\n\n\n---=== TOKENIZED TEXT ===---\n\n['N', 'omin', 'ations', ' for', ' the', ' ', '9', '0', 'th', ' Academy', ' Awards', ' will', ' be', ' officially', ' announced', ' on', ' January', ' ', '2', '3', ',', ' after', ' which', ' Hollywood', '\u2019s', ' annual', ' most', ' eye', '-catching', ' \u201c', 'O', 'scar', ' Time', '\u201d', ' will', ' commence', '.']"}], "[\n    [\"N\", \"\u041d\u043e\u043c\u0438\u043d\u0430\u0446\u0438\u0438\"],\n    [\"omin\", null],\n    [\"ations\", null],\n    [\" for\", null],\n    [\" the\", null],\n    [\" \", null],\n    [\"9\", null],\n    [\"0\", null],\n    [\"th\", null],\n    [\" Academy\", \"\u0410\u043a\u0430\u0434\u0435\u043c\u0438\u0438\"],\n    [\" Awards\", \"\u043f\u0440\u0435\u043c\u0438\u0438\"],\n    [\" will\", null],\n    [\" be\", null],\n    [\" officially\", \"\u043e\u0444\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u043e\"],\n    [\" announced\", \"\u043e\u0431\u044a\u044f\u0432\u043b\u0435\u043d\u044b\"],\n    [\" on\", null],\n    [\" January\", \"\u044f\u043d\u0432\u0430\u0440\u044f\"],\n    [\" \", null],\n    [\"2\", null],\n    [\"3\", null],\n    [\",\", null],\n    [\" after\", null],\n    [\" which\", null],\n    [\" Hollywood\", \"\u0413\u043e\u043b\u043b\u0438\u0432\u0443\u0434\u0430\"],\n    [\"'s\", null],\n    [\" annual\", \"\u0435\u0436\u0435\u0433\u043e\u0434\u043d\u043e\u0435\"],\n    [\" most\", null],\n    [\" eye\", \"\u043f\u0440\u0438\u0432\u043b\u0435\u043a\u0430\u044e\u0449\u0435\u0435\"],\n    [\"-catching\", null],\n    [\" \\\"\", null],\n    [\"O\", \"\u041e\u0441\u043a\u0430\u0440\u043e\u0432\u0441\u043a\u043e\u0435\"],\n    [\"scar\", null],\n    [\" Time\", \"\u0432\u0440\u0435\u043c\u044f\"],\n    [\"\\\"\", null],\n    [\" will\", null],\n    [\" commence\", \"\u043d\u0430\u0447\u043d\u0435\u0442\u0441\u044f\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nGuglielmi said police received a call from the Grand Plaza building manager who had just received a cryptic tip that said something had occurred in apartment 1008.\n\n\n---=== TOKENIZED TEXT ===---\n\n['G', 'ug', 'li', 'el', 'mi', ' said', ' police', ' received', ' a', ' call', ' from', ' the', ' Grand', ' Plaza', ' building', ' manager', ' who', ' had', ' just', ' received', ' a', ' crypt', 'ic', ' tip', ' that', ' said', ' something', ' had', ' occurred', ' in', ' apartment', ' ', '1', '0', '0', '8', '.']"}], "[\n  [\"G\", null],\n  [\"ug\", null],\n  [\"li\", null],\n  [\"el\", null],\n  [\"mi\", \"\u53e4\u5217\u5c14\u7c73\"],\n  [\" said\", null],\n  [\" police\", \"\u8b66\u65b9\"],\n  [\" received\", \"\u63a5\u5230\"],\n  [\" a\", null],\n  [\" call\", \"\u7535\u8bdd\"],\n  [\" from\", null],\n  [\" the\", null],\n  [\" Grand\", \"\u5927\"],\n  [\" Plaza\", \"\u5e7f\u573a\"],\n  [\" building\", \"\u5927\u697c\"],\n  [\" manager\", \"\u7ecf\u7406\"],\n  [\" who\", null],\n  [\" had\", null],\n  [\" just\", \"\u521a\u521a\"],\n  [\" received\", \"\u6536\u5230\"],\n  [\" a\", null],\n  [\" crypt\", \"\u795e\u79d8\"],\n  [\"ic\", null],\n  [\" tip\", \"\u63d0\u793a\"],\n  [\" that\", null],\n  [\" said\", \"\u79f0\"],\n  [\" something\", \"\u67d0\u4e8b\"],\n  [\" had\", null],\n  [\" occurred\", \"\u53d1\u751f\"],\n  [\" in\", null],\n  [\" apartment\", \"\u516c\u5bd3\"],\n  [\" \", null],\n  [\"1\", \"1008\"],\n  [\"0\", null],\n  [\"0\", null],\n  [\"8\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nGuglielmi said police received a call from the Grand Plaza building manager who had just received a cryptic tip that said something had occurred in apartment 1008.\n\n\n---=== TOKENIZED TEXT ===---\n\n['G', 'ug', 'li', 'el', 'mi', ' said', ' police', ' received', ' a', ' call', ' from', ' the', ' Grand', ' Plaza', ' building', ' manager', ' who', ' had', ' just', ' received', ' a', ' crypt', 'ic', ' tip', ' that', ' said', ' something', ' had', ' occurred', ' in', ' apartment', ' ', '1', '0', '0', '8', '.']"}], "[\n[\"G\", null],\n[\"ug\", null],\n[\"li\", null],\n[\"el\", null],\n[\"mi\", \"\u0413\u0443\u043b\u044c\u0435\u043b\u044c\u043c\u0438\"],\n[\" said\", null],\n[\" police\", \"\u043f\u043e\u043b\u0438\u0446\u0438\u044f\"],\n[\" received\", \"\u0430\u043b\u0434\u044b\"],\n[\" a\", null],\n[\" call\", \"\u049b\u043e\u04a3\u044b\u0440\u0430\u0443\"],\n[\" from\", null],\n[\" the\", null],\n[\" Grand\", \"\u0413\u0440\u0430\u043d\u0434\"],\n[\" Plaza\", \"\u041f\u043b\u0430\u0437\u0430\"],\n[\" building\", \"\u0493\u0438\u043c\u0430\u0440\u0430\u0442\u044b\u043d\u044b\u04a3\"],\n[\" manager\", \"\u043c\u0435\u043d\u0435\u0434\u0436\u0435\u0440\u0456\"],\n[\" who\", null],\n[\" had\", null],\n[\" just\", null],\n[\" received\", \"\u0430\u043b\u0493\u0430\u043d\"],\n[\" a\", null],\n[\" crypt\", \"\u0436\u04b1\u043c\u0431\u0430\u049b\"],\n[\" ic\", null],\n[\" tip\", \"\u0430\u049b\u043f\u0430\u0440\u0430\u0442\"],\n[\" that\", null],\n[\" said\", \"\u0430\u0439\u0442\u049b\u0430\u043d\"],\n[\" something\", \"\u0431\u0456\u0440\u043d\u04d9\u0440\u0441\u0435\"],\n[\" had\", null],\n[\" occurred\", \"\u0431\u043e\u043b\u0493\u0430\u043d\u044b\u043d\"],\n[\" in\", null],\n[\" apartment\", \"\u043f\u04d9\u0442\u0435\u0440\u0434\u0435\"],\n[\" \", null],\n[\"1\", null],\n[\"0\", null],\n[\"0\", null],\n[\"8\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nGuglielmi said police received a call from the Grand Plaza building manager who had just received a cryptic tip that said something had occurred in apartment 1008.\n\n\n---=== TOKENIZED TEXT ===---\n\n['G', 'ug', 'li', 'el', 'mi', ' said', ' police', ' received', ' a', ' call', ' from', ' the', ' Grand', ' Plaza', ' building', ' manager', ' who', ' had', ' just', ' received', ' a', ' crypt', 'ic', ' tip', ' that', ' said', ' something', ' had', ' occurred', ' in', ' apartment', ' ', '1', '0', '0', '8', '.']"}], "[\n  [\"G\", \"\u0413\u0443\u043b\u044c\u0435\u043b\u044c\u043c\u0438\"],\n  [\"ug\", null],\n  [\"li\", null],\n  [\"el\", null],\n  [\"mi\", null],\n  [\" said\", null],\n  [\" police\", \"\u043f\u043e\u043b\u0438\u0446\u0438\u044f\"],\n  [\" received\", \"\u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0430\"],\n  [\" a\", null],\n  [\" call\", \"\u0437\u0432\u043e\u043d\u043e\u043a\"],\n  [\" from\", null],\n  [\" the\", null],\n  [\" Grand\", \"\u0413\u0440\u0430\u043d\u0434\"],\n  [\" Plaza\", \"\u041f\u043b\u0430\u0437\u0430\"],\n  [\" building\", \"\u0437\u0434\u0430\u043d\u0438\u044f\"],\n  [\" manager\", \"\u0443\u043f\u0440\u0430\u0432\u043b\u044f\u044e\u0449\u0438\u0439\"],\n  [\" who\", null],\n  [\" had\", null],\n  [\" just\", null],\n  [\" received\", \"\u043f\u043e\u043b\u0443\u0447\u0438\u043b\"],\n  [\" a\", null],\n  [\" crypt\", \"\u0437\u0430\u0433\u0430\u0434\u043e\u0447\u043d\u044b\u0439\"],\n  [\"ic\", null],\n  [\" tip\", \"\u043d\u0430\u043c\u0451\u043a\"],\n  [\" that\", null],\n  [\" said\", \"\u0441\u043e\u043e\u0431\u0449\u0430\u0432\u0448\u0438\u0439\"],\n  [\" something\", \"\u0447\u0442\u043e-\u0442\u043e\"],\n  [\" had\", null],\n  [\" occurred\", \"\u043f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u043e\"],\n  [\" in\", null],\n  [\" apartment\", \"\u043a\u0432\u0430\u0440\u0442\u0438\u0440\u0435\"],\n  [\" \", null],\n  [\"1\", null],\n  [\"0\", null],\n  [\"0\", null],\n  [\"8\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nDuring his meeting with Sagintayev, Li Keqiang said he hopes both China and Kazakhstan will continue to work together\n\n\n---=== TOKENIZED TEXT ===---\n\n['During', ' his', ' meeting', ' with', ' Sag', 'int', 'ay', 'ev', ',', ' Li', ' Ke', 'q', 'iang', ' said', ' he', ' hopes', ' both', ' China', ' and', ' Kazakhstan', ' will', ' continue', ' to', ' work', ' together']"}], "[\n    [\"During\", \"\u5728\"],\n    [\" his\", null],\n    [\" meeting\", \"\u4f1a\u6664\"],\n    [\" with\", null],\n    [\" Sag\", \"\u8428\u91d1\u5854\u8036\u592b\"],\n    [\"int\", null],\n    [\"ay\", null],\n    [\"ev\", null],\n    [\",\", null],\n    [\" Li\", \"\u674e\"],\n    [\" Ke\", \"\u514b\u5f3a\"],\n    [\"q\", null],\n    [\"iang\", null],\n    [\" said\", \"\u8868\u793a\"],\n    [\" he\", null],\n    [\" hopes\", \"\u5e0c\u671b\"],\n    [\" both\", null],\n    [\" China\", \"\u4e2d\u56fd\"],\n    [\" and\", null],\n    [\" Kazakhstan\", \"\u54c8\u8428\u514b\u65af\u5766\"],\n    [\" will\", null],\n    [\" continue\", \"\u7ee7\u7eed\"],\n    [\" to\", null],\n    [\" work\", \"\u5408\u4f5c\"],\n    [\" together\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nDuring his meeting with Sagintayev, Li Keqiang said he hopes both China and Kazakhstan will continue to work together\n\n\n---=== TOKENIZED TEXT ===---\n\n['During', ' his', ' meeting', ' with', ' Sag', 'int', 'ay', 'ev', ',', ' Li', ' Ke', 'q', 'iang', ' said', ' he', ' hopes', ' both', ' China', ' and', ' Kazakhstan', ' will', ' continue', ' to', ' work', ' together']"}], "[\n  [\"During\", \"\u041a\u0435\u0437\u0456\u043d\u0434\u0435\"],\n  [\" his\", null],\n  [\" meeting\", \"\u043a\u0435\u0437\u0434\u0435\u0441\u0443\"],\n  [\" with\", null],\n  [\" Sag\", \"\u0421\u0430\u0493\u044b\u043d\u0442\u0430\u0435\u0432\"],\n  [\"int\", null],\n  [\"ay\", null],\n  [\"ev\", null],\n  [\",\", null],\n  [\" Li\", \"\u041b\u0438\"],\n  [\" Ke\", \"\u041a\u044d\u0446\u044f\u043d\"],\n  [\"q\", null],\n  [\"iang\", null],\n  [\" said\", \"\u0430\u0439\u0442\u0442\u044b\"],\n  [\" he\", null],\n  [\" hopes\", \"\u04af\u043c\u0456\u0442\u0442\u0435\u043d\u0435\u0434\u0456\"],\n  [\" both\", null],\n  [\" China\", \"\u049a\u044b\u0442\u0430\u0439\"],\n  [\" and\", null],\n  [\" Kazakhstan\", \"\u049a\u0430\u0437\u0430\u049b\u0441\u0442\u0430\u043d\"],\n  [\" will\", null],\n  [\" continue\", \"\u0436\u0430\u043b\u0493\u0430\u0441\u0442\u044b\u0440\u0430\u0434\u044b\"],\n  [\" to\", null],\n  [\" work\", \"\u0436\u04b1\u043c\u044b\u0441 \u0456\u0441\u0442\u0435\u0443\u0434\u0456\"],\n  [\" together\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nDuring his meeting with Sagintayev, Li Keqiang said he hopes both China and Kazakhstan will continue to work together\n\n\n---=== TOKENIZED TEXT ===---\n\n['During', ' his', ' meeting', ' with', ' Sag', 'int', 'ay', 'ev', ',', ' Li', ' Ke', 'q', 'iang', ' said', ' he', ' hopes', ' both', ' China', ' and', ' Kazakhstan', ' will', ' continue', ' to', ' work', ' together']"}], "[\n    [\"During\", \"\u0412\u043e \u0432\u0440\u0435\u043c\u044f\"],\n    [\" his\", null],\n    [\" meeting\", \"\u0432\u0441\u0442\u0440\u0435\u0447\u0438\"],\n    [\" with\", null],\n    [\" Sag\", \"\u0421\u0430\u0433\u0438\u043d\u0442\u0430\u0435\u0432\"],\n    [\"int\", null],\n    [\"ay\", null],\n    [\"ev\", null],\n    [\",\", null],\n    [\" Li\", \"\u041b\u0438\"],\n    [\" Ke\", \"\u041a\u044d\u0446\u044f\u043d\"],\n    [\"q\", null],\n    [\"iang\", null],\n    [\" said\", \"\u0441\u043a\u0430\u0437\u0430\u043b\"],\n    [\" he\", null],\n    [\" hopes\", \"\u043d\u0430\u0434\u0435\u0435\u0442\u0441\u044f\"],\n    [\" both\", null],\n    [\" China\", \"\u041a\u0438\u0442\u0430\u0439\"],\n    [\" and\", null],\n    [\" Kazakhstan\", \"\u041a\u0430\u0437\u0430\u0445\u0441\u0442\u0430\u043d\"],\n    [\" will\", null],\n    [\" continue\", \"\u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0430\u0442\"],\n    [\" to\", null],\n    [\" work\", \"\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c\"],\n    [\" together\", \"\u0432\u043c\u0435\u0441\u0442\u0435\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nThe U.S. Dollar Index, which measures the US$ against six major currencies, ended the day down 0.10%, falling to 96.479 at the closing session.\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', ' U', '.S', '.', ' Dollar', ' Index', ',', ' which', ' measures', ' the', ' US', '$', ' against', ' six', ' major', ' currencies', ',', ' ended', ' the', ' day', ' down', ' ', '0', '.', '1', '0', '%,', ' falling', ' to', ' ', '9', '6', '.', '4', '7', '9', ' at', ' the', ' closing', ' session', '.']"}], "[\n  [\"The\", null],\n  [\" U\", \"\u7f8e\u56fd\"],\n  [\".S\", null],\n  [\".\", null],\n  [\" Dollar\", \"\u7f8e\u5143\"],\n  [\" Index\", \"\u6307\u6570\"],\n  [\",\", null],\n  [\" which\", null],\n  [\" measures\", \"\u8861\u91cf\"],\n  [\" the\", null],\n  [\" US\", \"\u7f8e\u56fd\"],\n  [\"$\", null],\n  [\" against\", \"\u5bf9\u6bd4\"],\n  [\" six\", \"\u516d\u79cd\"],\n  [\" major\", \"\u4e3b\u8981\"],\n  [\" currencies\", \"\u8d27\u5e01\"],\n  [\",\", null],\n  [\" ended\", \"\u7ed3\u675f\"],\n  [\" the\", null],\n  [\" day\", \"\u5f53\u5929\"],\n  [\" down\", \"\u4e0b\u8dcc\"],\n  [\" \", null],\n  [\"0\", null],\n  [\".\", null],\n  [\"1\", null],\n  [\"0\", null],\n  [\"%,\", null],\n  [\" falling\", \"\u4e0b\u964d\"],\n  [\" to\", null],\n  [\" \", null],\n  [\"9\", null],\n  [\"6\", null],\n  [\".\", null],\n  [\"4\", null],\n  [\"7\", null],\n  [\"9\", null],\n  [\" at\", \"\u5728\"],\n  [\" the\", null],\n  [\" closing\", \"\u6536\u76d8\"],\n  [\" session\", \"\u65f6\u6bb5\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nThe U.S. Dollar Index, which measures the US$ against six major currencies, ended the day down 0.10%, falling to 96.479 at the closing session.\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', ' U', '.S', '.', ' Dollar', ' Index', ',', ' which', ' measures', ' the', ' US', '$', ' against', ' six', ' major', ' currencies', ',', ' ended', ' the', ' day', ' down', ' ', '0', '.', '1', '0', '%,', ' falling', ' to', ' ', '9', '6', '.', '4', '7', '9', ' at', ' the', ' closing', ' session', '.']"}], "[\n  [\"The\", null],\n  [\" U\", \"\u0410\u049a\u0428\"],\n  [\".S\", null],\n  [\".\", null],\n  [\" Dollar\", \"\u0434\u043e\u043b\u043b\u0430\u0440\"],\n  [\" Index\", \"\u0438\u043d\u0434\u0435\u043a\u0441\u0456\"],\n  [\",\", null],\n  [\" which\", null],\n  [\" measures\", \"\u04e9\u043b\u0448\u0435\u0439\u0442\u0456\u043d\"],\n  [\" the\", null],\n  [\" US\", \"\u0410\u049a\u0428\"],\n  [\"$\", null],\n  [\" against\", \"\u049b\u0430\u0440\u0441\u044b\"],\n  [\" six\", \"\u0430\u043b\u0442\u044b\"],\n  [\" major\", \"\u043d\u0435\u0433\u0456\u0437\u0433\u0456\"],\n  [\" currencies\", \"\u0432\u0430\u043b\u044e\u0442\u0430\u043b\u0430\u0440\u0493\u0430\"],\n  [\",\", null],\n  [\" ended\", \"\u0430\u044f\u049b\u0442\u0430\u043b\u0434\u044b\"],\n  [\" the\", null],\n  [\" day\", \"\u043a\u04af\u043d\u0434\u0456\"],\n  [\" down\", \"\u0442\u04e9\u043c\u0435\u043d\u0434\u0435\u043f\"],\n  [\" \", null],\n  [\"0\", null],\n  [\".\", null],\n  [\"1\", null],\n  [\"0\", null],\n  [\"%,\", null],\n  [\" falling\", \"\u0442\u04af\u0441\u0456\u043f\"],\n  [\" to\", null],\n  [\" \", null],\n  [\"9\", null],\n  [\"6\", null],\n  [\".\", null],\n  [\"4\", null],\n  [\"7\", null],\n  [\"9\", null],\n  [\" at\", null],\n  [\" the\", null],\n  [\" closing\", \"\u0436\u0430\u0431\u044b\u043b\u0443\"],\n  [\" session\", \"\u0441\u0435\u0441\u0441\u0438\u044f\u0441\u044b\u043d\u0434\u0430\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThe U.S. Dollar Index, which measures the US$ against six major currencies, ended the day down 0.10%, falling to 96.479 at the closing session.\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', ' U', '.S', '.', ' Dollar', ' Index', ',', ' which', ' measures', ' the', ' US', '$', ' against', ' six', ' major', ' currencies', ',', ' ended', ' the', ' day', ' down', ' ', '0', '.', '1', '0', '%,', ' falling', ' to', ' ', '9', '6', '.', '4', '7', '9', ' at', ' the', ' closing', ' session', '.']"}], "[\n[\"The\", null],\n[\" U\", \"\u0418\u043d\u0434\u0435\u043a\u0441\"],\n[\".S\", null],\n[\".\", null],\n[\" Dollar\", null],\n[\" Index\", null],\n[\",\", null],\n[\" which\", null],\n[\" measures\", \"\u0438\u0437\u043c\u0435\u0440\u044f\u0435\u0442\"],\n[\" the\", null],\n[\" US\", \"\u0421\u0428\u0410\"],\n[\"$\", null],\n[\" against\", \"\u043f\u0440\u043e\u0442\u0438\u0432\"],\n[\" six\", \"\u0448\u0435\u0441\u0442\u0438\"],\n[\" major\", \"\u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0445\"],\n[\" currencies\", \"\u0432\u0430\u043b\u044e\u0442\"],\n[\",\", null],\n[\" ended\", \"\u0437\u0430\u043a\u043e\u043d\u0447\u0438\u043b\"],\n[\" the\", null],\n[\" day\", \"\u0434\u0435\u043d\u044c\"],\n[\" down\", \"\u0441\u043d\u0438\u0436\u0435\u043d\u0438\u0435\u043c\"],\n[\" \", null],\n[\"0\", null],\n[\".\", null],\n[\"1\", null],\n[\"0\", null],\n[\"%,\", null],\n[\" falling\", \"\u0443\u043f\u0430\u0432\"],\n[\" to\", \"\u0434\u043e\"],\n[\" \", null],\n[\"9\", null],\n[\"6\", null],\n[\".\", null],\n[\"4\", null],\n[\"7\", null],\n[\"9\", null],\n[\" at\", \"\u043d\u0430\"],\n[\" the\", null],\n[\" closing\", \"\u0437\u0430\u043a\u0440\u044b\u0442\u0438\u0438\"],\n[\" session\", \"\u0441\u0435\u0441\u0441\u0438\u0438\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nThe RMB middle price is registered at 6.4401, a depreciation of 66 basis points. The rising slows down.\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', ' R', 'MB', ' middle', ' price', ' is', ' registered', ' at', ' ', '6', '.', '4', '4', '0', '1', ',', ' a', ' depreciation', ' of', ' ', '6', '6', ' basis', ' points', '.', ' The', ' rising', ' slows', ' down', '.']"}], "[\n  [\"The\", null],\n  [\" R\", \"\u4eba\u6c11\u5e01\"],\n  [\"MB\", null],\n  [\" middle\", null],\n  [\" price\", \"\u4e2d\u95f4\u4ef7\"],\n  [\" is\", null],\n  [\" registered\", \"\u62a5\"],\n  [\" at\", null],\n  [\" \", null],\n  [\"6\", \"6\"],\n  [\".\", null],\n  [\"4\", \"4\"],\n  [\"4\", \"4\"],\n  [\"0\", \"0\"],\n  [\"1\", \"1\"],\n  [\",\", null],\n  [\" a\", null],\n  [\" depreciation\", \"\u8d2c\u503c\"],\n  [\" of\", null],\n  [\" \", null],\n  [\"6\", \"6\"],\n  [\"6\", \"6\"],\n  [\" basis\", \"\u4e2a\u57fa\u70b9\"],\n  [\" points\", null],\n  [\".\", null],\n  [\" The\", null],\n  [\" rising\", \"\u4e0a\u6da8\"],\n  [\" slows\", \"\u653e\u7f13\"],\n  [\" down\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nThe RMB middle price is registered at 6.4401, a depreciation of 66 basis points. The rising slows down.\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', ' R', 'MB', ' middle', ' price', ' is', ' registered', ' at', ' ', '6', '.', '4', '4', '0', '1', ',', ' a', ' depreciation', ' of', ' ', '6', '6', ' basis', ' points', '.', ' The', ' rising', ' slows', ' down', '.']"}], "[\n[\"The\", null],\n[\" R\", \"RMB\"],\n[\"MB\", null],\n[\" middle\", \"\u043e\u0440\u0442\u0430\"],\n[\" price\", \"\u0431\u0430\u0493\u0430\"],\n[\" is\", null],\n[\" registered\", \"\u0442\u0456\u0440\u043a\u0435\u043b\u0433\u0435\u043d\"],\n[\" at\", null],\n[\" \", null],\n[\"6\", null],\n[\".\", null],\n[\"4\", null],\n[\"4\", null],\n[\"0\", null],\n[\"1\", null],\n[\",\", null],\n[\" a\", null],\n[\" depreciation\", \"\u049b\u04b1\u043d\u0441\u044b\u0437\u0434\u0430\u043d\u0443\"],\n[\" of\", null],\n[\" \", null],\n[\"6\", null],\n[\"6\", null],\n[\" basis\", \"\u0431\u0430\u0437\u0438\u0441\u0442\u0456\u043a\"],\n[\" points\", \"\u043f\u0443\u043d\u043a\u0442\"],\n[\".\", null],\n[\" The\", null],\n[\" rising\", \"\u043a\u04e9\u0442\u0435\u0440\u0456\u043b\u0443\"],\n[\" slows\", \"\u0431\u0430\u044f\u0443\u043b\u0430\u0439\u0434\u044b\"],\n[\" down\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThe RMB middle price is registered at 6.4401, a depreciation of 66 basis points. The rising slows down.\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', ' R', 'MB', ' middle', ' price', ' is', ' registered', ' at', ' ', '6', '.', '4', '4', '0', '1', ',', ' a', ' depreciation', ' of', ' ', '6', '6', ' basis', ' points', '.', ' The', ' rising', ' slows', ' down', '.']"}], "[\n    [\"The\", null],\n    [\" R\", \"\u042e\u0430\u043d\u044c\"],\n    [\"MB\", null],\n    [\" middle\", null],\n    [\" price\", \"\u043a\u0443\u0440\u0441\"],\n    [\" is\", null],\n    [\" registered\", \"\u0437\u0430\u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043d\"],\n    [\" at\", null],\n    [\" \", null],\n    [\"6\", null],\n    [\".\", null],\n    [\"4\", null],\n    [\"4\", null],\n    [\"0\", null],\n    [\"1\", null],\n    [\",\", null],\n    [\" a\", null],\n    [\" depreciation\", \"\u0441\u043d\u0438\u0436\u0435\u043d\u0438\u0435\"],\n    [\" of\", null],\n    [\" \", null],\n    [\"6\", null],\n    [\"6\", null],\n    [\" basis\", \"\u0431\u0430\u0437\u0438\u0441\u043d\u044b\u0445\"],\n    [\" points\", \"\u043f\u0443\u043d\u043a\u0442\u043e\u0432\"],\n    [\".\", null],\n    [\" The\", null],\n    [\" rising\", \"\u0440\u043e\u0441\u0442\"],\n    [\" slows\", \"\u0437\u0430\u043c\u0435\u0434\u043b\u044f\u0435\u0442\u0441\u044f\"],\n    [\" down\", null],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nA record number of Scottish athletes have been selected for London 2017, which starts on Friday.\n\n\n---=== TOKENIZED TEXT ===---\n\n['A', ' record', ' number', ' of', ' Scottish', ' athletes', ' have', ' been', ' selected', ' for', ' London', ' ', '2', '0', '1', '7', ',', ' which', ' starts', ' on', ' Friday', '.']"}], "[\n  [\"A\", null],\n  [\"record\", \"\u521b\u7eaa\u5f55\"],\n  [\"number\", \"\u6570\u91cf\"],\n  [\"of\", null],\n  [\"Scottish\", \"\u82cf\u683c\u5170\"],\n  [\"athletes\", \"\u8fd0\u52a8\u5458\"],\n  [\"have\", null],\n  [\"been\", null],\n  [\"selected\", \"\u88ab\u9009\u4e2d\"],\n  [\"for\", null],\n  [\"London\", \"\u4f26\u6566\"],\n  [\" \", null],\n  [\"2\", null],\n  [\"0\", null],\n  [\"1\", null],\n  [\"7\", null],\n  [\",\", null],\n  [\"which\", null],\n  [\"starts\", \"\u5f00\u59cb\"],\n  [\"on\", null],\n  [\"Friday\", \"\u661f\u671f\u4e94\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nA record number of Scottish athletes have been selected for London 2017, which starts on Friday.\n\n\n---=== TOKENIZED TEXT ===---\n\n['A', ' record', ' number', ' of', ' Scottish', ' athletes', ' have', ' been', ' selected', ' for', ' London', ' ', '2', '0', '1', '7', ',', ' which', ' starts', ' on', ' Friday', '.']"}], "[\n  [\"A\", null],\n  [\"record\", \"\u0440\u0435\u043a\u043e\u0440\u0434\u0442\u044b\u049b\"],\n  [\"number\", \"\u0441\u0430\u043d\"],\n  [\"of\", null],\n  [\"Scottish\", \"\u0448\u043e\u0442\u043b\u0430\u043d\u0434\u0438\u044f\u043b\u044b\u049b\"],\n  [\"athletes\", \"\u0441\u043f\u043e\u0440\u0442\u0448\u044b\u043b\u0430\u0440\"],\n  [\"have\", null],\n  [\"been\", null],\n  [\"selected\", \"\u0442\u0430\u04a3\u0434\u0430\u043b\u0434\u044b\"],\n  [\"for\", null],\n  [\"London\", \"\u041b\u043e\u043d\u0434\u043e\u043d\"],\n  [\" \", null],\n  [\"2\", null],\n  [\"0\", null],\n  [\"1\", null],\n  [\"7\", null],\n  [\",\", null],\n  [\"which\", null],\n  [\"starts\", \"\u0431\u0430\u0441\u0442\u0430\u043b\u0430\u0434\u044b\"],\n  [\"on\", null],\n  [\"Friday\", \"\u0436\u04b1\u043c\u0430\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nA record number of Scottish athletes have been selected for London 2017, which starts on Friday.\n\n\n---=== TOKENIZED TEXT ===---\n\n['A', ' record', ' number', ' of', ' Scottish', ' athletes', ' have', ' been', ' selected', ' for', ' London', ' ', '2', '0', '1', '7', ',', ' which', ' starts', ' on', ' Friday', '.']"}], "[\n[\"A\", null],\n[\"record\", \"\u0440\u0435\u043a\u043e\u0440\u0434\u043d\u043e\u0435\"],\n[\"number\", \"\u0447\u0438\u0441\u043b\u043e\"],\n[\"of\", null],\n[\"Scottish\", \"\u0448\u043e\u0442\u043b\u0430\u043d\u0434\u0441\u043a\u0438\u0445\"],\n[\"athletes\", \"\u0430\u0442\u043b\u0435\u0442\u043e\u0432\"],\n[\"have\", null],\n[\"been\", null],\n[\"selected\", \"\u043e\u0442\u043e\u0431\u0440\u0430\u043d\u043e\"],\n[\"for\", null],\n[\"London\", \"\u041b\u043e\u043d\u0434\u043e\u043d\"],\n[\" \", null],\n[\"2\", null],\n[\"0\", null],\n[\"1\", null],\n[\"7\", null],\n[\",\", null],\n[\"which\", null],\n[\"starts\", \"\u043d\u0430\u0447\u0438\u043d\u0430\u0435\u0442\u0441\u044f\"],\n[\"on\", null],\n[\"Friday\", \"\u043f\u044f\u0442\u043d\u0438\u0446\u0443\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nThe WADA roadmap was updated in this regard,\" Mutko stated on Thursday.\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', ' W', 'ADA', ' roadmap', ' was', ' updated', ' in', ' this', ' regard', ',\"', ' Mut', 'ko', ' stated', ' on', ' Thursday', '.']"}], "[\n    [\"The\", null],\n    [\" W\", \"\u4e16\u754c\"],\n    [\"ADA\", \"\u53cd\u5174\u594b\u5242\u673a\u6784\"],\n    [\" roadmap\", \"\u8def\u7ebf\u56fe\"],\n    [\" was\", null],\n    [\" updated\", \"\u66f4\u65b0\"],\n    [\" in\", null],\n    [\" this\", null],\n    [\" regard\", \"\u65b9\u9762\"],\n    [',\"', null],\n    [\" Mut\", \"\u7a46\u7279\u79d1\"],\n    [\"ko\", null],\n    [\" stated\", \"\u8868\u793a\"],\n    [\" on\", null],\n    [\" Thursday\", \"\u661f\u671f\u56db\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nThe WADA roadmap was updated in this regard,\" Mutko stated on Thursday.\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', ' W', 'ADA', ' roadmap', ' was', ' updated', ' in', ' this', ' regard', ',\"', ' Mut', 'ko', ' stated', ' on', ' Thursday', '.']"}], "[\n  [\"The\", null],\n  [\" W\", null],\n  [\"ADA\", \"\u0411\u0414\u0414\u04b0\"],\n  [\" roadmap\", \"\u0436\u043e\u043b \u043a\u0430\u0440\u0442\u0430\u0441\u044b\"],\n  [\" was\", null],\n  [\" updated\", \"\u0436\u0430\u04a3\u0430\u0440\u0442\u044b\u043b\u0434\u044b\"],\n  [\" in\", null],\n  [\" this\", null],\n  [\" regard\", \"\u0442\u04b1\u0440\u0493\u044b\u0434\u0430\"],\n  [',\"', null],\n  [\" Mut\", \"\u041c\u0443\u0442\u043a\u043e\"],\n  [\"ko\", null],\n  [\" stated\", \"\u043c\u04d9\u043b\u0456\u043c\u0434\u0435\u0434\u0456\"],\n  [\" on\", null],\n  [\" Thursday\", \"\u0431\u0435\u0439\u0441\u0435\u043d\u0431\u0456\u0434\u0435\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThe WADA roadmap was updated in this regard,\" Mutko stated on Thursday.\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', ' W', 'ADA', ' roadmap', ' was', ' updated', ' in', ' this', ' regard', ',\"', ' Mut', 'ko', ' stated', ' on', ' Thursday', '.']"}], "[\n  [\"The\", null],\n  [\" W\", \"\u0412\u0410\u0414\u0410\"],\n  [\"ADA\", null],\n  [\" roadmap\", \"\u0434\u043e\u0440\u043e\u0436\u043d\u0430\u044f \u043a\u0430\u0440\u0442\u0430\"],\n  [\" was\", null],\n  [\" updated\", \"\u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0430\"],\n  [\" in\", null],\n  [\" this\", null],\n  [\" regard\", \"\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0438\"],\n  [\",\" , null],\n  ['\"', null],\n  [\" Mut\", \"\u041c\u0443\u0442\u043a\u043e\"],\n  [\"ko\", null],\n  [\" stated\", \"\u0437\u0430\u044f\u0432\u0438\u043b\"],\n  [\" on\", null],\n  [\" Thursday\", \"\u0447\u0435\u0442\u0432\u0435\u0440\u0433\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nWhether the Tmall shopping platform or the Alipay payment platform, however, both belonged to Jack Ma\u2019s Alibaba Group.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Whether', ' the', ' T', 'mall', ' shopping', ' platform', ' or', ' the', ' Al', 'ipay', ' payment', ' platform', ',', ' however', ',', ' both', ' belonged', ' to', ' Jack', ' Ma', '\u2019s', ' Alibaba', ' Group', '.']"}], "[\n    [\"Whether\", \"\u65e0\u8bba\"],\n    [\" \", null],\n    [\"the\", null],\n    [\" \", null],\n    [\"T\", \"\u5929\"],\n    [\"mall\", null],\n    [\" \", null],\n    [\"shopping\", \"\u8d2d\u7269\"],\n    [\" \", null],\n    [\"platform\", \"\u5e73\u53f0\"],\n    [\" \", null],\n    [\"or\", null],\n    [\" \", null],\n    [\"the\", null],\n    [\" \", null],\n    [\"Al\", \"\u652f\"],\n    [\"ipay\", null],\n    [\" \", null],\n    [\"payment\", \"\u652f\u4ed8\"],\n    [\" \", null],\n    [\"platform\", \"\u5e73\u53f0\"],\n    [\",\", null],\n    [\" \", null],\n    [\"however\", \"\u7136\u800c\"],\n    [\",\", null],\n    [\" \", null],\n    [\"both\", \"\u4e24\u8005\"],\n    [\" \", null],\n    [\"belonged\", \"\u5c5e\u4e8e\"],\n    [\" \", null],\n    [\"to\", null],\n    [\" \", null],\n    [\"Jack\", \"\u9a6c\u4e91\"],\n    [\" \", null],\n    [\"Ma\", null],\n    [\"'s\", null],\n    [\" \", null],\n    [\"Alibaba\", \"\u963f\u91cc\u5df4\u5df4\"],\n    [\" \", null],\n    [\"Group\", \"\u96c6\u56e2\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nWhether the Tmall shopping platform or the Alipay payment platform, however, both belonged to Jack Ma\u2019s Alibaba Group.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Whether', ' the', ' T', 'mall', ' shopping', ' platform', ' or', ' the', ' Al', 'ipay', ' payment', ' platform', ',', ' however', ',', ' both', ' belonged', ' to', ' Jack', ' Ma', '\u2019s', ' Alibaba', ' Group', '.']"}], "[\n    [\"Whether\", null],\n    [\" the\", null],\n    [\" T\", \"Tmall\"],\n    [\"mall\", null],\n    [\" shopping\", \"\u0441\u0430\u0443\u0434\u0430\"],\n    [\" platform\", \"\u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u0430\u0441\u044b\"],\n    [\" or\", null],\n    [\" the\", null],\n    [\" Al\", \"Alipay\"],\n    [\"ipay\", null],\n    [\" payment\", \"\u0442\u04e9\u043b\u0435\u043c\"],\n    [\" platform\", \"\u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u0430\u0441\u044b\"],\n    [\",\", null],\n    [\" however\", null],\n    [\",\", null],\n    [\" both\", \"\u0435\u043a\u0435\u0443\u0456 \u0434\u0435\"],\n    [\" belonged\", \"\u0442\u0438\u0435\u0441\u0456\u043b\u0456 \u0431\u043e\u043b\u0434\u044b\"],\n    [\" to\", null],\n    [\" Jack\", \"\u0414\u0436\u0435\u043a\"],\n    [\" Ma\", \"\u041c\u0430\"],\n    [\"'s\", null],\n    [\" Alibaba\", \"Alibaba\"],\n    [\" Group\", \"\u0422\u043e\u0431\u044b\u043d\u0430\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nWhether the Tmall shopping platform or the Alipay payment platform, however, both belonged to Jack Ma\u2019s Alibaba Group.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Whether', ' the', ' T', 'mall', ' shopping', ' platform', ' or', ' the', ' Al', 'ipay', ' payment', ' platform', ',', ' however', ',', ' both', ' belonged', ' to', ' Jack', ' Ma', '\u2019s', ' Alibaba', ' Group', '.']"}], "[\n  [\"Whether\", \"\u041d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e \u043e\u0442 \u0442\u043e\u0433\u043e\"],\n  [\" the\", null],\n  [\" T\", \"\u0422\"],\n  [\"mall\", null],\n  [\" shopping\", \"\u0442\u043e\u0440\u0433\u043e\u0432\u0430\u044f\"],\n  [\" platform\", \"\u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u0430\"],\n  [\" or\", null],\n  [\" the\", null],\n  [\" Al\", \"\u0410\u043b\u0438\"],\n  [\"ipay\", null],\n  [\" payment\", \"\u043f\u043b\u0430\u0442\u0435\u0436\u043d\u0430\u044f\"],\n  [\" platform\", \"\u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u0430\"],\n  [\",\", null],\n  [\" however\", \"\u043e\u0434\u043d\u0430\u043a\u043e\"],\n  [\",\", null],\n  [\" both\", \"\u043e\u0431\u0435\"],\n  [\" belonged\", \"\u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u0430\u043b\u0438\"],\n  [\" to\", null],\n  [\" Jack\", \"\u0414\u0436\u0435\u043a\u0430\"],\n  [\" Ma\", \"\u041c\u0430\"],\n  [\"'s\", null],\n  [\" Alibaba\", \"Alibaba\"],\n  [\" Group\", \"Group\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nAvandia, which debuted in 1999, is an \u201cinsulin sensitizer\u201d, used for treating Type 2 diabetes.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Av', 'and', 'ia', ',', ' which', ' debuted', ' in', ' ', '1', '9', '9', '9', ',', ' is', ' an', ' \u201c', 'ins', 'ulin', ' sens', 'itizer', '\u201d,', ' used', ' for', ' treating', ' Type', ' ', '2', ' diabetes', '.']"}], "[\n  [\"Av\", \"\u963f\u51e1\"],\n  [\"and\", null],\n  [\"ia\", null],\n  [\",\", null],\n  [\" which\", null],\n  [\" debuted\", \"\u9996\u6b21\u63a8\u51fa\"],\n  [\" in\", null],\n  [\" \", null],\n  [\"1\", null],\n  [\"9\", null],\n  [\"9\", null],\n  [\"9\", null],\n  [\",\", null],\n  [\" is\", null],\n  [\" an\", null],\n  [\" \\\"\", null],\n  [\"ins\", \"\u80f0\u5c9b\u7d20\"],\n  [\"ulin\", null],\n  [\" sens\", \"\u589e\u654f\"],\n  [\"itizer\", null],\n  [\"\\\"\", null],\n  [\",\", null],\n  [\" used\", \"\u7528\u4e8e\"],\n  [\" for\", null],\n  [\" treating\", \"\u6cbb\u7597\"],\n  [\" Type\", \"2\u578b\"],\n  [\" \", null],\n  [\"2\", null],\n  [\" diabetes\", \"\u7cd6\u5c3f\u75c5\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nAvandia, which debuted in 1999, is an \u201cinsulin sensitizer\u201d, used for treating Type 2 diabetes.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Av', 'and', 'ia', ',', ' which', ' debuted', ' in', ' ', '1', '9', '9', '9', ',', ' is', ' an', ' \u201c', 'ins', 'ulin', ' sens', 'itizer', '\u201d,', ' used', ' for', ' treating', ' Type', ' ', '2', ' diabetes', '.']"}], "[\n[\"Av\", \"\u0410\u0432\u0430\u043d\u0434\u0438\u044f\"],\n[\"and\", null],\n[\"ia\", null],\n[\",\", null],\n[\" which\", null],\n[\" debuted\", \"\u0434\u0435\u0431\u044e\u0442 \u0436\u0430\u0441\u0430\u0434\u044b\"],\n[\" in\", null],\n[\" \", null],\n[\"1\", null],\n[\"9\", null],\n[\"9\", null],\n[\"9\", null],\n[\",\", null],\n[\" is\", null],\n[\" an\", null],\n[\" \\\"\", null],\n[\"ins\", \"\u0438\u043d\u0441\u0443\u043b\u0438\u043d\"],\n[\"ulin\", null],\n[\" sens\", \"\u0441\u0435\u043d\u0441\u0438\u0442\u0430\u0439\u0437\u0435\u0440\"],\n[\"itizer\", null],\n[\"\\\"\", null],\n[\",\", null],\n[\" used\", \"\u049b\u043e\u043b\u0434\u0430\u043d\u044b\u043b\u0430\u0434\u044b\"],\n[\" for\", null],\n[\" treating\", \"\u0435\u043c\u0434\u0435\u0443\"],\n[\" Type\", \"\u0422\u04af\u0440\u0456\"],\n[\" \", null],\n[\"2\", null],\n[\" diabetes\", \"\u0434\u0438\u0430\u0431\u0435\u0442\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nAvandia, which debuted in 1999, is an \u201cinsulin sensitizer\u201d, used for treating Type 2 diabetes.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Av', 'and', 'ia', ',', ' which', ' debuted', ' in', ' ', '1', '9', '9', '9', ',', ' is', ' an', ' \u201c', 'ins', 'ulin', ' sens', 'itizer', '\u201d,', ' used', ' for', ' treating', ' Type', ' ', '2', ' diabetes', '.']"}], "[\n  [\"Av\", \"\u0410\u0432\u0430\u043d\u0434\u0438\u044f\"],\n  [\"and\", null],\n  [\"ia\", null],\n  [\",\", null],\n  [\" which\", null],\n  [\" debuted\", \"\u0434\u0435\u0431\u044e\u0442\u0438\u0440\u043e\u0432\u0430\u043b\"],\n  [\" in\", null],\n  [\" \", null],\n  [\"1\", null],\n  [\"9\", null],\n  [\"9\", null],\n  [\"9\", null],\n  [\",\", null],\n  [\" is\", null],\n  [\" an\", null],\n  [\" \", null],\n  [\"ins\", \"\u0438\u043d\u0441\u0443\u043b\u0438\u043d\u043e\u0441\u0435\u043d\u0441\u0438\u0442\u0430\u0439\u0437\u0435\u0440\"],\n  [\"ulin\", null],\n  [\" sens\", null],\n  [\"itizer\", null],\n  [\"\"\", null],\n  [\",\", null],\n  [\" used\", \"\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044b\u0439\"],\n  [\" for\", null],\n  [\" treating\", \"\u043b\u0435\u0447\u0435\u043d\u0438\u044f\"],\n  [\" Type\", \"\u0414\u0438\u0430\u0431\u0435\u0442\u0430\"],\n  [\" \", null],\n  [\"2\", null],\n  [\" diabetes\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nAfter the competition ended, captain Jiang Cuo said that he hoped to gain experience from taking part in the competition, and do his best for Tibetan basketball.\n\n\n---=== TOKENIZED TEXT ===---\n\n['After', ' the', ' competition', ' ended', ',', ' captain', ' Jiang', ' C', 'uo', ' said', ' that', ' he', ' hoped', ' to', ' gain', ' experience', ' from', ' taking', ' part', ' in', ' the', ' competition', ',', ' and', ' do', ' his', ' best', ' for', ' Tibetan', ' basketball', '.']"}], "[\n  [\"After\", \"\u5728\"],\n  [\" \", null],\n  [\"the\", null],\n  [\" \", null],\n  [\"competition\", \"\u6bd4\u8d5b\"],\n  [\" \", null],\n  [\"ended\", \"\u7ed3\u675f\u540e\"],\n  [\",\", null],\n  [\" \", null],\n  [\"captain\", \"\u961f\u957f\"],\n  [\" \", null],\n  [\"Jiang\", \"\u59dc\"],\n  [\" \", null],\n  [\"C\", \"\u63aa\"],\n  [\"uo\", null],\n  [\" \", null],\n  [\"said\", \"\u8bf4\"],\n  [\" \", null],\n  [\"that\", null],\n  [\" \", null],\n  [\"he\", \"\u4ed6\"],\n  [\" \", null],\n  [\"hoped\", \"\u5e0c\u671b\"],\n  [\" \", null],\n  [\"to\", null],\n  [\" \", null],\n  [\"gain\", \"\u83b7\u5f97\"],\n  [\" \", null],\n  [\"experience\", \"\u7ecf\u9a8c\"],\n  [\" \", null],\n  [\"from\", \"\u4ece\"],\n  [\" \", null],\n  [\"taking\", \"\u53c2\u52a0\"],\n  [\" \", null],\n  [\"part\", null],\n  [\" \", null],\n  [\"in\", null],\n  [\" \", null],\n  [\"the\", null],\n  [\" \", null],\n  [\"competition\", \"\u6bd4\u8d5b\"],\n  [\",\", null],\n  [\" \", null],\n  [\"and\", \"\u5e76\"],\n  [\" \", null],\n  [\"do\", \"\u5c3d\"],\n  [\" \", null],\n  [\"his\", \"\u4ed6\u7684\"],\n  [\" \", null],\n  [\"best\", \"\u6700\u597d\"],\n  [\" \", null],\n  [\"for\", \"\u4e3a\"],\n  [\" \", null],\n  [\"Tibetan\", \"\u897f\u85cf\"],\n  [\" \", null],\n  [\"basketball\", \"\u7bee\u7403\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nAfter the competition ended, captain Jiang Cuo said that he hoped to gain experience from taking part in the competition, and do his best for Tibetan basketball.\n\n\n---=== TOKENIZED TEXT ===---\n\n['After', ' the', ' competition', ' ended', ',', ' captain', ' Jiang', ' C', 'uo', ' said', ' that', ' he', ' hoped', ' to', ' gain', ' experience', ' from', ' taking', ' part', ' in', ' the', ' competition', ',', ' and', ' do', ' his', ' best', ' for', ' Tibetan', ' basketball', '.']"}], "[\n  [\"After\", \"\u041a\u0435\u0439\u0456\u043d\"],\n  [\" \", null],\n  [\"the\", null],\n  [\" \", null],\n  [\"competition\", \"\u0436\u0430\u0440\u044b\u0441\"],\n  [\" \", null],\n  [\"ended\", \"\u0430\u044f\u049b\u0442\u0430\u043b\u0493\u0430\u043d\u043d\u0430\u043d\"],\n  [\",\", null],\n  [\" \", null],\n  [\"captain\", \"\u043a\u0430\u043f\u0438\u0442\u0430\u043d\"],\n  [\" \", null],\n  [\"Jiang\", \"\u0426\u0437\u044f\u043d\"],\n  [\" \", null],\n  [\"C\", null],\n  [\"uo\", null],\n  [\" \", null],\n  [\"said\", \"\u0430\u0439\u0442\u0442\u044b\"],\n  [\" \", null],\n  [\"that\", null],\n  [\" \", null],\n  [\"he\", \"\u043e\u043b\"],\n  [\" \", null],\n  [\"hoped\", \"\u04af\u043c\u0456\u0442\u0442\u0435\u043d\u0434\u0456\"],\n  [\" \", null],\n  [\"to\", null],\n  [\" \", null],\n  [\"gain\", \"\u0430\u043b\u0443\"],\n  [\" \", null],\n  [\"experience\", \"\u0442\u04d9\u0436\u0456\u0440\u0438\u0431\u0435\"],\n  [\" \", null],\n  [\"from\", null],\n  [\" \", null],\n  [\"taking\", \"\u049b\u0430\u0442\u044b\u0441\u0443\u0434\u0430\u043d\"],\n  [\" \", null],\n  [\"part\", null],\n  [\" \", null],\n  [\"in\", null],\n  [\" \", null],\n  [\"the\", null],\n  [\" \", null],\n  [\"competition\", \"\u0436\u0430\u0440\u044b\u0441\u049b\u0430\"],\n  [\",\", null],\n  [\" \", null],\n  [\"and\", \"\u0436\u04d9\u043d\u0435\"],\n  [\" \", null],\n  [\"do\", \"\u0436\u0430\u0441\u0430\u0443\"],\n  [\" \", null],\n  [\"his\", null],\n  [\" \", null],\n  [\"best\", \"\u0431\u0430\u0440\u044b\u043d\u0448\u0430\"],\n  [\" \", null],\n  [\"for\", null],\n  [\" \", null],\n  [\"Tibetan\", \"\u0422\u0438\u0431\u0435\u0442\"],\n  [\" \", null],\n  [\"basketball\", \"\u0431\u0430\u0441\u043a\u0435\u0442\u0431\u043e\u043b\u044b\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nAfter the competition ended, captain Jiang Cuo said that he hoped to gain experience from taking part in the competition, and do his best for Tibetan basketball.\n\n\n---=== TOKENIZED TEXT ===---\n\n['After', ' the', ' competition', ' ended', ',', ' captain', ' Jiang', ' C', 'uo', ' said', ' that', ' he', ' hoped', ' to', ' gain', ' experience', ' from', ' taking', ' part', ' in', ' the', ' competition', ',', ' and', ' do', ' his', ' best', ' for', ' Tibetan', ' basketball', '.']"}], "[\n  [\"After\", \"\u041f\u043e\u0441\u043b\u0435\"],\n  [\" \", null],\n  [\"the\", null],\n  [\" \", null],\n  [\"competition\", \"\u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u044f\"],\n  [\" \", null],\n  [\"ended\", \"\u0437\u0430\u043a\u043e\u043d\u0447\u0438\u043b\u0438\u0441\u044c\"],\n  [\",\", null],\n  [\" \", null],\n  [\"captain\", \"\u043a\u0430\u043f\u0438\u0442\u0430\u043d\"],\n  [\" \", null],\n  [\"Jiang\", \"\u0426\u0437\u044f\u043d\"],\n  [\" \", null],\n  [\"C\", \"\u0426\u043e\"],\n  [\"uo\", null],\n  [\" \", null],\n  [\"said\", \"\u0441\u043a\u0430\u0437\u0430\u043b\"],\n  [\" \", null],\n  [\"that\", null],\n  [\" \", null],\n  [\"he\", \"\u043e\u043d\"],\n  [\" \", null],\n  [\"hoped\", \"\u043d\u0430\u0434\u0435\u0435\u0442\u0441\u044f\"],\n  [\" \", null],\n  [\"to\", null],\n  [\" \", null],\n  [\"gain\", \"\u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c\"],\n  [\" \", null],\n  [\"experience\", \"\u043e\u043f\u044b\u0442\"],\n  [\" \", null],\n  [\"from\", \"\u043e\u0442\"],\n  [\" \", null],\n  [\"taking\", \"\u0443\u0447\u0430\u0441\u0442\u0438\u044f\"],\n  [\" \", null],\n  [\"part\", null],\n  [\" \", null],\n  [\"in\", \"\u0432\"],\n  [\" \", null],\n  [\"the\", null],\n  [\" \", null],\n  [\"competition\", \"\u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u0438\"],\n  [\",\", null],\n  [\" \", null],\n  [\"and\", \"\u0438\"],\n  [\" \", null],\n  [\"do\", \"\u0441\u0434\u0435\u043b\u0430\u0442\u044c\"],\n  [\" \", null],\n  [\"his\", null],\n  [\" \", null],\n  [\"best\", \"\u0432\u0441\u0451 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0435\"],\n  [\" \", null],\n  [\"for\", \"\u0434\u043b\u044f\"],\n  [\" \", null],\n  [\"Tibetan\", \"\u0442\u0438\u0431\u0435\u0442\u0441\u043a\u043e\u0433\u043e\"],\n  [\" \", null],\n  [\"basketball\", \"\u0431\u0430\u0441\u043a\u0435\u0442\u0431\u043e\u043b\u0430\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nAfter the policy was enhanced, officials stated that \u201cuniversity students are entitled to purchase residential property at a 20% discount to market price\u201d.\n\n\n---=== TOKENIZED TEXT ===---\n\n['After', ' the', ' policy', ' was', ' enhanced', ',', ' officials', ' stated', ' that', ' \u201c', 'un', 'iversity', ' students', ' are', ' entitled', ' to', ' purchase', ' residential', ' property', ' at', ' a', ' ', '2', '0', '%', ' discount', ' to', ' market', ' price', '\u201d.']"}], "[\n  [\"After\", \"\u5728\"],\n  [\" \", null],\n  [\"the\", null],\n  [\" \", null],\n  [\"policy\", \"\u653f\u7b56\"],\n  [\" \", null],\n  [\"was\", null],\n  [\" \", null],\n  [\"enhanced\", \"\u52a0\u5f3a\"],\n  [\",\", null],\n  [\" \", null],\n  [\"officials\", \"\u5b98\u5458\"],\n  [\" \", null],\n  [\"stated\", \"\u8868\u793a\"],\n  [\" \", null],\n  [\"that\", null],\n  [\" \", null],\n  [\"\"\", null],\n  [\"un\", \"\u5927\u5b66\u751f\"],\n  [\"iversity\", null],\n  [\" \", null],\n  [\"students\", null],\n  [\" \", null],\n  [\"are\", null],\n  [\" \", null],\n  [\"entitled\", \"\u6709\u6743\"],\n  [\" \", null],\n  [\"to\", null],\n  [\" \", null],\n  [\"purchase\", \"\u8d2d\u4e70\"],\n  [\" \", null],\n  [\"residential\", \"\u4f4f\u5b85\"],\n  [\" \", null],\n  [\"property\", \"\u623f\u4ea7\"],\n  [\" \", null],\n  [\"at\", null],\n  [\" \", null],\n  [\"a\", null],\n  [\" \", null],\n  [\"2\", \"20\"],\n  [\"0\", null],\n  [\"%\", null],\n  [\" \", null],\n  [\"discount\", \"\u6298\u6263\"],\n  [\" \", null],\n  [\"to\", null],\n  [\" \", null],\n  [\"market\", \"\u5e02\u573a\"],\n  [\" \", null],\n  [\"price\", \"\u4ef7\u683c\"],\n  [\"\"\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nAfter the policy was enhanced, officials stated that \u201cuniversity students are entitled to purchase residential property at a 20% discount to market price\u201d.\n\n\n---=== TOKENIZED TEXT ===---\n\n['After', ' the', ' policy', ' was', ' enhanced', ',', ' officials', ' stated', ' that', ' \u201c', 'un', 'iversity', ' students', ' are', ' entitled', ' to', ' purchase', ' residential', ' property', ' at', ' a', ' ', '2', '0', '%', ' discount', ' to', ' market', ' price', '\u201d.']"}], "[\n    [\"After\", \"\u041a\u0435\u0439\u0456\u043d\"],\n    [\" the\", null],\n    [\" policy\", \"\u0441\u0430\u044f\u0441\u0430\u0442\"],\n    [\" was\", null],\n    [\" enhanced\", \"\u0436\u0435\u0442\u0456\u043b\u0434\u0456\u0440\u0456\u043b\u0433\u0435\u043d\u043d\u0435\u043d\"],\n    [\",\", null],\n    [\" officials\", \"\u043b\u0430\u0443\u0430\u0437\u044b\u043c\u0434\u044b \u0442\u04b1\u043b\u0493\u0430\u043b\u0430\u0440\"],\n    [\" stated\", \"\u043c\u04d9\u043b\u0456\u043c\u0434\u0435\u0434\u0456\"],\n    [\" that\", null],\n    [\" \\\"\", null],\n    [\"un\", \"\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442\"],\n    [\"iversity\", null],\n    [\" students\", \"\u0441\u0442\u0443\u0434\u0435\u043d\u0442\u0442\u0435\u0440\u0456\"],\n    [\" are\", null],\n    [\" entitled\", \"\u049b\u04b1\u049b\u044b\u043b\u044b\"],\n    [\" to\", null],\n    [\" purchase\", \"\u0441\u0430\u0442\u044b\u043f \u0430\u043b\u0443\u0493\u0430\"],\n    [\" residential\", \"\u0442\u04b1\u0440\u0493\u044b\u043d \u04af\u0439\"],\n    [\" property\", \"\u043c\u04af\u043b\u043a\u0456\u043d\"],\n    [\" at\", null],\n    [\" a\", null],\n    [\" \", null],\n    [\"2\", null],\n    [\"0\", null],\n    [\"%\", null],\n    [\" discount\", \"\u0436\u0435\u04a3\u0456\u043b\u0434\u0456\u043a\u043f\u0435\u043d\"],\n    [\" to\", null],\n    [\" market\", \"\u043d\u0430\u0440\u044b\u049b\u0442\u044b\u049b\"],\n    [\" price\", \"\u0431\u0430\u0493\u0430\u0434\u0430\u043d\"],\n    [\"\\\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nAfter the policy was enhanced, officials stated that \u201cuniversity students are entitled to purchase residential property at a 20% discount to market price\u201d.\n\n\n---=== TOKENIZED TEXT ===---\n\n['After', ' the', ' policy', ' was', ' enhanced', ',', ' officials', ' stated', ' that', ' \u201c', 'un', 'iversity', ' students', ' are', ' entitled', ' to', ' purchase', ' residential', ' property', ' at', ' a', ' ', '2', '0', '%', ' discount', ' to', ' market', ' price', '\u201d.']"}], "[\n    [\"After\", \"\u041f\u043e\u0441\u043b\u0435\"],\n    [\" \", null],\n    [\"the\", null],\n    [\" \", null],\n    [\"policy\", \"\u043f\u043e\u043b\u0438\u0442\u0438\u043a\u0430\"],\n    [\" \", null],\n    [\"was\", null],\n    [\" \", null],\n    [\"enhanced\", \"\u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0430\"],\n    [\",\", null],\n    [\" \", null],\n    [\"officials\", \"\u0447\u0438\u043d\u043e\u0432\u043d\u0438\u043a\u0438\"],\n    [\" \", null],\n    [\"stated\", \"\u0437\u0430\u044f\u0432\u0438\u043b\u0438\"],\n    [\" \", null],\n    [\"that\", null],\n    [\" \", null],\n    [\"\"\", null],\n    [\"un\", \"\u0441\u0442\u0443\u0434\u0435\u043d\u0442\u044b\"],\n    [\"iversity\", null],\n    [\" \", null],\n    [\"students\", null],\n    [\" \", null],\n    [\"are\", null],\n    [\" \", null],\n    [\"entitled\", \"\u0438\u043c\u0435\u044e\u0442 \u043f\u0440\u0430\u0432\u043e\"],\n    [\" \", null],\n    [\"to\", null],\n    [\" \", null],\n    [\"purchase\", \"\u043f\u0440\u0438\u043e\u0431\u0440\u0435\u0442\u0430\u0442\u044c\"],\n    [\" \", null],\n    [\"residential\", \"\u0436\u0438\u043b\u0443\u044e\"],\n    [\" \", null],\n    [\"property\", \"\u043d\u0435\u0434\u0432\u0438\u0436\u0438\u043c\u043e\u0441\u0442\u044c\"],\n    [\" \", null],\n    [\"at\", null],\n    [\" \", null],\n    [\"a\", null],\n    [\" \", null],\n    [\"2\", \"20\"],\n    [\"0\", null],\n    [\"%\", null],\n    [\" \", null],\n    [\"discount\", \"\u0441\u043a\u0438\u0434\u043a\u043e\u0439\"],\n    [\" \", null],\n    [\"to\", null],\n    [\" \", null],\n    [\"market\", \"\u0440\u044b\u043d\u043e\u0447\u043d\u043e\u0439\"],\n    [\" \", null],\n    [\"price\", \"\u0446\u0435\u043d\u044b\"],\n    [\"\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nAt the beginning of the new year, when Shangpin Home Collection\u2019s IPO was first approved, news of the listing of this classic brand, which represented the customized home furnishing sector, attracted the attention of the entire home furnishing industry.\n\n\n---=== TOKENIZED TEXT ===---\n\n['At', ' the', ' beginning', ' of', ' the', ' new', ' year', ',', ' when', ' Sh', 'ang', 'pin', ' Home', ' Collection', '\u2019s', ' IPO', ' was', ' first', ' approved', ',', ' news', ' of', ' the', ' listing', ' of', ' this', ' classic', ' brand', ',', ' which', ' represented', ' the', ' customized', ' home', ' furn', 'ishing', ' sector', ',', ' attracted', ' the', ' attention', ' of', ' the', ' entire', ' home', ' furn', 'ishing', ' industry', '.']"}], "[\n[\"At\", null],\n[\" the\", null],\n[\" beginning\", \"\u5f00\u59cb\"],\n[\" of\", null],\n[\" the\", null],\n[\" new\", \"\u65b0\"],\n[\" year\", \"\u5e74\"],\n[\",\", null],\n[\" when\", null],\n[\" Sh\", \"\u5c1a\u54c1\"],\n[\"ang\", null],\n[\"pin\", null],\n[\" Home\", \"\u5bb6\u5c45\"],\n[\" Collection\", \"\u6536\u85cf\"],\n[\"'s\", null],\n[\" IPO\", \"\u9996\u6b21\u516c\u5f00\u52df\u80a1\"],\n[\" was\", null],\n[\" first\", \"\u9996\u6b21\"],\n[\" approved\", \"\u6279\u51c6\"],\n[\",\", null],\n[\" news\", \"\u65b0\u95fb\"],\n[\" of\", null],\n[\" the\", null],\n[\" listing\", \"\u4e0a\u5e02\"],\n[\" of\", null],\n[\" this\", null],\n[\" classic\", \"\u7ecf\u5178\"],\n[\" brand\", \"\u54c1\u724c\"],\n[\",\", null],\n[\" which\", null],\n[\" represented\", \"\u4ee3\u8868\"],\n[\" the\", null],\n[\" customized\", \"\u5b9a\u5236\"],\n[\" home\", \"\u5bb6\u5c45\"],\n[\" furn\", \"\u5bb6\u5177\"],\n[\"ishing\", null],\n[\" sector\", \"\u884c\u4e1a\"],\n[\",\", null],\n[\" attracted\", \"\u5438\u5f15\"],\n[\" the\", null],\n[\" attention\", \"\u5173\u6ce8\"],\n[\" of\", null],\n[\" the\", null],\n[\" entire\", \"\u6574\u4e2a\"],\n[\" home\", \"\u5bb6\u5c45\"],\n[\" furn\", \"\u5bb6\u5177\"],\n[\"ishing\", null],\n[\" industry\", \"\u884c\u4e1a\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nAt the beginning of the new year, when Shangpin Home Collection\u2019s IPO was first approved, news of the listing of this classic brand, which represented the customized home furnishing sector, attracted the attention of the entire home furnishing industry.\n\n\n---=== TOKENIZED TEXT ===---\n\n['At', ' the', ' beginning', ' of', ' the', ' new', ' year', ',', ' when', ' Sh', 'ang', 'pin', ' Home', ' Collection', '\u2019s', ' IPO', ' was', ' first', ' approved', ',', ' news', ' of', ' the', ' listing', ' of', ' this', ' classic', ' brand', ',', ' which', ' represented', ' the', ' customized', ' home', ' furn', 'ishing', ' sector', ',', ' attracted', ' the', ' attention', ' of', ' the', ' entire', ' home', ' furn', 'ishing', ' industry', '.']"}], "[\n[\"At\", null],\n[\" the\", null],\n[\" beginning\", \"\u0431\u0430\u0441\u044b\u043d\u0434\u0430\"],\n[\" of\", null],\n[\" the\", null],\n[\" new\", \"\u0436\u0430\u04a3\u0430\"],\n[\" year\", \"\u0436\u044b\u043b\"],\n[\",\", null],\n[\" when\", null],\n[\" Sh\", \"\u0428\u0430\u04a3\u043f\u0438\u043d\"],\n[\"ang\", null],\n[\"pin\", null],\n[\" Home\", \"\u04ae\u0439\"],\n[\" Collection\", \"\u041a\u043e\u043b\u043b\u0435\u043a\u0446\u0438\u044f\u0441\u044b\"],\n[\"'s\", null],\n[\" IPO\", \"IPO\"],\n[\" was\", null],\n[\" first\", \"\u0430\u043b\u0493\u0430\u0448\"],\n[\" approved\", \"\u043c\u0430\u049b\u04b1\u043b\u0434\u0430\u043d\u0434\u044b\"],\n[\",\", null],\n[\" news\", \"\u0436\u0430\u04a3\u0430\u043b\u044b\u049b\"],\n[\" of\", null],\n[\" the\", null],\n[\" listing\", \"\u043b\u0438\u0441\u0442\u0438\u043d\u0433\"],\n[\" of\", null],\n[\" this\", null],\n[\" classic\", \"\u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430\u043b\u044b\u049b\"],\n[\" brand\", \"\u0431\u0440\u0435\u043d\u0434\"],\n[\",\", null],\n[\" which\", null],\n[\" represented\", \"\u04b1\u0441\u044b\u043d\u0493\u0430\u043d\"],\n[\" the\", null],\n[\" customized\", \"\u0436\u0435\u043a\u0435\u043b\u0435\u043d\u0433\u0435\u043d\"],\n[\" home\", \"\u04af\u0439\"],\n[\" furn\", \"\u0436\u0438\u04bb\u0430\u0437\"],\n[\"ishing\", null],\n[\" sector\", \"\u0441\u0435\u043a\u0442\u043e\u0440\u044b\"],\n[\",\", null],\n[\" attracted\", \"\u0442\u0430\u0440\u0442\u0442\u044b\"],\n[\" the\", null],\n[\" attention\", \"\u043d\u0430\u0437\u0430\u0440\u044b\u043d\"],\n[\" of\", null],\n[\" the\", null],\n[\" entire\", \"\u0431\u04af\u043a\u0456\u043b\"],\n[\" home\", \"\u04af\u0439\"],\n[\" furn\", \"\u0436\u0438\u04bb\u0430\u0437\"],\n[\"ishing\", null],\n[\" industry\", \"\u0438\u043d\u0434\u0443\u0441\u0442\u0440\u0438\u044f\u0441\u044b\u043d\u044b\u04a3\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nAt the beginning of the new year, when Shangpin Home Collection\u2019s IPO was first approved, news of the listing of this classic brand, which represented the customized home furnishing sector, attracted the attention of the entire home furnishing industry.\n\n\n---=== TOKENIZED TEXT ===---\n\n['At', ' the', ' beginning', ' of', ' the', ' new', ' year', ',', ' when', ' Sh', 'ang', 'pin', ' Home', ' Collection', '\u2019s', ' IPO', ' was', ' first', ' approved', ',', ' news', ' of', ' the', ' listing', ' of', ' this', ' classic', ' brand', ',', ' which', ' represented', ' the', ' customized', ' home', ' furn', 'ishing', ' sector', ',', ' attracted', ' the', ' attention', ' of', ' the', ' entire', ' home', ' furn', 'ishing', ' industry', '.']"}], "[\n[\"At\", \"\u0412\"],\n[\" the\", null],\n[\" beginning\", \"\u043d\u0430\u0447\u0430\u043b\u0435\"],\n[\" of\", null],\n[\" the\", null],\n[\" new\", \"\u043d\u043e\u0432\u043e\u0433\u043e\"],\n[\" year\", \"\u0433\u043e\u0434\u0430\"],\n[\",\", null],\n[\" when\", \"\u043a\u043e\u0433\u0434\u0430\"],\n[\" Sh\", \"\u0428\u0430\u043d\u043f\u0438\u043d\"],\n[\"ang\", null],\n[\"pin\", null],\n[\" Home\", \"\u0425\u043e\u0443\u043c\"],\n[\" Collection\", \"\u041a\u043e\u043b\u043b\u0435\u043a\u0448\u043d\"],\n[\"'s\", null],\n[\" IPO\", \"IPO\"],\n[\" was\", null],\n[\" first\", \"\u0432\u043f\u0435\u0440\u0432\u044b\u0435\"],\n[\" approved\", \"\u043e\u0434\u043e\u0431\u0440\u0435\u043d\u043e\"],\n[\",\", null],\n[\" news\", \"\u043d\u043e\u0432\u043e\u0441\u0442\u044c\"],\n[\" of\", null],\n[\" the\", null],\n[\" listing\", \"\u043b\u0438\u0441\u0442\u0438\u043d\u0433\u0435\"],\n[\" of\", null],\n[\" this\", \"\u044d\u0442\u043e\u0433\u043e\"],\n[\" classic\", \"\u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e\"],\n[\" brand\", \"\u0431\u0440\u0435\u043d\u0434\u0430\"],\n[\",\", null],\n[\" which\", \"\u043a\u043e\u0442\u043e\u0440\u044b\u0439\"],\n[\" represented\", \"\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u043b\"],\n[\" the\", null],\n[\" customized\", \"\u0438\u043d\u0434\u0438\u0432\u0438\u0434\u0443\u0430\u043b\u044c\u043d\u044b\u0439\"],\n[\" home\", \"\u0434\u043e\u043c\u0430\u0448\u043d\u0438\u0439\"],\n[\" furn\", \"\u043c\u0435\u0431\u0435\u043b\u044c\u043d\u044b\u0439\"],\n[\"ishing\", null],\n[\" sector\", \"\u0441\u0435\u043a\u0442\u043e\u0440\"],\n[\",\", null],\n[\" attracted\", \"\u043f\u0440\u0438\u0432\u043b\u0435\u043a\u043b\u0430\"],\n[\" the\", null],\n[\" attention\", \"\u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435\"],\n[\" of\", null],\n[\" the\", null],\n[\" entire\", \"\u0432\u0441\u0435\u0439\"],\n[\" home\", \"\u0434\u043e\u043c\u0430\u0448\u043d\u0435\u0439\"],\n[\" furn\", \"\u043c\u0435\u0431\u0435\u043b\u044c\u043d\u043e\u0439\"],\n[\"ishing\", null],\n[\" industry\", \"\u0438\u043d\u0434\u0443\u0441\u0442\u0440\u0438\u0438\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\n\"We believe in dialogue with, and being responsive to, our state partners,\" EPA Chief Scott Pruitt said in a statement.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\"We', ' believe', ' in', ' dialogue', ' with', ',', ' and', ' being', ' responsive', ' to', ',', ' our', ' state', ' partners', ',\"', ' EPA', ' Chief', ' Scott', ' Pruitt', ' said', ' in', ' a', ' statement', '.']"}], "[\n  [\"\\\"We\", \"\\\"\u6211\u4eec\"],\n  [\" believe\", \"\u76f8\u4fe1\"],\n  [\" in\", null],\n  [\" dialogue\", \"\u5bf9\u8bdd\"],\n  [\" with\", null],\n  [\",\", null],\n  [\" and\", null],\n  [\" being\", \"\u4fdd\u6301\"],\n  [\" responsive\", \"\u54cd\u5e94\"],\n  [\" to\", null],\n  [\",\", null],\n  [\" our\", \"\u6211\u4eec\u7684\"],\n  [\" state\", \"\u5dde\"],\n  [\" partners\", \"\u5408\u4f5c\u4f19\u4f34\"],\n  [\",\\\"\", null],\n  [\" EPA\", \"\u73af\u4fdd\u7f72\"],\n  [\" Chief\", \"\u5c40\u957f\"],\n  [\" Scott\", \"\u65af\u79d1\u7279\"],\n  [\" Pruitt\", \"\u666e\u9c81\u4f0a\u7279\"],\n  [\" said\", \"\u8bf4\"],\n  [\" in\", null],\n  [\" a\", null],\n  [\" statement\", \"\u58f0\u660e\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\n\"We believe in dialogue with, and being responsive to, our state partners,\" EPA Chief Scott Pruitt said in a statement.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\"We', ' believe', ' in', ' dialogue', ' with', ',', ' and', ' being', ' responsive', ' to', ',', ' our', ' state', ' partners', ',\"', ' EPA', ' Chief', ' Scott', ' Pruitt', ' said', ' in', ' a', ' statement', '.']"}], "[\n  [\"\\\"We\", \"\\\"\u0411\u0456\u0437\"],\n  [\" believe\", \" \u0441\u0435\u043d\u0435\u043c\u0456\u0437\"],\n  [\" in\", null],\n  [\" dialogue\", \"\u0434\u0438\u0430\u043b\u043e\u0433\u049b\u0430\"],\n  [\" with\", null],\n  [\",\", null],\n  [\" and\", null],\n  [\" being\", null],\n  [\" responsive\", \"\u0436\u0430\u0443\u0430\u043f\u0442\u044b\"],\n  [\" to\", null],\n  [\",\", null],\n  [\" our\", null],\n  [\" state\", \"\u043c\u0435\u043c\u043b\u0435\u043a\u0435\u0442\u0442\u0456\u043a\"],\n  [\" partners\", \"\u0441\u0435\u0440\u0456\u043a\u0442\u0435\u0441\u0442\u0435\u0440\u0456\u043c\u0456\u0437\u0433\u0435\"],\n  [\",\\\"\", null],\n  [\" EPA\", \"\u0410\u049a\u0428 \u049a\u043e\u0440\u0448\u0430\u0493\u0430\u043d \u043e\u0440\u0442\u0430\u043d\u044b \u049b\u043e\u0440\u0493\u0430\u0443 \u0430\u0433\u0435\u043d\u0442\u0442\u0456\u0433\u0456\u043d\u0456\u04a3\"],\n  [\" Chief\", \"\u0431\u0430\u0441\u0448\u044b\u0441\u044b\"],\n  [\" Scott\", \"\u0421\u043a\u043e\u0442\u0442\"],\n  [\" Pruitt\", \"\u041f\u0440\u044e\u0438\u0442\u0442\"],\n  [\" said\", \"\u0434\u0435\u0434\u0456\"],\n  [\" in\", null],\n  [\" a\", null],\n  [\" statement\", \"\u043c\u04d9\u043b\u0456\u043c\u0434\u0435\u043c\u0435\u0434\u0435\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\n\"We believe in dialogue with, and being responsive to, our state partners,\" EPA Chief Scott Pruitt said in a statement.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\"We', ' believe', ' in', ' dialogue', ' with', ',', ' and', ' being', ' responsive', ' to', ',', ' our', ' state', ' partners', ',\"', ' EPA', ' Chief', ' Scott', ' Pruitt', ' said', ' in', ' a', ' statement', '.']"}], "[\n  [\"We\", \"\u041c\u044b\"],\n  [\" believe\", \"\u0432\u0435\u0440\u0438\u043c\"],\n  [\" in\", null],\n  [\" dialogue\", \"\u0434\u0438\u0430\u043b\u043e\u0433\"],\n  [\" with\", null],\n  [\",\", null],\n  [\" and\", null],\n  [\" being\", null],\n  [\" responsive\", \"\u043e\u0442\u0437\u044b\u0432\u0447\u0438\u0432\u044b\u043c\u0438\"],\n  [\" to\", null],\n  [\",\", null],\n  [\" our\", null],\n  [\" state\", \"\u0433\u043e\u0441\u0443\u0434\u0430\u0440\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u043c\u0438\"],\n  [\" partners\", \"\u043f\u0430\u0440\u0442\u043d\u0435\u0440\u0430\u043c\u0438\"],\n  [\",\\\"\", null],\n  [\" EPA\", \"EPA\"],\n  [\" Chief\", \"\u0433\u043b\u0430\u0432\u0430\"],\n  [\" Scott\", \"\u0421\u043a\u043e\u0442\u0442\"],\n  [\" Pruitt\", \"\u041f\u0440\u044e\u0438\u0442\u0442\"],\n  [\" said\", \"\u0441\u043a\u0430\u0437\u0430\u043b\"],\n  [\" in\", null],\n  [\" a\", null],\n  [\" statement\", \"\u0437\u0430\u044f\u0432\u043b\u0435\u043d\u0438\u0438\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nThis concludes tonight's Xinwen Lianbo broadcast. Thank you for watching.\n\n\n---=== TOKENIZED TEXT ===---\n\n['This', ' concludes', ' tonight', \"'s\", ' Xin', 'wen', ' L', 'ian', 'bo', ' broadcast', '.', ' Thank', ' you', ' for', ' watching', '.']"}], "[\n  [\"This\", \"\u8fd9\"],\n  [\" \", null],\n  [\"concludes\", \"\u7ed3\u675f\"],\n  [\" \", null],\n  [\"tonight\", \"\u4eca\u665a\"],\n  [\"'s\", null],\n  [\" \", null],\n  [\"Xin\", \"\u65b0\u95fb\u8054\u64ad\"],\n  [\"wen\", null],\n  [\" \", null],\n  [\"L\", null],\n  [\"ian\", null],\n  [\"bo\", null],\n  [\" \", null],\n  [\"broadcast\", \"\u64ad\u51fa\"],\n  [\".\", null],\n  [\" \", null],\n  [\"Thank\", \"\u8c22\u8c22\"],\n  [\" \", null],\n  [\"you\", \"\u60a8\"],\n  [\" \", null],\n  [\"for\", null],\n  [\" \", null],\n  [\"watching\", \"\u6536\u770b\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nThis concludes tonight's Xinwen Lianbo broadcast. Thank you for watching.\n\n\n---=== TOKENIZED TEXT ===---\n\n['This', ' concludes', ' tonight', \"'s\", ' Xin', 'wen', ' L', 'ian', 'bo', ' broadcast', '.', ' Thank', ' you', ' for', ' watching', '.']"}], "[\n    [\"This\", \"\u0411\u04b1\u043b\"],\n    [\" concludes\", \"\u0430\u044f\u049b\u0442\u0430\u0439\u0434\u044b\"],\n    [\" tonight\", \"\u0431\u04af\u0433\u0456\u043d\u0433\u0456 \u043a\u0435\u0448\u043a\u0456\"],\n    [\"'s\", null],\n    [\" Xin\", \"\u0421\u0438\u043d\u044c\u0432\u044d\u043d\u044c\"],\n    [\"wen\", null],\n    [\" L\", null],\n    [\"ian\", null],\n    [\"bo\", null],\n    [\" broadcast\", \"\u0445\u0430\u0431\u0430\u0440\u044b\"],\n    [\".\", null],\n    [\" Thank\", \"\u0420\u0430\u0445\u043c\u0435\u0442\"],\n    [\" you\", \"\u0441\u0456\u0437\u0433\u0435\"],\n    [\" for\", null],\n    [\" watching\", \"\u043a\u04e9\u0440\u0433\u0435\u043d\u0456\u04a3\u0456\u0437\u0433\u0435\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThis concludes tonight's Xinwen Lianbo broadcast. Thank you for watching.\n\n\n---=== TOKENIZED TEXT ===---\n\n['This', ' concludes', ' tonight', \"'s\", ' Xin', 'wen', ' L', 'ian', 'bo', ' broadcast', '.', ' Thank', ' you', ' for', ' watching', '.']"}], "[\n  [\"This\", \"\u042d\u0442\u043e\"],\n  [\" concludes\", \"\u0437\u0430\u043a\u0430\u043d\u0447\u0438\u0432\u0430\u0435\u0442\"],\n  [\" tonight\", \"\u0441\u0435\u0433\u043e\u0434\u043d\u044f\u0448\u043d\u0438\u0439\"],\n  [\"'s\", null],\n  [\" Xin\", \"\u0421\u0438\u043d\u044c\u0432\u044d\u043d\u044c\"],\n  [\"wen\", null],\n  [\" L\", null],\n  [\"ian\", null],\n  [\"bo\", null],\n  [\" broadcast\", \"\u0432\u044b\u043f\u0443\u0441\u043a\"],\n  [\".\", null],\n  [\" Thank\", \"\u0421\u043f\u0430\u0441\u0438\u0431\u043e\"],\n  [\" you\", null],\n  [\" for\", null],\n  [\" watching\", \"\u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nWhen newspaper Svenska Dagbladet investigated the 581 results released however it found that in 243 of the cases the person had openly stated that they turn 18 this year.\n\n\n---=== TOKENIZED TEXT ===---\n\n['When', ' newspaper', ' Svens', 'ka', ' Dag', 'bl', 'ad', 'et', ' investigated', ' the', ' ', '5', '8', '1', ' results', ' released', ' however', ' it', ' found', ' that', ' in', ' ', '2', '4', '3', ' of', ' the', ' cases', ' the', ' person', ' had', ' openly', ' stated', ' that', ' they', ' turn', ' ', '1', '8', ' this', ' year', '.']"}], "[\n    [\"When\", \"\u5f53\"],\n    [\" \", null],\n    [\"newspaper\", \"\u62a5\u7eb8\"],\n    [\" \", null],\n    [\"Svens\", \"\u745e\u5178\u65e5\u62a5\"],\n    [\"ka\", null],\n    [\" \", null],\n    [\"Dag\", null],\n    [\"bl\", null],\n    [\"ad\", null],\n    [\"et\", null],\n    [\" \", null],\n    [\"investigated\", \"\u8c03\u67e5\"],\n    [\" \", null],\n    [\"the\", null],\n    [\" \", null],\n    [\"5\", null],\n    [\"8\", null],\n    [\"1\", null],\n    [\" \", null],\n    [\"results\", \"\u7ed3\u679c\"],\n    [\" \", null],\n    [\"released\", \"\u53d1\u5e03\"],\n    [\" \", null],\n    [\"however\", \"\u7136\u800c\"],\n    [\" \", null],\n    [\"it\", \"\u5b83\"],\n    [\" \", null],\n    [\"found\", \"\u53d1\u73b0\"],\n    [\" \", null],\n    [\"that\", null],\n    [\" \", null],\n    [\"in\", \"\u5728\"],\n    [\" \", null],\n    [\"2\", null],\n    [\"4\", null],\n    [\"3\", null],\n    [\" \", null],\n    [\"of\", null],\n    [\" \", null],\n    [\"the\", null],\n    [\" \", null],\n    [\"cases\", \"\u6848\u4f8b\"],\n    [\" \", null],\n    [\"the\", null],\n    [\" \", null],\n    [\"person\", \"\u4eba\"],\n    [\" \", null],\n    [\"had\", null],\n    [\" \", null],\n    [\"openly\", \"\u516c\u5f00\"],\n    [\" \", null],\n    [\"stated\", \"\u58f0\u660e\"],\n    [\" \", null],\n    [\"that\", null],\n    [\" \", null],\n    [\"they\", \"\u4ed6\u4eec\"],\n    [\" \", null],\n    [\"turn\", \"\u5e74\u6ee1\"],\n    [\" \", null],\n    [\"1\", null],\n    [\"8\", null],\n    [\" \", null],\n    [\"this\", \"\u4eca\"],\n    [\" \", null],\n    [\"year\", \"\u5e74\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nWhen newspaper Svenska Dagbladet investigated the 581 results released however it found that in 243 of the cases the person had openly stated that they turn 18 this year.\n\n\n---=== TOKENIZED TEXT ===---\n\n['When', ' newspaper', ' Svens', 'ka', ' Dag', 'bl', 'ad', 'et', ' investigated', ' the', ' ', '5', '8', '1', ' results', ' released', ' however', ' it', ' found', ' that', ' in', ' ', '2', '4', '3', ' of', ' the', ' cases', ' the', ' person', ' had', ' openly', ' stated', ' that', ' they', ' turn', ' ', '1', '8', ' this', ' year', '.']"}], "[\n[\"When\", \"\u049a\u0430\u0448\u0430\u043d\"],\n[\" newspaper\", \"\u0433\u0430\u0437\u0435\u0442\"],\n[\" Svens\", \"\u0421\u0432\u0435\u043d\u0441\u043a\u0430\"],\n[\"ka\", null],\n[\" Dag\", \"\u0414\u0430\u0433\"],\n[\"bl\", null],\n[\"ad\", null],\n[\"et\", null],\n[\" investigated\", \"\u0437\u0435\u0440\u0442\u0442\u0435\u0433\u0435\u043d\u0434\u0435\"],\n[\" the\", null],\n[\" \", null],\n[\"5\", null],\n[\"8\", null],\n[\"1\", null],\n[\" results\", \"\u043d\u04d9\u0442\u0438\u0436\u0435\u043d\u0456\"],\n[\" released\", \"\u0436\u0430\u0440\u0438\u044f\u043b\u0430\u0493\u0430\u043d\"],\n[\" however\", \"\u0430\u043b\u0430\u0439\u0434\u0430\"],\n[\" it\", \"\u043e\u043b\"],\n[\" found\", \"\u0430\u043d\u044b\u049b\u0442\u0430\u0434\u044b\"],\n[\" that\", null],\n[\" in\", null],\n[\" \", null],\n[\"2\", null],\n[\"4\", null],\n[\"3\", null],\n[\" of\", null],\n[\" the\", null],\n[\" cases\", \"\u0436\u0430\u0493\u0434\u0430\u0439\u0434\u0430\"],\n[\" the\", null],\n[\" person\", \"\u0430\u0434\u0430\u043c\"],\n[\" had\", null],\n[\" openly\", \"\u0430\u0448\u044b\u049b\"],\n[\" stated\", \"\u043c\u04d9\u043b\u0456\u043c\u0434\u0435\u0433\u0435\u043d\"],\n[\" that\", null],\n[\" they\", \"\u043e\u043b\u0430\u0440\"],\n[\" turn\", \"\u0442\u043e\u043b\u0430\u0442\u044b\u043d\u044b\u043d\"],\n[\" \", null],\n[\"1\", null],\n[\"8\", null],\n[\" this\", \"\u0431\u0438\u044b\u043b\"],\n[\" year\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nWhen newspaper Svenska Dagbladet investigated the 581 results released however it found that in 243 of the cases the person had openly stated that they turn 18 this year.\n\n\n---=== TOKENIZED TEXT ===---\n\n['When', ' newspaper', ' Svens', 'ka', ' Dag', 'bl', 'ad', 'et', ' investigated', ' the', ' ', '5', '8', '1', ' results', ' released', ' however', ' it', ' found', ' that', ' in', ' ', '2', '4', '3', ' of', ' the', ' cases', ' the', ' person', ' had', ' openly', ' stated', ' that', ' they', ' turn', ' ', '1', '8', ' this', ' year', '.']"}], "[\n  [\"When\", \"\u041a\u043e\u0433\u0434\u0430\"],\n  [\" newspaper\", \"\u0433\u0430\u0437\u0435\u0442\u0430\"],\n  [\" Svens\", \"\u0421\u0432\u0435\u043d\u0441\u043a\u0430\"],\n  [\"ka\", null],\n  [\" Dag\", \"\u0414\u0430\u0433\u0431\u043b\u0430\u0434\u0435\u0442\"],\n  [\"bl\", null],\n  [\"ad\", null],\n  [\"et\", null],\n  [\" investigated\", \"\u0440\u0430\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043b\u0430\"],\n  [\" the\", null],\n  [\" \", null],\n  [\"5\", null],\n  [\"8\", null],\n  [\"1\", null],\n  [\" results\", \"\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\"],\n  [\" released\", \"\u043e\u043f\u0443\u0431\u043b\u0438\u043a\u043e\u0432\u0430\u043d\u043d\u044b\u0445\"],\n  [\" however\", \"\u043e\u0434\u043d\u0430\u043a\u043e\"],\n  [\" it\", \"\u043e\u043d\u0430\"],\n  [\" found\", \"\u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0438\u043b\u0430\"],\n  [\" that\", \"\u0447\u0442\u043e\"],\n  [\" in\", \"\u0432\"],\n  [\" \", null],\n  [\"2\", null],\n  [\"4\", null],\n  [\"3\", null],\n  [\" of\", \"\u0438\u0437\"],\n  [\" the\", null],\n  [\" cases\", \"\u0441\u043b\u0443\u0447\u0430\u0435\u0432\"],\n  [\" the\", null],\n  [\" person\", \"\u0447\u0435\u043b\u043e\u0432\u0435\u043a\"],\n  [\" had\", null],\n  [\" openly\", \"\u043e\u0442\u043a\u0440\u044b\u0442\u043e\"],\n  [\" stated\", \"\u0437\u0430\u044f\u0432\u0438\u043b\"],\n  [\" that\", \"\u0447\u0442\u043e\"],\n  [\" they\", \"\u043e\u043d\u0438\"],\n  [\" turn\", \"\u0438\u0441\u043f\u043e\u043b\u043d\u044f\u0435\u0442\u0441\u044f\"],\n  [\" \", null],\n  [\"1\", null],\n  [\"8\", null],\n  [\" this\", \"\u0432 \u044d\u0442\u043e\u043c\"],\n  [\" year\", \"\u0433\u043e\u0434\u0443\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nThe aim behind these sex robots is to create as much of a physical likeness to actual women (albeit with porn star proportions) as technologically possible.\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', ' aim', ' behind', ' these', ' sex', ' robots', ' is', ' to', ' create', ' as', ' much', ' of', ' a', ' physical', ' likeness', ' to', ' actual', ' women', ' (', 'al', 'beit', ' with', ' porn', ' star', ' proportions', ')', ' as', ' techn', 'ologically', ' possible', '.']"}], "[\n    [\"The\", null],\n    [\"aim\", \"\u76ee\u6807\"],\n    [\" \", null],\n    [\"behind\", null],\n    [\" \", null],\n    [\"these\", null],\n    [\" \", null],\n    [\"sex\", \"\u6027\"],\n    [\" \", null],\n    [\"robots\", \"\u673a\u5668\u4eba\"],\n    [\" \", null],\n    [\"is\", null],\n    [\" \", null],\n    [\"to\", null],\n    [\" \", null],\n    [\"create\", \"\u521b\u9020\"],\n    [\" \", null],\n    [\"as\", null],\n    [\" \", null],\n    [\"much\", null],\n    [\" \", null],\n    [\"of\", null],\n    [\" \", null],\n    [\"a\", null],\n    [\" \", null],\n    [\"physical\", \"\u7269\u7406\"],\n    [\" \", null],\n    [\"likeness\", \"\u76f8\u4f3c\u6027\"],\n    [\" \", null],\n    [\"to\", null],\n    [\" \", null],\n    [\"actual\", \"\u771f\u5b9e\"],\n    [\" \", null],\n    [\"women\", \"\u5973\u6027\"],\n    [\" \", null],\n    [\"(\", null],\n    [\"al\", \"\u5c3d\u7ba1\"],\n    [\"beit\", null],\n    [\" \", null],\n    [\"with\", null],\n    [\" \", null],\n    [\"porn\", \"\u8272\u60c5\"],\n    [\" \", null],\n    [\"star\", \"\u660e\u661f\"],\n    [\" \", null],\n    [\"proportions\", \"\u6bd4\u4f8b\"],\n    [\")\", null],\n    [\" \", null],\n    [\"as\", null],\n    [\" \", null],\n    [\"techn\", \"\u6280\u672f\"],\n    [\"ologically\", null],\n    [\" \", null],\n    [\"possible\", \"\u53ef\u80fd\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nThe aim behind these sex robots is to create as much of a physical likeness to actual women (albeit with porn star proportions) as technologically possible.\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', ' aim', ' behind', ' these', ' sex', ' robots', ' is', ' to', ' create', ' as', ' much', ' of', ' a', ' physical', ' likeness', ' to', ' actual', ' women', ' (', 'al', 'beit', ' with', ' porn', ' star', ' proportions', ')', ' as', ' techn', 'ologically', ' possible', '.']"}], "[\n  [\"The\", null],\n  [\"aim\", \"\u043c\u0430\u049b\u0441\u0430\u0442\"],\n  [\"behind\", null],\n  [\"these\", null],\n  [\"sex\", \"\u0436\u044b\u043d\u044b\u0441\u0442\u044b\u049b\"],\n  [\"robots\", \"\u0440\u043e\u0431\u043e\u0442\u0442\u0430\u0440\"],\n  [\"is\", null],\n  [\"to\", null],\n  [\"create\", \"\u0436\u0430\u0441\u0430\u0443\"],\n  [\"as\", null],\n  [\"much\", null],\n  [\"of\", null],\n  [\"a\", null],\n  [\"physical\", \"\u0444\u0438\u0437\u0438\u043a\u0430\u043b\u044b\u049b\"],\n  [\"likeness\", \"\u04b1\u049b\u0441\u0430\u0441\u0442\u044b\u049b\"],\n  [\"to\", null],\n  [\"actual\", \"\u043d\u0430\u049b\u0442\u044b\"],\n  [\"women\", \"\u04d9\u0439\u0435\u043b\u0434\u0435\u0440\"],\n  [\"(\", null],\n  [\"al\", null],\n  [\"beit\", null],\n  [\"with\", null],\n  [\"porn\", \"\u043f\u043e\u0440\u043d\u043e\"],\n  [\"star\", \"\u0436\u04b1\u043b\u0434\u044b\u0437\"],\n  [\"proportions\", \"\u043f\u0440\u043e\u043f\u043e\u0440\u0446\u0438\u044f\u043b\u0430\u0440\"],\n  [\")\", null],\n  [\"as\", null],\n  [\"techn\", null],\n  [\"ologically\", null],\n  [\"possible\", \"\u043c\u04af\u043c\u043a\u0456\u043d\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThe aim behind these sex robots is to create as much of a physical likeness to actual women (albeit with porn star proportions) as technologically possible.\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', ' aim', ' behind', ' these', ' sex', ' robots', ' is', ' to', ' create', ' as', ' much', ' of', ' a', ' physical', ' likeness', ' to', ' actual', ' women', ' (', 'al', 'beit', ' with', ' porn', ' star', ' proportions', ')', ' as', ' techn', 'ologically', ' possible', '.']"}], "[\n[\"The\", null],\n[\" aim\", \"\u0446\u0435\u043b\u044c\"],\n[\" behind\", null],\n[\" these\", null],\n[\" sex\", \"\u0441\u0435\u043a\u0441\"],\n[\" robots\", \"\u0440\u043e\u0431\u043e\u0442\u044b\"],\n[\" is\", null],\n[\" to\", null],\n[\" create\", \"\u0441\u043e\u0437\u0434\u0430\u0442\u044c\"],\n[\" as\", null],\n[\" much\", null],\n[\" of\", null],\n[\" a\", null],\n[\" physical\", \"\u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u043e\u0435\"],\n[\" likeness\", \"\u0441\u0445\u043e\u0434\u0441\u0442\u0432\u043e\"],\n[\" to\", null],\n[\" actual\", \"\u0440\u0435\u0430\u043b\u044c\u043d\u044b\u043c\u0438\"],\n[\" women\", \"\u0436\u0435\u043d\u0449\u0438\u043d\u0430\u043c\u0438\"],\n[\" (\", null],\n[\"al\", null],\n[\"beit\", null],\n[\" with\", null],\n[\" porn\", \"\u043f\u043e\u0440\u043d\u043e\u0437\u0432\u0435\u0437\u0434\"],\n[\" star\", null],\n[\" proportions\", \"\u043f\u0440\u043e\u043f\u043e\u0440\u0446\u0438\u044f\u043c\u0438\"],\n[\")\", null],\n[\" as\", null],\n[\" techn\", null],\n[\"ologically\", null],\n[\" possible\", \"\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\n\"At the end of the day, floor space is what you are going to sell - or how your property is valued,\" Erfan says.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\"At', ' the', ' end', ' of', ' the', ' day', ',', ' floor', ' space', ' is', ' what', ' you', ' are', ' going', ' to', ' sell', ' -', ' or', ' how', ' your', ' property', ' is', ' valued', ',\"', ' Erf', 'an', ' says', '.']"}], "[\n['\"At', null],\n[' the', null],\n[' end', '\u7ed3\u675f'],\n[' of', null],\n[' the', null],\n[' day', '\u5929'],\n[',', null],\n[' floor', '\u5730\u677f'],\n[' space', '\u7a7a\u95f4'],\n[' is', null],\n[' what', null],\n[' you', null],\n[' are', null],\n[' going', null],\n[' to', null],\n[' sell', '\u51fa\u552e'],\n[' -', null],\n[' or', null],\n[' how', null],\n[' your', null],\n[' property', '\u8d22\u4ea7'],\n[' is', null],\n[' valued', '\u4f30\u4ef7'],\n[',\"', null],\n[' Erf', '\u57c3\u5c14\u51e1'],\n['an', null],\n[' says', '\u8bf4'],\n['.', null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\n\"At the end of the day, floor space is what you are going to sell - or how your property is valued,\" Erfan says.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\"At', ' the', ' end', ' of', ' the', ' day', ',', ' floor', ' space', ' is', ' what', ' you', ' are', ' going', ' to', ' sell', ' -', ' or', ' how', ' your', ' property', ' is', ' valued', ',\"', ' Erf', 'an', ' says', '.']"}], "[\n  ['\"At', null],\n  [\" the\", null],\n  [\" end\", \"\u0441\u043e\u04a3\u044b\u043d\u0434\u0430\"],\n  [\" of\", null],\n  [\" the\", null],\n  [\" day\", \"\u043a\u04af\u043d\u043d\u0456\u04a3\"],\n  [\",\", null],\n  [\" floor\", \"\u0435\u0434\u0435\u043d\"],\n  [\" space\", \"\u043a\u0435\u04a3\u0456\u0441\u0442\u0456\u0433\u0456\"],\n  [\" is\", null],\n  [\" what\", null],\n  [\" you\", null],\n  [\" are\", null],\n  [\" going\", null],\n  [\" to\", null],\n  [\" sell\", \"\u0441\u0430\u0442\u0430\u0442\u044b\u043d\"],\n  [\" -\", null],\n  [\" or\", null],\n  [\" how\", null],\n  [\" your\", null],\n  [\" property\", \"\u0436\u044b\u043b\u0436\u044b\u043c\u0430\u0439\u0442\u044b\u043d \u043c\u04af\u043b\u0456\u043a\"],\n  [\" is\", null],\n  [\" valued\", \"\u0431\u0430\u0493\u0430\u043b\u0430\u043d\u0430\u0442\u044b\u043d\"],\n  [',\"', null],\n  [\" Erf\", \"\u042d\u0440\u0444\"],\n  [\"an\", null],\n  [\" says\", \"\u0434\u0435\u0439\u0434\u0456\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\n\"At the end of the day, floor space is what you are going to sell - or how your property is valued,\" Erfan says.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\"At', ' the', ' end', ' of', ' the', ' day', ',', ' floor', ' space', ' is', ' what', ' you', ' are', ' going', ' to', ' sell', ' -', ' or', ' how', ' your', ' property', ' is', ' valued', ',\"', ' Erf', 'an', ' says', '.']"}], "[\n  ['\"At', null],\n  [\" the\", null],\n  [\" end\", \"\u043a\u043e\u043d\u0435\u0446\"],\n  [\" of\", null],\n  [\" the\", null],\n  [\" day\", \"\u0434\u043d\u044f\"],\n  [\",\", null],\n  [\" floor\", \"\u043f\u043b\u043e\u0449\u0430\u0434\u044c\"],\n  [\" space\", null],\n  [\" is\", null],\n  [\" what\", null],\n  [\" you\", null],\n  [\" are\", null],\n  [\" going\", null],\n  [\" to\", null],\n  [\" sell\", \"\u043f\u0440\u043e\u0434\u0430\u0432\u0430\u0442\u044c\"],\n  [\" -\", null],\n  [\" or\", null],\n  [\" how\", null],\n  [\" your\", null],\n  [\" property\", \"\u043d\u0435\u0434\u0432\u0438\u0436\u0438\u043c\u043e\u0441\u0442\u044c\"],\n  [\" is\", null],\n  [\" valued\", \"\u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u0442\u0441\u044f\"],\n  [',\"', null],\n  [\" Erf\", \"\u042d\u0440\u0444\"],\n  [\"an\", null],\n  [\" says\", \"\u0433\u043e\u0432\u043e\u0440\u0438\u0442\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nAmong the few to defend Carter was Amanda Knox, who was convicted of murdering her British roommate in Italy in 2007 before she was cleared two years ago.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Among', ' the', ' few', ' to', ' defend', ' Carter', ' was', ' Amanda', ' Knox', ',', ' who', ' was', ' convicted', ' of', ' murdering', ' her', ' British', ' roommate', ' in', ' Italy', ' in', ' ', '2', '0', '0', '7', ' before', ' she', ' was', ' cleared', ' two', ' years', ' ago', '.']"}], "[\n  [\"Among\", \"\u5728...\u4e4b\u4e2d\"],\n  [\" \", null],\n  [\"the\", null],\n  [\" \", null],\n  [\"few\", \"\u5c11\u6570\"],\n  [\" \", null],\n  [\"to\", null],\n  [\" \", null],\n  [\"defend\", \"\u4e3a...\u8fa9\u62a4\"],\n  [\" \", null],\n  [\"Carter\", \"\u5361\u7279\"],\n  [\" \", null],\n  [\"was\", null],\n  [\" \", null],\n  [\"Amanda\", \"\u963f\u66fc\u8fbe\"],\n  [\" \", null],\n  [\"Knox\", \"\u8bfa\u514b\u65af\"],\n  [\",\", null],\n  [\" \", null],\n  [\"who\", null],\n  [\" \", null],\n  [\"was\", null],\n  [\" \", null],\n  [\"convicted\", \"\u88ab\u5b9a\u7f6a\"],\n  [\" \", null],\n  [\"of\", null],\n  [\" \", null],\n  [\"murdering\", \"\u8c0b\u6740\"],\n  [\" \", null],\n  [\"her\", null],\n  [\" \", null],\n  [\"British\", \"\u82f1\u56fd\"],\n  [\" \", null],\n  [\"roommate\", \"\u5ba4\u53cb\"],\n  [\" \", null],\n  [\"in\", null],\n  [\" \", null],\n  [\"Italy\", \"\u610f\u5927\u5229\"],\n  [\" \", null],\n  [\"in\", null],\n  [\" \", null],\n  [\"2\", null],\n  [\"0\", null],\n  [\"0\", null],\n  [\"7\", null],\n  [\" \", null],\n  [\"before\", \"\u4e4b\u524d\"],\n  [\" \", null],\n  [\"she\", null],\n  [\" \", null],\n  [\"was\", null],\n  [\" \", null],\n  [\"cleared\", \"\u88ab\u6e05\u767d\"],\n  [\" \", null],\n  [\"two\", \"\u4e24\"],\n  [\" \", null],\n  [\"years\", \"\u5e74\"],\n  [\" \", null],\n  [\"ago\", \"\u524d\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nAmong the few to defend Carter was Amanda Knox, who was convicted of murdering her British roommate in Italy in 2007 before she was cleared two years ago.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Among', ' the', ' few', ' to', ' defend', ' Carter', ' was', ' Amanda', ' Knox', ',', ' who', ' was', ' convicted', ' of', ' murdering', ' her', ' British', ' roommate', ' in', ' Italy', ' in', ' ', '2', '0', '0', '7', ' before', ' she', ' was', ' cleared', ' two', ' years', ' ago', '.']"}], "[\n[\"Among\", \"\u0410\u0440\u0430\u0441\u044b\u043d\u0434\u0430\"],\n[\" the\", null],\n[\" few\", \"\u0430\u0437\u0434\u0430\u0440\u0434\u044b\u04a3\"],\n[\" to\", null],\n[\" defend\", \"\u049b\u043e\u0440\u0493\u0430\u0443\u0493\u0430\"],\n[\"Carter\", \"\u041a\u0430\u0440\u0442\u0435\u0440\u0434\u0456\"],\n[\" was\", null],\n[\"Amanda\", \"\u0410\u043c\u0430\u043d\u0434\u0430\"],\n[\"Knox\", \"\u041d\u043e\u043a\u0441\"],\n[\",\", null],\n[\" who\", null],\n[\" was\", null],\n[\" convicted\", \"\u0441\u043e\u0442\u0442\u0430\u043b\u0493\u0430\u043d\"],\n[\" of\", null],\n[\" murdering\", \"\u04e9\u043b\u0442\u0456\u0440\u0433\u0435\u043d\u0456\"],\n[\" her\", null],\n[\"British\", \"\u0431\u0440\u0438\u0442\u0430\u043d\u0434\u044b\u049b\"],\n[\" roommate\", \"\u0431\u04e9\u043b\u043c\u0435\u043b\u0435\u0441\u0456\u043d\"],\n[\" in\", null],\n[\"Italy\", \"\u0418\u0442\u0430\u043b\u0438\u044f\u0434\u0430\"],\n[\" in\", null],\n[\" \", null],\n[\"2\", null],\n[\"0\", null],\n[\"0\", null],\n[\"7\", null],\n[\" before\", \"\u0431\u04b1\u0440\u044b\u043d\"],\n[\" she\", null],\n[\" was\", null],\n[\" cleared\", \"\u0430\u049b\u0442\u0430\u043b\u0493\u0430\u043d\"],\n[\" two\", \"\u0435\u043a\u0456\"],\n[\" years\", \"\u0436\u044b\u043b\"],\n[\" ago\", \"\u0431\u04b1\u0440\u044b\u043d\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nAmong the few to defend Carter was Amanda Knox, who was convicted of murdering her British roommate in Italy in 2007 before she was cleared two years ago.\n\n\n---=== TOKENIZED TEXT ===---\n\n['Among', ' the', ' few', ' to', ' defend', ' Carter', ' was', ' Amanda', ' Knox', ',', ' who', ' was', ' convicted', ' of', ' murdering', ' her', ' British', ' roommate', ' in', ' Italy', ' in', ' ', '2', '0', '0', '7', ' before', ' she', ' was', ' cleared', ' two', ' years', ' ago', '.']"}], "[\n[\"Among\", \"\u0421\u0440\u0435\u0434\u0438\"],\n[\" the\", null],\n[\" few\", \"\u043d\u0435\u043c\u043d\u043e\u0433\u0438\u0445\"],\n[\" to\", null],\n[\" defend\", \"\u0437\u0430\u0449\u0438\u0442\u0438\u0442\u044c\"],\n[\" Carter\", \"\u041a\u0430\u0440\u0442\u0435\u0440\"],\n[\" was\", null],\n[\" Amanda\", \"\u0410\u043c\u0430\u043d\u0434\u0430\"],\n[\" Knox\", \"\u041d\u043e\u043a\u0441\"],\n[\",\", null],\n[\" who\", \"\u043a\u043e\u0442\u043e\u0440\u0430\u044f\"],\n[\" was\", null],\n[\" convicted\", \"\u0431\u044b\u043b\u0430 \u043e\u0441\u0443\u0436\u0434\u0435\u043d\u0430\"],\n[\" of\", null],\n[\" murdering\", \"\u0443\u0431\u0438\u0439\u0441\u0442\u0432\u043e\"],\n[\" her\", null],\n[\" British\", \"\u0431\u0440\u0438\u0442\u0430\u043d\u0441\u043a\u0443\u044e\"],\n[\" roommate\", \"\u0441\u043e\u0441\u0435\u0434\u043a\u0443\"],\n[\" in\", null],\n[\" Italy\", \"\u0418\u0442\u0430\u043b\u0438\u0438\"],\n[\" in\", null],\n[\" \", null],\n[\"2\", null],\n[\"0\", null],\n[\"0\", null],\n[\"7\", null],\n[\" before\", \"\u043f\u0440\u0435\u0436\u0434\u0435 \u0447\u0435\u043c\"],\n[\" she\", \"\u043e\u043d\u0430\"],\n[\" was\", null],\n[\" cleared\", \"\u0431\u044b\u043b\u0430 \u043e\u043f\u0440\u0430\u0432\u0434\u0430\u043d\u0430\"],\n[\" two\", \"\u0434\u0432\u0430\"],\n[\" years\", \"\u0433\u043e\u0434\u0430\"],\n[\" ago\", \"\u043d\u0430\u0437\u0430\u0434\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nThe 19th National Congress of the Communist Party of China first proposed the construction of a modern economic system and demanded that\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', ' ', '1', '9', 'th', ' National', ' Congress', ' of', ' the', ' Communist', ' Party', ' of', ' China', ' first', ' proposed', ' the', ' construction', ' of', ' a', ' modern', ' economic', ' system', ' and', ' demanded', ' that']"}], "[\n[\"The\", null],\n[\" \", null],\n[\"1\", null],\n[\"9\", null],\n[\"th\", \"\u7b2c19\u6b21\"],\n[\" National\", \"\u5168\u56fd\"],\n[\" Congress\", \"\u4ee3\u8868\u5927\u4f1a\"],\n[\" of\", null],\n[\" the\", null],\n[\" Communist\", \"\u5171\u4ea7\u515a\"],\n[\" Party\", null],\n[\" of\", null],\n[\" China\", \"\u4e2d\u56fd\"],\n[\" first\", \"\u9996\u6b21\"],\n[\" proposed\", \"\u63d0\u51fa\"],\n[\" the\", null],\n[\" construction\", \"\u5efa\u8bbe\"],\n[\" of\", null],\n[\" a\", null],\n[\" modern\", \"\u73b0\u4ee3\"],\n[\" economic\", \"\u7ecf\u6d4e\"],\n[\" system\", \"\u4f53\u7cfb\"],\n[\" and\", null],\n[\" demanded\", \"\u8981\u6c42\"],\n[\" that\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nThe 19th National Congress of the Communist Party of China first proposed the construction of a modern economic system and demanded that\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', ' ', '1', '9', 'th', ' National', ' Congress', ' of', ' the', ' Communist', ' Party', ' of', ' China', ' first', ' proposed', ' the', ' construction', ' of', ' a', ' modern', ' economic', ' system', ' and', ' demanded', ' that']"}], "[\n  [\"The\", null],\n  [\" \", null],\n  [\"1\", null],\n  [\"9\", null],\n  [\"th\", null],\n  [\" National\", \"\u04b0\u043b\u0442\u0442\u044b\u049b\"],\n  [\" Congress\", \"\u041a\u043e\u043d\u0433\u0440\u0435\u0441\u0441\"],\n  [\" of\", null],\n  [\" the\", null],\n  [\" Communist\", \"\u041a\u043e\u043c\u043c\u0443\u043d\u0438\u0441\u0442\u0456\u043a\"],\n  [\" Party\", \"\u041f\u0430\u0440\u0442\u0438\u044f\"],\n  [\" of\", null],\n  [\" China\", \"\u049a\u044b\u0442\u0430\u0439\"],\n  [\" first\", \"\u0430\u043b\u0493\u0430\u0448\"],\n  [\" proposed\", \"\u04b1\u0441\u044b\u043d\u0434\u044b\"],\n  [\" the\", null],\n  [\" construction\", \"\u049b\u04b1\u0440\u044b\u043b\u044b\u0441\"],\n  [\" of\", null],\n  [\" a\", null],\n  [\" modern\", \"\u0437\u0430\u043c\u0430\u043d\u0430\u0443\u0438\"],\n  [\" economic\", \"\u044d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0430\u043b\u044b\u049b\"],\n  [\" system\", \"\u0436\u04af\u0439\u0435\"],\n  [\" and\", null],\n  [\" demanded\", \"\u0442\u0430\u043b\u0430\u043f \u0435\u0442\u0442\u0456\"],\n  [\" that\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nThe 19th National Congress of the Communist Party of China first proposed the construction of a modern economic system and demanded that\n\n\n---=== TOKENIZED TEXT ===---\n\n['The', ' ', '1', '9', 'th', ' National', ' Congress', ' of', ' the', ' Communist', ' Party', ' of', ' China', ' first', ' proposed', ' the', ' construction', ' of', ' a', ' modern', ' economic', ' system', ' and', ' demanded', ' that']"}], "[\n    [\"The\", null],\n    [\" \", null],\n    [\"1\", null],\n    [\"9\", null],\n    [\"th\", null],\n    [\" National\", \"\u041d\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439\"],\n    [\" Congress\", \"\u0421\u044a\u0435\u0437\u0434\"],\n    [\" of\", null],\n    [\" the\", null],\n    [\" Communist\", \"\u041a\u043e\u043c\u043c\u0443\u043d\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439\"],\n    [\" Party\", \"\u041f\u0430\u0440\u0442\u0438\u0438\"],\n    [\" of\", null],\n    [\" China\", \"\u041a\u0438\u0442\u0430\u044f\"],\n    [\" first\", \"\u0432\u043f\u0435\u0440\u0432\u044b\u0435\"],\n    [\" proposed\", \"\u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0438\u043b\"],\n    [\" the\", null],\n    [\" construction\", \"\u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435\"],\n    [\" of\", null],\n    [\" a\", null],\n    [\" modern\", \"\u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439\"],\n    [\" economic\", \"\u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0439\"],\n    [\" system\", \"\u0441\u0438\u0441\u0442\u0435\u043c\u044b\"],\n    [\" and\", null],\n    [\" demanded\", \"\u043f\u043e\u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043b\"],\n    [\" that\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\n\"I can get closer to that holistic, 360-degree view into the employment experience,\" said Vitrano, who oversees Ruby Tuesday's risk management group.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\"I', ' can', ' get', ' closer', ' to', ' that', ' holistic', ',', ' ', '3', '6', '0', '-degree', ' view', ' into', ' the', ' employment', ' experience', ',\"', ' said', ' V', 'itr', 'ano', ',', ' who', ' oversees', ' Ruby', ' Tuesday', \"'s\", ' risk', ' management', ' group', '.']"}], "[\n    ['\"I', null],\n    [\" can\", null],\n    [\" get\", \"\u83b7\u5f97\"],\n    [\" closer\", \"\u66f4\u63a5\u8fd1\"],\n    [\" to\", null],\n    [\" that\", null],\n    [\" holistic\", \"\u5168\u9762\"],\n    [\",\", null],\n    [\" \", null],\n    [\"3\", null],\n    [\"6\", null],\n    [\"0\", null],\n    [\"-degree\", \"\u5ea6\"],\n    [\" view\", \"\u89c6\u89d2\"],\n    [\" into\", null],\n    [\" the\", null],\n    [\" employment\", \"\u5c31\u4e1a\"],\n    [\" experience\", \"\u4f53\u9a8c\"],\n    [',\"', null],\n    [\" said\", \"\u8bf4\"],\n    [\" V\", \"\u7ef4\"],\n    [\"itr\", null],\n    [\"ano\", null],\n    [\",\", null],\n    [\" who\", null],\n    [\" oversees\", \"\u76d1\u7ba1\"],\n    [\" Ruby\", \"\u7ea2\u5b9d\u77f3\"],\n    [\" Tuesday\", \"\u661f\u671f\u4e8c\"],\n    [\"'s\", null],\n    [\" risk\", \"\u98ce\u9669\"],\n    [\" management\", \"\u7ba1\u7406\"],\n    [\" group\", \"\u56e2\u961f\"],\n    [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\n\"I can get closer to that holistic, 360-degree view into the employment experience,\" said Vitrano, who oversees Ruby Tuesday's risk management group.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\"I', ' can', ' get', ' closer', ' to', ' that', ' holistic', ',', ' ', '3', '6', '0', '-degree', ' view', ' into', ' the', ' employment', ' experience', ',\"', ' said', ' V', 'itr', 'ano', ',', ' who', ' oversees', ' Ruby', ' Tuesday', \"'s\", ' risk', ' management', ' group', '.']"}], "[\n    ['\"I', null],\n    [' can', null],\n    [' get', \"\u0430\u043b\u0443\"],\n    [' closer', \"\u0436\u0430\u049b\u044b\u043d\u044b\u0440\u0430\u049b\"],\n    [' to', null],\n    [' that', null],\n    [' holistic', \"\u0442\u04b1\u0442\u0430\u0441\"],\n    [',', null],\n    [' ', null],\n    ['3', null],\n    ['6', null],\n    ['0', null],\n    ['-degree', null],\n    [' view', \"\u043a\u04e9\u0437\u049b\u0430\u0440\u0430\u0441\"],\n    [' into', null],\n    [' the', null],\n    [' employment', \"\u0436\u04b1\u043c\u044b\u0441\u043f\u0435\u043d \u049b\u0430\u043c\u0442\u0443\"],\n    [' experience', \"\u0442\u04d9\u0436\u0456\u0440\u0438\u0431\u0435\"],\n    [',\"', null],\n    [' said', \"\u0434\u0435\u0434\u0456\"],\n    [' V', \"\u0412\u0438\u0442\u0440\u0430\u043d\u043e\"],\n    ['itr', null],\n    ['ano', null],\n    [',', null],\n    [' who', null],\n    [' oversees', \"\u0431\u0430\u0441\u049b\u0430\u0440\u0430\u0434\u044b\"],\n    [' Ruby', \"\u0420\u0443\u0431\u0438\"],\n    [' Tuesday', \"\u0422\u044c\u044e\u0437\u0434\u0435\u0439\"],\n    [\"'s\", null],\n    [' risk', \"\u0442\u04d9\u0443\u0435\u043a\u0435\u043b\"],\n    [' management', \"\u0431\u0430\u0441\u049b\u0430\u0440\u0443\"],\n    [' group', \"\u0442\u043e\u0431\u044b\"],\n    ['.', null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\n\"I can get closer to that holistic, 360-degree view into the employment experience,\" said Vitrano, who oversees Ruby Tuesday's risk management group.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\"I', ' can', ' get', ' closer', ' to', ' that', ' holistic', ',', ' ', '3', '6', '0', '-degree', ' view', ' into', ' the', ' employment', ' experience', ',\"', ' said', ' V', 'itr', 'ano', ',', ' who', ' oversees', ' Ruby', ' Tuesday', \"'s\", ' risk', ' management', ' group', '.']"}], "[\n  [\"\\\"I\", \"\u00ab\u042f\"],\n  [\" can\", null],\n  [\" get\", null],\n  [\" closer\", \"\u0431\u043b\u0438\u0436\u0435\"],\n  [\" to\", null],\n  [\" that\", null],\n  [\" holistic\", \"\u0446\u0435\u043b\u043e\u0441\u0442\u043d\u044b\u0439\"],\n  [\",\", null],\n  [\" \", null],\n  [\"3\", null],\n  [\"6\", null],\n  [\"0\", null],\n  [\"-degree\", \"\u0433\u0440\u0430\u0434\u0443\u0441\u043d\u044b\u0439\"],\n  [\" view\", \"\u0432\u0437\u0433\u043b\u044f\u0434\"],\n  [\" into\", null],\n  [\" the\", null],\n  [\" employment\", \"\u0442\u0440\u0443\u0434\u043e\u0432\u043e\u0439\"],\n  [\" experience\", \"\u043e\u043f\u044b\u0442\"],\n  [\",\\\"\", null],\n  [\" said\", \"\u0441\u043a\u0430\u0437\u0430\u043b\"],\n  [\" V\", \"\u0412\u0438\u0442\u0440\u0430\u043d\u043e\"],\n  [\"itr\", null],\n  [\"ano\", null],\n  [\",\", null],\n  [\" who\", null],\n  [\" oversees\", \"\u043a\u0443\u0440\u0438\u0440\u0443\u0435\u0442\"],\n  [\" Ruby\", \"Ruby\"],\n  [\" Tuesday\", \"Tuesday\"],\n  [\"'s\", null],\n  [\" risk\", \"\u0440\u0438\u0441\u043a\"],\n  [\" management\", \"\u043c\u0435\u043d\u0435\u0434\u0436\u043c\u0435\u043d\u0442\"],\n  [\" group\", \"\u0433\u0440\u0443\u043f\u043f\u0443\"],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Chinese.\n\n---=== TEXT ===---\n\nIt is understood that Chongqing currently owns 18 shale gas exploration rights established by Ministry of Land and Resources as the first domestic shale gas mining rights - the Sinopec Fuling shale gas field.\n\n\n---=== TOKENIZED TEXT ===---\n\n['It', ' is', ' understood', ' that', ' Ch', 'ong', 'qing', ' currently', ' owns', ' ', '1', '8', ' shale', ' gas', ' exploration', ' rights', ' established', ' by', ' Ministry', ' of', ' Land', ' and', ' Resources', ' as', ' the', ' first', ' domestic', ' shale', ' gas', ' mining', ' rights', ' -', ' the', ' S', 'ino', 'pec', ' F', 'uling', ' shale', ' gas', ' field', '.']"}], "[\n[\"It\", null],\n[\" is\", null],\n[\" understood\", \"\u636e\u4e86\u89e3\"],\n[\" that\", null],\n[\" Ch\", \"\u91cd\u5e86\"],\n[\"ong\", null],\n[\"qing\", null],\n[\" currently\", \"\u76ee\u524d\"],\n[\" owns\", \"\u62e5\u6709\"],\n[\" \", null],\n[\"1\", \"18\"],\n[\"8\", null],\n[\" shale\", \"\u9875\u5ca9\"],\n[\" gas\", \"\u6c14\"],\n[\" exploration\", \"\u52d8\u63a2\"],\n[\" rights\", \"\u6743\"],\n[\" established\", \"\u8bbe\u7acb\"],\n[\" by\", null],\n[\" Ministry\", \"\u56fd\u571f\u8d44\u6e90\u90e8\"],\n[\" of\", null],\n[\" Land\", null],\n[\" and\", null],\n[\" Resources\", null],\n[\" as\", null],\n[\" the\", null],\n[\" first\", \"\u7b2c\u4e00\u4e2a\"],\n[\" domestic\", \"\u56fd\u5185\"],\n[\" shale\", \"\u9875\u5ca9\"],\n[\" gas\", \"\u6c14\"],\n[\" mining\", \"\u5f00\u91c7\"],\n[\" rights\", \"\u6743\"],\n[\" -\", null],\n[\" the\", null],\n[\" S\", \"\u4e2d\u77f3\u5316\"],\n[\"ino\", null],\n[\"pec\", null],\n[\" F\", \"\u6daa\u9675\"],\n[\"uling\", null],\n[\" shale\", \"\u9875\u5ca9\"],\n[\" gas\", \"\u6c14\"],\n[\" field\", \"\u7530\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Kazakh.\n\n---=== TEXT ===---\n\nIt is understood that Chongqing currently owns 18 shale gas exploration rights established by Ministry of Land and Resources as the first domestic shale gas mining rights - the Sinopec Fuling shale gas field.\n\n\n---=== TOKENIZED TEXT ===---\n\n['It', ' is', ' understood', ' that', ' Ch', 'ong', 'qing', ' currently', ' owns', ' ', '1', '8', ' shale', ' gas', ' exploration', ' rights', ' established', ' by', ' Ministry', ' of', ' Land', ' and', ' Resources', ' as', ' the', ' first', ' domestic', ' shale', ' gas', ' mining', ' rights', ' -', ' the', ' S', 'ino', 'pec', ' F', 'uling', ' shale', ' gas', ' field', '.']"}], "[\n[\"It\", null],\n[\" is\", null],\n[\" understood\", \"\u0442\u04af\u0441\u0456\u043d\u0456\u043a\u0442\u0456\"],\n[\" that\", null],\n[\" Ch\", \"\u0427\u0443\u043d\u0446\u0438\u043d\"],\n[\"ong\", null],\n[\"qing\", null],\n[\" currently\", \"\u049b\u0430\u0437\u0456\u0440\u0433\u0456 \u0443\u0430\u049b\u044b\u0442\u0442\u0430\"],\n[\" owns\", \"\u0438\u0435\u043b\u0435\u043d\u0435\u0434\u0456\"],\n[\" \", null],\n[\"1\", null],\n[\"8\", null],\n[\" shale\", \"\u0442\u0430\u049b\u0442\u0430\u0442\u0430\u0441\"],\n[\" gas\", \"\u0433\u0430\u0437\"],\n[\" exploration\", \"\u0431\u0430\u0440\u043b\u0430\u0443\"],\n[\" rights\", \"\u049b\u04b1\u049b\u044b\u049b\u0442\u0430\u0440\u044b\"],\n[\" established\", \"\u0431\u0435\u043b\u0433\u0456\u043b\u0435\u043d\u0433\u0435\u043d\"],\n[\" by\", null],\n[\" Ministry\", \"\u041c\u0438\u043d\u0438\u0441\u0442\u0440\u043b\u0456\u0433\u0456\"],\n[\" of\", null],\n[\" Land\", \"\u0416\u0435\u0440\"],\n[\" and\", null],\n[\" Resources\", \"\u0420\u0435\u0441\u0443\u0440\u0441\u0442\u0430\u0440\"],\n[\" as\", null],\n[\" the\", null],\n[\" first\", \"\u0431\u0456\u0440\u0456\u043d\u0448\u0456\"],\n[\" domestic\", \"\u043e\u0442\u0430\u043d\u0434\u044b\u049b\"],\n[\" shale\", \"\u0442\u0430\u049b\u0442\u0430\u0442\u0430\u0441\"],\n[\" gas\", \"\u0433\u0430\u0437\"],\n[\" mining\", \"\u04e9\u043d\u0434\u0456\u0440\u0443\"],\n[\" rights\", \"\u049b\u04b1\u049b\u044b\u049b\u0442\u0430\u0440\u044b\"],\n[\" -\", null],\n[\" the\", null],\n[\" S\", \"\u0421\u0438\u043d\u043e\u043f\u0435\u043a\"],\n[\"ino\", null],\n[\"pec\", null],\n[\" F\", \"\u0424\u0443\u043b\u0438\u043d\"],\n[\"uling\", null],\n[\" shale\", \"\u0442\u0430\u049b\u0442\u0430\u0442\u0430\u0441\"],\n[\" gas\", \"\u0433\u0430\u0437\"],\n[\" field\", \"\u043a\u0435\u043d \u043e\u0440\u043d\u044b\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from English to Russian.\n\n---=== TEXT ===---\n\nIt is understood that Chongqing currently owns 18 shale gas exploration rights established by Ministry of Land and Resources as the first domestic shale gas mining rights - the Sinopec Fuling shale gas field.\n\n\n---=== TOKENIZED TEXT ===---\n\n['It', ' is', ' understood', ' that', ' Ch', 'ong', 'qing', ' currently', ' owns', ' ', '1', '8', ' shale', ' gas', ' exploration', ' rights', ' established', ' by', ' Ministry', ' of', ' Land', ' and', ' Resources', ' as', ' the', ' first', ' domestic', ' shale', ' gas', ' mining', ' rights', ' -', ' the', ' S', 'ino', 'pec', ' F', 'uling', ' shale', ' gas', ' field', '.']"}], "[\n[\"It\", \"\u042d\u0442\u043e\"],\n[\" is\", null],\n[\" understood\", \"\u043f\u043e\u043d\u044f\u0442\u043d\u043e\"],\n[\" that\", null],\n[\" Ch\", \"\u0427\u0443\u043d\u0446\u0438\u043d\"],\n[\"ong\", null],\n[\"qing\", null],\n[\" currently\", \"\u0432 \u043d\u0430\u0441\u0442\u043e\u044f\u0449\u0435\u0435 \u0432\u0440\u0435\u043c\u044f\"],\n[\" owns\", \"\u0432\u043b\u0430\u0434\u0435\u0435\u0442\"],\n[\" \", null],\n[\"1\", null],\n[\"8\", null],\n[\" shale\", \"\u0441\u043b\u0430\u043d\u0446\u0435\u0432\u043e\u0433\u043e\"],\n[\" gas\", \"\u0433\u0430\u0437\u0430\"],\n[\" exploration\", \"\u0440\u0430\u0437\u0432\u0435\u0434\u043e\u0447\u043d\u044b\u0445\"],\n[\" rights\", \"\u043f\u0440\u0430\u0432\"],\n[\" established\", \"\u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d\u043d\u044b\u0445\"],\n[\" by\", null],\n[\" Ministry\", \"\u041c\u0438\u043d\u0438\u0441\u0442\u0435\u0440\u0441\u0442\u0432\u043e\u043c\"],\n[\" of\", null],\n[\" Land\", \"\u0437\u0435\u043c\u0435\u043b\u044c\u043d\u044b\u0445\"],\n[\" and\", null],\n[\" Resources\", \"\u0440\u0435\u0441\u0443\u0440\u0441\u043e\u0432\"],\n[\" as\", null],\n[\" the\", null],\n[\" first\", \"\u043f\u0435\u0440\u0432\u043e\u0435\"],\n[\" domestic\", \"\u043e\u0442\u0435\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0435\"],\n[\" shale\", \"\u0441\u043b\u0430\u043d\u0446\u0435\u0432\u043e\u0433\u043e\"],\n[\" gas\", \"\u0433\u0430\u0437\u0430\"],\n[\" mining\", \"\u0434\u043e\u0431\u044b\u0447\u043d\u043e\u0435\"],\n[\" rights\", \"\u043f\u0440\u0430\u0432\u043e\"],\n[\" -\", null],\n[\" the\", null],\n[\" S\", \"\u0421\u0438\u043d\u043e\u043f\u0435\u043a\"],\n[\"ino\", null],\n[\"pec\", null],\n[\" F\", \"\u0424\u0443\u043b\u0438\u043d\"],\n[\"uling\", null],\n[\" shale\", \"\u0441\u043b\u0430\u043d\u0446\u0435\u0432\u043e\u0433\u043e\"],\n[\" gas\", \"\u0433\u0430\u0437\u0430\"],\n[\" field\", \"\u043c\u0435\u0441\u0442\u043e\u0440\u043e\u0436\u0434\u0435\u043d\u0438\u0435\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u6d3b\u52a8\u524d\u4e00\u5929\uff0cHMX-1 \u6307\u6325\u5b98\u5728\u5411\u4e0a\u7ea7\u6d77\u519b\u9646\u6218\u961f\u98de\u884c\u526f\u53f8\u4ee4\u53d1\u9001\u7684\u7535\u5b50\u90ae\u4ef6\u4e2d\u8868\u793a\u201c\u6211\u53ea\u60f3\u786e\u8ba4\u60a8\u77e5\u6653\uff0c\u56e0\u4e3a\u8fd9\u6b64\u6b21\u6d3b\u52a8\u53d7\u5230\u9ad8\u5ea6\u91cd\u89c6\u3002\u201d\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u6d3b\u52a8', '\u524d\u4e00\u5929', '\uff0c', 'H', 'MX', '-', '1', '', '', '', '\u6325', '\u5b98', '\u5728', '\u5411\u4e0a', '\u7ea7', '\u6d77\u519b', '\u9646', '\u6218\u961f', '\u98de\u884c', '\u526f', '\u53f8\u4ee4', '\u53d1\u9001', '\u7684', '\u7535\u5b50\u90ae\u4ef6', '\u4e2d', '\u8868\u793a', '\u201c', '\u6211', '\u53ea\u60f3', '\u786e\u8ba4', '\u60a8', '\u77e5\u6653', '\uff0c', '\u56e0\u4e3a', '\u8fd9', '\u6b64\u6b21\u6d3b\u52a8', '\u53d7\u5230', '\u9ad8\u5ea6\u91cd\u89c6', '\u3002\u201d']"}], "[\n  [\"\u6d3b\u52a8\", \"event\"],\n  [\"\u524d\u4e00\u5929\", \"the day before\"],\n  [\"\uff0c\", null],\n  [\"H\", \"H\"],\n  [\"MX\", \"MX\"],\n  [\"-\", null],\n  [\"1\", \"1\"],\n  [\"\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\u6325\", null],\n  [\"\u5b98\", null],\n  [\"\u5728\", null],\n  [\"\u5411\u4e0a\", \"superior\"],\n  [\"\u7ea7\", null],\n  [\"\u6d77\u519b\", \"Navy\"],\n  [\"\u9646\", null],\n  [\"\u6218\u961f\", \"Marine Corps\"],\n  [\"\u98de\u884c\", \"aviation\"],\n  [\"\u526f\", \"deputy\"],\n  [\"\u53f8\u4ee4\", \"commander\"],\n  [\"\u53d1\u9001\", \"sent\"],\n  [\"\u7684\", null],\n  [\"\u7535\u5b50\u90ae\u4ef6\", \"email\"],\n  [\"\u4e2d\", null],\n  [\"\u8868\u793a\", \"stated\"],\n  [\"\"\", null],\n  [\"\u6211\", \"I\"],\n  [\"\u53ea\u60f3\", \"just want to\"],\n  [\"\u786e\u8ba4\", \"confirm\"],\n  [\"\u60a8\", \"you\"],\n  [\"\u77e5\u6653\", \"are aware\"],\n  [\"\uff0c\", null],\n  [\"\u56e0\u4e3a\", \"because\"],\n  [\"\u8fd9\", \"this\"],\n  [\"\u6b64\u6b21\u6d3b\u52a8\", \"event\"],\n  [\"\u53d7\u5230\", \"receives\"],\n  [\"\u9ad8\u5ea6\u91cd\u89c6\", \"high attention\"],\n  [\"\u3002\"\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u6d3b\u52a8\u524d\u4e00\u5929\uff0cHMX-1 \u6307\u6325\u5b98\u5728\u5411\u4e0a\u7ea7\u6d77\u519b\u9646\u6218\u961f\u98de\u884c\u526f\u53f8\u4ee4\u53d1\u9001\u7684\u7535\u5b50\u90ae\u4ef6\u4e2d\u8868\u793a\u201c\u6211\u53ea\u60f3\u786e\u8ba4\u60a8\u77e5\u6653\uff0c\u56e0\u4e3a\u8fd9\u6b64\u6b21\u6d3b\u52a8\u53d7\u5230\u9ad8\u5ea6\u91cd\u89c6\u3002\u201d\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u6d3b\u52a8', '\u524d\u4e00\u5929', '\uff0c', 'H', 'MX', '-', '1', '', '', '', '\u6325', '\u5b98', '\u5728', '\u5411\u4e0a', '\u7ea7', '\u6d77\u519b', '\u9646', '\u6218\u961f', '\u98de\u884c', '\u526f', '\u53f8\u4ee4', '\u53d1\u9001', '\u7684', '\u7535\u5b50\u90ae\u4ef6', '\u4e2d', '\u8868\u793a', '\u201c', '\u6211', '\u53ea\u60f3', '\u786e\u8ba4', '\u60a8', '\u77e5\u6653', '\uff0c', '\u56e0\u4e3a', '\u8fd9', '\u6b64\u6b21\u6d3b\u52a8', '\u53d7\u5230', '\u9ad8\u5ea6\u91cd\u89c6', '\u3002\u201d']"}], "[\n[\"\u6d3b\u52a8\", \"\u0456\u0441-\u0448\u0430\u0440\u0430\"],\n[\"\u524d\u4e00\u5929\", \"\u0430\u043b\u0434\u044b\u04a3\u0493\u044b \u043a\u04af\u043d\u0456\"],\n[\"\uff0c\", null],\n[\"H\", null],\n[\"MX\", null],\n[\"-\", null],\n[\"1\", null],\n[\"\", null],\n[\"\", null],\n[\"\", null],\n[\"\u6325\", null],\n[\"\u5b98\", null],\n[\"\u5728\", null],\n[\"\u5411\u4e0a\", \"\u0436\u043e\u0493\u0430\u0440\u044b\"],\n[\"\u7ea7\", null],\n[\"\u6d77\u519b\", \"\u04d9\u0441\u043a\u0435\u0440\u0438-\u0442\u0435\u04a3\u0456\u0437 \u043a\u04af\u0448\u0442\u0435\u0440\u0456\"],\n[\"\u9646\", null],\n[\"\u6218\u961f\", \"\u04d9\u0441\u043a\u0435\u0440\u0438 \u0431\u04e9\u043b\u0456\u043c\"],\n[\"\u98de\u884c\", \"\u04b1\u0448\u0443\"],\n[\"\u526f\", \"\u043e\u0440\u044b\u043d\u0431\u0430\u0441\u0430\u0440\"],\n[\"\u53f8\u4ee4\", \"\u049b\u043e\u043b\u0431\u0430\u0441\u0448\u044b\"],\n[\"\u53d1\u9001\", \"\u0436\u0456\u0431\u0435\u0440\u0443\"],\n[\"\u7684\", null],\n[\"\u7535\u5b50\u90ae\u4ef6\", \"\u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u0434\u044b\u049b \u043f\u043e\u0448\u0442\u0430\"],\n[\"\u4e2d\", null],\n[\"\u8868\u793a\", \"\u0431\u0456\u043b\u0434\u0456\u0440\u0443\"],\n[\"\"\", null],\n[\"\u6211\", \"\u043c\u0435\u043d\"],\n[\"\u53ea\u60f3\", \"\u0442\u0435\u043a \u049b\u0430\u043b\u0430\u0439\u043c\u044b\u043d\"],\n[\"\u786e\u8ba4\", \"\u0440\u0430\u0441\u0442\u0430\u0443\"],\n[\"\u60a8\", \"\u0441\u0456\u0437\"],\n[\"\u77e5\u6653\", \"\u0431\u0456\u043b\u0443\"],\n[\"\uff0c\", null],\n[\"\u56e0\u4e3a\", \"\u0441\u0435\u0431\u0435\u0431\u0456\"],\n[\"\u8fd9\", \"\u0431\u04b1\u043b\"],\n[\"\u6b64\u6b21\u6d3b\u52a8\", \"\u0431\u04b1\u043b \u0456\u0441-\u0448\u0430\u0440\u0430\"],\n[\"\u53d7\u5230\", \"\u049b\u0430\u0431\u044b\u043b\u0434\u0430\u0443\"],\n[\"\u9ad8\u5ea6\u91cd\u89c6\", \"\u0436\u043e\u0493\u0430\u0440\u044b \u0431\u0430\u0493\u0430\u043b\u0430\u043d\u0443\"],\n[\"\u3002\"\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u6d3b\u52a8\u524d\u4e00\u5929\uff0cHMX-1 \u6307\u6325\u5b98\u5728\u5411\u4e0a\u7ea7\u6d77\u519b\u9646\u6218\u961f\u98de\u884c\u526f\u53f8\u4ee4\u53d1\u9001\u7684\u7535\u5b50\u90ae\u4ef6\u4e2d\u8868\u793a\u201c\u6211\u53ea\u60f3\u786e\u8ba4\u60a8\u77e5\u6653\uff0c\u56e0\u4e3a\u8fd9\u6b64\u6b21\u6d3b\u52a8\u53d7\u5230\u9ad8\u5ea6\u91cd\u89c6\u3002\u201d\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u6d3b\u52a8', '\u524d\u4e00\u5929', '\uff0c', 'H', 'MX', '-', '1', '', '', '', '\u6325', '\u5b98', '\u5728', '\u5411\u4e0a', '\u7ea7', '\u6d77\u519b', '\u9646', '\u6218\u961f', '\u98de\u884c', '\u526f', '\u53f8\u4ee4', '\u53d1\u9001', '\u7684', '\u7535\u5b50\u90ae\u4ef6', '\u4e2d', '\u8868\u793a', '\u201c', '\u6211', '\u53ea\u60f3', '\u786e\u8ba4', '\u60a8', '\u77e5\u6653', '\uff0c', '\u56e0\u4e3a', '\u8fd9', '\u6b64\u6b21\u6d3b\u52a8', '\u53d7\u5230', '\u9ad8\u5ea6\u91cd\u89c6', '\u3002\u201d']"}], "[\n[\"\u6d3b\u52a8\", \"\u043c\u0435\u0440\u043e\u043f\u0440\u0438\u044f\u0442\u0438\u0435\"],\n[\"\u524d\u4e00\u5929\", \"\u043d\u0430\u043a\u0430\u043d\u0443\u043d\u0435\"],\n[\"\uff0c\", null],\n[\"H\", null],\n[\"MX\", null],\n[\"-\", null],\n[\"1\", null],\n[\"\", null],\n[\"\", null],\n[\"\", null],\n[\"\u6325\", null],\n[\"\u5b98\", null],\n[\"\u5728\", null],\n[\"\u5411\u4e0a\", null],\n[\"\u7ea7\", null],\n[\"\u6d77\u519b\", \"\u0432\u043e\u0435\u043d\u043d\u043e-\u043c\u043e\u0440\u0441\u043a\u043e\u0439\"],\n[\"\u9646\", null],\n[\"\u6218\u961f\", \"\u043a\u043e\u0440\u043f\u0443\u0441\"],\n[\"\u98de\u884c\", \"\u0430\u0432\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439\"],\n[\"\u526f\", \"\u0437\u0430\u043c\u0435\u0441\u0442\u0438\u0442\u0435\u043b\u044c\"],\n[\"\u53f8\u4ee4\", \"\u043a\u043e\u043c\u0430\u043d\u0434\u0443\u044e\u0449\u0438\u0439\"],\n[\"\u53d1\u9001\", \"\u043e\u0442\u043f\u0440\u0430\u0432\u0438\u0442\u044c\"],\n[\"\u7684\", null],\n[\"\u7535\u5b50\u90ae\u4ef6\", \"\u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u043d\u043e\u0435 \u043f\u0438\u0441\u044c\u043c\u043e\"],\n[\"\u4e2d\", null],\n[\"\u8868\u793a\", \"\u0437\u0430\u044f\u0432\u0438\u0442\u044c\"],\n[\"\"\", null],\n[\"\u6211\", \"\u044f\"],\n[\"\u53ea\u60f3\", \"\u043f\u0440\u043e\u0441\u0442\u043e \u0445\u043e\u0447\u0443\"],\n[\"\u786e\u8ba4\", \"\u043f\u043e\u0434\u0442\u0432\u0435\u0440\u0434\u0438\u0442\u044c\"],\n[\"\u60a8\", \"\u0432\u044b\"],\n[\"\u77e5\u6653\", \"\u0437\u043d\u0430\u0442\u044c\"],\n[\"\uff0c\", null],\n[\"\u56e0\u4e3a\", \"\u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e\"],\n[\"\u8fd9\", \"\u044d\u0442\u043e\"],\n[\"\u6b64\u6b21\u6d3b\u52a8\", \"\u044d\u0442\u043e \u043c\u0435\u0440\u043e\u043f\u0440\u0438\u044f\u0442\u0438\u0435\"],\n[\"\u53d7\u5230\", \"\u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c\"],\n[\"\u9ad8\u5ea6\u91cd\u89c6\", \"\u0432\u044b\u0441\u043e\u043a\u043e\u0435 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435\"],\n[\"\u3002\"\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u201c\u56db\u5ddd\u9020\u201d\u65b0\u80fd\u6e90\u6c7d\u8f66\u9006\u52bf\u4e0a\u884c\u8d22\u653f\u8865\u8d34\u529b\u5ea6\u51cf\u5f31\u3001\u5145\u7535\u8bbe\u65bd\u666e\u53ca\u7387\u4e0d\u9ad8\u3001\u4e09\u5143\u9502\u7535\u6c60\u6682\u505c\u5728\u65b0\u80fd\u6e90\u5ba2\u8f66\u4e0a\u4f7f\u7528\uff0c\u9762\u5bf9\u56fd\u5185\u65b0\u80fd\u6e90\u6c7d\u8f66\u5e02\u573a\u5f53\u524d\u7684\u4e0a\u884c\u538b\u529b\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u201c', '\u56db\u5ddd', '\u9020', '\u201d', '\u65b0\u80fd\u6e90', '\u6c7d\u8f66', '\u9006\u52bf', '\u4e0a', '\u884c', '\u8d22\u653f', '\u8865\u8d34', '\u529b\u5ea6', '\u51cf\u5f31', '\u3001', '\u5145\u7535', '\u8bbe\u65bd', '\u666e\u53ca', '\u7387', '\u4e0d\u9ad8', '\u3001', '\u4e09', '\u5143', '\u9502\u7535\u6c60', '\u6682\u505c', '\u5728', '\u65b0\u80fd\u6e90', '\u5ba2\u8f66', '\u4e0a', '\u4f7f\u7528', '\uff0c', '\u9762\u5bf9', '\u56fd\u5185', '\u65b0\u80fd\u6e90', '\u6c7d\u8f66', '\u5e02\u573a', '\u5f53\u524d', '\u7684', '\u4e0a', '\u884c', '\u538b\u529b']"}], "[\n  ['\"', null],\n  [\"\u56db\u5ddd\", \"Sichuan\"],\n  [\"\u9020\", null],\n  ['\"', null],\n  [\"\u65b0\u80fd\u6e90\", \"new energy\"],\n  [\"\u6c7d\u8f66\", \"vehicles\"],\n  [\"\u9006\u52bf\", \"against the trend\"],\n  [\"\u4e0a\", null],\n  [\"\u884c\", \"perform\"],\n  [\"\u8d22\u653f\", \"financial\"],\n  [\"\u8865\u8d34\", \"subsidies\"],\n  [\"\u529b\u5ea6\", \"intensity\"],\n  [\"\u51cf\u5f31\", \"weaken\"],\n  [\"\u3001\", null],\n  [\"\u5145\u7535\", \"charging\"],\n  [\"\u8bbe\u65bd\", \"facilities\"],\n  [\"\u666e\u53ca\", \"popularization\"],\n  [\"\u7387\", null],\n  [\"\u4e0d\u9ad8\", \"not high\"],\n  [\"\u3001\", null],\n  [\"\u4e09\", \"three\"],\n  [\"\u5143\", null],\n  [\"\u9502\u7535\u6c60\", \"lithium batteries\"],\n  [\"\u6682\u505c\", \"suspended\"],\n  [\"\u5728\", null],\n  [\"\u65b0\u80fd\u6e90\", \"new energy\"],\n  [\"\u5ba2\u8f66\", \"buses\"],\n  [\"\u4e0a\", null],\n  [\"\u4f7f\u7528\", \"use\"],\n  [\"\uff0c\", null],\n  [\"\u9762\u5bf9\", \"facing\"],\n  [\"\u56fd\u5185\", \"domestic\"],\n  [\"\u65b0\u80fd\u6e90\", \"new energy\"],\n  [\"\u6c7d\u8f66\", \"vehicle\"],\n  [\"\u5e02\u573a\", \"market\"],\n  [\"\u5f53\u524d\", \"current\"],\n  [\"\u7684\", null],\n  [\"\u4e0a\", null],\n  [\"\u884c\", \"upward\"],\n  [\"\u538b\u529b\", \"pressure\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u201c\u56db\u5ddd\u9020\u201d\u65b0\u80fd\u6e90\u6c7d\u8f66\u9006\u52bf\u4e0a\u884c\u8d22\u653f\u8865\u8d34\u529b\u5ea6\u51cf\u5f31\u3001\u5145\u7535\u8bbe\u65bd\u666e\u53ca\u7387\u4e0d\u9ad8\u3001\u4e09\u5143\u9502\u7535\u6c60\u6682\u505c\u5728\u65b0\u80fd\u6e90\u5ba2\u8f66\u4e0a\u4f7f\u7528\uff0c\u9762\u5bf9\u56fd\u5185\u65b0\u80fd\u6e90\u6c7d\u8f66\u5e02\u573a\u5f53\u524d\u7684\u4e0a\u884c\u538b\u529b\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u201c', '\u56db\u5ddd', '\u9020', '\u201d', '\u65b0\u80fd\u6e90', '\u6c7d\u8f66', '\u9006\u52bf', '\u4e0a', '\u884c', '\u8d22\u653f', '\u8865\u8d34', '\u529b\u5ea6', '\u51cf\u5f31', '\u3001', '\u5145\u7535', '\u8bbe\u65bd', '\u666e\u53ca', '\u7387', '\u4e0d\u9ad8', '\u3001', '\u4e09', '\u5143', '\u9502\u7535\u6c60', '\u6682\u505c', '\u5728', '\u65b0\u80fd\u6e90', '\u5ba2\u8f66', '\u4e0a', '\u4f7f\u7528', '\uff0c', '\u9762\u5bf9', '\u56fd\u5185', '\u65b0\u80fd\u6e90', '\u6c7d\u8f66', '\u5e02\u573a', '\u5f53\u524d', '\u7684', '\u4e0a', '\u884c', '\u538b\u529b']"}], "[\n    [\"\\\"\", null],\n    [\"\u56db\u5ddd\", \"\u0421\u044b\u0447\u0443\u0430\u043d\u044c\"],\n    [\"\u9020\", null],\n    [\"\\\"\", null],\n    [\"\u65b0\u80fd\u6e90\", \"\u0436\u0430\u04a3\u0430 \u044d\u043d\u0435\u0440\u0433\u0438\u044f\"],\n    [\"\u6c7d\u8f66\", \"\u043a\u04e9\u043b\u0456\u043a\"],\n    [\"\u9006\u52bf\", \"\u049b\u0430\u0440\u0441\u044b\"],\n    [\"\u4e0a\", null],\n    [\"\u884c\", \"\u0436\u04af\u0440\u0443\"],\n    [\"\u8d22\u653f\", \"\u049b\u0430\u0440\u0436\u044b\u043b\u044b\u049b\"],\n    [\"\u8865\u8d34\", \"\u0441\u0443\u0431\u0441\u0438\u0434\u0438\u044f\"],\n    [\"\u529b\u5ea6\", \"\u043a\u04af\u0448\u0456\"],\n    [\"\u51cf\u5f31\", \"\u04d9\u043b\u0441\u0456\u0440\u0435\u0443\"],\n    [\"\u3001\", null],\n    [\"\u5145\u7535\", \"\u0437\u0430\u0440\u044f\u0434\u0442\u0430\u0443\"],\n    [\"\u8bbe\u65bd\", \"\u049b\u04b1\u0440\u044b\u043b\u0493\u044b\u043b\u0430\u0440\u044b\"],\n    [\"\u666e\u53ca\", \"\u0442\u0430\u0440\u0430\u0442\u0443\"],\n    [\"\u7387\", null],\n    [\"\u4e0d\u9ad8\", \"\u0442\u04e9\u043c\u0435\u043d\"],\n    [\"\u3001\", null],\n    [\"\u4e09\", \"\u04af\u0448\"],\n    [\"\u5143\", null],\n    [\"\u9502\u7535\u6c60\", \"\u043b\u0438\u0442\u0438\u0439 \u0431\u0430\u0442\u0430\u0440\u0435\u044f\u0441\u044b\"],\n    [\"\u6682\u505c\", \"\u0442\u043e\u049b\u0442\u0430\u0442\u0443\"],\n    [\"\u5728\", null],\n    [\"\u65b0\u80fd\u6e90\", \"\u0436\u0430\u04a3\u0430 \u044d\u043d\u0435\u0440\u0433\u0438\u044f\"],\n    [\"\u5ba2\u8f66\", \"\u0430\u0432\u0442\u043e\u0431\u0443\u0441\"],\n    [\"\u4e0a\", null],\n    [\"\u4f7f\u7528\", \"\u043f\u0430\u0439\u0434\u0430\u043b\u0430\u043d\u0443\"],\n    [\"\uff0c\", null],\n    [\"\u9762\u5bf9\", \"\u0431\u0435\u0442\u043f\u0435-\u0431\u0435\u0442\"],\n    [\"\u56fd\u5185\", \"\u0456\u0448\u043a\u0456\"],\n    [\"\u65b0\u80fd\u6e90\", \"\u0436\u0430\u04a3\u0430 \u044d\u043d\u0435\u0440\u0433\u0438\u044f\"],\n    [\"\u6c7d\u8f66\", \"\u043a\u04e9\u043b\u0456\u043a\"],\n    [\"\u5e02\u573a\", \"\u043d\u0430\u0440\u044b\u0493\u044b\"],\n    [\"\u5f53\u524d\", \"\u049b\u0430\u0437\u0456\u0440\u0433\u0456\"],\n    [\"\u7684\", null],\n    [\"\u4e0a\", null],\n    [\"\u884c\", \"\u0436\u04af\u0440\u0443\"],\n    [\"\u538b\u529b\", \"\u049b\u044b\u0441\u044b\u043c\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u201c\u56db\u5ddd\u9020\u201d\u65b0\u80fd\u6e90\u6c7d\u8f66\u9006\u52bf\u4e0a\u884c\u8d22\u653f\u8865\u8d34\u529b\u5ea6\u51cf\u5f31\u3001\u5145\u7535\u8bbe\u65bd\u666e\u53ca\u7387\u4e0d\u9ad8\u3001\u4e09\u5143\u9502\u7535\u6c60\u6682\u505c\u5728\u65b0\u80fd\u6e90\u5ba2\u8f66\u4e0a\u4f7f\u7528\uff0c\u9762\u5bf9\u56fd\u5185\u65b0\u80fd\u6e90\u6c7d\u8f66\u5e02\u573a\u5f53\u524d\u7684\u4e0a\u884c\u538b\u529b\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u201c', '\u56db\u5ddd', '\u9020', '\u201d', '\u65b0\u80fd\u6e90', '\u6c7d\u8f66', '\u9006\u52bf', '\u4e0a', '\u884c', '\u8d22\u653f', '\u8865\u8d34', '\u529b\u5ea6', '\u51cf\u5f31', '\u3001', '\u5145\u7535', '\u8bbe\u65bd', '\u666e\u53ca', '\u7387', '\u4e0d\u9ad8', '\u3001', '\u4e09', '\u5143', '\u9502\u7535\u6c60', '\u6682\u505c', '\u5728', '\u65b0\u80fd\u6e90', '\u5ba2\u8f66', '\u4e0a', '\u4f7f\u7528', '\uff0c', '\u9762\u5bf9', '\u56fd\u5185', '\u65b0\u80fd\u6e90', '\u6c7d\u8f66', '\u5e02\u573a', '\u5f53\u524d', '\u7684', '\u4e0a', '\u884c', '\u538b\u529b']"}], "[\n    [\"\\\"\", null],\n    [\"\u56db\u5ddd\", \"\u0421\u044b\u0447\u0443\u0430\u043d\u044c\"],\n    [\"\u9020\", null],\n    [\"\\\"\", null],\n    [\"\u65b0\u80fd\u6e90\", \"\u043d\u043e\u0432\u044b\u0435 \u044d\u043d\u0435\u0440\u0433\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435\"],\n    [\"\u6c7d\u8f66\", \"\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u0438\"],\n    [\"\u9006\u52bf\", \"\u043f\u0440\u043e\u0442\u0438\u0432 \u0442\u0435\u043d\u0434\u0435\u043d\u0446\u0438\u0438\"],\n    [\"\u4e0a\", null],\n    [\"\u884c\", \"\u0438\u0434\u0443\u0442\"],\n    [\"\u8d22\u653f\", \"\u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u044b\u0435\"],\n    [\"\u8865\u8d34\", \"\u0441\u0443\u0431\u0441\u0438\u0434\u0438\u0438\"],\n    [\"\u529b\u5ea6\", \"\u0438\u043d\u0442\u0435\u043d\u0441\u0438\u0432\u043d\u043e\u0441\u0442\u044c\"],\n    [\"\u51cf\u5f31\", \"\u043e\u0441\u043b\u0430\u0431\u0435\u0432\u0430\u0435\u0442\"],\n    [\"\u3001\", null],\n    [\"\u5145\u7535\", \"\u0437\u0430\u0440\u044f\u0434\u043d\u044b\u0435\"],\n    [\"\u8bbe\u65bd\", \"\u0443\u0441\u0442\u0440\u043e\u0439\u0441\u0442\u0432\u0430\"],\n    [\"\u666e\u53ca\", \"\u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u0438\u0435\"],\n    [\"\u7387\", null],\n    [\"\u4e0d\u9ad8\", \"\u043d\u0435\u0432\u044b\u0441\u043e\u043a\u043e\u0435\"],\n    [\"\u3001\", null],\n    [\"\u4e09\", \"\u0442\u0440\u0435\u0445\"],\n    [\"\u5143\", null],\n    [\"\u9502\u7535\u6c60\", \"\u043b\u0438\u0442\u0438\u0435\u0432\u044b\u0435 \u0431\u0430\u0442\u0430\u0440\u0435\u0438\"],\n    [\"\u6682\u505c\", \"\u043f\u0440\u0438\u043e\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d\u043e\"],\n    [\"\u5728\", null],\n    [\"\u65b0\u80fd\u6e90\", \"\u043d\u043e\u0432\u044b\u0445 \u044d\u043d\u0435\u0440\u0433\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445\"],\n    [\"\u5ba2\u8f66\", \"\u0430\u0432\u0442\u043e\u0431\u0443\u0441\u0430\u0445\"],\n    [\"\u4e0a\", null],\n    [\"\u4f7f\u7528\", \"\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\"],\n    [\"\uff0c\", null],\n    [\"\u9762\u5bf9\", \"\u043f\u0435\u0440\u0435\u0434 \u043b\u0438\u0446\u043e\u043c\"],\n    [\"\u56fd\u5185\", \"\u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0435\u0433\u043e\"],\n    [\"\u65b0\u80fd\u6e90\", \"\u043d\u043e\u0432\u044b\u0445 \u044d\u043d\u0435\u0440\u0433\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445\"],\n    [\"\u6c7d\u8f66\", \"\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u0435\u0439\"],\n    [\"\u5e02\u573a\", \"\u0440\u044b\u043d\u043a\u0430\"],\n    [\"\u5f53\u524d\", \"\u0442\u0435\u043a\u0443\u0449\u0435\u0433\u043e\"],\n    [\"\u7684\", null],\n    [\"\u4e0a\", null],\n    [\"\u884c\", \"\u043f\u043e\u0434\u044a\u0435\u043c\u0430\"],\n    [\"\u538b\u529b\", \"\u0434\u0430\u0432\u043b\u0435\u043d\u0438\u044f\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u65b0\u95fb\u62a5\u7eb8 Svenska Dagbladet \u8c03\u67e5\u4e86\u6b64\u6b21\u516c\u5e03\u7684 581 \u4f8b\u6d4b\u8bd5\u7ed3\u679c\uff0c\u53d1\u73b0\u5176\u4e2d 243 \u4f8b\u53d7\u8bd5\u8005\u5df2\u516c\u5f00\u8868\u793a\u4ed6\u4eec\u4eca\u5e74\u4f1a\u5e74\u6ee1 18 \u5c81\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u65b0\u95fb', '\u62a5\u7eb8', ' Svens', 'ka', ' Dag', 'bl', 'ad', 'et', '', '', '', '\u67e5', '\u4e86', '\u6b64\u6b21', '\u516c\u5e03\u7684', ' ', '5', '8', '1', ' ', '\u4f8b', '\u6d4b\u8bd5', '\u7ed3\u679c', '\uff0c', '\u53d1\u73b0', '\u5176\u4e2d', ' ', '2', '4', '3', ' ', '\u4f8b', '\u53d7', '\u8bd5', '\u8005', '\u5df2', '\u516c\u5f00', '\u8868\u793a', '\u4ed6\u4eec', '\u4eca\u5e74', '\u4f1a', '\u5e74', '\u6ee1', ' ', '1', '8', '', '', '', '\u3002']"}], "[\n  [\"\u65b0\u95fb\", \"news\"],\n  [\"\u62a5\u7eb8\", \"newspaper\"],\n  [\" Svens\", null],\n  [\"ka\", null],\n  [\" Dag\", null],\n  [\"bl\", null],\n  [\"ad\", null],\n  [\"et\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\u67e5\", \"investigated\"],\n  [\"\u4e86\", null],\n  [\"\u6b64\u6b21\", \"this\"],\n  [\"\u516c\u5e03\u7684\", \"published\"],\n  [\" \", null],\n  [\"5\", null],\n  [\"8\", null],\n  [\"1\", null],\n  [\" \", null],\n  [\"\u4f8b\", \"cases\"],\n  [\"\u6d4b\u8bd5\", \"test\"],\n  [\"\u7ed3\u679c\", \"results\"],\n  [\"\uff0c\", null],\n  [\"\u53d1\u73b0\", \"found\"],\n  [\"\u5176\u4e2d\", \"among them\"],\n  [\" \", null],\n  [\"2\", null],\n  [\"4\", null],\n  [\"3\", null],\n  [\" \", null],\n  [\"\u4f8b\", \"cases\"],\n  [\"\u53d7\", null],\n  [\"\u8bd5\", null],\n  [\"\u8005\", \"subjects\"],\n  [\"\u5df2\", \"have\"],\n  [\"\u516c\u5f00\", \"publicly\"],\n  [\"\u8868\u793a\", \"stated\"],\n  [\"\u4ed6\u4eec\", \"they\"],\n  [\"\u4eca\u5e74\", \"this year\"],\n  [\"\u4f1a\", \"will\"],\n  [\"\u5e74\", null],\n  [\"\u6ee1\", \"turn\"],\n  [\" \", null],\n  [\"1\", null],\n  [\"8\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u65b0\u95fb\u62a5\u7eb8 Svenska Dagbladet \u8c03\u67e5\u4e86\u6b64\u6b21\u516c\u5e03\u7684 581 \u4f8b\u6d4b\u8bd5\u7ed3\u679c\uff0c\u53d1\u73b0\u5176\u4e2d 243 \u4f8b\u53d7\u8bd5\u8005\u5df2\u516c\u5f00\u8868\u793a\u4ed6\u4eec\u4eca\u5e74\u4f1a\u5e74\u6ee1 18 \u5c81\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u65b0\u95fb', '\u62a5\u7eb8', ' Svens', 'ka', ' Dag', 'bl', 'ad', 'et', '', '', '', '\u67e5', '\u4e86', '\u6b64\u6b21', '\u516c\u5e03\u7684', ' ', '5', '8', '1', ' ', '\u4f8b', '\u6d4b\u8bd5', '\u7ed3\u679c', '\uff0c', '\u53d1\u73b0', '\u5176\u4e2d', ' ', '2', '4', '3', ' ', '\u4f8b', '\u53d7', '\u8bd5', '\u8005', '\u5df2', '\u516c\u5f00', '\u8868\u793a', '\u4ed6\u4eec', '\u4eca\u5e74', '\u4f1a', '\u5e74', '\u6ee1', ' ', '1', '8', '', '', '', '\u3002']"}], "[\n[\"\u65b0\u95fb\", \"\u0436\u0430\u04a3\u0430\u043b\u044b\u049b\u0442\u0430\u0440\"],\n[\"\u62a5\u7eb8\", \"\u0433\u0430\u0437\u0435\u0442\"],\n[\" Svens\", null],\n[\"ka\", null],\n[\" Dag\", null],\n[\"bl\", null],\n[\"ad\", null],\n[\"et\", null],\n[\"\", null],\n[\"\", null],\n[\"\", null],\n[\"\u67e5\", \"\u0437\u0435\u0440\u0442\u0442\u0435\u0443\"],\n[\"\u4e86\", null],\n[\"\u6b64\u6b21\", \"\u0431\u04b1\u043b \u0436\u043e\u043b\u044b\"],\n[\"\u516c\u5e03\u7684\", \"\u0436\u0430\u0440\u0438\u044f\u043b\u0430\u043d\u0493\u0430\u043d\"],\n[\" \", null],\n[\"5\", null],\n[\"8\", null],\n[\"1\", null],\n[\" \", null],\n[\"\u4f8b\", \"\u0436\u0430\u0493\u0434\u0430\u0439\"],\n[\"\u6d4b\u8bd5\", \"\u0441\u044b\u043d\u0430\u049b\"],\n[\"\u7ed3\u679c\", \"\u043d\u04d9\u0442\u0438\u0436\u0435\u043b\u0435\u0440\u0456\"],\n[\"\uff0c\", null],\n[\"\u53d1\u73b0\", \"\u0430\u043d\u044b\u049b\u0442\u0430\u0434\u044b\"],\n[\"\u5176\u4e2d\", \"\u043e\u043b\u0430\u0440\u0434\u044b\u04a3 \u0456\u0448\u0456\u043d\u0434\u0435\"],\n[\" \", null],\n[\"2\", null],\n[\"4\", null],\n[\"3\", null],\n[\" \", null],\n[\"\u4f8b\", \"\u0436\u0430\u0493\u0434\u0430\u0439\"],\n[\"\u53d7\", null],\n[\"\u8bd5\", null],\n[\"\u8005\", \"\u049b\u0430\u0442\u044b\u0441\u0443\u0448\u044b\u043b\u0430\u0440\"],\n[\"\u5df2\", null],\n[\"\u516c\u5f00\", \"\u0436\u0430\u0440\u0438\u044f\u043b\u044b \u0442\u04af\u0440\u0434\u0435\"],\n[\"\u8868\u793a\", \"\u043c\u04d9\u043b\u0456\u043c\u0434\u0435\u0434\u0456\"],\n[\"\u4ed6\u4eec\", \"\u043e\u043b\u0430\u0440\"],\n[\"\u4eca\u5e74\", \"\u0431\u0438\u044b\u043b\"],\n[\"\u4f1a\", null],\n[\"\u5e74\", null],\n[\"\u6ee1\", \"\u0442\u043e\u043b\u0430\u0434\u044b\"],\n[\" \", null],\n[\"1\", null],\n[\"8\", null],\n[\"\", null],\n[\"\", null],\n[\"\", null],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u65b0\u95fb\u62a5\u7eb8 Svenska Dagbladet \u8c03\u67e5\u4e86\u6b64\u6b21\u516c\u5e03\u7684 581 \u4f8b\u6d4b\u8bd5\u7ed3\u679c\uff0c\u53d1\u73b0\u5176\u4e2d 243 \u4f8b\u53d7\u8bd5\u8005\u5df2\u516c\u5f00\u8868\u793a\u4ed6\u4eec\u4eca\u5e74\u4f1a\u5e74\u6ee1 18 \u5c81\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u65b0\u95fb', '\u62a5\u7eb8', ' Svens', 'ka', ' Dag', 'bl', 'ad', 'et', '', '', '', '\u67e5', '\u4e86', '\u6b64\u6b21', '\u516c\u5e03\u7684', ' ', '5', '8', '1', ' ', '\u4f8b', '\u6d4b\u8bd5', '\u7ed3\u679c', '\uff0c', '\u53d1\u73b0', '\u5176\u4e2d', ' ', '2', '4', '3', ' ', '\u4f8b', '\u53d7', '\u8bd5', '\u8005', '\u5df2', '\u516c\u5f00', '\u8868\u793a', '\u4ed6\u4eec', '\u4eca\u5e74', '\u4f1a', '\u5e74', '\u6ee1', ' ', '1', '8', '', '', '', '\u3002']"}], "[\n[\"\u65b0\u95fb\", \"\u041d\u043e\u0432\u043e\u0441\u0442\u0438\"],\n[\"\u62a5\u7eb8\", \"\u0433\u0430\u0437\u0435\u0442\u0430\"],\n[\" Svens\", null],\n[\"ka\", null],\n[\" Dag\", null],\n[\"bl\", null],\n[\"ad\", null],\n[\"et\", null],\n[\"\", null],\n[\"\", null],\n[\"\", null],\n[\"\u67e5\", \"\u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043b\u0430\"],\n[\"\u4e86\", null],\n[\"\u6b64\u6b21\", \"\u044d\u0442\u043e\u0442\"],\n[\"\u516c\u5e03\u7684\", \"\u043e\u043f\u0443\u0431\u043b\u0438\u043a\u043e\u0432\u0430\u043d\u043d\u044b\u0439\"],\n[\" \", null],\n[\"5\", null],\n[\"8\", null],\n[\"1\", null],\n[\" \", null],\n[\"\u4f8b\", \"\u0441\u043b\u0443\u0447\u0430\u0435\u0432\"],\n[\"\u6d4b\u8bd5\", \"\u0442\u0435\u0441\u0442\u043e\u0432\"],\n[\"\u7ed3\u679c\", \"\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\"],\n[\"\uff0c\", null],\n[\"\u53d1\u73b0\", \"\u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0438\u043b\u0430\"],\n[\"\u5176\u4e2d\", \"\u0441\u0440\u0435\u0434\u0438 \u043d\u0438\u0445\"],\n[\" \", null],\n[\"2\", null],\n[\"4\", null],\n[\"3\", null],\n[\" \", null],\n[\"\u4f8b\", \"\u0441\u043b\u0443\u0447\u0430\u0435\u0432\"],\n[\"\u53d7\", null],\n[\"\u8bd5\", null],\n[\"\u8005\", \"\u0438\u0441\u043f\u044b\u0442\u0443\u0435\u043c\u044b\u0445\"],\n[\"\u5df2\", null],\n[\"\u516c\u5f00\", \"\u043f\u0443\u0431\u043b\u0438\u0447\u043d\u043e\"],\n[\"\u8868\u793a\", \"\u0437\u0430\u044f\u0432\u0438\u043b\u0438\"],\n[\"\u4ed6\u4eec\", \"\u043e\u043d\u0438\"],\n[\"\u4eca\u5e74\", \"\u0432 \u044d\u0442\u043e\u043c \u0433\u043e\u0434\u0443\"],\n[\"\u4f1a\", null],\n[\"\u5e74\", null],\n[\"\u6ee1\", \"\u0438\u0441\u043f\u043e\u043b\u043d\u0438\u0442\u0441\u044f\"],\n[\" \", null],\n[\"1\", null],\n[\"8\", null],\n[\"\", null],\n[\"\", null],\n[\"\", null],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u66ff\u5361\u7279\u8fa9\u62a4\u7684\u4eba\u4e3a\u6570\u4e0d\u591a\uff0c\u963f\u66fc\u8fbe?\u8bfa\u514b\u65af (Amanda Knox) \u5c31\u662f\u5176\u4e2d\u4e4b\u4e00\uff0c\u5979\u4e8e 2007 \u5e74\u88ab\u6307\u63a7\u5728\u610f\u5927\u5229\u8c0b\u6740\u5979\u7684\u82f1\u56fd\u5ba4\u53cb\uff0c\u4e24\u5e74\u540e\u88ab\u5ba3\u5e03\u65e0\u7f6a\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u66ff', '\u5361', '\u7279', '\u8fa9\u62a4', '\u7684\u4eba', '\u4e3a', '\u6570', '\u4e0d\u591a', '\uff0c', '\u963f', '\u66fc', '\u8fbe', '?', '\u8bfa', '\u514b\u65af', ' (', 'A', 'manda', ' Knox', ')', '', '', '\u662f', '\u5176\u4e2d\u4e4b\u4e00', '\uff0c', '\u5979', '\u4e8e', ' ', '2', '0', '0', '7', '', '', '\u88ab', '\u6307\u63a7', '\u5728', '\u610f\u5927\u5229', '\u8c0b', '\u6740', '\u5979\u7684', '\u82f1\u56fd', '\u5ba4\u53cb', '\uff0c', '\u4e24\u5e74', '\u540e', '\u88ab', '\u5ba3\u5e03', '\u65e0', '\u7f6a', '\u3002']"}], "[\n[\"\u66ff\", null],\n[\"\u5361\", null],\n[\"\u7279\", \"Cate\"],\n[\"\u8fa9\u62a4\", \"defend\"],\n[\"\u7684\u4eba\", null],\n[\"\u4e3a\", null],\n[\"\u6570\", null],\n[\"\u4e0d\u591a\", \"few\"],\n[\"\uff0c\", null],\n[\"\u963f\", null],\n[\"\u66fc\", null],\n[\"\u8fbe\", null],\n[\"?\", null],\n[\"\u8bfa\", null],\n[\"\u514b\u65af\", \"Knox\"],\n[\" (\", null],\n[\"A\", null],\n[\"manda\", null],\n[\" Knox\", null],\n[\")\", null],\n[\"\", null],\n[\"\", null],\n[\"\u662f\", null],\n[\"\u5176\u4e2d\u4e4b\u4e00\", \"one of them\"],\n[\"\uff0c\", null],\n[\"\u5979\", \"she\"],\n[\"\u4e8e\", null],\n[\" \", null],\n[\"2\", null],\n[\"0\", null],\n[\"0\", null],\n[\"7\", null],\n[\"\", null],\n[\"\", null],\n[\"\u88ab\", null],\n[\"\u6307\u63a7\", \"accused\"],\n[\"\u5728\", null],\n[\"\u610f\u5927\u5229\", \"Italy\"],\n[\"\u8c0b\", null],\n[\"\u6740\", \"murder\"],\n[\"\u5979\u7684\", null],\n[\"\u82f1\u56fd\", \"British\"],\n[\"\u5ba4\u53cb\", \"roommate\"],\n[\"\uff0c\", null],\n[\"\u4e24\u5e74\", \"two years\"],\n[\"\u540e\", \"later\"],\n[\"\u88ab\", null],\n[\"\u5ba3\u5e03\", \"declared\"],\n[\"\u65e0\", null],\n[\"\u7f6a\", \"innocent\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u66ff\u5361\u7279\u8fa9\u62a4\u7684\u4eba\u4e3a\u6570\u4e0d\u591a\uff0c\u963f\u66fc\u8fbe?\u8bfa\u514b\u65af (Amanda Knox) \u5c31\u662f\u5176\u4e2d\u4e4b\u4e00\uff0c\u5979\u4e8e 2007 \u5e74\u88ab\u6307\u63a7\u5728\u610f\u5927\u5229\u8c0b\u6740\u5979\u7684\u82f1\u56fd\u5ba4\u53cb\uff0c\u4e24\u5e74\u540e\u88ab\u5ba3\u5e03\u65e0\u7f6a\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u66ff', '\u5361', '\u7279', '\u8fa9\u62a4', '\u7684\u4eba', '\u4e3a', '\u6570', '\u4e0d\u591a', '\uff0c', '\u963f', '\u66fc', '\u8fbe', '?', '\u8bfa', '\u514b\u65af', ' (', 'A', 'manda', ' Knox', ')', '', '', '\u662f', '\u5176\u4e2d\u4e4b\u4e00', '\uff0c', '\u5979', '\u4e8e', ' ', '2', '0', '0', '7', '', '', '\u88ab', '\u6307\u63a7', '\u5728', '\u610f\u5927\u5229', '\u8c0b', '\u6740', '\u5979\u7684', '\u82f1\u56fd', '\u5ba4\u53cb', '\uff0c', '\u4e24\u5e74', '\u540e', '\u88ab', '\u5ba3\u5e03', '\u65e0', '\u7f6a', '\u3002']"}], "[\n  [\"\u66ff\", null],\n  [\"\u5361\", null],\n  [\"\u7279\", null],\n  [\"\u8fa9\u62a4\", \"\u049b\u043e\u0440\u0493\u0430\u0443\"],\n  [\"\u7684\u4eba\", \"\u0430\u0434\u0430\u043c\u0434\u0430\u0440\"],\n  [\"\u4e3a\", null],\n  [\"\u6570\", null],\n  [\"\u4e0d\u591a\", \"\u0430\u0437\"],\n  [\"\uff0c\", null],\n  [\"\u963f\", null],\n  [\"\u66fc\", null],\n  [\"\u8fbe\", null],\n  [\"?\", null],\n  [\"\u8bfa\", null],\n  [\"\u514b\u65af\", null],\n  [\" (\", null],\n  [\"A\", null],\n  [\"manda\", null],\n  [\" Knox\", null],\n  [\")\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\u662f\", null],\n  [\"\u5176\u4e2d\u4e4b\u4e00\", \"\u0431\u0456\u0440\u0435\u0443\u0456\"],\n  [\"\uff0c\", null],\n  [\"\u5979\", \"\u043e\u043b\"],\n  [\"\u4e8e\", null],\n  [\" \", null],\n  [\"2\", null],\n  [\"0\", null],\n  [\"0\", null],\n  [\"7\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\u88ab\", null],\n  [\"\u6307\u63a7\", \"\u0430\u0439\u044b\u043f\u0442\u0430\u043b\u0493\u0430\u043d\"],\n  [\"\u5728\", null],\n  [\"\u610f\u5927\u5229\", \"\u0418\u0442\u0430\u043b\u0438\u044f\u0434\u0430\"],\n  [\"\u8c0b\", null],\n  [\"\u6740\", null],\n  [\"\u5979\u7684\", null],\n  [\"\u82f1\u56fd\", \"\u0430\u0493\u044b\u043b\u0448\u044b\u043d\"],\n  [\"\u5ba4\u53cb\", \"\u0431\u04e9\u043b\u043c\u0435\u043b\u0435\u0441\u0456\u043d\"],\n  [\"\uff0c\", null],\n  [\"\u4e24\u5e74\", \"\u0435\u043a\u0456 \u0436\u044b\u043b\u0434\u0430\u043d\"],\n  [\"\u540e\", \"\u043a\u0435\u0439\u0456\u043d\"],\n  [\"\u88ab\", null],\n  [\"\u5ba3\u5e03\", \"\u0436\u0430\u0440\u0438\u044f\u043b\u0430\u043d\u0434\u044b\"],\n  [\"\u65e0\", null],\n  [\"\u7f6a\", \"\u043a\u0456\u043d\u04d9\u0441\u0456\u0437\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u66ff\u5361\u7279\u8fa9\u62a4\u7684\u4eba\u4e3a\u6570\u4e0d\u591a\uff0c\u963f\u66fc\u8fbe?\u8bfa\u514b\u65af (Amanda Knox) \u5c31\u662f\u5176\u4e2d\u4e4b\u4e00\uff0c\u5979\u4e8e 2007 \u5e74\u88ab\u6307\u63a7\u5728\u610f\u5927\u5229\u8c0b\u6740\u5979\u7684\u82f1\u56fd\u5ba4\u53cb\uff0c\u4e24\u5e74\u540e\u88ab\u5ba3\u5e03\u65e0\u7f6a\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u66ff', '\u5361', '\u7279', '\u8fa9\u62a4', '\u7684\u4eba', '\u4e3a', '\u6570', '\u4e0d\u591a', '\uff0c', '\u963f', '\u66fc', '\u8fbe', '?', '\u8bfa', '\u514b\u65af', ' (', 'A', 'manda', ' Knox', ')', '', '', '\u662f', '\u5176\u4e2d\u4e4b\u4e00', '\uff0c', '\u5979', '\u4e8e', ' ', '2', '0', '0', '7', '', '', '\u88ab', '\u6307\u63a7', '\u5728', '\u610f\u5927\u5229', '\u8c0b', '\u6740', '\u5979\u7684', '\u82f1\u56fd', '\u5ba4\u53cb', '\uff0c', '\u4e24\u5e74', '\u540e', '\u88ab', '\u5ba3\u5e03', '\u65e0', '\u7f6a', '\u3002']"}], "[\n  [\"\u66ff\", null],\n  [\"\u5361\", null],\n  [\"\u7279\", null],\n  [\"\u8fa9\u62a4\", \"\u0437\u0430\u0449\u0438\u0449\u0430\u0442\u044c\"],\n  [\"\u7684\u4eba\", null],\n  [\"\u4e3a\", null],\n  [\"\u6570\", null],\n  [\"\u4e0d\u591a\", \"\u043d\u0435\u043c\u043d\u043e\u0433\u043e\"],\n  [\"\uff0c\", null],\n  [\"\u963f\", null],\n  [\"\u66fc\", null],\n  [\"\u8fbe\", null],\n  [\"?\", null],\n  [\"\u8bfa\", null],\n  [\"\u514b\u65af\", null],\n  [\" (\", null],\n  [\"A\", null],\n  [\"manda\", null],\n  [\" Knox\", null],\n  [\")\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\u662f\", null],\n  [\"\u5176\u4e2d\u4e4b\u4e00\", \"\u043e\u0434\u043d\u0430 \u0438\u0437\"],\n  [\"\uff0c\", null],\n  [\"\u043e\u043d\u0430\", \"\u5979\"],\n  [\"\u4e8e\", null],\n  [\" \", null],\n  [\"2\", null],\n  [\"0\", null],\n  [\"0\", null],\n  [\"7\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\u88ab\", null],\n  [\"\u6307\u63a7\", \"\u043e\u0431\u0432\u0438\u043d\u0435\u043d\u0430\"],\n  [\"\u5728\", null],\n  [\"\u610f\u5927\u5229\", \"\u0418\u0442\u0430\u043b\u0438\u0438\"],\n  [\"\u8c0b\", null],\n  [\"\u6740\", null],\n  [\"\u5979\u7684\", null],\n  [\"\u82f1\u56fd\", \"\u0431\u0440\u0438\u0442\u0430\u043d\u0441\u043a\u043e\u0439\"],\n  [\"\u5ba4\u53cb\", \"\u0441\u043e\u0441\u0435\u0434\u043a\u0438\"],\n  [\"\uff0c\", null],\n  [\"\u4e24\u5e74\", \"\u0434\u0432\u0430 \u0433\u043e\u0434\u0430\"],\n  [\"\u540e\", null],\n  [\"\u88ab\", null],\n  [\"\u5ba3\u5e03\", \"\u043e\u0431\u044a\u044f\u0432\u043b\u0435\u043d\u0430\"],\n  [\"\u65e0\", null],\n  [\"\u7f6a\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u4eca\u5e74\u7684\u6f14\u51fa\u4e8e\u516d\u6708\u5f00\u59cb\uff0c\u7b2c\u4e00\u5929\u6709\u5c06\u8fd1 30 \u540d\u6f14\u5458 - \u5341\u51e0\u4f4d\u767d\u4eba\uff0c\u5176\u4f59\u4e3a\u9ed1\u4eba - \u6324\u5728\u4e00\u95f4\u5c4b\u5b50\u91cc\uff0c\u542c\u53d6\u683c\u6797\u53d1\u5e03\u6392\u7ec3\u597d\u7684\u6f14\u51fa\u6982\u89c8\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u4eca\u5e74', '\u7684', '\u6f14\u51fa', '\u4e8e', '\u516d', '\u6708', '\u5f00\u59cb', '\uff0c', '\u7b2c\u4e00\u5929', '\u6709', '\u5c06\u8fd1', ' ', '3', '0', ' \u540d', '\u6f14\u5458', ' -', '', '', '\u51e0\u4f4d', '\u767d', '\u4eba', '\uff0c', '\u5176\u4f59', '\u4e3a', '\u9ed1', '\u4eba', ' -', '', '', '', '\u5728', '\u4e00\u95f4', '\u5c4b', '\u5b50\u91cc', '\uff0c', '\u542c\u53d6', '\u683c\u6797', '\u53d1\u5e03', '\u6392', '\u7ec3', '\u597d\u7684', '\u6f14\u51fa', '\u6982', '\u89c8', '\u3002']"}], "[\n  [\"\u4eca\u5e74\", \"This year\"],\n  [\"\u7684\", null],\n  [\"\u6f14\u51fa\", \"performance\"],\n  [\"\u4e8e\", null],\n  [\"\u516d\", \"June\"],\n  [\"\u6708\", null],\n  [\"\u5f00\u59cb\", \"begin\"],\n  [\"\uff0c\", null],\n  [\"\u7b2c\u4e00\u5929\", \"first day\"],\n  [\"\u6709\", null],\n  [\"\u5c06\u8fd1\", \"nearly\"],\n  [\" \", null],\n  [\"3\", null],\n  [\"0\", null],\n  [\" \u540d\", null],\n  [\"\u6f14\u5458\", \"actors\"],\n  [\" -\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\u51e0\u4f4d\", \"several\"],\n  [\"\u767d\", \"white\"],\n  [\"\u4eba\", null],\n  [\"\uff0c\", null],\n  [\"\u5176\u4f59\", \"rest\"],\n  [\"\u4e3a\", null],\n  [\"\u9ed1\", \"black\"],\n  [\"\u4eba\", null],\n  [\" -\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\u5728\", null],\n  [\"\u4e00\u95f4\", \"one\"],\n  [\"\u5c4b\", \"room\"],\n  [\"\u5b50\u91cc\", null],\n  [\"\uff0c\", null],\n  [\"\u542c\u53d6\", \"listen to\"],\n  [\"\u683c\u6797\", \"Green\"],\n  [\"\u53d1\u5e03\", \"deliver\"],\n  [\"\u6392\", null],\n  [\"\u7ec3\", null],\n  [\"\u597d\u7684\", \"rehearsed\"],\n  [\"\u6f14\u51fa\", \"performance\"],\n  [\"\u6982\", null],\n  [\"\u89c8\", \"overview\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u4eca\u5e74\u7684\u6f14\u51fa\u4e8e\u516d\u6708\u5f00\u59cb\uff0c\u7b2c\u4e00\u5929\u6709\u5c06\u8fd1 30 \u540d\u6f14\u5458 - \u5341\u51e0\u4f4d\u767d\u4eba\uff0c\u5176\u4f59\u4e3a\u9ed1\u4eba - \u6324\u5728\u4e00\u95f4\u5c4b\u5b50\u91cc\uff0c\u542c\u53d6\u683c\u6797\u53d1\u5e03\u6392\u7ec3\u597d\u7684\u6f14\u51fa\u6982\u89c8\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u4eca\u5e74', '\u7684', '\u6f14\u51fa', '\u4e8e', '\u516d', '\u6708', '\u5f00\u59cb', '\uff0c', '\u7b2c\u4e00\u5929', '\u6709', '\u5c06\u8fd1', ' ', '3', '0', ' \u540d', '\u6f14\u5458', ' -', '', '', '\u51e0\u4f4d', '\u767d', '\u4eba', '\uff0c', '\u5176\u4f59', '\u4e3a', '\u9ed1', '\u4eba', ' -', '', '', '', '\u5728', '\u4e00\u95f4', '\u5c4b', '\u5b50\u91cc', '\uff0c', '\u542c\u53d6', '\u683c\u6797', '\u53d1\u5e03', '\u6392', '\u7ec3', '\u597d\u7684', '\u6f14\u51fa', '\u6982', '\u89c8', '\u3002']"}], "[\n  [\"\u4eca\u5e74\", \"\u0431\u0438\u044b\u043b\"],\n  [\"\u7684\", null],\n  [\"\u6f14\u51fa\", \"\u04e9\u043d\u0435\u0440 \u043a\u04e9\u0440\u0441\u0435\u0442\u0443\"],\n  [\"\u4e8e\", null],\n  [\"\u516d\", \"\u0430\u043b\u0442\u044b\"],\n  [\"\u6708\", \"\u0430\u0439\"],\n  [\"\u5f00\u59cb\", \"\u0431\u0430\u0441\u0442\u0430\u043b\u0434\u044b\"],\n  [\"\uff0c\", null],\n  [\"\u7b2c\u4e00\u5929\", \"\u0431\u0456\u0440\u0456\u043d\u0448\u0456 \u043a\u04af\u043d\u0456\"],\n  [\"\u6709\", null],\n  [\"\u5c06\u8fd1\", \"\u0448\u0430\u043c\u0430\u043c\u0435\u043d\"],\n  [\" \", null],\n  [\"3\", null],\n  [\"0\", null],\n  [\" \u540d\", null],\n  [\"\u6f14\u5458\", \"\u0430\u043a\u0442\u0435\u0440\"],\n  [\" -\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\u51e0\u4f4d\", \"\u0431\u0456\u0440\u043d\u0435\u0448\u0435\"],\n  [\"\u767d\", \"\u0430\u049b\"],\n  [\"\u4eba\", \"\u0430\u0434\u0430\u043c\"],\n  [\"\uff0c\", null],\n  [\"\u5176\u4f59\", \"\u049b\u0430\u043b\u0493\u0430\u043d\u0434\u0430\u0440\u044b\"],\n  [\"\u4e3a\", null],\n  [\"\u9ed1\", \"\u049b\u0430\u0440\u0430\"],\n  [\"\u4eba\", \"\u0430\u0434\u0430\u043c\"],\n  [\" -\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\u5728\", null],\n  [\"\u4e00\u95f4\", \"\u0431\u0456\u0440\"],\n  [\"\u5c4b\", \"\u0431\u04e9\u043b\u043c\u0435\"],\n  [\"\u5b50\u91cc\", null],\n  [\"\uff0c\", null],\n  [\"\u542c\u53d6\", \"\u0442\u044b\u04a3\u0434\u0430\u0434\u044b\"],\n  [\"\u683c\u6797\", \"\u0413\u0440\u0438\u043d\"],\n  [\"\u53d1\u5e03\", \"\u0436\u0430\u0440\u0438\u044f\u043b\u0430\u0493\u0430\u043d\"],\n  [\"\u6392\", null],\n  [\"\u7ec3\", null],\n  [\"\u597d\u7684\", null],\n  [\"\u6f14\u51fa\", \"\u049b\u043e\u0439\u044b\u043b\u044b\u043c\"],\n  [\"\u6982\", null],\n  [\"\u89c8\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u4eca\u5e74\u7684\u6f14\u51fa\u4e8e\u516d\u6708\u5f00\u59cb\uff0c\u7b2c\u4e00\u5929\u6709\u5c06\u8fd1 30 \u540d\u6f14\u5458 - \u5341\u51e0\u4f4d\u767d\u4eba\uff0c\u5176\u4f59\u4e3a\u9ed1\u4eba - \u6324\u5728\u4e00\u95f4\u5c4b\u5b50\u91cc\uff0c\u542c\u53d6\u683c\u6797\u53d1\u5e03\u6392\u7ec3\u597d\u7684\u6f14\u51fa\u6982\u89c8\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u4eca\u5e74', '\u7684', '\u6f14\u51fa', '\u4e8e', '\u516d', '\u6708', '\u5f00\u59cb', '\uff0c', '\u7b2c\u4e00\u5929', '\u6709', '\u5c06\u8fd1', ' ', '3', '0', ' \u540d', '\u6f14\u5458', ' -', '', '', '\u51e0\u4f4d', '\u767d', '\u4eba', '\uff0c', '\u5176\u4f59', '\u4e3a', '\u9ed1', '\u4eba', ' -', '', '', '', '\u5728', '\u4e00\u95f4', '\u5c4b', '\u5b50\u91cc', '\uff0c', '\u542c\u53d6', '\u683c\u6797', '\u53d1\u5e03', '\u6392', '\u7ec3', '\u597d\u7684', '\u6f14\u51fa', '\u6982', '\u89c8', '\u3002']"}], "[\n[\"\u4eca\u5e74\", \"\u0412 \u044d\u0442\u043e\u043c \u0433\u043e\u0434\u0443\"],\n[\"\u7684\", null],\n[\"\u6f14\u51fa\", \"\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435\"],\n[\"\u4e8e\", null],\n[\"\u516d\", \"\u0438\u044e\u043d\u044c\"],\n[\"\u6708\", null],\n[\"\u5f00\u59cb\", \"\u043d\u0430\u0447\u0438\u043d\u0430\u0435\u0442\u0441\u044f\"],\n[\"\uff0c\", null],\n[\"\u7b2c\u4e00\u5929\", \"\u0432 \u043f\u0435\u0440\u0432\u044b\u0439 \u0434\u0435\u043d\u044c\"],\n[\"\u6709\", null],\n[\"\u5c06\u8fd1\", \"\u043e\u043a\u043e\u043b\u043e\"],\n[\" \", null],\n[\"3\", null],\n[\"0\", null],\n[\" \u540d\", null],\n[\"\u6f14\u5458\", \"\u0430\u043a\u0442\u0435\u0440\u043e\u0432\"],\n[\" -\", null],\n[\"\", null],\n[\"\", null],\n[\"\u51e0\u4f4d\", \"\u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e\"],\n[\"\u767d\", \"\u0431\u0435\u043b\u044b\u0445\"],\n[\"\u4eba\", null],\n[\"\uff0c\", null],\n[\"\u5176\u4f59\", \"\u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0435\"],\n[\"\u4e3a\", null],\n[\"\u9ed1\", \"\u0447\u0435\u0440\u043d\u044b\u0435\"],\n[\"\u4eba\", null],\n[\" -\", null],\n[\"\", null],\n[\"\", null],\n[\"\", null],\n[\"\u5728\", null],\n[\"\u4e00\u95f4\", \"\u0432 \u043e\u0434\u043d\u043e\u0439\"],\n[\"\u5c4b\", \"\u043a\u043e\u043c\u043d\u0430\u0442\u0435\"],\n[\"\u5b50\u91cc\", null],\n[\"\uff0c\", null],\n[\"\u542c\u53d6\", \"\u0441\u043b\u0443\u0448\u0430\u043b\u0438\"],\n[\"\u683c\u6797\", \"\u0413\u0440\u0438\u043d\"],\n[\"\u53d1\u5e03\", \"\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u043b\"],\n[\"\u6392\", null],\n[\"\u7ec3\", null],\n[\"\u597d\u7684\", null],\n[\"\u6f14\u51fa\", \"\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f\"],\n[\"\u6982\", \"\u043e\u0431\u0437\u043e\u0440\"],\n[\"\u89c8\", null],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u6bd4\u5982\uff0c\u4e9a\u9a6c\u900a\u5728\u7f8e\u56fd\u5728\u7ebf\u96f6\u552e\u5e02\u573a\u7684\u4efd\u989d\u5df2\u8fbe\uff13\uff14\uff05\uff0c\uff12\uff10\uff12\uff10\u5e74\u66f4\u6709\u671b\u5347\u81f3\uff15\uff10\uff05\uff0c\u4f46\u5728\u4e2d\u56fd\uff0c\u4e9a\u9a6c\u900a\u7684\u5e02\u573a\u4efd\u989d\u8fd8\u4e0d\u5230\uff11\uff05\uff0c\u8fdc\u8fdc\u843d\u540e\u4e8e\u963f\u91cc\u5df4\u5df4\u548c\u4eac\u4e1c\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u6bd4\u5982', '\uff0c', '\u4e9a\u9a6c\u900a', '\u5728\u7f8e\u56fd', '\u5728\u7ebf', '\u96f6\u552e', '\u5e02\u573a\u7684', '\u4efd\u989d', '\u5df2\u8fbe', '\uff13', '\uff14', '\uff05', '\uff0c', '\uff12', '\uff10', '\uff12', '\uff10', '\u5e74', '\u66f4\u6709', '\u671b', '\u5347', '\u81f3', '\uff15', '\uff10', '\uff05', '\uff0c', '\u4f46', '\u5728\u4e2d\u56fd', '\uff0c', '\u4e9a\u9a6c\u900a', '\u7684', '\u5e02\u573a\u4efd\u989d', '\u8fd8', '\u4e0d\u5230', '\uff11', '\uff05', '\uff0c', '\u8fdc\u8fdc', '\u843d\u540e', '\u4e8e', '\u963f\u91cc\u5df4\u5df4', '\u548c', '\u4eac\u4e1c', '\u3002']"}], "[\n[\"\u6bd4\u5982\", \"For example\"],\n[\"\uff0c\", null],\n[\"\u4e9a\u9a6c\u900a\", \"Amazon\"],\n[\"\u5728\u7f8e\u56fd\", \"in the United States\"],\n[\"\u5728\u7ebf\", \"online\"],\n[\"\u96f6\u552e\", \"retail\"],\n[\"\u5e02\u573a\u7684\", null],\n[\"\u4efd\u989d\", \"market share\"],\n[\"\u5df2\u8fbe\", \"has reached\"],\n[\"\uff13\", null],\n[\"\uff14\", null],\n[\"\uff05\", null],\n[\"\uff0c\", null],\n[\"\uff12\", null],\n[\"\uff10\", null],\n[\"\uff12\", null],\n[\"\uff10\", null],\n[\"\u5e74\", \"year\"],\n[\"\u66f4\u6709\", \"is even more\"],\n[\"\u671b\", \"likely\"],\n[\"\u5347\", \"to rise\"],\n[\"\u81f3\", \"to\"],\n[\"\uff15\", null],\n[\"\uff10\", null],\n[\"\uff05\", null],\n[\"\uff0c\", null],\n[\"\u4f46\", \"but\"],\n[\"\u5728\u4e2d\u56fd\", \"in China\"],\n[\"\uff0c\", null],\n[\"\u4e9a\u9a6c\u900a\", \"Amazon\"],\n[\"\u7684\", null],\n[\"\u5e02\u573a\u4efd\u989d\", \"market share\"],\n[\"\u8fd8\", \"still\"],\n[\"\u4e0d\u5230\", \"less than\"],\n[\"\uff11\", null],\n[\"\uff05\", null],\n[\"\uff0c\", null],\n[\"\u8fdc\u8fdc\", \"far\"],\n[\"\u843d\u540e\", \"behind\"],\n[\"\u4e8e\", null],\n[\"\u963f\u91cc\u5df4\u5df4\", \"Alibaba\"],\n[\"\u548c\", \"and\"],\n[\"\u4eac\u4e1c\", \"JD.com\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u6bd4\u5982\uff0c\u4e9a\u9a6c\u900a\u5728\u7f8e\u56fd\u5728\u7ebf\u96f6\u552e\u5e02\u573a\u7684\u4efd\u989d\u5df2\u8fbe\uff13\uff14\uff05\uff0c\uff12\uff10\uff12\uff10\u5e74\u66f4\u6709\u671b\u5347\u81f3\uff15\uff10\uff05\uff0c\u4f46\u5728\u4e2d\u56fd\uff0c\u4e9a\u9a6c\u900a\u7684\u5e02\u573a\u4efd\u989d\u8fd8\u4e0d\u5230\uff11\uff05\uff0c\u8fdc\u8fdc\u843d\u540e\u4e8e\u963f\u91cc\u5df4\u5df4\u548c\u4eac\u4e1c\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u6bd4\u5982', '\uff0c', '\u4e9a\u9a6c\u900a', '\u5728\u7f8e\u56fd', '\u5728\u7ebf', '\u96f6\u552e', '\u5e02\u573a\u7684', '\u4efd\u989d', '\u5df2\u8fbe', '\uff13', '\uff14', '\uff05', '\uff0c', '\uff12', '\uff10', '\uff12', '\uff10', '\u5e74', '\u66f4\u6709', '\u671b', '\u5347', '\u81f3', '\uff15', '\uff10', '\uff05', '\uff0c', '\u4f46', '\u5728\u4e2d\u56fd', '\uff0c', '\u4e9a\u9a6c\u900a', '\u7684', '\u5e02\u573a\u4efd\u989d', '\u8fd8', '\u4e0d\u5230', '\uff11', '\uff05', '\uff0c', '\u8fdc\u8fdc', '\u843d\u540e', '\u4e8e', '\u963f\u91cc\u5df4\u5df4', '\u548c', '\u4eac\u4e1c', '\u3002']"}], "[\n  [\"\u6bd4\u5982\", \"\u043c\u044b\u0441\u0430\u043b\u044b\"],\n  [\"\uff0c\", null],\n  [\"\u4e9a\u9a6c\u900a\", \"Amazon\"],\n  [\"\u5728\u7f8e\u56fd\", \"\u0410\u049a\u0428-\u0442\u0430\u0493\u044b\"],\n  [\"\u5728\u7ebf\", \"\u043e\u043d\u043b\u0430\u0439\u043d\"],\n  [\"\u96f6\u552e\", \"\u0431\u04e9\u043b\u0448\u0435\u043a \u0441\u0430\u0443\u0434\u0430\"],\n  [\"\u5e02\u573a\u7684\", null],\n  [\"\u4efd\u989d\", \"\u04af\u043b\u0435\u0441\u0456\"],\n  [\"\u5df2\u8fbe\", \"\u0436\u0435\u0442\u0442\u0456\"],\n  [\"\uff13\", null],\n  [\"\uff14\", null],\n  [\"\uff05\", null],\n  [\"\uff0c\", null],\n  [\"\uff12\", null],\n  [\"\uff10\", null],\n  [\"\uff12\", null],\n  [\"\uff10\", null],\n  [\"\u5e74\", \"\u0436\u044b\u043b\u044b\"],\n  [\"\u66f4\u6709\", \"\u043e\u0434\u0430\u043d \u0434\u0430\"],\n  [\"\u671b\", \"\u04af\u043c\u0456\u0442\"],\n  [\"\u5347\", \"\u04e9\u0441\u0443\"],\n  [\"\u81f3\", \"\u0434\u0435\u0439\u0456\u043d\"],\n  [\"\uff15\", null],\n  [\"\uff10\", null],\n  [\"\uff05\", null],\n  [\"\uff0c\", null],\n  [\"\u4f46\", \"\u0431\u0456\u0440\u0430\u049b\"],\n  [\"\u5728\u4e2d\u56fd\", \"\u049a\u044b\u0442\u0430\u0439\u0434\u0430\"],\n  [\"\uff0c\", null],\n  [\"\u4e9a\u9a6c\u900a\", \"Amazon\"],\n  [\"\u7684\", null],\n  [\"\u5e02\u573a\u4efd\u989d\", \"\u043d\u0430\u0440\u044b\u049b \u04af\u043b\u0435\u0441\u0456\"],\n  [\"\u8fd8\", \"\u04d9\u043b\u0456\"],\n  [\"\u4e0d\u5230\", \"\u0436\u0435\u0442\u043f\u0435\u0439\u0434\u0456\"],\n  [\"\uff11\", null],\n  [\"\uff05\", null],\n  [\"\uff0c\", null],\n  [\"\u8fdc\u8fdc\", \"\u04d9\u043b\u0434\u0435\u049b\u0430\u0439\u0434\u0430\"],\n  [\"\u843d\u540e\", \"\u0430\u0440\u0442\u0442\u0430 \u049b\u0430\u043b\u0434\u044b\"],\n  [\"\u4e8e\", null],\n  [\"\u963f\u91cc\u5df4\u5df4\", \"Alibaba\"],\n  [\"\u548c\", \"\u0436\u04d9\u043d\u0435\"],\n  [\"\u4eac\u4e1c\", \"JD.com\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u6bd4\u5982\uff0c\u4e9a\u9a6c\u900a\u5728\u7f8e\u56fd\u5728\u7ebf\u96f6\u552e\u5e02\u573a\u7684\u4efd\u989d\u5df2\u8fbe\uff13\uff14\uff05\uff0c\uff12\uff10\uff12\uff10\u5e74\u66f4\u6709\u671b\u5347\u81f3\uff15\uff10\uff05\uff0c\u4f46\u5728\u4e2d\u56fd\uff0c\u4e9a\u9a6c\u900a\u7684\u5e02\u573a\u4efd\u989d\u8fd8\u4e0d\u5230\uff11\uff05\uff0c\u8fdc\u8fdc\u843d\u540e\u4e8e\u963f\u91cc\u5df4\u5df4\u548c\u4eac\u4e1c\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u6bd4\u5982', '\uff0c', '\u4e9a\u9a6c\u900a', '\u5728\u7f8e\u56fd', '\u5728\u7ebf', '\u96f6\u552e', '\u5e02\u573a\u7684', '\u4efd\u989d', '\u5df2\u8fbe', '\uff13', '\uff14', '\uff05', '\uff0c', '\uff12', '\uff10', '\uff12', '\uff10', '\u5e74', '\u66f4\u6709', '\u671b', '\u5347', '\u81f3', '\uff15', '\uff10', '\uff05', '\uff0c', '\u4f46', '\u5728\u4e2d\u56fd', '\uff0c', '\u4e9a\u9a6c\u900a', '\u7684', '\u5e02\u573a\u4efd\u989d', '\u8fd8', '\u4e0d\u5230', '\uff11', '\uff05', '\uff0c', '\u8fdc\u8fdc', '\u843d\u540e', '\u4e8e', '\u963f\u91cc\u5df4\u5df4', '\u548c', '\u4eac\u4e1c', '\u3002']"}], "[\n    [\"\u6bd4\u5982\", \"\u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440\"],\n    [\"\uff0c\", null],\n    [\"\u4e9a\u9a6c\u900a\", \"Amazon\"],\n    [\"\u5728\u7f8e\u56fd\", \"\u0432 \u0421\u0428\u0410\"],\n    [\"\u5728\u7ebf\", \"\u043e\u043d\u043b\u0430\u0439\u043d\"],\n    [\"\u96f6\u552e\", \"\u0440\u043e\u0437\u043d\u0438\u0447\u043d\u0430\u044f \u0442\u043e\u0440\u0433\u043e\u0432\u043b\u044f\"],\n    [\"\u5e02\u573a\u7684\", null],\n    [\"\u4efd\u989d\", \"\u0434\u043e\u043b\u044f\"],\n    [\"\u5df2\u8fbe\", \"\u0434\u043e\u0441\u0442\u0438\u0433\u043b\u0430\"],\n    [\"\uff13\", null],\n    [\"\uff14\", null],\n    [\"\uff05\", null],\n    [\"\uff0c\", null],\n    [\"\uff12\", null],\n    [\"\uff10\", null],\n    [\"\uff12\", null],\n    [\"\uff10\", null],\n    [\"\u5e74\", \"\u0433\u043e\u0434\"],\n    [\"\u66f4\u6709\", \"\u0435\u0449\u0435 \u0431\u043e\u043b\u0435\u0435\"],\n    [\"\u671b\", \"\u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\"],\n    [\"\u5347\", \"\u0432\u044b\u0440\u0430\u0441\u0442\u0435\u0442\"],\n    [\"\u81f3\", \"\u0434\u043e\"],\n    [\"\uff15\", null],\n    [\"\uff10\", null],\n    [\"\uff05\", null],\n    [\"\uff0c\", null],\n    [\"\u4f46\", \"\u043d\u043e\"],\n    [\"\u5728\u4e2d\u56fd\", \"\u0432 \u041a\u0438\u0442\u0430\u0435\"],\n    [\"\uff0c\", null],\n    [\"\u4e9a\u9a6c\u900a\", \"Amazon\"],\n    [\"\u7684\", null],\n    [\"\u5e02\u573a\u4efd\u989d\", \"\u0434\u043e\u043b\u044f \u0440\u044b\u043d\u043a\u0430\"],\n    [\"\u8fd8\", \"\u0432\u0441\u0435 \u0435\u0449\u0435\"],\n    [\"\u4e0d\u5230\", \"\u043c\u0435\u043d\u0435\u0435\"],\n    [\"\uff11\", null],\n    [\"\uff05\", null],\n    [\"\uff0c\", null],\n    [\"\u8fdc\u8fdc\", \"\u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e\"],\n    [\"\u843d\u540e\", \"\u043e\u0442\u0441\u0442\u0430\u0435\u0442\"],\n    [\"\u4e8e\", \"\u043e\u0442\"],\n    [\"\u963f\u91cc\u5df4\u5df4\", \"Alibaba\"],\n    [\"\u548c\", \"\u0438\"],\n    [\"\u4eac\u4e1c\", \"JD.com\"],\n    [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u7136\u800c\uff0c\u8fd8\u6709\u4e00\u4e2a\u66f4\u52a0\u4ee4\u4eba\u62c5\u5fe7\u7684\u957f\u671f\u573a\u666f\uff0c\u7279\u6717\u666e\u8981\u201c\u6b63\u5e38\u5316\u201d\u7684\u662f\u4e0e\u53f3\u7ffc\u5171\u548c\u515a\u4eba\u7684\u5173\u7cfb\uff0c\u8fd9\u4e9b\u5171\u548c\u515a\u4eba\u4e4b\u524d\u66fe\u53cd\u5bf9\u603b\u7edf\u5148\u751f\u7684\u6807\u51c6\uff0c\u73b0\u5728\u5219\u7a81\u7136\u95f4\u770b\u8d77\u6765\u6210\u4e86\u7406\u6027\u7684\u8c61\u5f81\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u7136\u800c', '\uff0c', '\u8fd8\u6709\u4e00\u4e2a', '\u66f4\u52a0', '\u4ee4\u4eba', '\u62c5\u5fe7', '\u7684', '\u957f\u671f', '\u573a\u666f', '\uff0c', '\u7279\u6717\u666e', '\u8981', '\u201c', '\u6b63\u5e38', '\u5316', '\u201d\u7684', '\u662f', '\u4e0e', '\u53f3', '\u7ffc', '\u5171\u548c', '\u515a', '\u4eba\u7684', '\u5173\u7cfb', '\uff0c', '\u8fd9\u4e9b', '\u5171\u548c', '\u515a', '\u4eba', '\u4e4b\u524d', '\u66fe', '\u53cd\u5bf9', '\u603b\u7edf', '\u5148\u751f', '\u7684\u6807\u51c6', '\uff0c', '\u73b0\u5728', '\u5219', '\u7a81\u7136', '\u95f4', '\u770b\u8d77\u6765', '\u6210\u4e86', '\u7406', '\u6027\u7684', '\u8c61\u5f81', '\u3002']"}], "[\n    [\"\u7136\u800c\", \"However\"],\n    [\"\uff0c\", null],\n    [\"\u8fd8\u6709\u4e00\u4e2a\", \"there is another\"],\n    [\"\u66f4\u52a0\", \"more\"],\n    [\"\u4ee4\u4eba\", null],\n    [\"\u62c5\u5fe7\", \"worrying\"],\n    [\"\u7684\", null],\n    [\"\u957f\u671f\", \"long-term\"],\n    [\"\u573a\u666f\", \"scenario\"],\n    [\"\uff0c\", null],\n    [\"\u7279\u6717\u666e\", \"Trump\"],\n    [\"\u8981\", null],\n    [\"\"\", null],\n    [\"\u6b63\u5e38\", \"normalize\"],\n    [\"\u5316\", null],\n    [\"\"\u7684\", null],\n    [\"\u662f\", null],\n    [\"\u4e0e\", null],\n    [\"\u53f3\", \"right\"],\n    [\"\u7ffc\", null],\n    [\"\u5171\u548c\", \"Republican\"],\n    [\"\u515a\", null],\n    [\"\u4eba\u7684\", null],\n    [\"\u5173\u7cfb\", \"relationship\"],\n    [\"\uff0c\", null],\n    [\"\u8fd9\u4e9b\", \"these\"],\n    [\"\u5171\u548c\", \"Republican\"],\n    [\"\u515a\", null],\n    [\"\u4eba\", null],\n    [\"\u4e4b\u524d\", \"previously\"],\n    [\"\u66fe\", null],\n    [\"\u53cd\u5bf9\", \"opposed\"],\n    [\"\u603b\u7edf\", \"president\"],\n    [\"\u5148\u751f\", null],\n    [\"\u7684\u6807\u51c6\", \"standards\"],\n    [\"\uff0c\", null],\n    [\"\u73b0\u5728\", \"now\"],\n    [\"\u5219\", null],\n    [\"\u7a81\u7136\", \"suddenly\"],\n    [\"\u95f4\", null],\n    [\"\u770b\u8d77\u6765\", \"seem\"],\n    [\"\u6210\u4e86\", \"become\"],\n    [\"\u7406\", \"rational\"],\n    [\"\u6027\u7684\", null],\n    [\"\u8c61\u5f81\", \"symbol\"],\n    [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u7136\u800c\uff0c\u8fd8\u6709\u4e00\u4e2a\u66f4\u52a0\u4ee4\u4eba\u62c5\u5fe7\u7684\u957f\u671f\u573a\u666f\uff0c\u7279\u6717\u666e\u8981\u201c\u6b63\u5e38\u5316\u201d\u7684\u662f\u4e0e\u53f3\u7ffc\u5171\u548c\u515a\u4eba\u7684\u5173\u7cfb\uff0c\u8fd9\u4e9b\u5171\u548c\u515a\u4eba\u4e4b\u524d\u66fe\u53cd\u5bf9\u603b\u7edf\u5148\u751f\u7684\u6807\u51c6\uff0c\u73b0\u5728\u5219\u7a81\u7136\u95f4\u770b\u8d77\u6765\u6210\u4e86\u7406\u6027\u7684\u8c61\u5f81\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u7136\u800c', '\uff0c', '\u8fd8\u6709\u4e00\u4e2a', '\u66f4\u52a0', '\u4ee4\u4eba', '\u62c5\u5fe7', '\u7684', '\u957f\u671f', '\u573a\u666f', '\uff0c', '\u7279\u6717\u666e', '\u8981', '\u201c', '\u6b63\u5e38', '\u5316', '\u201d\u7684', '\u662f', '\u4e0e', '\u53f3', '\u7ffc', '\u5171\u548c', '\u515a', '\u4eba\u7684', '\u5173\u7cfb', '\uff0c', '\u8fd9\u4e9b', '\u5171\u548c', '\u515a', '\u4eba', '\u4e4b\u524d', '\u66fe', '\u53cd\u5bf9', '\u603b\u7edf', '\u5148\u751f', '\u7684\u6807\u51c6', '\uff0c', '\u73b0\u5728', '\u5219', '\u7a81\u7136', '\u95f4', '\u770b\u8d77\u6765', '\u6210\u4e86', '\u7406', '\u6027\u7684', '\u8c61\u5f81', '\u3002']"}], "[\n[\"\u7136\u800c\", \"\u0430\u043b\u0430\u0439\u0434\u0430\"],\n[\"\uff0c\", null],\n[\"\u8fd8\u6709\u4e00\u4e2a\", \"\u0442\u0430\u0493\u044b \u0431\u0456\u0440\"],\n[\"\u66f4\u52a0\", \"\u043e\u0434\u0430\u043d \u0434\u0430\"],\n[\"\u4ee4\u4eba\", null],\n[\"\u62c5\u5fe7\", \"\u0430\u043b\u0430\u04a3\u0434\u0430\u0442\u0430\u0440\u043b\u044b\u049b\"],\n[\"\u7684\", null],\n[\"\u957f\u671f\", \"\u04b1\u0437\u0430\u049b \u043c\u0435\u0440\u0437\u0456\u043c\u0434\u0456\"],\n[\"\u573a\u666f\", \"\u0441\u0446\u0435\u043d\u0430\u0440\u0438\u0439\"],\n[\"\uff0c\", null],\n[\"\u7279\u6717\u666e\", \"\u0422\u0440\u0430\u043c\u043f\"],\n[\"\u8981\", null],\n['\"', null],\n[\"\u6b63\u5e38\", \"\u049b\u0430\u043b\u044b\u043f\u0442\u044b\"],\n[\"\u5316\", null],\n['\"\u7684', null],\n[\"\u662f\", null],\n[\"\u4e0e\", null],\n[\"\u53f3\", \"\u043e\u04a3\"],\n[\"\u7ffc\", null],\n[\"\u5171\u548c\", \"\u0440\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430\u043b\u044b\u049b\"],\n[\"\u515a\", null],\n[\"\u4eba\u7684\", null],\n[\"\u5173\u7cfb\", \"\u049b\u0430\u0442\u044b\u043d\u0430\u0441\u0442\u0430\u0440\"],\n[\"\uff0c\", null],\n[\"\u8fd9\u4e9b\", \"\u0431\u04b1\u043b\"],\n[\"\u5171\u548c\", \"\u0440\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430\u043b\u044b\u049b\"],\n[\"\u515a\", null],\n[\"\u4eba\", null],\n[\"\u4e4b\u524d\", \"\u0431\u04b1\u0440\u044b\u043d\"],\n[\"\u66fe\", null],\n[\"\u53cd\u5bf9\", \"\u049b\u0430\u0440\u0441\u044b \u0431\u043e\u043b\u0493\u0430\u043d\"],\n[\"\u603b\u7edf\", \"\u043f\u0440\u0435\u0437\u0438\u0434\u0435\u043d\u0442\"],\n[\"\u5148\u751f\", null],\n[\"\u7684\u6807\u51c6\", \"\u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0442\u0430\u0440\u044b\"],\n[\"\uff0c\", null],\n[\"\u73b0\u5728\", \"\u049b\u0430\u0437\u0456\u0440\"],\n[\"\u5219\", null],\n[\"\u7a81\u7136\", \"\u043a\u0435\u043d\u0435\u0442\u0442\u0435\u043d\"],\n[\"\u95f4\", null],\n[\"\u770b\u8d77\u6765\", \"\u043a\u04e9\u0440\u0456\u043d\u0435\u0434\u0456\"],\n[\"\u6210\u4e86\", null],\n[\"\u7406\", \"\u0430\u049b\u044b\u043b\"],\n[\"\u6027\u7684\", null],\n[\"\u8c61\u5f81\", \"\u0441\u0438\u043c\u0432\u043e\u043b\u044b\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u7136\u800c\uff0c\u8fd8\u6709\u4e00\u4e2a\u66f4\u52a0\u4ee4\u4eba\u62c5\u5fe7\u7684\u957f\u671f\u573a\u666f\uff0c\u7279\u6717\u666e\u8981\u201c\u6b63\u5e38\u5316\u201d\u7684\u662f\u4e0e\u53f3\u7ffc\u5171\u548c\u515a\u4eba\u7684\u5173\u7cfb\uff0c\u8fd9\u4e9b\u5171\u548c\u515a\u4eba\u4e4b\u524d\u66fe\u53cd\u5bf9\u603b\u7edf\u5148\u751f\u7684\u6807\u51c6\uff0c\u73b0\u5728\u5219\u7a81\u7136\u95f4\u770b\u8d77\u6765\u6210\u4e86\u7406\u6027\u7684\u8c61\u5f81\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u7136\u800c', '\uff0c', '\u8fd8\u6709\u4e00\u4e2a', '\u66f4\u52a0', '\u4ee4\u4eba', '\u62c5\u5fe7', '\u7684', '\u957f\u671f', '\u573a\u666f', '\uff0c', '\u7279\u6717\u666e', '\u8981', '\u201c', '\u6b63\u5e38', '\u5316', '\u201d\u7684', '\u662f', '\u4e0e', '\u53f3', '\u7ffc', '\u5171\u548c', '\u515a', '\u4eba\u7684', '\u5173\u7cfb', '\uff0c', '\u8fd9\u4e9b', '\u5171\u548c', '\u515a', '\u4eba', '\u4e4b\u524d', '\u66fe', '\u53cd\u5bf9', '\u603b\u7edf', '\u5148\u751f', '\u7684\u6807\u51c6', '\uff0c', '\u73b0\u5728', '\u5219', '\u7a81\u7136', '\u95f4', '\u770b\u8d77\u6765', '\u6210\u4e86', '\u7406', '\u6027\u7684', '\u8c61\u5f81', '\u3002']"}], "[\n[\"\u7136\u800c\", \"\u041e\u0434\u043d\u0430\u043a\u043e\"],\n[\"\uff0c\", null],\n[\"\u8fd8\u6709\u4e00\u4e2a\", \"\u0415\u0441\u0442\u044c \u0435\u0449\u0435 \u043e\u0434\u0438\u043d\"],\n[\"\u66f4\u52a0\", \"\u0431\u043e\u043b\u0435\u0435\"],\n[\"\u4ee4\u4eba\", null],\n[\"\u62c5\u5fe7\", \"\u0442\u0440\u0435\u0432\u043e\u0436\u043d\u044b\u0439\"],\n[\"\u7684\", null],\n[\"\u957f\u671f\", \"\u0434\u043e\u043b\u0433\u043e\u0441\u0440\u043e\u0447\u043d\u044b\u0439\"],\n[\"\u573a\u666f\", \"\u0441\u0446\u0435\u043d\u0430\u0440\u0438\u0439\"],\n[\"\uff0c\", null],\n[\"\u7279\u6717\u666e\", \"\u0422\u0440\u0430\u043c\u043f\"],\n[\"\u8981\", null],\n[\"\"\", null],\n[\"\u6b63\u5e38\", \"\u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c\"],\n[\"\u5316\", null],\n[\"\"\u7684\", null],\n[\"\u662f\", null],\n[\"\u4e0e\", null],\n[\"\u53f3\", \"\u043f\u0440\u0430\u0432\u044b\u0445\"],\n[\"\u7ffc\", null],\n[\"\u5171\u548c\", \"\u0440\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430\u043d\u0446\u0435\u0432\"],\n[\"\u515a\", null],\n[\"\u4eba\u7684\", null],\n[\"\u5173\u7cfb\", \"\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044f\"],\n[\"\uff0c\", null],\n[\"\u8fd9\u4e9b\", \"\u044d\u0442\u0438\"],\n[\"\u5171\u548c\", \"\u0440\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430\u043d\u0446\u044b\"],\n[\"\u515a\", null],\n[\"\u4eba\", null],\n[\"\u4e4b\u524d\", \"\u0440\u0430\u043d\u0435\u0435\"],\n[\"\u66fe\", null],\n[\"\u53cd\u5bf9\", \"\u0432\u044b\u0441\u0442\u0443\u043f\u0430\u043b\u0438 \u043f\u0440\u043e\u0442\u0438\u0432\"],\n[\"\u603b\u7edf\", \"\u043f\u0440\u0435\u0437\u0438\u0434\u0435\u043d\u0442\u0430\"],\n[\"\u5148\u751f\", null],\n[\"\u7684\u6807\u51c6\", \"\u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043e\u0432\"],\n[\"\uff0c\", null],\n[\"\u73b0\u5728\", \"\u0442\u0435\u043f\u0435\u0440\u044c\"],\n[\"\u5219\", null],\n[\"\u7a81\u7136\", \"\u0432\u043d\u0435\u0437\u0430\u043f\u043d\u043e\"],\n[\"\u95f4\", null],\n[\"\u770b\u8d77\u6765\", \"\u043a\u0430\u0436\u0443\u0442\u0441\u044f\"],\n[\"\u6210\u4e86\", null],\n[\"\u7406\", \"\u0440\u0430\u0437\u0443\u043c\u0430\"],\n[\"\u6027\u7684\", null],\n[\"\u8c61\u5f81\", \"\u0441\u0438\u043c\u0432\u043e\u043b\u043e\u043c\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u56e0\u6b64\uff0c\u4ece\u5c31\u4e1a\u3001\u85aa\u916c\u548c\u7ecf\u6d4e\u7e41\u8363\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u5728\u82f1\u56fd\u8131\u6b27\uff0c\u751f\u6d3b\u6210\u672c\u4e0a\u6da8\u548c\u5236\u9020\u4e1a\u957f\u671f\u8870\u9000\u7684\u80cc\u666f\u4e0b\uff0c\u683c\u62c9\u65af\u54e5\u4e1c\u90e8\u5730\u533a\u8feb\u5207\u9700\u8981\u9ea6\u7ef4\u4ed6\u4e0d\u65ad\u6269\u5927\u751f\u4ea7\u7f51\u7edc\u3002\u201d\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u56e0\u6b64', '\uff0c', '\u4ece', '\u5c31\u4e1a', '\u3001', '\u85aa\u916c', '\u548c', '\u7ecf\u6d4e', '\u7e41\u8363', '\u7684\u89d2\u5ea6', '\u6765\u770b', '\uff0c\u5728', '\u82f1\u56fd', '\u8131', '\u6b27', '\uff0c', '\u751f\u6d3b', '\u6210\u672c', '\u4e0a\u6da8', '\u548c', '\u5236\u9020\u4e1a', '\u957f\u671f', '\u8870\u9000', '\u7684', '\u80cc\u666f\u4e0b', '\uff0c', '\u683c', '\u62c9\u65af', '\u54e5', '\u4e1c\u90e8', '\u5730\u533a', '\u8feb\u5207', '\u9700\u8981', '\u9ea6', '\u7ef4', '\u4ed6', '\u4e0d\u65ad\u6269\u5927', '\u751f\u4ea7', '\u7f51\u7edc', '\u3002\u201d']"}], "[\n  [\"\u56e0\u6b64\", \"Therefore\"],\n  [\"\uff0c\", null],\n  [\"\u4ece\", null],\n  [\"\u5c31\u4e1a\", \"employment\"],\n  [\"\u3001\", null],\n  [\"\u85aa\u916c\", \"wages\"],\n  [\"\u548c\", null],\n  [\"\u7ecf\u6d4e\", \"economic\"],\n  [\"\u7e41\u8363\", \"prosperity\"],\n  [\"\u7684\u89d2\u5ea6\", null],\n  [\"\u6765\u770b\", null],\n  [\"\uff0c\u5728\", null],\n  [\"\u82f1\u56fd\", \"UK\"],\n  [\"\u8131\", null],\n  [\"\u6b27\", \"Brexit\"],\n  [\"\uff0c\", null],\n  [\"\u751f\u6d3b\", \"living\"],\n  [\"\u6210\u672c\", \"costs\"],\n  [\"\u4e0a\u6da8\", \"rising\"],\n  [\"\u548c\", null],\n  [\"\u5236\u9020\u4e1a\", \"manufacturing\"],\n  [\"\u957f\u671f\", \"long-term\"],\n  [\"\u8870\u9000\", \"decline\"],\n  [\"\u7684\", null],\n  [\"\u80cc\u666f\u4e0b\", null],\n  [\"\uff0c\", null],\n  [\"\u683c\", null],\n  [\"\u62c9\u65af\", null],\n  [\"\u54e5\", \"Glasgow\"],\n  [\"\u4e1c\u90e8\", \"East\"],\n  [\"\u5730\u533a\", \"area\"],\n  [\"\u8feb\u5207\", \"urgently\"],\n  [\"\u9700\u8981\", \"needs\"],\n  [\"\u9ea6\", null],\n  [\"\u7ef4\", null],\n  [\"\u4ed6\", \"McVitie's\"],\n  [\"\u4e0d\u65ad\u6269\u5927\", \"continuously expand\"],\n  [\"\u751f\u4ea7\", \"production\"],\n  [\"\u7f51\u7edc\", \"network\"],\n  [\"\u3002\"\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u56e0\u6b64\uff0c\u4ece\u5c31\u4e1a\u3001\u85aa\u916c\u548c\u7ecf\u6d4e\u7e41\u8363\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u5728\u82f1\u56fd\u8131\u6b27\uff0c\u751f\u6d3b\u6210\u672c\u4e0a\u6da8\u548c\u5236\u9020\u4e1a\u957f\u671f\u8870\u9000\u7684\u80cc\u666f\u4e0b\uff0c\u683c\u62c9\u65af\u54e5\u4e1c\u90e8\u5730\u533a\u8feb\u5207\u9700\u8981\u9ea6\u7ef4\u4ed6\u4e0d\u65ad\u6269\u5927\u751f\u4ea7\u7f51\u7edc\u3002\u201d\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u56e0\u6b64', '\uff0c', '\u4ece', '\u5c31\u4e1a', '\u3001', '\u85aa\u916c', '\u548c', '\u7ecf\u6d4e', '\u7e41\u8363', '\u7684\u89d2\u5ea6', '\u6765\u770b', '\uff0c\u5728', '\u82f1\u56fd', '\u8131', '\u6b27', '\uff0c', '\u751f\u6d3b', '\u6210\u672c', '\u4e0a\u6da8', '\u548c', '\u5236\u9020\u4e1a', '\u957f\u671f', '\u8870\u9000', '\u7684', '\u80cc\u666f\u4e0b', '\uff0c', '\u683c', '\u62c9\u65af', '\u54e5', '\u4e1c\u90e8', '\u5730\u533a', '\u8feb\u5207', '\u9700\u8981', '\u9ea6', '\u7ef4', '\u4ed6', '\u4e0d\u65ad\u6269\u5927', '\u751f\u4ea7', '\u7f51\u7edc', '\u3002\u201d']"}], "[\n  [\"\u56e0\u6b64\", \"\u0421\u043e\u043d\u0434\u044b\u049b\u0442\u0430\u043d\"],\n  [\"\uff0c\", null],\n  [\"\u4ece\", null],\n  [\"\u5c31\u4e1a\", \"\u0436\u04b1\u043c\u044b\u0441\u043f\u0435\u043d \u049b\u0430\u043c\u0442\u0443\"],\n  [\"\u3001\", null],\n  [\"\u85aa\u916c\", \"\u0436\u0430\u043b\u0430\u049b\u044b\"],\n  [\"\u548c\", null],\n  [\"\u7ecf\u6d4e\", \"\u044d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0430\u043b\u044b\u049b\"],\n  [\"\u7e41\u8363\", \"\u04e9\u0440\u043a\u0435\u043d\u0434\u0435\u0443\"],\n  [\"\u7684\u89d2\u5ea6\", null],\n  [\"\u6765\u770b\", null],\n  [\"\uff0c\u5728\", null],\n  [\"\u82f1\u56fd\", \"\u04b0\u043b\u044b\u0431\u0440\u0438\u0442\u0430\u043d\u0438\u044f\"],\n  [\"\u8131\", null],\n  [\"\u6b27\", null],\n  [\"\uff0c\", null],\n  [\"\u751f\u6d3b\", \"\u0442\u04b1\u0440\u043c\u044b\u0441\"],\n  [\"\u6210\u672c\", \"\u0448\u044b\u0493\u044b\u043d\u0434\u0430\u0440\"],\n  [\"\u4e0a\u6da8\", \"\u04e9\u0441\u0443\u0456\"],\n  [\"\u548c\", null],\n  [\"\u5236\u9020\u4e1a\", \"\u04e9\u043d\u0434\u0456\u0440\u0456\u0441\"],\n  [\"\u957f\u671f\", \"\u04b1\u0437\u0430\u049b \u043c\u0435\u0440\u0437\u0456\u043c\u0434\u0456\"],\n  [\"\u8870\u9000\", \"\u049b\u04b1\u043b\u0434\u044b\u0440\u0430\u0443\"],\n  [\"\u7684\", null],\n  [\"\u80cc\u666f\u4e0b\", null],\n  [\"\uff0c\", null],\n  [\"\u683c\", null],\n  [\"\u62c9\u65af\", null],\n  [\"\u54e5\", null],\n  [\"\u4e1c\u90e8\", \"\u0448\u044b\u0493\u044b\u0441\"],\n  [\"\u5730\u533a\", \"\u0430\u0439\u043c\u0430\u049b\"],\n  [\"\u8feb\u5207\", \"\u0448\u04b1\u0493\u044b\u043b\"],\n  [\"\u9700\u8981\", \"\u049b\u0430\u0436\u0435\u0442\"],\n  [\"\u9ea6\", null],\n  [\"\u7ef4\", null],\n  [\"\u4ed6\", null],\n  [\"\u4e0d\u65ad\u6269\u5927\", \"\u04af\u0437\u0434\u0456\u043a\u0441\u0456\u0437 \u043a\u0435\u04a3\u0435\u0439\u0442\u0443\"],\n  [\"\u751f\u4ea7\", \"\u04e9\u043d\u0434\u0456\u0440\u0456\u0441\"],\n  [\"\u7f51\u7edc\", \"\u0436\u0435\u043b\u0456\u0441\u0456\"],\n  [\"\u3002\"\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u56e0\u6b64\uff0c\u4ece\u5c31\u4e1a\u3001\u85aa\u916c\u548c\u7ecf\u6d4e\u7e41\u8363\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u5728\u82f1\u56fd\u8131\u6b27\uff0c\u751f\u6d3b\u6210\u672c\u4e0a\u6da8\u548c\u5236\u9020\u4e1a\u957f\u671f\u8870\u9000\u7684\u80cc\u666f\u4e0b\uff0c\u683c\u62c9\u65af\u54e5\u4e1c\u90e8\u5730\u533a\u8feb\u5207\u9700\u8981\u9ea6\u7ef4\u4ed6\u4e0d\u65ad\u6269\u5927\u751f\u4ea7\u7f51\u7edc\u3002\u201d\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u56e0\u6b64', '\uff0c', '\u4ece', '\u5c31\u4e1a', '\u3001', '\u85aa\u916c', '\u548c', '\u7ecf\u6d4e', '\u7e41\u8363', '\u7684\u89d2\u5ea6', '\u6765\u770b', '\uff0c\u5728', '\u82f1\u56fd', '\u8131', '\u6b27', '\uff0c', '\u751f\u6d3b', '\u6210\u672c', '\u4e0a\u6da8', '\u548c', '\u5236\u9020\u4e1a', '\u957f\u671f', '\u8870\u9000', '\u7684', '\u80cc\u666f\u4e0b', '\uff0c', '\u683c', '\u62c9\u65af', '\u54e5', '\u4e1c\u90e8', '\u5730\u533a', '\u8feb\u5207', '\u9700\u8981', '\u9ea6', '\u7ef4', '\u4ed6', '\u4e0d\u65ad\u6269\u5927', '\u751f\u4ea7', '\u7f51\u7edc', '\u3002\u201d']"}], "[\n[\"\u56e0\u6b64\", \"\u041f\u043e\u044d\u0442\u043e\u043c\u0443\"],\n[\"\uff0c\", null],\n[\"\u4ece\", null],\n[\"\u5c31\u4e1a\", \"\u0437\u0430\u043d\u044f\u0442\u043e\u0441\u0442\u044c\"],\n[\"\u3001\", null],\n[\"\u85aa\u916c\", \"\u0437\u0430\u0440\u043f\u043b\u0430\u0442\u0430\"],\n[\"\u548c\", null],\n[\"\u7ecf\u6d4e\", \"\u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0439\"],\n[\"\u7e41\u8363\", \"\u043f\u0440\u043e\u0446\u0432\u0435\u0442\u0430\u043d\u0438\u0435\"],\n[\"\u7684\u89d2\u5ea6\", null],\n[\"\u6765\u770b\", null],\n[\"\uff0c\u5728\", null],\n[\"\u82f1\u56fd\", \"\u0412\u0435\u043b\u0438\u043a\u043e\u0431\u0440\u0438\u0442\u0430\u043d\u0438\u044f\"],\n[\"\u8131\", null],\n[\"\u6b27\", null],\n[\"\uff0c\", null],\n[\"\u751f\u6d3b\", \"\u0436\u0438\u0437\u043d\u044c\"],\n[\"\u6210\u672c\", \"\u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u044c\"],\n[\"\u4e0a\u6da8\", \"\u0440\u043e\u0441\u0442\"],\n[\"\u548c\", null],\n[\"\u5236\u9020\u4e1a\", \"\u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0441\u0442\u0432\u043e\"],\n[\"\u957f\u671f\", \"\u0434\u043e\u043b\u0433\u043e\u0441\u0440\u043e\u0447\u043d\u044b\u0439\"],\n[\"\u8870\u9000\", \"\u0441\u043f\u0430\u0434\"],\n[\"\u7684\", null],\n[\"\u80cc\u666f\u4e0b\", null],\n[\"\uff0c\", null],\n[\"\u683c\", null],\n[\"\u62c9\u65af\", null],\n[\"\u54e5\", null],\n[\"\u4e1c\u90e8\", \"\u0432\u043e\u0441\u0442\u043e\u0447\u043d\u044b\u0439\"],\n[\"\u5730\u533a\", \"\u0440\u0430\u0439\u043e\u043d\"],\n[\"\u8feb\u5207\", \"\u0441\u0440\u043e\u0447\u043d\u043e\"],\n[\"\u9700\u8981\", \"\u043d\u0443\u0436\u0434\u0430\u0442\u044c\u0441\u044f\"],\n[\"\u9ea6\", null],\n[\"\u7ef4\", null],\n[\"\u4ed6\", null],\n[\"\u4e0d\u65ad\u6269\u5927\", \"\u0440\u0430\u0441\u0448\u0438\u0440\u044f\u0442\u044c\"],\n[\"\u751f\u4ea7\", \"\u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0441\u0442\u0432\u043e\"],\n[\"\u7f51\u7edc\", \"\u0441\u0435\u0442\u044c\"],\n[\"\u3002\"\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u526f\u603b\u7406\u7a46\u7279\u79d1\u5728\u8bc4\u8bba\u4e16\u754c\u53cd\u5174\u594b\u5242\u673a\u6784\u8def\u7ebf\u56fe\u300a\u9075\u5b88\u89c4\u5b9a\u300b\u7684\u7b2c\u4e8c\u90e8\u5206\u65f6\u505a\u51fa\u4e0a\u8ff0\u8868\u8ff0\u3002\u8be5\u90e8\u5206\u89c4\u5b9a\u4e86\u6062\u590d RUSADA \u6743\u5229\u7684\u6807\u51c6\uff0c\u5e76\u4e8e\u6700\u8fd1\u5728\u8be5\u7ec4\u7ec7\u7f51\u7ad9\u4e0a\u53d1\u5e03\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u526f\u603b', '\u7406', '\u7a46', '\u7279', '\u79d1', '\u5728', '\u8bc4\u8bba', '\u4e16\u754c', '\u53cd', '\u5174\u594b', '\u5242', '\u673a\u6784', '\u8def\u7ebf', '\u56fe', '\u300a', '\u9075\u5b88', '\u89c4\u5b9a', '\u300b', '\u7684', '\u7b2c\u4e8c', '\u90e8\u5206', '\u65f6', '\u505a\u51fa', '\u4e0a\u8ff0', '\u8868\u8ff0', '\u3002', '\u8be5', '\u90e8\u5206', '\u89c4\u5b9a', '\u4e86', '\u6062\u590d', ' R', 'US', 'ADA', '', '', '\u5229', '\u7684\u6807\u51c6', '\uff0c\u5e76', '\u4e8e', '\u6700\u8fd1', '\u5728', '\u8be5', '\u7ec4\u7ec7', '\u7f51\u7ad9', '\u4e0a', '\u53d1\u5e03', '\u3002']"}], "[\n[\"\u526f\u603b\", \"Deputy Prime Minister\"],\n[\"\u7406\", null],\n[\"\u7a46\", \"Mutko\"],\n[\"\u7279\", null],\n[\"\u79d1\", null],\n[\"\u5728\", null],\n[\"\u8bc4\u8bba\", \"comment\"],\n[\"\u4e16\u754c\", \"World\"],\n[\"\u53cd\", \"Anti\"],\n[\"\u5174\u594b\", \"Doping\"],\n[\"\u5242\", null],\n[\"\u673a\u6784\", \"Agency\"],\n[\"\u8def\u7ebf\", \"roadmap\"],\n[\"\u56fe\", null],\n[\"\u300a\", null],\n[\"\u9075\u5b88\", \"Compliance\"],\n[\"\u89c4\u5b9a\", \"Regulations\"],\n[\"\u300b\", null],\n[\"\u7684\", null],\n[\"\u7b2c\u4e8c\", \"second\"],\n[\"\u90e8\u5206\", \"part\"],\n[\"\u65f6\", null],\n[\"\u505a\u51fa\", \"made\"],\n[\"\u4e0a\u8ff0\", \"above-mentioned\"],\n[\"\u8868\u8ff0\", \"statement\"],\n[\"\u3002\", null],\n[\"\u8be5\", null],\n[\"\u90e8\u5206\", \"part\"],\n[\"\u89c4\u5b9a\", \"stipulates\"],\n[\"\u4e86\", null],\n[\"\u6062\u590d\", \"restoration\"],\n[\" R\", \"RUSADA\"],\n[\"US\", null],\n[\"ADA\", null],\n[\"\", null],\n[\"\", null],\n[\"\u5229\", null],\n[\"\u7684\u6807\u51c6\", \"standards\"],\n[\"\uff0c\u5e76\", null],\n[\"\u4e8e\", null],\n[\"\u6700\u8fd1\", \"recently\"],\n[\"\u5728\", null],\n[\"\u8be5\", null],\n[\"\u7ec4\u7ec7\", \"organization\"],\n[\"\u7f51\u7ad9\", \"website\"],\n[\"\u4e0a\", null],\n[\"\u53d1\u5e03\", \"published\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u526f\u603b\u7406\u7a46\u7279\u79d1\u5728\u8bc4\u8bba\u4e16\u754c\u53cd\u5174\u594b\u5242\u673a\u6784\u8def\u7ebf\u56fe\u300a\u9075\u5b88\u89c4\u5b9a\u300b\u7684\u7b2c\u4e8c\u90e8\u5206\u65f6\u505a\u51fa\u4e0a\u8ff0\u8868\u8ff0\u3002\u8be5\u90e8\u5206\u89c4\u5b9a\u4e86\u6062\u590d RUSADA \u6743\u5229\u7684\u6807\u51c6\uff0c\u5e76\u4e8e\u6700\u8fd1\u5728\u8be5\u7ec4\u7ec7\u7f51\u7ad9\u4e0a\u53d1\u5e03\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u526f\u603b', '\u7406', '\u7a46', '\u7279', '\u79d1', '\u5728', '\u8bc4\u8bba', '\u4e16\u754c', '\u53cd', '\u5174\u594b', '\u5242', '\u673a\u6784', '\u8def\u7ebf', '\u56fe', '\u300a', '\u9075\u5b88', '\u89c4\u5b9a', '\u300b', '\u7684', '\u7b2c\u4e8c', '\u90e8\u5206', '\u65f6', '\u505a\u51fa', '\u4e0a\u8ff0', '\u8868\u8ff0', '\u3002', '\u8be5', '\u90e8\u5206', '\u89c4\u5b9a', '\u4e86', '\u6062\u590d', ' R', 'US', 'ADA', '', '', '\u5229', '\u7684\u6807\u51c6', '\uff0c\u5e76', '\u4e8e', '\u6700\u8fd1', '\u5728', '\u8be5', '\u7ec4\u7ec7', '\u7f51\u7ad9', '\u4e0a', '\u53d1\u5e03', '\u3002']"}], "[\n  [\"\u526f\u603b\", \"\u043e\u0440\u044b\u043d\u0431\u0430\u0441\u0430\u0440 \u043f\u0440\u0435\u043c\u044c\u0435\u0440-\u043c\u0438\u043d\u0438\u0441\u0442\u0440\"],\n  [\"\u7406\", null],\n  [\"\u7a46\", \"\u041c\u0443\u0442\u043a\u043e\"],\n  [\"\u7279\", null],\n  [\"\u79d1\", null],\n  [\"\u5728\", null],\n  [\"\u8bc4\u8bba\", \"\u043f\u0456\u043a\u0456\u0440 \u0431\u0456\u043b\u0434\u0456\u0440\u0443\"],\n  [\"\u4e16\u754c\", \"\u04d9\u043b\u0435\u043c\u0434\u0456\u043a\"],\n  [\"\u53cd\", \"\u049b\u0430\u0440\u0441\u044b\"],\n  [\"\u5174\u594b\", \"\u0434\u043e\u043f\u0438\u043d\u0433\"],\n  [\"\u5242\", null],\n  [\"\u673a\u6784\", \"\u0430\u0433\u0435\u043d\u0442\u0442\u0456\u043a\"],\n  [\"\u8def\u7ebf\", \"\u0436\u043e\u043b \u043a\u0430\u0440\u0442\u0430\u0441\u044b\"],\n  [\"\u56fe\", null],\n  [\"\u300a\", null],\n  [\"\u9075\u5b88\", \"\u0441\u04d9\u0439\u043a\u0435\u0441\u0442\u0456\u043a\"],\n  [\"\u89c4\u5b9a\", \"\u0435\u0440\u0435\u0436\u0435\u043b\u0435\u0440\"],\n  [\"\u300b\", null],\n  [\"\u7684\", null],\n  [\"\u7b2c\u4e8c\", \"\u0435\u043a\u0456\u043d\u0448\u0456\"],\n  [\"\u90e8\u5206\", \"\u0431\u04e9\u043b\u0456\u043c\"],\n  [\"\u65f6\", null],\n  [\"\u505a\u51fa\", \"\u0436\u0430\u0441\u0430\u0443\"],\n  [\"\u4e0a\u8ff0\", \"\u0436\u043e\u0493\u0430\u0440\u044b\u0434\u0430 \u0430\u0442\u0430\u043b\u0493\u0430\u043d\"],\n  [\"\u8868\u8ff0\", \"\u043c\u04d9\u043b\u0456\u043c\u0434\u0435\u043c\u0435\"],\n  [\"\u3002\", null],\n  [\"\u8be5\", \"\u0431\u04b1\u043b\"],\n  [\"\u90e8\u5206\", \"\u0431\u04e9\u043b\u0456\u043c\"],\n  [\"\u89c4\u5b9a\", \"\u0430\u043d\u044b\u049b\u0442\u0430\u0443\"],\n  [\"\u4e86\", null],\n  [\"\u6062\u590d\", \"\u049b\u0430\u043b\u043f\u044b\u043d\u0430 \u043a\u0435\u043b\u0442\u0456\u0440\u0443\"],\n  [\" R\", null],\n  [\"US\", null],\n  [\"ADA\", \"\u0420\u0423\u0421\u0410\u0414\u0410\"],\n  [\"\", null],\n  [\"\", null],\n  [\"\u5229\", null],\n  [\"\u7684\u6807\u51c6\", \"\u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0442\u0430\u0440\u044b\"],\n  [\"\uff0c\u5e76\", null],\n  [\"\u4e8e\", null],\n  [\"\u6700\u8fd1\", \"\u0436\u0430\u049b\u044b\u043d\u0434\u0430\"],\n  [\"\u5728\", null],\n  [\"\u8be5\", \"\u0431\u04b1\u043b\"],\n  [\"\u7ec4\u7ec7\", \"\u04b1\u0439\u044b\u043c\"],\n  [\"\u7f51\u7ad9\", \"\u0432\u0435\u0431-\u0441\u0430\u0439\u0442\"],\n  [\"\u4e0a\", null],\n  [\"\u53d1\u5e03\", \"\u0436\u0430\u0440\u0438\u044f\u043b\u0430\u0443\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u526f\u603b\u7406\u7a46\u7279\u79d1\u5728\u8bc4\u8bba\u4e16\u754c\u53cd\u5174\u594b\u5242\u673a\u6784\u8def\u7ebf\u56fe\u300a\u9075\u5b88\u89c4\u5b9a\u300b\u7684\u7b2c\u4e8c\u90e8\u5206\u65f6\u505a\u51fa\u4e0a\u8ff0\u8868\u8ff0\u3002\u8be5\u90e8\u5206\u89c4\u5b9a\u4e86\u6062\u590d RUSADA \u6743\u5229\u7684\u6807\u51c6\uff0c\u5e76\u4e8e\u6700\u8fd1\u5728\u8be5\u7ec4\u7ec7\u7f51\u7ad9\u4e0a\u53d1\u5e03\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u526f\u603b', '\u7406', '\u7a46', '\u7279', '\u79d1', '\u5728', '\u8bc4\u8bba', '\u4e16\u754c', '\u53cd', '\u5174\u594b', '\u5242', '\u673a\u6784', '\u8def\u7ebf', '\u56fe', '\u300a', '\u9075\u5b88', '\u89c4\u5b9a', '\u300b', '\u7684', '\u7b2c\u4e8c', '\u90e8\u5206', '\u65f6', '\u505a\u51fa', '\u4e0a\u8ff0', '\u8868\u8ff0', '\u3002', '\u8be5', '\u90e8\u5206', '\u89c4\u5b9a', '\u4e86', '\u6062\u590d', ' R', 'US', 'ADA', '', '', '\u5229', '\u7684\u6807\u51c6', '\uff0c\u5e76', '\u4e8e', '\u6700\u8fd1', '\u5728', '\u8be5', '\u7ec4\u7ec7', '\u7f51\u7ad9', '\u4e0a', '\u53d1\u5e03', '\u3002']"}], "[\n    [\"\u526f\u603b\", \"\u0412\u0438\u0446\u0435-\u043f\u0440\u0435\u043c\u044c\u0435\u0440\"],\n    [\"\u7406\", null],\n    [\"\u7a46\", \"\u041c\u0443\u0442\u043a\u043e\"],\n    [\"\u7279\", null],\n    [\"\u79d1\", null],\n    [\"\u5728\", null],\n    [\"\u8bc4\u8bba\", \"\u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0438\u0440\u0443\u044f\"],\n    [\"\u4e16\u754c\", \"\u0412\u0441\u0435\u043c\u0438\u0440\u043d\u043e\u0433\u043e\"],\n    [\"\u53cd\", null],\n    [\"\u5174\u594b\", \"\u0430\u043d\u0442\u0438\u0434\u043e\u043f\u0438\u043d\u0433\u043e\u0432\u043e\u0433\u043e\"],\n    [\"\u5242\", null],\n    [\"\u673a\u6784\", \"\u0430\u0433\u0435\u043d\u0442\u0441\u0442\u0432\u0430\"],\n    [\"\u8def\u7ebf\", \"\u0434\u043e\u0440\u043e\u0436\u043d\u0443\u044e\"],\n    [\"\u56fe\", \"\u043a\u0430\u0440\u0442\u0443\"],\n    [\"\u300a\", null],\n    [\"\u9075\u5b88\", \"\u0421\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0435\"],\n    [\"\u89c4\u5b9a\", \"\u043a\u043e\u0434\u0435\u043a\u0441\u0443\"],\n    [\"\u300b\", null],\n    [\"\u7684\", null],\n    [\"\u7b2c\u4e8c\", \"\u0432\u0442\u043e\u0440\u0443\u044e\"],\n    [\"\u90e8\u5206\", \"\u0447\u0430\u0441\u0442\u044c\"],\n    [\"\u65f6\", null],\n    [\"\u505a\u51fa\", \"\u0441\u0434\u0435\u043b\u0430\u043b\"],\n    [\"\u4e0a\u8ff0\", \"\u0432\u044b\u0448\u0435\u0443\u043f\u043e\u043c\u044f\u043d\u0443\u0442\u043e\u0435\"],\n    [\"\u8868\u8ff0\", \"\u0437\u0430\u044f\u0432\u043b\u0435\u043d\u0438\u0435\"],\n    [\"\u3002\", null],\n    [\"\u8be5\", \"\u042d\u0442\u0430\"],\n    [\"\u90e8\u5206\", \"\u0447\u0430\u0441\u0442\u044c\"],\n    [\"\u89c4\u5b9a\", \"\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442\"],\n    [\"\u4e86\", null],\n    [\"\u6062\u590d\", \"\u0432\u043e\u0441\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u044f\"],\n    [\" R\", \"\u0420\u0423\u0421\u0410\u0414\u0410\"],\n    [\"US\", null],\n    [\"ADA\", null],\n    [\"\", null],\n    [\"\", null],\n    [\"\u5229\", null],\n    [\"\u7684\u6807\u51c6\", \"\u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0438\"],\n    [\"\uff0c\u5e76\", null],\n    [\"\u4e8e\", null],\n    [\"\u6700\u8fd1\", \"\u043d\u0435\u0434\u0430\u0432\u043d\u043e\"],\n    [\"\u5728\", null],\n    [\"\u8be5\", \"\u044d\u0442\u043e\u0439\"],\n    [\"\u7ec4\u7ec7\", \"\u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u0438\"],\n    [\"\u7f51\u7ad9\", \"\u0441\u0430\u0439\u0442\u0435\"],\n    [\"\u4e0a\", null],\n    [\"\u53d1\u5e03\", \"\u043e\u043f\u0443\u0431\u043b\u0438\u043a\u043e\u0432\u0430\u043d\u0430\"],\n    [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u8be5\u4fe1\u606f\u8868\u660e\u8be5\u76f4\u5347\u673a\u5c06\u4e8e 7 \u6708 17 \u65e5\u5927\u7ea6\u4e0a\u5348 7 \u70b9\u62b5\u8fbe\u5357\u8349\u576a\u53c2\u52a0\u4e0b\u5348 3 \u70b9\u7684\u6d3b\u52a8\uff0c\u5728\u5f53\u5929\u665a\u4e0a 6 \u70b9\u81f3 9 \u70b9\u8349\u576a\u6e05\u7a7a\u540e\u624d\u79bb\u5f00\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u8be5', '\u4fe1\u606f', '\u8868\u660e', '\u8be5', '\u76f4\u5347\u673a', '\u5c06\u4e8e', ' ', '7', ' ', '\u6708', ' ', '1', '7', ' \u65e5', '\u5927\u7ea6', '\u4e0a\u5348', ' ', '7', '', '', '\u62b5\u8fbe', '\u5357', '\u8349\u576a', '\u53c2\u52a0', '\u4e0b\u5348', ' ', '3', '', '', '\u7684', '\u6d3b\u52a8', '\uff0c\u5728', '\u5f53\u5929', '\u665a\u4e0a', ' ', '6', '', '', '\u81f3', ' ', '9', '', '', '\u8349\u576a', '\u6e05', '\u7a7a', '\u540e', '\u624d', '\u79bb\u5f00', '\u3002']"}], "[\n[\"\u8be5\", \"This\"],\n[\"\u4fe1\u606f\", \"information\"],\n[\"\u8868\u660e\", \"indicates\"],\n[\"\u8be5\", \"the\"],\n[\"\u76f4\u5347\u673a\", \"helicopter\"],\n[\"\u5c06\u4e8e\", null],\n[\" \", null],\n[\"7\", null],\n[\" \", null],\n[\"\u6708\", \"July\"],\n[\" \", null],\n[\"1\", null],\n[\"7\", null],\n[\" \u65e5\", null],\n[\"\u5927\u7ea6\", \"around\"],\n[\"\u4e0a\u5348\", \"morning\"],\n[\" \", null],\n[\"7\", null],\n[\"\", null],\n[\"\", null],\n[\"\u62b5\u8fbe\", \"arrive\"],\n[\"\u5357\", \"South\"],\n[\"\u8349\u576a\", \"Lawn\"],\n[\"\u53c2\u52a0\", \"attend\"],\n[\"\u4e0b\u5348\", \"afternoon\"],\n[\" \", null],\n[\"3\", null],\n[\"\", null],\n[\"\", null],\n[\"\u7684\", null],\n[\"\u6d3b\u52a8\", \"event\"],\n[\"\uff0c\u5728\", null],\n[\"\u5f53\u5929\", \"that day\"],\n[\"\u665a\u4e0a\", \"evening\"],\n[\" \", null],\n[\"6\", null],\n[\"\", null],\n[\"\", null],\n[\"\u81f3\", \"to\"],\n[\" \", null],\n[\"9\", null],\n[\"\", null],\n[\"\", null],\n[\"\u8349\u576a\", \"lawn\"],\n[\"\u6e05\", null],\n[\"\u7a7a\", \"cleared\"],\n[\"\u540e\", \"after\"],\n[\"\u624d\", \"only\"],\n[\"\u79bb\u5f00\", \"leave\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u8be5\u4fe1\u606f\u8868\u660e\u8be5\u76f4\u5347\u673a\u5c06\u4e8e 7 \u6708 17 \u65e5\u5927\u7ea6\u4e0a\u5348 7 \u70b9\u62b5\u8fbe\u5357\u8349\u576a\u53c2\u52a0\u4e0b\u5348 3 \u70b9\u7684\u6d3b\u52a8\uff0c\u5728\u5f53\u5929\u665a\u4e0a 6 \u70b9\u81f3 9 \u70b9\u8349\u576a\u6e05\u7a7a\u540e\u624d\u79bb\u5f00\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u8be5', '\u4fe1\u606f', '\u8868\u660e', '\u8be5', '\u76f4\u5347\u673a', '\u5c06\u4e8e', ' ', '7', ' ', '\u6708', ' ', '1', '7', ' \u65e5', '\u5927\u7ea6', '\u4e0a\u5348', ' ', '7', '', '', '\u62b5\u8fbe', '\u5357', '\u8349\u576a', '\u53c2\u52a0', '\u4e0b\u5348', ' ', '3', '', '', '\u7684', '\u6d3b\u52a8', '\uff0c\u5728', '\u5f53\u5929', '\u665a\u4e0a', ' ', '6', '', '', '\u81f3', ' ', '9', '', '', '\u8349\u576a', '\u6e05', '\u7a7a', '\u540e', '\u624d', '\u79bb\u5f00', '\u3002']"}], "[\n  [\"\u8be5\", null],\n  [\"\u4fe1\u606f\", \"\u0430\u049b\u043f\u0430\u0440\u0430\u0442\"],\n  [\"\u8868\u660e\", \"\u043a\u04e9\u0440\u0441\u0435\u0442\u0435\u0434\u0456\"],\n  [\"\u8be5\", null],\n  [\"\u76f4\u5347\u673a\", \"\u0442\u0456\u043a\u04b1\u0448\u0430\u049b\"],\n  [\"\u5c06\u4e8e\", null],\n  [\" \", null],\n  [\"7\", null],\n  [\" \", null],\n  [\"\u6708\", \"\u0430\u0439\"],\n  [\" \", null],\n  [\"1\", null],\n  [\"7\", null],\n  [\" \u65e5\", \"\u043a\u04af\u043d\u0456\"],\n  [\"\u5927\u7ea6\", \"\u0448\u0430\u043c\u0430\u043c\u0435\u043d\"],\n  [\"\u4e0a\u5348\", \"\u0442\u0430\u04a3\u0435\u0440\u0442\u0435\u04a3\"],\n  [\" \", null],\n  [\"7\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\u62b5\u8fbe\", \"\u043a\u0435\u043b\u0435\u0434\u0456\"],\n  [\"\u5357\", \"\u043e\u04a3\u0442\u04af\u0441\u0442\u0456\u043a\"],\n  [\"\u8349\u576a\", \"\u043a\u04e9\u0433\u0430\u043b\u0434\u044b\u049b\"],\n  [\"\u53c2\u52a0\", \"\u049b\u0430\u0442\u044b\u0441\u0443\"],\n  [\"\u4e0b\u5348\", \"\u0442\u04af\u0441\u0442\u0435\u043d \u043a\u0435\u0439\u0456\u043d\"],\n  [\" \", null],\n  [\"3\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\u7684\", null],\n  [\"\u6d3b\u52a8\", \"\u0456\u0441-\u0448\u0430\u0440\u0430\"],\n  [\"\uff0c\u5728\", null],\n  [\"\u5f53\u5929\", \"\u0441\u043e\u043b \u043a\u04af\u043d\u0456\"],\n  [\"\u665a\u4e0a\", \"\u043a\u0435\u0448\u043a\u0435\"],\n  [\" \", null],\n  [\"6\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\u81f3\", \"\u0431\u0430\u0441\u0442\u0430\u043f\"],\n  [\" \", null],\n  [\"9\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\u8349\u576a\", \"\u043a\u04e9\u0433\u0430\u043b\u0434\u044b\u049b\"],\n  [\"\u6e05\", null],\n  [\"\u7a7a\", \"\u0431\u043e\u0441\u0430\u0442\u044b\u043b\u0493\u0430\u043d\u043d\u0430\u043d\"],\n  [\"\u540e\", \"\u043a\u0435\u0439\u0456\u043d\"],\n  [\"\u624d\", \"\u0493\u0430\u043d\u0430\"],\n  [\"\u79bb\u5f00\", \"\u043a\u0435\u0442\u0435\u0434\u0456\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u8be5\u4fe1\u606f\u8868\u660e\u8be5\u76f4\u5347\u673a\u5c06\u4e8e 7 \u6708 17 \u65e5\u5927\u7ea6\u4e0a\u5348 7 \u70b9\u62b5\u8fbe\u5357\u8349\u576a\u53c2\u52a0\u4e0b\u5348 3 \u70b9\u7684\u6d3b\u52a8\uff0c\u5728\u5f53\u5929\u665a\u4e0a 6 \u70b9\u81f3 9 \u70b9\u8349\u576a\u6e05\u7a7a\u540e\u624d\u79bb\u5f00\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u8be5', '\u4fe1\u606f', '\u8868\u660e', '\u8be5', '\u76f4\u5347\u673a', '\u5c06\u4e8e', ' ', '7', ' ', '\u6708', ' ', '1', '7', ' \u65e5', '\u5927\u7ea6', '\u4e0a\u5348', ' ', '7', '', '', '\u62b5\u8fbe', '\u5357', '\u8349\u576a', '\u53c2\u52a0', '\u4e0b\u5348', ' ', '3', '', '', '\u7684', '\u6d3b\u52a8', '\uff0c\u5728', '\u5f53\u5929', '\u665a\u4e0a', ' ', '6', '', '', '\u81f3', ' ', '9', '', '', '\u8349\u576a', '\u6e05', '\u7a7a', '\u540e', '\u624d', '\u79bb\u5f00', '\u3002']"}], "[\n  [\"\u8be5\", null],\n  [\"\u4fe1\u606f\", \"\u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f\"],\n  [\"\u8868\u660e\", \"\u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442\"],\n  [\"\u8be5\", null],\n  [\"\u76f4\u5347\u673a\", \"\u0432\u0435\u0440\u0442\u043e\u043b\u0435\u0442\"],\n  [\"\u5c06\u4e8e\", null],\n  [\" \", null],\n  [\"7\", null],\n  [\" \", null],\n  [\"\u6708\", \"\u0438\u044e\u043b\u044f\"],\n  [\" \", null],\n  [\"1\", null],\n  [\"7\", null],\n  [\" \u65e5\", null],\n  [\"\u5927\u7ea6\", \"\u043e\u043a\u043e\u043b\u043e\"],\n  [\"\u4e0a\u5348\", \"\u0443\u0442\u0440\u0430\"],\n  [\" \", null],\n  [\"7\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\u62b5\u8fbe\", \"\u043f\u0440\u0438\u0431\u0443\u0434\u0435\u0442\"],\n  [\"\u5357\", \"\u044e\u0436\u043d\u0443\u044e\"],\n  [\"\u8349\u576a\", \"\u043b\u0443\u0436\u0430\u0439\u043a\u0443\"],\n  [\"\u53c2\u52a0\", \"\u0434\u043b\u044f \u0443\u0447\u0430\u0441\u0442\u0438\u044f\"],\n  [\"\u4e0b\u5348\", \"\u0434\u043d\u044f\"],\n  [\" \", null],\n  [\"3\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\u7684\", null],\n  [\"\u6d3b\u52a8\", \"\u043c\u0435\u0440\u043e\u043f\u0440\u0438\u044f\u0442\u0438\u0438\"],\n  [\"\uff0c\u5728\", null],\n  [\"\u5f53\u5929\", \"\u0442\u043e\u0433\u043e \u0436\u0435 \u0434\u043d\u044f\"],\n  [\"\u665a\u4e0a\", \"\u0432\u0435\u0447\u0435\u0440\u043e\u043c\"],\n  [\" \", null],\n  [\"6\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\u81f3\", \"\u0434\u043e\"],\n  [\" \", null],\n  [\"9\", null],\n  [\"\", null],\n  [\"\", null],\n  [\"\u8349\u576a\", \"\u043b\u0443\u0436\u0430\u0439\u043a\u0430\"],\n  [\"\u6e05\", null],\n  [\"\u7a7a\", \"\u043e\u0447\u0438\u0449\u0435\u043d\u0430\"],\n  [\"\u540e\", \"\u043f\u043e\u0441\u043b\u0435\"],\n  [\"\u624d\", null],\n  [\"\u79bb\u5f00\", \"\u0443\u043b\u0435\u0442\u0438\u0442\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u622a\u81f32017\u5e741\u6708\uff0c\u91cd\u5e86\u5168\u5e02\u5171\u5b8c\u94bb\u9875\u5ca9\u6c14\u4e95368\u53e3(\u4e0d\u542b\u53c2\u6570\u4e95)\uff0c\u6295\u4ea7261\u53e3\uff0c\u8fd1\u671f\uff0c\u968f\u7740\u4e2d\u56fd\u77f3\u5316\u5357\u5ddd\u533a\u5e73\u6865\u5357\u533a\u57df\u51854\u53e3\u9875\u5ca9\u6c14\u63a2\u4e95\u6d4b\u8bd5\u65e5\u4ea7\u91cf\u5747\u8d85\u8fc720\u4e07\u7acb\u65b9\u7c73\u7684\u597d\u6d88\u606f\u4f20\u6765\uff0c\u5168\u7403\u5317\u7f8e\u4ee5\u5916\u9996\u4e2a\u5546\u4e1a\u5316\u5f00\u91c7\u7684\u5927\u578b\u9875\u5ca9\u6c14\u7530\u4e2d\u56fd\u77f3\u5316\u6daa\u9675\u9875\u5ca9\u6c14\u7530\u6709\u671b\u6309\u7167\u8ba1\u5212\u5728\u4eca\u5e74\u5efa\u6210100\u4ebf\u7acb\u65b9\u7c73\u4ea7\u80fd\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u622a\u81f3', '2', '0', '1', '7', '\u5e74', '1', '\u6708', '\uff0c', '\u91cd\u5e86', '\u5168\u5e02', '\u5171', '\u5b8c', '\u94bb', '\u9875', '\u5ca9', '\u6c14', '\u4e95', '3', '6', '8', '\u53e3', '(', '\u4e0d\u542b', '\u53c2\u6570', '\u4e95', ')\uff0c', '\u6295\u4ea7', '2', '6', '1', '\u53e3', '\uff0c', '\u8fd1\u671f', '\uff0c', '\u968f\u7740', '\u4e2d\u56fd', '\u77f3\u5316', '\u5357', '\u5ddd', '\u533a', '\u5e73', '\u6865', '\u5357', '\u533a\u57df\u5185', '4', '\u53e3', '\u9875', '\u5ca9', '\u6c14', '\u63a2', '\u4e95', '\u6d4b\u8bd5', '\u65e5', '\u4ea7\u91cf', '\u5747', '\u8d85\u8fc7', '2', '0', '\u4e07', '\u7acb\u65b9\u7c73', '\u7684\u597d', '\u6d88\u606f', '\u4f20\u6765', '\uff0c', '\u5168\u7403', '\u5317\u7f8e', '\u4ee5\u5916', '\u9996\u4e2a', '\u5546\u4e1a\u5316', '\u5f00\u91c7', '\u7684', '\u5927\u578b', '\u9875', '\u5ca9', '\u6c14', '\u7530', '\u4e2d\u56fd', '\u77f3\u5316', '\u6daa', '\u9675', '\u9875', '\u5ca9', '\u6c14', '\u7530', '\u6709\u671b', '\u6309\u7167', '\u8ba1\u5212', '\u5728\u4eca\u5e74', '\u5efa\u6210', '1', '0', '0', '\u4ebf', '\u7acb\u65b9\u7c73', '\u4ea7\u80fd', '\u3002']"}], "[\n  [\"\u622a\u81f3\", \"As of\"],\n  [\"2\", null],\n  [\"0\", null],\n  [\"1\", null],\n  [\"7\", null],\n  [\"\u5e74\", null],\n  [\"1\", null],\n  [\"\u6708\", null],\n  [\"\uff0c\", null],\n  [\"\u91cd\u5e86\", \"Chongqing\"],\n  [\"\u5168\u5e02\", \"entire city\"],\n  [\"\u5171\", null],\n  [\"\u5b8c\", null],\n  [\"\u94bb\", \"drilled\"],\n  [\"\u9875\", null],\n  [\"\u5ca9\", null],\n  [\"\u6c14\", null],\n  [\"\u4e95\", \"wells\"],\n  [\"3\", null],\n  [\"6\", null],\n  [\"8\", null],\n  [\"\u53e3\", null],\n  [\"(\", null],\n  [\"\u4e0d\u542b\", \"excluding\"],\n  [\"\u53c2\u6570\", \"parameter\"],\n  [\"\u4e95\", null],\n  [\")\uff0c\", null],\n  [\"\u6295\u4ea7\", \"put into production\"],\n  [\"2\", null],\n  [\"6\", null],\n  [\"1\", null],\n  [\"\u53e3\", null],\n  [\"\uff0c\", null],\n  [\"\u8fd1\u671f\", \"Recently\"],\n  [\"\uff0c\", null],\n  [\"\u968f\u7740\", \"with\"],\n  [\"\u4e2d\u56fd\", \"China\"],\n  [\"\u77f3\u5316\", \"Petrochemical\"],\n  [\"\u5357\", null],\n  [\"\u5ddd\", null],\n  [\"\u533a\", null],\n  [\"\u5e73\", null],\n  [\"\u6865\", null],\n  [\"\u5357\", null],\n  [\"\u533a\u57df\u5185\", \"in the area\"],\n  [\"4\", null],\n  [\"\u53e3\", null],\n  [\"\u9875\", null],\n  [\"\u5ca9\", null],\n  [\"\u6c14\", null],\n  [\"\u63a2\", null],\n  [\"\u4e95\", \"wells\"],\n  [\"\u6d4b\u8bd5\", \"test\"],\n  [\"\u65e5\", \"daily\"],\n  [\"\u4ea7\u91cf\", \"production\"],\n  [\"\u5747\", \"average\"],\n  [\"\u8d85\u8fc7\", \"exceeding\"],\n  [\"2\", null],\n  [\"0\", null],\n  [\"\u4e07\", null],\n  [\"\u7acb\u65b9\u7c73\", \"cubic meters\"],\n  [\"\u7684\u597d\", \"good\"],\n  [\"\u6d88\u606f\", \"news\"],\n  [\"\u4f20\u6765\", \"arrived\"],\n  [\"\uff0c\", null],\n  [\"\u5168\u7403\", \"global\"],\n  [\"\u5317\u7f8e\", \"North America\"],\n  [\"\u4ee5\u5916\", \"outside\"],\n  [\"\u9996\u4e2a\", \"first\"],\n  [\"\u5546\u4e1a\u5316\", \"commercialized\"],\n  [\"\u5f00\u91c7\", \"exploitation\"],\n  [\"\u7684\", null],\n  [\"\u5927\u578b\", \"large-scale\"],\n  [\"\u9875\", null],\n  [\"\u5ca9\", null],\n  [\"\u6c14\", null],\n  [\"\u7530\", \"field\"],\n  [\"\u4e2d\u56fd\", \"China\"],\n  [\"\u77f3\u5316\", \"Petrochemical\"],\n  [\"\u6daa\", null],\n  [\"\u9675\", null],\n  [\"\u9875\", null],\n  [\"\u5ca9\", null],\n  [\"\u6c14\", null],\n  [\"\u7530\", \"field\"],\n  [\"\u6709\u671b\", \"expected\"],\n  [\"\u6309\u7167\", \"according to\"],\n  [\"\u8ba1\u5212\", \"plan\"],\n  [\"\u5728\u4eca\u5e74\", \"this year\"],\n  [\"\u5efa\u6210\", \"complete\"],\n  [\"1\", null],\n  [\"0\", null],\n  [\"0\", null],\n  [\"\u4ebf\", \"billion\"],\n  [\"\u7acb\u65b9\u7c73\", \"cubic meters\"],\n  [\"\u4ea7\u80fd\", \"production capacity\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u622a\u81f32017\u5e741\u6708\uff0c\u91cd\u5e86\u5168\u5e02\u5171\u5b8c\u94bb\u9875\u5ca9\u6c14\u4e95368\u53e3(\u4e0d\u542b\u53c2\u6570\u4e95)\uff0c\u6295\u4ea7261\u53e3\uff0c\u8fd1\u671f\uff0c\u968f\u7740\u4e2d\u56fd\u77f3\u5316\u5357\u5ddd\u533a\u5e73\u6865\u5357\u533a\u57df\u51854\u53e3\u9875\u5ca9\u6c14\u63a2\u4e95\u6d4b\u8bd5\u65e5\u4ea7\u91cf\u5747\u8d85\u8fc720\u4e07\u7acb\u65b9\u7c73\u7684\u597d\u6d88\u606f\u4f20\u6765\uff0c\u5168\u7403\u5317\u7f8e\u4ee5\u5916\u9996\u4e2a\u5546\u4e1a\u5316\u5f00\u91c7\u7684\u5927\u578b\u9875\u5ca9\u6c14\u7530\u4e2d\u56fd\u77f3\u5316\u6daa\u9675\u9875\u5ca9\u6c14\u7530\u6709\u671b\u6309\u7167\u8ba1\u5212\u5728\u4eca\u5e74\u5efa\u6210100\u4ebf\u7acb\u65b9\u7c73\u4ea7\u80fd\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u622a\u81f3', '2', '0', '1', '7', '\u5e74', '1', '\u6708', '\uff0c', '\u91cd\u5e86', '\u5168\u5e02', '\u5171', '\u5b8c', '\u94bb', '\u9875', '\u5ca9', '\u6c14', '\u4e95', '3', '6', '8', '\u53e3', '(', '\u4e0d\u542b', '\u53c2\u6570', '\u4e95', ')\uff0c', '\u6295\u4ea7', '2', '6', '1', '\u53e3', '\uff0c', '\u8fd1\u671f', '\uff0c', '\u968f\u7740', '\u4e2d\u56fd', '\u77f3\u5316', '\u5357', '\u5ddd', '\u533a', '\u5e73', '\u6865', '\u5357', '\u533a\u57df\u5185', '4', '\u53e3', '\u9875', '\u5ca9', '\u6c14', '\u63a2', '\u4e95', '\u6d4b\u8bd5', '\u65e5', '\u4ea7\u91cf', '\u5747', '\u8d85\u8fc7', '2', '0', '\u4e07', '\u7acb\u65b9\u7c73', '\u7684\u597d', '\u6d88\u606f', '\u4f20\u6765', '\uff0c', '\u5168\u7403', '\u5317\u7f8e', '\u4ee5\u5916', '\u9996\u4e2a', '\u5546\u4e1a\u5316', '\u5f00\u91c7', '\u7684', '\u5927\u578b', '\u9875', '\u5ca9', '\u6c14', '\u7530', '\u4e2d\u56fd', '\u77f3\u5316', '\u6daa', '\u9675', '\u9875', '\u5ca9', '\u6c14', '\u7530', '\u6709\u671b', '\u6309\u7167', '\u8ba1\u5212', '\u5728\u4eca\u5e74', '\u5efa\u6210', '1', '0', '0', '\u4ebf', '\u7acb\u65b9\u7c73', '\u4ea7\u80fd', '\u3002']"}], "[\n  [\"\u622a\u81f3\", \"\u0434\u0435\u0439\u0456\u043d\"],\n  [\"2\", null],\n  [\"0\", null],\n  [\"1\", null],\n  [\"7\", null],\n  [\"\u5e74\", null],\n  [\"1\", null],\n  [\"\u6708\", null],\n  [\"\uff0c\", null],\n  [\"\u91cd\u5e86\", \"\u0427\u0443\u043d\u0446\u0438\u043d\"],\n  [\"\u5168\u5e02\", \"\u0431\u04af\u043a\u0456\u043b \u049b\u0430\u043b\u0430\"],\n  [\"\u5171\", \"\u0431\u0430\u0440\u043b\u044b\u0493\u044b\"],\n  [\"\u5b8c\", null],\n  [\"\u94bb\", null],\n  [\"\u9875\", null],\n  [\"\u5ca9\", null],\n  [\"\u6c14\", null],\n  [\"\u4e95\", \"\u04b1\u04a3\u0493\u044b\u043c\u0430\"],\n  [\"3\", null],\n  [\"6\", null],\n  [\"8\", null],\n  [\"\u53e3\", null],\n  [\"(\", null],\n  [\"\u4e0d\u542b\", \"\u049b\u043e\u0441\u043f\u0430\u0493\u0430\u043d\u0434\u0430\"],\n  [\"\u53c2\u6570\", \"\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043b\u0456\u043a\"],\n  [\"\u4e95\", null],\n  [\")\uff0c\", null],\n  [\"\u6295\u4ea7\", \"\u043f\u0430\u0439\u0434\u0430\u043b\u0430\u043d\u0443\u0493\u0430 \u0431\u0435\u0440\u0456\u043b\u0434\u0456\"],\n  [\"2\", null],\n  [\"6\", null],\n  [\"1\", null],\n  [\"\u53e3\", null],\n  [\"\uff0c\", null],\n  [\"\u8fd1\u671f\", \"\u0436\u0430\u049b\u044b\u043d\u0434\u0430\"],\n  [\"\uff0c\", null],\n  [\"\u968f\u7740\", \"\u0441\u04d9\u0439\u043a\u0435\u0441\"],\n  [\"\u4e2d\u56fd\", \"\u049a\u044b\u0442\u0430\u0439\"],\n  [\"\u77f3\u5316\", \"\u043c\u04b1\u043d\u0430\u0439-\u0445\u0438\u043c\u0438\u044f\"],\n  [\"\u5357\", null],\n  [\"\u5ddd\", null],\n  [\"\u533a\", null],\n  [\"\u5e73\", null],\n  [\"\u6865\", null],\n  [\"\u5357\", null],\n  [\"\u533a\u57df\u5185\", \"\u0430\u0439\u043c\u0430\u0493\u044b\u043d\u0434\u0430\"],\n  [\"4\", null],\n  [\"\u53e3\", null],\n  [\"\u9875\", null],\n  [\"\u5ca9\", null],\n  [\"\u6c14\", null],\n  [\"\u63a2\", null],\n  [\"\u4e95\", \"\u0431\u0430\u0440\u043b\u0430\u0443 \u04b1\u04a3\u0493\u044b\u043c\u0430\u0441\u044b\"],\n  [\"\u6d4b\u8bd5\", \"\u0441\u044b\u043d\u0430\u049b\"],\n  [\"\u65e5\", \"\u043a\u04af\u043d\u0434\u0456\u043a\"],\n  [\"\u4ea7\u91cf\", \"\u04e9\u043d\u0456\u043c\"],\n  [\"\u5747\", \"\u043e\u0440\u0442\u0430\u0448\u0430\"],\n  [\"\u8d85\u8fc7\", \"\u0430\u0441\u0442\u044b\"],\n  [\"2\", null],\n  [\"0\", null],\n  [\"\u4e07\", null],\n  [\"\u7acb\u65b9\u7c73\", \"\u0442\u0435\u043a\u0448\u0435 \u043c\u0435\u0442\u0440\"],\n  [\"\u7684\u597d\", null],\n  [\"\u6d88\u606f\", \"\u0436\u0430\u04a3\u0430\u043b\u044b\u049b\"],\n  [\"\u4f20\u6765\", \"\u043a\u0435\u043b\u0434\u0456\"],\n  [\"\uff0c\", null],\n  [\"\u5168\u7403\", \"\u04d9\u043b\u0435\u043c\u0434\u0456\u043a\"],\n  [\"\u5317\u7f8e\", \"\u0421\u043e\u043b\u0442\u04af\u0441\u0442\u0456\u043a \u0410\u043c\u0435\u0440\u0438\u043a\u0430\"],\n  [\"\u4ee5\u5916\", \"\u0442\u044b\u0441\"],\n  [\"\u9996\u4e2a\", \"\u0430\u043b\u0493\u0430\u0448\u049b\u044b\"],\n  [\"\u5546\u4e1a\u5316\", \"\u043a\u043e\u043c\u043c\u0435\u0440\u0446\u0438\u044f\u043b\u044b\u049b\"],\n  [\"\u5f00\u91c7\", \"\u04e9\u043d\u0434\u0456\u0440\u0443\"],\n  [\"\u7684\", null],\n  [\"\u5927\u578b\", \"\u0456\u0440\u0456\"],\n  [\"\u9875\", null],\n  [\"\u5ca9\", null],\n  [\"\u6c14\", null],\n  [\"\u7530\", \"\u043a\u0435\u043d \u043e\u0440\u043d\u044b\"],\n  [\"\u4e2d\u56fd\", \"\u049a\u044b\u0442\u0430\u0439\"],\n  [\"\u77f3\u5316\", \"\u043c\u04b1\u043d\u0430\u0439-\u0445\u0438\u043c\u0438\u044f\"],\n  [\"\u6daa\", null],\n  [\"\u9675\", null],\n  [\"\u9875\", null],\n  [\"\u5ca9\", null],\n  [\"\u6c14\", null],\n  [\"\u7530\", \"\u043a\u0435\u043d \u043e\u0440\u043d\u044b\"],\n  [\"\u6709\u671b\", \"\u04af\u043c\u0456\u0442 \u0431\u0430\u0440\"],\n  [\"\u6309\u7167\", \"\u0441\u04d9\u0439\u043a\u0435\u0441\"],\n  [\"\u8ba1\u5212\", \"\u0436\u043e\u0441\u043f\u0430\u0440\"],\n  [\"\u5728\u4eca\u5e74\", \"\u0431\u0438\u044b\u043b\"],\n  [\"\u5efa\u6210\", \"\u049b\u04b1\u0440\u0443\"],\n  [\"1\", null],\n  [\"0\", null],\n  [\"0\", null],\n  [\"\u4ebf\", \"\u043c\u0438\u043b\u043b\u0438\u0430\u0440\u0434\"],\n  [\"\u7acb\u65b9\u7c73\", \"\u0442\u0435\u043a\u0448\u0435 \u043c\u0435\u0442\u0440\"],\n  [\"\u4ea7\u80fd\", \"\u04e9\u043d\u0434\u0456\u0440\u0456\u0441\u0442\u0456\u043a \u049b\u0443\u0430\u0442\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u622a\u81f32017\u5e741\u6708\uff0c\u91cd\u5e86\u5168\u5e02\u5171\u5b8c\u94bb\u9875\u5ca9\u6c14\u4e95368\u53e3(\u4e0d\u542b\u53c2\u6570\u4e95)\uff0c\u6295\u4ea7261\u53e3\uff0c\u8fd1\u671f\uff0c\u968f\u7740\u4e2d\u56fd\u77f3\u5316\u5357\u5ddd\u533a\u5e73\u6865\u5357\u533a\u57df\u51854\u53e3\u9875\u5ca9\u6c14\u63a2\u4e95\u6d4b\u8bd5\u65e5\u4ea7\u91cf\u5747\u8d85\u8fc720\u4e07\u7acb\u65b9\u7c73\u7684\u597d\u6d88\u606f\u4f20\u6765\uff0c\u5168\u7403\u5317\u7f8e\u4ee5\u5916\u9996\u4e2a\u5546\u4e1a\u5316\u5f00\u91c7\u7684\u5927\u578b\u9875\u5ca9\u6c14\u7530\u4e2d\u56fd\u77f3\u5316\u6daa\u9675\u9875\u5ca9\u6c14\u7530\u6709\u671b\u6309\u7167\u8ba1\u5212\u5728\u4eca\u5e74\u5efa\u6210100\u4ebf\u7acb\u65b9\u7c73\u4ea7\u80fd\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u622a\u81f3', '2', '0', '1', '7', '\u5e74', '1', '\u6708', '\uff0c', '\u91cd\u5e86', '\u5168\u5e02', '\u5171', '\u5b8c', '\u94bb', '\u9875', '\u5ca9', '\u6c14', '\u4e95', '3', '6', '8', '\u53e3', '(', '\u4e0d\u542b', '\u53c2\u6570', '\u4e95', ')\uff0c', '\u6295\u4ea7', '2', '6', '1', '\u53e3', '\uff0c', '\u8fd1\u671f', '\uff0c', '\u968f\u7740', '\u4e2d\u56fd', '\u77f3\u5316', '\u5357', '\u5ddd', '\u533a', '\u5e73', '\u6865', '\u5357', '\u533a\u57df\u5185', '4', '\u53e3', '\u9875', '\u5ca9', '\u6c14', '\u63a2', '\u4e95', '\u6d4b\u8bd5', '\u65e5', '\u4ea7\u91cf', '\u5747', '\u8d85\u8fc7', '2', '0', '\u4e07', '\u7acb\u65b9\u7c73', '\u7684\u597d', '\u6d88\u606f', '\u4f20\u6765', '\uff0c', '\u5168\u7403', '\u5317\u7f8e', '\u4ee5\u5916', '\u9996\u4e2a', '\u5546\u4e1a\u5316', '\u5f00\u91c7', '\u7684', '\u5927\u578b', '\u9875', '\u5ca9', '\u6c14', '\u7530', '\u4e2d\u56fd', '\u77f3\u5316', '\u6daa', '\u9675', '\u9875', '\u5ca9', '\u6c14', '\u7530', '\u6709\u671b', '\u6309\u7167', '\u8ba1\u5212', '\u5728\u4eca\u5e74', '\u5efa\u6210', '1', '0', '0', '\u4ebf', '\u7acb\u65b9\u7c73', '\u4ea7\u80fd', '\u3002']"}], "[\n[\"\u622a\u81f3\", \"\u041f\u043e \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u044e \u043d\u0430\"],\n[\"2\", null],\n[\"0\", null],\n[\"1\", null],\n[\"7\", null],\n[\"\u5e74\", null],\n[\"1\", null],\n[\"\u6708\", null],\n[\"\uff0c\", null],\n[\"\u91cd\u5e86\", \"\u0427\u0443\u043d\u0446\u0438\u043d\"],\n[\"\u5168\u5e02\", \"\u0432\u0435\u0441\u044c \u0433\u043e\u0440\u043e\u0434\"],\n[\"\u5171\", null],\n[\"\u5b8c\", null],\n[\"\u94bb\", \"\u043f\u0440\u043e\u0431\u0443\u0440\u0438\u0442\u044c\"],\n[\"\u9875\", null],\n[\"\u5ca9\", null],\n[\"\u6c14\", null],\n[\"\u4e95\", \"\u0441\u043a\u0432\u0430\u0436\u0438\u043d\u0430\"],\n[\"3\", null],\n[\"6\", null],\n[\"8\", null],\n[\"\u53e3\", null],\n[\"(\", null],\n[\"\u4e0d\u542b\", \"\u043d\u0435 \u0432\u043a\u043b\u044e\u0447\u0430\u044f\"],\n[\"\u53c2\u6570\", \"\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u0435\"],\n[\"\u4e95\", null],\n[\")\uff0c\", null],\n[\"\u6295\u4ea7\", \"\u0432\u0432\u0435\u0434\u0435\u043d\u043e \u0432 \u044d\u043a\u0441\u043f\u043b\u0443\u0430\u0442\u0430\u0446\u0438\u044e\"],\n[\"2\", null],\n[\"6\", null],\n[\"1\", null],\n[\"\u53e3\", null],\n[\"\uff0c\", null],\n[\"\u8fd1\u671f\", \"\u043d\u0435\u0434\u0430\u0432\u043d\u043e\"],\n[\"\uff0c\", null],\n[\"\u968f\u7740\", \"\u0441\"],\n[\"\u4e2d\u56fd\", \"\u041a\u0438\u0442\u0430\u0439\u0441\u043a\u0430\u044f\"],\n[\"\u77f3\u5316\", \"\u043d\u0435\u0444\u0442\u0435\u0445\u0438\u043c\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043a\u043e\u0440\u043f\u043e\u0440\u0430\u0446\u0438\u044f\"],\n[\"\u5357\", null],\n[\"\u5ddd\", null],\n[\"\u533a\", null],\n[\"\u5e73\", null],\n[\"\u6865\", null],\n[\"\u5357\", null],\n[\"\u533a\u57df\u5185\", \"\u0432 \u0440\u0430\u0439\u043e\u043d\u0435\"],\n[\"4\", null],\n[\"\u53e3\", null],\n[\"\u9875\", null],\n[\"\u5ca9\", null],\n[\"\u6c14\", null],\n[\"\u63a2\", null],\n[\"\u4e95\", \"\u0440\u0430\u0437\u0432\u0435\u0434\u043e\u0447\u043d\u044b\u0435 \u0441\u043a\u0432\u0430\u0436\u0438\u043d\u044b\"],\n[\"\u6d4b\u8bd5\", \"\u0442\u0435\u0441\u0442\"],\n[\"\u65e5\", null],\n[\"\u4ea7\u91cf\", \"\u0441\u0443\u0442\u043e\u0447\u043d\u0430\u044f \u0434\u043e\u0431\u044b\u0447\u0430\"],\n[\"\u5747\", \"\u0432 \u0441\u0440\u0435\u0434\u043d\u0435\u043c\"],\n[\"\u8d85\u8fc7\", \"\u043f\u0440\u0435\u0432\u044b\u0441\u0438\u043b\u0430\"],\n[\"2\", null],\n[\"0\", null],\n[\"\u4e07\", null],\n[\"\u7acb\u65b9\u7c73\", \"\u043a\u0443\u0431\u043e\u043c\u0435\u0442\u0440\u043e\u0432\"],\n[\"\u7684\u597d\", null],\n[\"\u6d88\u606f\", \"\u043d\u043e\u0432\u043e\u0441\u0442\u044c\"],\n[\"\u4f20\u6765\", \"\u043f\u043e\u0441\u0442\u0443\u043f\u0438\u043b\u0430\"],\n[\"\uff0c\", null],\n[\"\u5168\u7403\", \"\u043c\u0438\u0440\u043e\u0432\u043e\u0439\"],\n[\"\u5317\u7f8e\", \"\u0421\u0435\u0432\u0435\u0440\u043d\u0430\u044f \u0410\u043c\u0435\u0440\u0438\u043a\u0430\"],\n[\"\u4ee5\u5916\", \"\u0437\u0430 \u043f\u0440\u0435\u0434\u0435\u043b\u0430\u043c\u0438\"],\n[\"\u9996\u4e2a\", \"\u043f\u0435\u0440\u0432\u043e\u0435\"],\n[\"\u5546\u4e1a\u5316\", \"\u043a\u043e\u043c\u043c\u0435\u0440\u0447\u0435\u0441\u043a\u043e\u0435\"],\n[\"\u5f00\u91c7\", \"\u043e\u0441\u0432\u043e\u0435\u043d\u0438\u0435\"],\n[\"\u7684\", null],\n[\"\u5927\u578b\", \"\u043a\u0440\u0443\u043f\u043d\u043e\u0435\"],\n[\"\u9875\", null],\n[\"\u5ca9\", null],\n[\"\u6c14\", null],\n[\"\u7530\", \"\u043c\u0435\u0441\u0442\u043e\u0440\u043e\u0436\u0434\u0435\u043d\u0438\u0435\"],\n[\"\u4e2d\u56fd\", null],\n[\"\u77f3\u5316\", null],\n[\"\u6daa\", null],\n[\"\u9675\", null],\n[\"\u9875\", null],\n[\"\u5ca9\", null],\n[\"\u6c14\", null],\n[\"\u7530\", \"\u043c\u0435\u0441\u0442\u043e\u0440\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u0441\u043b\u0430\u043d\u0446\u0435\u0432\u043e\u0433\u043e \u0433\u0430\u0437\u0430 \u0424\u0443\u043b\u0438\u043d\"],\n[\"\u6709\u671b\", \"\u043f\u043b\u0430\u043d\u0438\u0440\u0443\u0435\u0442\"],\n[\"\u6309\u7167\", \"\u0432 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0438 \u0441\"],\n[\"\u8ba1\u5212\", \"\u043f\u043b\u0430\u043d\u043e\u043c\"],\n[\"\u5728\u4eca\u5e74\", \"\u0432 \u044d\u0442\u043e\u043c \u0433\u043e\u0434\u0443\"],\n[\"\u5efa\u6210\", \"\u0434\u043e\u0441\u0442\u0438\u0447\u044c\"],\n[\"1\", null],\n[\"0\", null],\n[\"0\", null],\n[\"\u4ebf\", null],\n[\"\u7acb\u65b9\u7c73\", \"\u043a\u0443\u0431\u043e\u043c\u0435\u0442\u0440\u043e\u0432\"],\n[\"\u4ea7\u80fd\", \"\u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0439 \u043c\u043e\u0449\u043d\u043e\u0441\u0442\u0438\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u7eff\u8272\u4ee3\u8868\u6bd4\u7279\u5e01\u7684\u5408\u6cd5\u5e02\u573a\uff1b\u6a58\u9ec4\u8272\u4ee3\u8868\u4e2d\u6027\u5e02\u573a\uff0c\u867d\u5c1a\u672a\u516c\u5f00\u5408\u6cd5\u5316\u6bd4\u7279\u5e01\u5730\u4f4d\uff0c\u4f46\u4e5f\u6ca1\u6709\u4e3b\u8981\u7684\u4f7f\u7528\u9650\u5236\uff1b\u6de1\u7c89\u8272\u4ee3\u8868\u53d7\u9650\u5236\u7684\u5e02\u573a\uff0c\u5373\u5f53\u5730\u653f\u5e9c\u5c1d\u8bd5\u901a\u8fc7\u7ea2\u7ebf\u653f\u7b56\u6216\u76d1\u7ba1\u6765\u65b9\u6cd5\u6570\u5b57\u8d27\u5e01\u7684\u4f7f\u7528\uff1b\u6df1\u7c89\u8272\u4ee3\u8868\u6bd4\u7279\u5e01\u88ab\u5b9a\u4e49\u4e3a\u975e\u6cd5\u7684\u56fd\u5bb6\uff1b\u7070\u8272\u662f\u5c1a\u672a\u7f6e\u8bc4\u7684\u56fd\u5bb6\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u5982\u4e0b', '\u56fe', '\u6240\u793a', '\uff0c', '\u7eff\u8272', '\u4ee3\u8868', '\u6bd4\u7279\u5e01', '\u7684', '\u5408\u6cd5', '\u5e02\u573a', '\uff1b', '\u6a58', '\u9ec4\u8272', '\u4ee3\u8868', '\u4e2d', '\u6027', '\u5e02\u573a', '\uff0c', '\u867d', '\u5c1a\u672a', '\u516c\u5f00', '\u5408\u6cd5', '\u5316', '\u6bd4\u7279\u5e01', '\u5730\u4f4d', '\uff0c', '\u4f46', '\u4e5f\u6ca1\u6709', '\u4e3b\u8981', '\u7684', '\u4f7f\u7528', '\u9650\u5236', '\uff1b', '\u6de1', '\u7c89\u8272', '\u4ee3\u8868', '\u53d7', '\u9650\u5236', '\u7684', '\u5e02\u573a', '\uff0c', '\u5373', '\u5f53\u5730\u653f\u5e9c', '\u5c1d\u8bd5', '\u901a\u8fc7', '\u7ea2\u7ebf', '\u653f\u7b56', '\u6216', '\u76d1\u7ba1', '\u6765', '\u65b9\u6cd5', '\u6570\u5b57\u8d27\u5e01', '\u7684', '\u4f7f\u7528', '\uff1b', '\u6df1', '\u7c89\u8272', '\u4ee3\u8868', '\u6bd4\u7279\u5e01', '\u88ab', '\u5b9a\u4e49', '\u4e3a', '\u975e\u6cd5', '\u7684', '\u56fd\u5bb6', '\uff1b', '\u7070\u8272', '\u662f', '\u5c1a\u672a', '\u7f6e', '\u8bc4', '\u7684', '\u56fd\u5bb6', '\u3002']"}], "[\n[\"\u5982\u4e0b\", \"As shown\"],\n[\"\u56fe\", \"figure\"],\n[\"\u6240\u793a\", null],\n[\"\uff0c\", null],\n[\"\u7eff\u8272\", \"green\"],\n[\"\u4ee3\u8868\", \"represents\"],\n[\"\u6bd4\u7279\u5e01\", \"Bitcoin\"],\n[\"\u7684\", null],\n[\"\u5408\u6cd5\", \"legal\"],\n[\"\u5e02\u573a\", \"markets\"],\n[\"\uff1b\", null],\n[\"\u6a58\", \"orange\"],\n[\"\u9ec4\u8272\", null],\n[\"\u4ee3\u8868\", \"represents\"],\n[\"\u4e2d\", null],\n[\"\u6027\", null],\n[\"\u5e02\u573a\", \"markets\"],\n[\"\uff0c\", null],\n[\"\u867d\", \"although\"],\n[\"\u5c1a\u672a\", \"not yet\"],\n[\"\u516c\u5f00\", \"publicly\"],\n[\"\u5408\u6cd5\", \"legalized\"],\n[\"\u5316\", null],\n[\"\u6bd4\u7279\u5e01\", \"Bitcoin\"],\n[\"\u5730\u4f4d\", \"status\"],\n[\"\uff0c\", null],\n[\"\u4f46\", \"but\"],\n[\"\u4e5f\u6ca1\u6709\", \"no\"],\n[\"\u4e3b\u8981\", \"major\"],\n[\"\u7684\", null],\n[\"\u4f7f\u7528\", \"usage\"],\n[\"\u9650\u5236\", \"restrictions\"],\n[\"\uff1b\", null],\n[\"\u6de1\", \"light\"],\n[\"\u7c89\u8272\", \"pink\"],\n[\"\u4ee3\u8868\", \"represents\"],\n[\"\u53d7\", null],\n[\"\u9650\u5236\", \"restricted\"],\n[\"\u7684\", null],\n[\"\u5e02\u573a\", \"markets\"],\n[\"\uff0c\", null],\n[\"\u5373\", \"namely\"],\n[\"\u5f53\u5730\u653f\u5e9c\", \"local governments\"],\n[\"\u5c1d\u8bd5\", \"attempt\"],\n[\"\u901a\u8fc7\", \"through\"],\n[\"\u7ea2\u7ebf\", \"red line\"],\n[\"\u653f\u7b56\", \"policies\"],\n[\"\u6216\", \"or\"],\n[\"\u76d1\u7ba1\", \"regulations\"],\n[\"\u6765\", null],\n[\"\u65b9\u6cd5\", \"methods\"],\n[\"\u6570\u5b57\u8d27\u5e01\", \"digital currencies\"],\n[\"\u7684\", null],\n[\"\u4f7f\u7528\", \"use\"],\n[\"\uff1b\", null],\n[\"\u6df1\", \"dark\"],\n[\"\u7c89\u8272\", \"pink\"],\n[\"\u4ee3\u8868\", \"represents\"],\n[\"\u6bd4\u7279\u5e01\", \"Bitcoin\"],\n[\"\u88ab\", null],\n[\"\u5b9a\u4e49\", \"defined\"],\n[\"\u4e3a\", \"as\"],\n[\"\u975e\u6cd5\", \"illegal\"],\n[\"\u7684\", null],\n[\"\u56fd\u5bb6\", \"countries\"],\n[\"\uff1b\", null],\n[\"\u7070\u8272\", \"gray\"],\n[\"\u662f\", \"is\"],\n[\"\u5c1a\u672a\", \"not yet\"],\n[\"\u7f6e\", null],\n[\"\u8bc4\", \"commented\"],\n[\"\u7684\", null],\n[\"\u56fd\u5bb6\", \"countries\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u7eff\u8272\u4ee3\u8868\u6bd4\u7279\u5e01\u7684\u5408\u6cd5\u5e02\u573a\uff1b\u6a58\u9ec4\u8272\u4ee3\u8868\u4e2d\u6027\u5e02\u573a\uff0c\u867d\u5c1a\u672a\u516c\u5f00\u5408\u6cd5\u5316\u6bd4\u7279\u5e01\u5730\u4f4d\uff0c\u4f46\u4e5f\u6ca1\u6709\u4e3b\u8981\u7684\u4f7f\u7528\u9650\u5236\uff1b\u6de1\u7c89\u8272\u4ee3\u8868\u53d7\u9650\u5236\u7684\u5e02\u573a\uff0c\u5373\u5f53\u5730\u653f\u5e9c\u5c1d\u8bd5\u901a\u8fc7\u7ea2\u7ebf\u653f\u7b56\u6216\u76d1\u7ba1\u6765\u65b9\u6cd5\u6570\u5b57\u8d27\u5e01\u7684\u4f7f\u7528\uff1b\u6df1\u7c89\u8272\u4ee3\u8868\u6bd4\u7279\u5e01\u88ab\u5b9a\u4e49\u4e3a\u975e\u6cd5\u7684\u56fd\u5bb6\uff1b\u7070\u8272\u662f\u5c1a\u672a\u7f6e\u8bc4\u7684\u56fd\u5bb6\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u5982\u4e0b', '\u56fe', '\u6240\u793a', '\uff0c', '\u7eff\u8272', '\u4ee3\u8868', '\u6bd4\u7279\u5e01', '\u7684', '\u5408\u6cd5', '\u5e02\u573a', '\uff1b', '\u6a58', '\u9ec4\u8272', '\u4ee3\u8868', '\u4e2d', '\u6027', '\u5e02\u573a', '\uff0c', '\u867d', '\u5c1a\u672a', '\u516c\u5f00', '\u5408\u6cd5', '\u5316', '\u6bd4\u7279\u5e01', '\u5730\u4f4d', '\uff0c', '\u4f46', '\u4e5f\u6ca1\u6709', '\u4e3b\u8981', '\u7684', '\u4f7f\u7528', '\u9650\u5236', '\uff1b', '\u6de1', '\u7c89\u8272', '\u4ee3\u8868', '\u53d7', '\u9650\u5236', '\u7684', '\u5e02\u573a', '\uff0c', '\u5373', '\u5f53\u5730\u653f\u5e9c', '\u5c1d\u8bd5', '\u901a\u8fc7', '\u7ea2\u7ebf', '\u653f\u7b56', '\u6216', '\u76d1\u7ba1', '\u6765', '\u65b9\u6cd5', '\u6570\u5b57\u8d27\u5e01', '\u7684', '\u4f7f\u7528', '\uff1b', '\u6df1', '\u7c89\u8272', '\u4ee3\u8868', '\u6bd4\u7279\u5e01', '\u88ab', '\u5b9a\u4e49', '\u4e3a', '\u975e\u6cd5', '\u7684', '\u56fd\u5bb6', '\uff1b', '\u7070\u8272', '\u662f', '\u5c1a\u672a', '\u7f6e', '\u8bc4', '\u7684', '\u56fd\u5bb6', '\u3002']"}], "[\n  [\"\u5982\u4e0b\", \"\u0442\u04e9\u043c\u0435\u043d\u0434\u0435\u0433\u0456\u0434\u0435\u0439\"],\n  [\"\u56fe\", \"\u0441\u0443\u0440\u0435\u0442\"],\n  [\"\u6240\u793a\", null],\n  [\"\uff0c\", null],\n  [\"\u7eff\u8272\", \"\u0436\u0430\u0441\u044b\u043b\"],\n  [\"\u4ee3\u8868\", \"\u0431\u0456\u043b\u0434\u0456\u0440\u0435\u0434\u0456\"],\n  [\"\u6bd4\u7279\u5e01\", \"\u0431\u0438\u0442\u043a\u043e\u0438\u043d\"],\n  [\"\u7684\", null],\n  [\"\u5408\u6cd5\", \"\u0437\u0430\u04a3\u0434\u044b\"],\n  [\"\u5e02\u573a\", \"\u043d\u0430\u0440\u044b\u049b\"],\n  [\"\uff1b\", null],\n  [\"\u6a58\", \"\u049b\u044b\u0437\u0493\u044b\u043b\u0442\"],\n  [\"\u9ec4\u8272\", \"\u0441\u0430\u0440\u044b\"],\n  [\"\u4ee3\u8868\", \"\u0431\u0456\u043b\u0434\u0456\u0440\u0435\u0434\u0456\"],\n  [\"\u4e2d\", null],\n  [\"\u6027\", null],\n  [\"\u5e02\u573a\", \"\u043d\u0430\u0440\u044b\u049b\"],\n  [\"\uff0c\", null],\n  [\"\u867d\", null],\n  [\"\u5c1a\u672a\", \"\u04d9\u043b\u0456\"],\n  [\"\u516c\u5f00\", \"\u0430\u0448\u044b\u049b\"],\n  [\"\u5408\u6cd5\", \"\u0437\u0430\u04a3\u0434\u044b\"],\n  [\"\u5316\", null],\n  [\"\u6bd4\u7279\u5e01\", \"\u0431\u0438\u0442\u043a\u043e\u0438\u043d\"],\n  [\"\u5730\u4f4d\", \"\u043c\u04d9\u0440\u0442\u0435\u0431\u0435\"],\n  [\"\uff0c\", null],\n  [\"\u4f46\", null],\n  [\"\u4e5f\u6ca1\u6709\", \"\u0434\u0430 \u0436\u043e\u049b\"],\n  [\"\u4e3b\u8981\", \"\u043d\u0435\u0433\u0456\u0437\u0433\u0456\"],\n  [\"\u7684\", null],\n  [\"\u4f7f\u7528\", \"\u043f\u0430\u0439\u0434\u0430\u043b\u0430\u043d\u0443\"],\n  [\"\u9650\u5236\", \"\u0448\u0435\u043a\u0442\u0435\u0443\u043b\u0435\u0440\"],\n  [\"\uff1b\", null],\n  [\"\u6de1\", \"\u0430\u0448\u044b\u049b\"],\n  [\"\u7c89\u8272\", \"\u049b\u044b\u0437\u0493\u044b\u043b\u0442\"],\n  [\"\u4ee3\u8868\", \"\u0431\u0456\u043b\u0434\u0456\u0440\u0435\u0434\u0456\"],\n  [\"\u53d7\", null],\n  [\"\u9650\u5236\", \"\u0448\u0435\u043a\u0442\u0435\u0443\u043b\u0456\"],\n  [\"\u7684\", null],\n  [\"\u5e02\u573a\", \"\u043d\u0430\u0440\u044b\u049b\"],\n  [\"\uff0c\", null],\n  [\"\u5373\", null],\n  [\"\u5f53\u5730\u653f\u5e9c\", \"\u0436\u0435\u0440\u0433\u0456\u043b\u0456\u043a\u0442\u0456 \u04af\u043a\u0456\u043c\u0435\u0442\"],\n  [\"\u5c1d\u8bd5\", \"\u0442\u044b\u0440\u044b\u0441\u0430\u0434\u044b\"],\n  [\"\u901a\u8fc7\", \"\u0430\u0440\u049b\u044b\u043b\u044b\"],\n  [\"\u7ea2\u7ebf\", \"\u049b\u044b\u0437\u044b\u043b \u0441\u044b\u0437\u044b\u049b\"],\n  [\"\u653f\u7b56\", \"\u0441\u0430\u044f\u0441\u0430\u0442\"],\n  [\"\u6216\", \"\u043d\u0435\u043c\u0435\u0441\u0435\"],\n  [\"\u76d1\u7ba1\", \"\u0440\u0435\u0442\u0442\u0435\u0443\"],\n  [\"\u6765\", null],\n  [\"\u65b9\u6cd5\", \"\u04d9\u0434\u0456\u0441\"],\n  [\"\u6570\u5b57\u8d27\u5e01\", \"\u0441\u0430\u043d\u0434\u044b\u049b \u0432\u0430\u043b\u044e\u0442\u0430\"],\n  [\"\u7684\", null],\n  [\"\u4f7f\u7528\", \"\u043f\u0430\u0439\u0434\u0430\u043b\u0430\u043d\u0443\"],\n  [\"\uff1b\", null],\n  [\"\u6df1\", \"\u049b\u043e\u044e\"],\n  [\"\u7c89\u8272\", \"\u049b\u044b\u0437\u0493\u044b\u043b\u0442\"],\n  [\"\u4ee3\u8868\", \"\u0431\u0456\u043b\u0434\u0456\u0440\u0435\u0434\u0456\"],\n  [\"\u6bd4\u7279\u5e01\", \"\u0431\u0438\u0442\u043a\u043e\u0438\u043d\"],\n  [\"\u88ab\", null],\n  [\"\u5b9a\u4e49\", \"\u0430\u043d\u044b\u049b\u0442\u0430\u043b\u0493\u0430\u043d\"],\n  [\"\u4e3a\", null],\n  [\"\u975e\u6cd5\", \"\u0437\u0430\u04a3\u0441\u044b\u0437\"],\n  [\"\u7684\", null],\n  [\"\u56fd\u5bb6\", \"\u0435\u043b\u0434\u0435\u0440\"],\n  [\"\uff1b\", null],\n  [\"\u7070\u8272\", \"\u0441\u04b1\u0440\"],\n  [\"\u662f\", null],\n  [\"\u5c1a\u672a\", \"\u04d9\u043b\u0456\"],\n  [\"\u7f6e\", null],\n  [\"\u8bc4\", \"\u0431\u0430\u0493\u0430\u043b\u0430\u043d\u0431\u0430\u0493\u0430\u043d\"],\n  [\"\u7684\", null],\n  [\"\u56fd\u5bb6\", \"\u0435\u043b\u0434\u0435\u0440\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u7eff\u8272\u4ee3\u8868\u6bd4\u7279\u5e01\u7684\u5408\u6cd5\u5e02\u573a\uff1b\u6a58\u9ec4\u8272\u4ee3\u8868\u4e2d\u6027\u5e02\u573a\uff0c\u867d\u5c1a\u672a\u516c\u5f00\u5408\u6cd5\u5316\u6bd4\u7279\u5e01\u5730\u4f4d\uff0c\u4f46\u4e5f\u6ca1\u6709\u4e3b\u8981\u7684\u4f7f\u7528\u9650\u5236\uff1b\u6de1\u7c89\u8272\u4ee3\u8868\u53d7\u9650\u5236\u7684\u5e02\u573a\uff0c\u5373\u5f53\u5730\u653f\u5e9c\u5c1d\u8bd5\u901a\u8fc7\u7ea2\u7ebf\u653f\u7b56\u6216\u76d1\u7ba1\u6765\u65b9\u6cd5\u6570\u5b57\u8d27\u5e01\u7684\u4f7f\u7528\uff1b\u6df1\u7c89\u8272\u4ee3\u8868\u6bd4\u7279\u5e01\u88ab\u5b9a\u4e49\u4e3a\u975e\u6cd5\u7684\u56fd\u5bb6\uff1b\u7070\u8272\u662f\u5c1a\u672a\u7f6e\u8bc4\u7684\u56fd\u5bb6\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u5982\u4e0b', '\u56fe', '\u6240\u793a', '\uff0c', '\u7eff\u8272', '\u4ee3\u8868', '\u6bd4\u7279\u5e01', '\u7684', '\u5408\u6cd5', '\u5e02\u573a', '\uff1b', '\u6a58', '\u9ec4\u8272', '\u4ee3\u8868', '\u4e2d', '\u6027', '\u5e02\u573a', '\uff0c', '\u867d', '\u5c1a\u672a', '\u516c\u5f00', '\u5408\u6cd5', '\u5316', '\u6bd4\u7279\u5e01', '\u5730\u4f4d', '\uff0c', '\u4f46', '\u4e5f\u6ca1\u6709', '\u4e3b\u8981', '\u7684', '\u4f7f\u7528', '\u9650\u5236', '\uff1b', '\u6de1', '\u7c89\u8272', '\u4ee3\u8868', '\u53d7', '\u9650\u5236', '\u7684', '\u5e02\u573a', '\uff0c', '\u5373', '\u5f53\u5730\u653f\u5e9c', '\u5c1d\u8bd5', '\u901a\u8fc7', '\u7ea2\u7ebf', '\u653f\u7b56', '\u6216', '\u76d1\u7ba1', '\u6765', '\u65b9\u6cd5', '\u6570\u5b57\u8d27\u5e01', '\u7684', '\u4f7f\u7528', '\uff1b', '\u6df1', '\u7c89\u8272', '\u4ee3\u8868', '\u6bd4\u7279\u5e01', '\u88ab', '\u5b9a\u4e49', '\u4e3a', '\u975e\u6cd5', '\u7684', '\u56fd\u5bb6', '\uff1b', '\u7070\u8272', '\u662f', '\u5c1a\u672a', '\u7f6e', '\u8bc4', '\u7684', '\u56fd\u5bb6', '\u3002']"}], "[\n[\"\u5982\u4e0b\", \"\u043a\u0430\u043a \u043f\u043e\u043a\u0430\u0437\u0430\u043d\u043e\"],\n[\"\u56fe\", \"\u043d\u0430 \u0440\u0438\u0441\u0443\u043d\u043a\u0435\"],\n[\"\u6240\u793a\", null],\n[\"\uff0c\", null],\n[\"\u7eff\u8272\", \"\u0437\u0435\u043b\u0435\u043d\u044b\u0439\"],\n[\"\u4ee3\u8868\", \"\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442\"],\n[\"\u6bd4\u7279\u5e01\", \"\u0431\u0438\u0442\u043a\u043e\u0438\u043d\"],\n[\"\u7684\", null],\n[\"\u5408\u6cd5\", \"\u043b\u0435\u0433\u0430\u043b\u044c\u043d\u044b\u0439\"],\n[\"\u5e02\u573a\", \"\u0440\u044b\u043d\u043e\u043a\"],\n[\"\uff1b\", null],\n[\"\u6a58\", null],\n[\"\u9ec4\u8272\", \"\u043e\u0440\u0430\u043d\u0436\u0435\u0432\u044b\u0439\"],\n[\"\u4ee3\u8868\", \"\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442\"],\n[\"\u4e2d\", null],\n[\"\u6027\", null],\n[\"\u5e02\u573a\", \"\u0440\u044b\u043d\u043e\u043a\"],\n[\"\uff0c\", null],\n[\"\u867d\", null],\n[\"\u5c1a\u672a\", \"\u0435\u0449\u0435 \u043d\u0435\"],\n[\"\u516c\u5f00\", \"\u043f\u0443\u0431\u043b\u0438\u0447\u043d\u043e\"],\n[\"\u5408\u6cd5\", \"\u043b\u0435\u0433\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043b\"],\n[\"\u5316\", null],\n[\"\u6bd4\u7279\u5e01\", \"\u0431\u0438\u0442\u043a\u043e\u0438\u043d\"],\n[\"\u5730\u4f4d\", \"\u0441\u0442\u0430\u0442\u0443\u0441\"],\n[\"\uff0c\", null],\n[\"\u4f46\", \"\u043d\u043e\"],\n[\"\u4e5f\u6ca1\u6709\", \"\u0442\u0430\u043a\u0436\u0435 \u043d\u0435\u0442\"],\n[\"\u4e3b\u8981\", \"\u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0445\"],\n[\"\u7684\", null],\n[\"\u4f7f\u7528\", \"\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f\"],\n[\"\u9650\u5236\", \"\u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0439\"],\n[\"\uff1b\", null],\n[\"\u6de1\", \"\u0441\u0432\u0435\u0442\u043b\u043e\"],\n[\"\u7c89\u8272\", \"\u0440\u043e\u0437\u043e\u0432\u044b\u0439\"],\n[\"\u4ee3\u8868\", \"\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442\"],\n[\"\u53d7\", null],\n[\"\u9650\u5236\", \"\u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u043d\u044b\u0439\"],\n[\"\u7684\", null],\n[\"\u5e02\u573a\", \"\u0440\u044b\u043d\u043e\u043a\"],\n[\"\uff0c\", null],\n[\"\u5373\", \"\u0442\u043e \u0435\u0441\u0442\u044c\"],\n[\"\u5f53\u5730\u653f\u5e9c\", \"\u043c\u0435\u0441\u0442\u043d\u043e\u0435 \u043f\u0440\u0430\u0432\u0438\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u043e\"],\n[\"\u5c1d\u8bd5\", \"\u043f\u044b\u0442\u0430\u0435\u0442\u0441\u044f\"],\n[\"\u901a\u8fc7\", \"\u0447\u0435\u0440\u0435\u0437\"],\n[\"\u7ea2\u7ebf\", \"\u043a\u0440\u0430\u0441\u043d\u0443\u044e \u043b\u0438\u043d\u0438\u044e\"],\n[\"\u653f\u7b56\", \"\u043f\u043e\u043b\u0438\u0442\u0438\u043a\u0443\"],\n[\"\u6216\", \"\u0438\u043b\u0438\"],\n[\"\u76d1\u7ba1\", \"\u0440\u0435\u0433\u0443\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\"],\n[\"\u6765\", null],\n[\"\u65b9\u6cd5\", \"\u043c\u0435\u0442\u043e\u0434\"],\n[\"\u6570\u5b57\u8d27\u5e01\", \"\u0446\u0438\u0444\u0440\u043e\u0432\u0443\u044e \u0432\u0430\u043b\u044e\u0442\u0443\"],\n[\"\u7684\", null],\n[\"\u4f7f\u7528\", \"\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\"],\n[\"\uff1b\", null],\n[\"\u6df1\", \"\u0442\u0435\u043c\u043d\u043e\"],\n[\"\u7c89\u8272\", \"\u0440\u043e\u0437\u043e\u0432\u044b\u0439\"],\n[\"\u4ee3\u8868\", \"\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442\"],\n[\"\u6bd4\u7279\u5e01\", \"\u0431\u0438\u0442\u043a\u043e\u0438\u043d\"],\n[\"\u88ab\", null],\n[\"\u5b9a\u4e49\", \"\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\"],\n[\"\u4e3a\", \"\u043a\u0430\u043a\"],\n[\"\u975e\u6cd5\", \"\u043d\u0435\u0437\u0430\u043a\u043e\u043d\u043d\u044b\u0439\"],\n[\"\u7684\", null],\n[\"\u56fd\u5bb6\", \"\u0441\u0442\u0440\u0430\u043d\u044b\"],\n[\"\uff1b\", null],\n[\"\u7070\u8272\", \"\u0441\u0435\u0440\u044b\u0439\"],\n[\"\u662f\", null],\n[\"\u5c1a\u672a\", \"\u0435\u0449\u0435 \u043d\u0435\"],\n[\"\u7f6e\", null],\n[\"\u8bc4\", \"\u043e\u0446\u0435\u043d\u0435\u043d\u044b\"],\n[\"\u7684\", null],\n[\"\u56fd\u5bb6\", \"\u0441\u0442\u0440\u0430\u043d\u044b\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u800c\u9f99\u6e56\u6631\u6e56\u58f9\u53f7\uff0c\u4e34\u8fd1\u91cd\u5e86\u9996\u4e2a\u4f4e\u5bc6\u5ea6\u6ee8\u6c34\u5546\u5708\u2014\u2014\u793c\u5609\u6ee8\u6c34\u5546\u5708\uff0c\u88ab\u4e5d\u4e2a\u516c\u56ed\u73af\u7ed5\uff0c\u5546\u4e1a\u8857\u533a\u5bc6\u5ea6\u4f4e\uff0c\u5f00\u653e\u7a0b\u5ea6\u9ad8\uff0c\u4f7f\u5f97\u4eba\u4eec\u8d2d\u7269\u65f6\u66f4\u613f\u610f\u505c\u7559\u3001\u9a7b\u8db3\u3001\u6162\u6162\u4eab\u53d7\u8d2d\u7269\u7684\u4e50\u8da3\uff0c\u800c\u4e0d\u662f\u88ab\u540e\u9762\u7684\u4eba\u6d41\u88f9\u631f\u7740\u6025\u6025\u5fd9\u5fd9\u5730\u5411\u524d\u8d76\uff0c\u771f\u6b63\u5b9e\u73b0\u4e86\u201c\u6563\u6b65\u5f0f\u8d2d\u7269\u201d\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u800c', '\u9f99\u6e56', '\u6631', '\u6e56', '\u58f9', '\u53f7', '\uff0c', '\u4e34\u8fd1', '\u91cd\u5e86', '\u9996\u4e2a', '\u4f4e', '\u5bc6\u5ea6', '\u6ee8', '\u6c34', '\u5546\u5708', '\u2014\u2014', '\u793c', '\u5609', '\u6ee8', '\u6c34', '\u5546\u5708', '\uff0c', '\u88ab', '\u4e5d', '\u4e2a', '\u516c\u56ed', '\u73af\u7ed5', '\uff0c', '\u5546\u4e1a', '\u8857\u533a', '\u5bc6\u5ea6', '\u4f4e', '\uff0c', '\u5f00\u653e', '\u7a0b\u5ea6', '\u9ad8', '\uff0c', '\u4f7f\u5f97', '\u4eba\u4eec', '\u8d2d\u7269', '\u65f6', '\u66f4', '\u613f\u610f', '\u505c\u7559', '\u3001', '\u9a7b', '\u8db3', '\u3001', '\u6162\u6162', '\u4eab\u53d7', '\u8d2d\u7269', '\u7684\u4e50\u8da3', '\uff0c', '\u800c\u4e0d\u662f', '\u88ab', '\u540e\u9762', '\u7684\u4eba', '\u6d41', '\u88f9', '\u631f', '\u7740\u6025', '\u6025\u5fd9', '\u5fd9', '\u5730', '\u5411\u524d', '\u8d76', '\uff0c', '\u771f\u6b63', '\u5b9e\u73b0\u4e86', '\u201c', '\u6563\u6b65', '\u5f0f', '\u8d2d\u7269', '\u201d\u3002']"}], "[\n  [\"\u800c\", null],\n  [\"\u9f99\u6e56\", \"Longhu\"],\n  [\"\u6631\", null],\n  [\"\u6e56\", null],\n  [\"\u58f9\", null],\n  [\"\u53f7\", \"No. 1\"],\n  [\"\uff0c\", null],\n  [\"\u4e34\u8fd1\", \"near\"],\n  [\"\u91cd\u5e86\", \"Chongqing\"],\n  [\"\u9996\u4e2a\", \"first\"],\n  [\"\u4f4e\", null],\n  [\"\u5bc6\u5ea6\", \"density\"],\n  [\"\u6ee8\", null],\n  [\"\u6c34\", null],\n  [\"\u5546\u5708\", \"commercial area\"],\n  [\"\u2014\u2014\", null],\n  [\"\u793c\", null],\n  [\"\u5609\", null],\n  [\"\u6ee8\", null],\n  [\"\u6c34\", null],\n  [\"\u5546\u5708\", \"commercial area\"],\n  [\"\uff0c\", null],\n  [\"\u88ab\", null],\n  [\"\u4e5d\", \"nine\"],\n  [\"\u4e2a\", null],\n  [\"\u516c\u56ed\", \"parks\"],\n  [\"\u73af\u7ed5\", \"surrounded\"],\n  [\"\uff0c\", null],\n  [\"\u5546\u4e1a\", \"commercial\"],\n  [\"\u8857\u533a\", \"district\"],\n  [\"\u5bc6\u5ea6\", \"density\"],\n  [\"\u4f4e\", \"low\"],\n  [\"\uff0c\", null],\n  [\"\u5f00\u653e\", \"open\"],\n  [\"\u7a0b\u5ea6\", \"degree\"],\n  [\"\u9ad8\", \"high\"],\n  [\"\uff0c\", null],\n  [\"\u4f7f\u5f97\", \"making\"],\n  [\"\u4eba\u4eec\", \"people\"],\n  [\"\u8d2d\u7269\", \"shopping\"],\n  [\"\u65f6\", null],\n  [\"\u66f4\", \"more\"],\n  [\"\u613f\u610f\", \"willing\"],\n  [\"\u505c\u7559\", \"stay\"],\n  [\"\u3001\", null],\n  [\"\u9a7b\", null],\n  [\"\u8db3\", null],\n  [\"\u3001\", null],\n  [\"\u6162\u6162\", \"slowly\"],\n  [\"\u4eab\u53d7\", \"enjoy\"],\n  [\"\u8d2d\u7269\", \"shopping\"],\n  [\"\u7684\u4e50\u8da3\", \"pleasure\"],\n  [\"\uff0c\", null],\n  [\"\u800c\u4e0d\u662f\", \"rather than\"],\n  [\"\u88ab\", null],\n  [\"\u540e\u9762\", \"behind\"],\n  [\"\u7684\u4eba\", \"people\"],\n  [\"\u6d41\", null],\n  [\"\u88f9\", null],\n  [\"\u631f\", null],\n  [\"\u7740\u6025\", \"hurriedly\"],\n  [\"\u6025\u5fd9\", null],\n  [\"\u5fd9\", null],\n  [\"\u5730\", null],\n  [\"\u5411\u524d\", \"forward\"],\n  [\"\u8d76\", \"rush\"],\n  [\"\uff0c\", null],\n  [\"\u771f\u6b63\", \"truly\"],\n  [\"\u5b9e\u73b0\u4e86\", \"realized\"],\n  [\"\\\"\", null],\n  [\"\u6563\u6b65\", \"stroll\"],\n  [\"\u5f0f\", null],\n  [\"\u8d2d\u7269\", \"shopping\"],\n  [\"\\\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u800c\u9f99\u6e56\u6631\u6e56\u58f9\u53f7\uff0c\u4e34\u8fd1\u91cd\u5e86\u9996\u4e2a\u4f4e\u5bc6\u5ea6\u6ee8\u6c34\u5546\u5708\u2014\u2014\u793c\u5609\u6ee8\u6c34\u5546\u5708\uff0c\u88ab\u4e5d\u4e2a\u516c\u56ed\u73af\u7ed5\uff0c\u5546\u4e1a\u8857\u533a\u5bc6\u5ea6\u4f4e\uff0c\u5f00\u653e\u7a0b\u5ea6\u9ad8\uff0c\u4f7f\u5f97\u4eba\u4eec\u8d2d\u7269\u65f6\u66f4\u613f\u610f\u505c\u7559\u3001\u9a7b\u8db3\u3001\u6162\u6162\u4eab\u53d7\u8d2d\u7269\u7684\u4e50\u8da3\uff0c\u800c\u4e0d\u662f\u88ab\u540e\u9762\u7684\u4eba\u6d41\u88f9\u631f\u7740\u6025\u6025\u5fd9\u5fd9\u5730\u5411\u524d\u8d76\uff0c\u771f\u6b63\u5b9e\u73b0\u4e86\u201c\u6563\u6b65\u5f0f\u8d2d\u7269\u201d\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u800c', '\u9f99\u6e56', '\u6631', '\u6e56', '\u58f9', '\u53f7', '\uff0c', '\u4e34\u8fd1', '\u91cd\u5e86', '\u9996\u4e2a', '\u4f4e', '\u5bc6\u5ea6', '\u6ee8', '\u6c34', '\u5546\u5708', '\u2014\u2014', '\u793c', '\u5609', '\u6ee8', '\u6c34', '\u5546\u5708', '\uff0c', '\u88ab', '\u4e5d', '\u4e2a', '\u516c\u56ed', '\u73af\u7ed5', '\uff0c', '\u5546\u4e1a', '\u8857\u533a', '\u5bc6\u5ea6', '\u4f4e', '\uff0c', '\u5f00\u653e', '\u7a0b\u5ea6', '\u9ad8', '\uff0c', '\u4f7f\u5f97', '\u4eba\u4eec', '\u8d2d\u7269', '\u65f6', '\u66f4', '\u613f\u610f', '\u505c\u7559', '\u3001', '\u9a7b', '\u8db3', '\u3001', '\u6162\u6162', '\u4eab\u53d7', '\u8d2d\u7269', '\u7684\u4e50\u8da3', '\uff0c', '\u800c\u4e0d\u662f', '\u88ab', '\u540e\u9762', '\u7684\u4eba', '\u6d41', '\u88f9', '\u631f', '\u7740\u6025', '\u6025\u5fd9', '\u5fd9', '\u5730', '\u5411\u524d', '\u8d76', '\uff0c', '\u771f\u6b63', '\u5b9e\u73b0\u4e86', '\u201c', '\u6563\u6b65', '\u5f0f', '\u8d2d\u7269', '\u201d\u3002']"}], "[\n[\"\u800c\", null],\n[\"\u9f99\u6e56\", \"\u041b\u0443\u043d\u0445\u0443\"],\n[\"\u6631\", null],\n[\"\u6e56\", null],\n[\"\u58f9\", null],\n[\"\u53f7\", null],\n[\"\uff0c\", null],\n[\"\u4e34\u8fd1\", \"\u0436\u0430\u049b\u044b\u043d\"],\n[\"\u91cd\u5e86\", \"\u0427\u0443\u043d\u0446\u0438\u043d\"],\n[\"\u9996\u4e2a\", \"\u0431\u0456\u0440\u0456\u043d\u0448\u0456\"],\n[\"\u4f4e\", \"\u0442\u04e9\u043c\u0435\u043d\"],\n[\"\u5bc6\u5ea6\", \"\u0442\u044b\u0493\u044b\u0437\u0434\u044b\u049b\"],\n[\"\u6ee8\", null],\n[\"\u6c34\", null],\n[\"\u5546\u5708\", \"\u0441\u0430\u0443\u0434\u0430 \u0430\u0439\u043c\u0430\u0493\u044b\"],\n[\"\u2014\u2014\", null],\n[\"\u793c\", null],\n[\"\u5609\", null],\n[\"\u6ee8\", null],\n[\"\u6c34\", null],\n[\"\u5546\u5708\", \"\u0441\u0430\u0443\u0434\u0430 \u0430\u0439\u043c\u0430\u0493\u044b\"],\n[\"\uff0c\", null],\n[\"\u88ab\", null],\n[\"\u4e5d\", \"\u0442\u043e\u0493\u044b\u0437\"],\n[\"\u4e2a\", null],\n[\"\u516c\u56ed\", \"\u0441\u0430\u044f\u0431\u0430\u049b\"],\n[\"\u73af\u7ed5\", \"\u049b\u043e\u0440\u0448\u0430\u043b\u0493\u0430\u043d\"],\n[\"\uff0c\", null],\n[\"\u5546\u4e1a\", \"\u043a\u043e\u043c\u043c\u0435\u0440\u0446\u0438\u044f\u043b\u044b\u049b\"],\n[\"\u8857\u533a\", \"\u0430\u0443\u0434\u0430\u043d\"],\n[\"\u5bc6\u5ea6\", \"\u0442\u044b\u0493\u044b\u0437\u0434\u044b\u049b\"],\n[\"\u4f4e\", \"\u0442\u04e9\u043c\u0435\u043d\"],\n[\"\uff0c\", null],\n[\"\u5f00\u653e\", \"\u0430\u0448\u044b\u049b\"],\n[\"\u7a0b\u5ea6\", \"\u0434\u0435\u04a3\u0433\u0435\u0439\"],\n[\"\u9ad8\", \"\u0436\u043e\u0493\u0430\u0440\u044b\"],\n[\"\uff0c\", null],\n[\"\u4f7f\u5f97\", null],\n[\"\u4eba\u4eec\", \"\u0430\u0434\u0430\u043c\u0434\u0430\u0440\"],\n[\"\u8d2d\u7269\", \"\u0441\u0430\u0443\u0434\u0430 \u0436\u0430\u0441\u0430\u0443\"],\n[\"\u65f6\", null],\n[\"\u66f4\", \"\u043a\u04e9\u0431\u0456\u0440\u0435\u043a\"],\n[\"\u613f\u610f\", \"\u049b\u0430\u043b\u0430\u0439\u0434\u044b\"],\n[\"\u505c\u7559\", \"\u0442\u043e\u049b\u0442\u0430\u0443\"],\n[\"\u3001\", null],\n[\"\u9a7b\", null],\n[\"\u8db3\", null],\n[\"\u3001\", null],\n[\"\u6162\u6162\", \"\u0431\u0430\u044f\u0443\"],\n[\"\u4eab\u53d7\", \"\u0440\u0430\u0445\u0430\u0442\u0442\u0430\u043d\u0443\"],\n[\"\u8d2d\u7269\", \"\u0441\u0430\u0443\u0434\u0430 \u0436\u0430\u0441\u0430\u0443\"],\n[\"\u7684\u4e50\u8da3\", \"\u049b\u0443\u0430\u043d\u044b\u0448\u044b\u043d\"],\n[\"\uff0c\", null],\n[\"\u800c\u4e0d\u662f\", \"\u0435\u043c\u0435\u0441\"],\n[\"\u88ab\", null],\n[\"\u540e\u9762\", \"\u0430\u0440\u0442\u044b\u043d\u0434\u0430\u0493\u044b\"],\n[\"\u7684\u4eba\", \"\u0430\u0434\u0430\u043c\u0434\u0430\u0440\"],\n[\"\u6d41\", null],\n[\"\u88f9\", null],\n[\"\u631f\", null],\n[\"\u7740\u6025\", \"\u0430\u0441\u044b\u0493\u044b\u0441\"],\n[\"\u6025\u5fd9\", null],\n[\"\u5fd9\", null],\n[\"\u5730\", null],\n[\"\u5411\u524d\", \"\u0430\u043b\u0493\u0430\"],\n[\"\u8d76\", \"\u0436\u04af\u0433\u0456\u0440\u0443\"],\n[\"\uff0c\", null],\n[\"\u771f\u6b63\", \"\u0448\u044b\u043d\u0430\u0439\u044b\"],\n[\"\u5b9e\u73b0\u4e86\", \"\u0436\u04af\u0437\u0435\u0433\u0435 \u0430\u0441\u044b\u0440\u0434\u044b\"],\n[\"\"\", null],\n[\"\u6563\u6b65\", \"\u0441\u0435\u0440\u0443\u0435\u043d\u0434\u0435\u0443\"],\n[\"\u5f0f\", null],\n[\"\u8d2d\u7269\", \"\u0441\u0430\u0443\u0434\u0430 \u0436\u0430\u0441\u0430\u0443\"],\n[\"\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u800c\u9f99\u6e56\u6631\u6e56\u58f9\u53f7\uff0c\u4e34\u8fd1\u91cd\u5e86\u9996\u4e2a\u4f4e\u5bc6\u5ea6\u6ee8\u6c34\u5546\u5708\u2014\u2014\u793c\u5609\u6ee8\u6c34\u5546\u5708\uff0c\u88ab\u4e5d\u4e2a\u516c\u56ed\u73af\u7ed5\uff0c\u5546\u4e1a\u8857\u533a\u5bc6\u5ea6\u4f4e\uff0c\u5f00\u653e\u7a0b\u5ea6\u9ad8\uff0c\u4f7f\u5f97\u4eba\u4eec\u8d2d\u7269\u65f6\u66f4\u613f\u610f\u505c\u7559\u3001\u9a7b\u8db3\u3001\u6162\u6162\u4eab\u53d7\u8d2d\u7269\u7684\u4e50\u8da3\uff0c\u800c\u4e0d\u662f\u88ab\u540e\u9762\u7684\u4eba\u6d41\u88f9\u631f\u7740\u6025\u6025\u5fd9\u5fd9\u5730\u5411\u524d\u8d76\uff0c\u771f\u6b63\u5b9e\u73b0\u4e86\u201c\u6563\u6b65\u5f0f\u8d2d\u7269\u201d\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u800c', '\u9f99\u6e56', '\u6631', '\u6e56', '\u58f9', '\u53f7', '\uff0c', '\u4e34\u8fd1', '\u91cd\u5e86', '\u9996\u4e2a', '\u4f4e', '\u5bc6\u5ea6', '\u6ee8', '\u6c34', '\u5546\u5708', '\u2014\u2014', '\u793c', '\u5609', '\u6ee8', '\u6c34', '\u5546\u5708', '\uff0c', '\u88ab', '\u4e5d', '\u4e2a', '\u516c\u56ed', '\u73af\u7ed5', '\uff0c', '\u5546\u4e1a', '\u8857\u533a', '\u5bc6\u5ea6', '\u4f4e', '\uff0c', '\u5f00\u653e', '\u7a0b\u5ea6', '\u9ad8', '\uff0c', '\u4f7f\u5f97', '\u4eba\u4eec', '\u8d2d\u7269', '\u65f6', '\u66f4', '\u613f\u610f', '\u505c\u7559', '\u3001', '\u9a7b', '\u8db3', '\u3001', '\u6162\u6162', '\u4eab\u53d7', '\u8d2d\u7269', '\u7684\u4e50\u8da3', '\uff0c', '\u800c\u4e0d\u662f', '\u88ab', '\u540e\u9762', '\u7684\u4eba', '\u6d41', '\u88f9', '\u631f', '\u7740\u6025', '\u6025\u5fd9', '\u5fd9', '\u5730', '\u5411\u524d', '\u8d76', '\uff0c', '\u771f\u6b63', '\u5b9e\u73b0\u4e86', '\u201c', '\u6563\u6b65', '\u5f0f', '\u8d2d\u7269', '\u201d\u3002']"}], "[\n[\"\u800c\", null],\n[\"\u9f99\u6e56\", \"\u041b\u0443\u043d\u0445\u0443\"],\n[\"\u6631\", null],\n[\"\u6e56\", null],\n[\"\u58f9\", null],\n[\"\u53f7\", null],\n[\"\uff0c\", null],\n[\"\u4e34\u8fd1\", \"\u0412\u0431\u043b\u0438\u0437\u0438\"],\n[\"\u91cd\u5e86\", \"\u0427\u0443\u043d\u0446\u0438\u043d\"],\n[\"\u9996\u4e2a\", \"\u043f\u0435\u0440\u0432\u044b\u0439\"],\n[\"\u4f4e\", \"\u043d\u0438\u0437\u043a\u0438\u0439\"],\n[\"\u5bc6\u5ea6\", \"\u043f\u043b\u043e\u0442\u043d\u043e\u0441\u0442\u044c\"],\n[\"\u6ee8\", null],\n[\"\u6c34\", null],\n[\"\u5546\u5708\", \"\u0442\u043e\u0440\u0433\u043e\u0432\u044b\u0439 \u0440\u0430\u0439\u043e\u043d\"],\n[\"\u2014\u2014\", null],\n[\"\u793c\", null],\n[\"\u5609\", null],\n[\"\u6ee8\", null],\n[\"\u6c34\", null],\n[\"\u5546\u5708\", \"\u0442\u043e\u0440\u0433\u043e\u0432\u044b\u0439 \u0440\u0430\u0439\u043e\u043d\"],\n[\"\uff0c\", null],\n[\"\u88ab\", null],\n[\"\u4e5d\", \"\u0434\u0435\u0432\u044f\u0442\u044c\"],\n[\"\u4e2a\", null],\n[\"\u516c\u56ed\", \"\u043f\u0430\u0440\u043a\u043e\u0432\"],\n[\"\u73af\u7ed5\", \"\u043e\u043a\u0440\u0443\u0436\u0430\u044e\u0442\"],\n[\"\uff0c\", null],\n[\"\u5546\u4e1a\", \"\u043a\u043e\u043c\u043c\u0435\u0440\u0447\u0435\u0441\u043a\u0438\u0439\"],\n[\"\u8857\u533a\", \"\u043a\u0432\u0430\u0440\u0442\u0430\u043b\"],\n[\"\u5bc6\u5ea6\", \"\u043f\u043b\u043e\u0442\u043d\u043e\u0441\u0442\u044c\"],\n[\"\u4f4e\", \"\u043d\u0438\u0437\u043a\u0430\u044f\"],\n[\"\uff0c\", null],\n[\"\u5f00\u653e\", \"\u043e\u0442\u043a\u0440\u044b\u0442\u043e\u0441\u0442\u044c\"],\n[\"\u7a0b\u5ea6\", \"\u0441\u0442\u0435\u043f\u0435\u043d\u044c\"],\n[\"\u9ad8\", \"\u0432\u044b\u0441\u043e\u043a\u0430\u044f\"],\n[\"\uff0c\", null],\n[\"\u4f7f\u5f97\", \"\u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442\"],\n[\"\u4eba\u4eec\", \"\u043b\u044e\u0434\u044f\u043c\"],\n[\"\u8d2d\u7269\", \"\u043f\u043e\u043a\u0443\u043f\u043a\u0438\"],\n[\"\u65f6\", null],\n[\"\u66f4\", \"\u0431\u043e\u043b\u0435\u0435\"],\n[\"\u613f\u610f\", \"\u043e\u0445\u043e\u0442\u043d\u043e\"],\n[\"\u505c\u7559\", \"\u0437\u0430\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0442\u044c\u0441\u044f\"],\n[\"\u3001\", null],\n[\"\u9a7b\", null],\n[\"\u8db3\", null],\n[\"\u3001\", null],\n[\"\u6162\u6162\", \"\u043c\u0435\u0434\u043b\u0435\u043d\u043d\u043e\"],\n[\"\u4eab\u53d7\", \"\u043d\u0430\u0441\u043b\u0430\u0436\u0434\u0430\u0442\u044c\u0441\u044f\"],\n[\"\u8d2d\u7269\", \"\u043f\u043e\u043a\u0443\u043f\u043a\u0430\u043c\u0438\"],\n[\"\u7684\u4e50\u8da3\", \"\u0443\u0434\u043e\u0432\u043e\u043b\u044c\u0441\u0442\u0432\u0438\u0435\u043c\"],\n[\"\uff0c\", null],\n[\"\u800c\u4e0d\u662f\", \"\u0430 \u043d\u0435\"],\n[\"\u88ab\", null],\n[\"\u540e\u9762\", \"\u043f\u043e\u0437\u0430\u0434\u0438\"],\n[\"\u7684\u4eba\", \"\u043b\u044e\u0434\u0435\u0439\"],\n[\"\u6d41\", null],\n[\"\u88f9\", null],\n[\"\u631f\", null],\n[\"\u7740\u6025\", \"\u0441\u043f\u0435\u0448\u0438\u0442\u044c\"],\n[\"\u6025\u5fd9\", null],\n[\"\u5fd9\", null],\n[\"\u5730\", null],\n[\"\u5411\u524d\", \"\u0432\u043f\u0435\u0440\u0451\u0434\"],\n[\"\u8d76\", \"\u0442\u043e\u0440\u043e\u043f\u0438\u0442\u044c\u0441\u044f\"],\n[\"\uff0c\", null],\n[\"\u771f\u6b63\", \"\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e\"],\n[\"\u5b9e\u73b0\u4e86\", \"\u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c\"],\n[\"\"\", null],\n[\"\u6563\u6b65\", \"\u043f\u0440\u043e\u0433\u0443\u043b\u043e\u0447\u043d\u044b\u0439\"],\n[\"\u5f0f\", null],\n[\"\u8d2d\u7269\", \"\u0448\u043e\u043f\u0438\u043d\u0433\"],\n[\"\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u6574\u4e2a\u666f\u89c2\u5e26\u501f\u52a9\u539f\u6709\u81ea\u7136\u751f\u6001\u548c\u5386\u53f2\u6587\u5316\u8d44\u6e90\u4f18\u52bf\uff0c\u5bf9\u5357\u8fd0\u6cb3\u4e24\u5cb8\u8fdb\u884c\u4e86\u91cd\u65b0\u89c4\u5212\u548c\u63d0\u5347\u6539\u9020\uff0c\u6253\u9020\u51fa\u5929\u5b50\u6d25\u6e21\u9057\u5740\u516c\u56ed\u60e0\u7075\u987f\u5e7f\u573a\u65b0\u5929\u5730\u98ce\u60c5\u8857\u897f\u7ad9\u8d70\u5eca\u6c34\u897f\u9057\u97f5\u7eff\u5eca\u4e94\u4e2a\u6cbf\u6cb3\u666f\u89c2\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u6574\u4e2a', '\u666f\u89c2', '\u5e26', '\u501f\u52a9', '\u539f\u6709', '\u81ea\u7136', '\u751f\u6001', '\u548c', '\u5386\u53f2\u6587\u5316', '\u8d44\u6e90\u4f18\u52bf', '\uff0c', '\u5bf9', '\u5357', '\u8fd0\u6cb3', '\u4e24\u5cb8', '\u8fdb\u884c\u4e86', '\u91cd\u65b0', '\u89c4\u5212', '\u548c', '\u63d0\u5347', '\u6539\u9020', '\uff0c', '\u6253\u9020\u51fa', '\u5929', '\u5b50', '\u6d25', '\u6e21', '\u9057\u5740', '\u516c\u56ed', '\u60e0', '\u7075', '\u987f', '\u5e7f\u573a', '\u65b0', '\u5929\u5730', '\u98ce\u60c5', '\u8857', '\u897f', '\u7ad9', '\u8d70\u5eca', '\u6c34', '\u897f', '\u9057', '\u97f5', '\u7eff', '\u5eca', '\u4e94\u4e2a', '\u6cbf', '\u6cb3', '\u666f\u89c2', '\u3002']"}], "[\n    [\"\u6574\u4e2a\", \"entire\"],\n    [\"\u666f\u89c2\", \"landscape\"],\n    [\"\u5e26\", null],\n    [\"\u501f\u52a9\", \"utilizing\"],\n    [\"\u539f\u6709\", \"existing\"],\n    [\"\u81ea\u7136\", \"natural\"],\n    [\"\u751f\u6001\", \"ecology\"],\n    [\"\u548c\", null],\n    [\"\u5386\u53f2\u6587\u5316\", \"historical and cultural\"],\n    [\"\u8d44\u6e90\u4f18\u52bf\", \"resource advantages\"],\n    [\"\uff0c\", null],\n    [\"\u5bf9\", null],\n    [\"\u5357\", \"South\"],\n    [\"\u8fd0\u6cb3\", \"Canal\"],\n    [\"\u4e24\u5cb8\", \"both banks\"],\n    [\"\u8fdb\u884c\u4e86\", null],\n    [\"\u91cd\u65b0\", \"re-\"],\n    [\"\u89c4\u5212\", \"planning\"],\n    [\"\u548c\", null],\n    [\"\u63d0\u5347\", \"improvement\"],\n    [\"\u6539\u9020\", \"renovation\"],\n    [\"\uff0c\", null],\n    [\"\u6253\u9020\u51fa\", \"created\"],\n    [\"\u5929\", \"Emperor's\"],\n    [\"\u5b50\", null],\n    [\"\u6d25\", null],\n    [\"\u6e21\", null],\n    [\"\u9057\u5740\", \"Site\"],\n    [\"\u516c\u56ed\", \"Park\"],\n    [\"\u60e0\", \"Wellington\"],\n    [\"\u7075\", null],\n    [\"\u987f\", null],\n    [\"\u5e7f\u573a\", \"Square\"],\n    [\"\u65b0\", \"New\"],\n    [\"\u5929\u5730\", \"World\"],\n    [\"\u98ce\u60c5\", \"Style\"],\n    [\"\u8857\", \"Street\"],\n    [\"\u897f\", \"West\"],\n    [\"\u7ad9\", \"Station\"],\n    [\"\u8d70\u5eca\", \"Corridor\"],\n    [\"\u6c34\", \"Water\"],\n    [\"\u897f\", \"West\"],\n    [\"\u9057\", \"Heritage\"],\n    [\"\u97f5\", null],\n    [\"\u7eff\", \"Green\"],\n    [\"\u5eca\", \"Corridor\"],\n    [\"\u4e94\u4e2a\", \"five\"],\n    [\"\u6cbf\", \"along\"],\n    [\"\u6cb3\", \"river\"],\n    [\"\u666f\u89c2\", \"landscapes\"],\n    [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u6574\u4e2a\u666f\u89c2\u5e26\u501f\u52a9\u539f\u6709\u81ea\u7136\u751f\u6001\u548c\u5386\u53f2\u6587\u5316\u8d44\u6e90\u4f18\u52bf\uff0c\u5bf9\u5357\u8fd0\u6cb3\u4e24\u5cb8\u8fdb\u884c\u4e86\u91cd\u65b0\u89c4\u5212\u548c\u63d0\u5347\u6539\u9020\uff0c\u6253\u9020\u51fa\u5929\u5b50\u6d25\u6e21\u9057\u5740\u516c\u56ed\u60e0\u7075\u987f\u5e7f\u573a\u65b0\u5929\u5730\u98ce\u60c5\u8857\u897f\u7ad9\u8d70\u5eca\u6c34\u897f\u9057\u97f5\u7eff\u5eca\u4e94\u4e2a\u6cbf\u6cb3\u666f\u89c2\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u6574\u4e2a', '\u666f\u89c2', '\u5e26', '\u501f\u52a9', '\u539f\u6709', '\u81ea\u7136', '\u751f\u6001', '\u548c', '\u5386\u53f2\u6587\u5316', '\u8d44\u6e90\u4f18\u52bf', '\uff0c', '\u5bf9', '\u5357', '\u8fd0\u6cb3', '\u4e24\u5cb8', '\u8fdb\u884c\u4e86', '\u91cd\u65b0', '\u89c4\u5212', '\u548c', '\u63d0\u5347', '\u6539\u9020', '\uff0c', '\u6253\u9020\u51fa', '\u5929', '\u5b50', '\u6d25', '\u6e21', '\u9057\u5740', '\u516c\u56ed', '\u60e0', '\u7075', '\u987f', '\u5e7f\u573a', '\u65b0', '\u5929\u5730', '\u98ce\u60c5', '\u8857', '\u897f', '\u7ad9', '\u8d70\u5eca', '\u6c34', '\u897f', '\u9057', '\u97f5', '\u7eff', '\u5eca', '\u4e94\u4e2a', '\u6cbf', '\u6cb3', '\u666f\u89c2', '\u3002']"}], "[\n[\"\u6574\u4e2a\", \"\u0442\u04b1\u0442\u0430\u0441\"],\n[\"\u666f\u89c2\", \"\u043b\u0430\u043d\u0434\u0448\u0430\u0444\u0442\"],\n[\"\u5e26\", \"\u0431\u0435\u043b\u0434\u0435\u0443\"],\n[\"\u501f\u52a9\", \"\u043f\u0430\u0439\u0434\u0430\u043b\u0430\u043d\u0443\"],\n[\"\u539f\u6709\", \"\u0431\u0430\u0441\u0442\u0430\u043f\u049b\u044b\"],\n[\"\u81ea\u7136\", \"\u0442\u0430\u0431\u0438\u0493\u0438\"],\n[\"\u751f\u6001\", \"\u044d\u043a\u043e\u043b\u043e\u0433\u0438\u044f\"],\n[\"\u548c\", null],\n[\"\u5386\u53f2\u6587\u5316\", \"\u0442\u0430\u0440\u0438\u0445\u0438-\u043c\u04d9\u0434\u0435\u043d\u0438\"],\n[\"\u8d44\u6e90\u4f18\u52bf\", \"\u0440\u0435\u0441\u0443\u0440\u0441\u0442\u044b\u049b \u0430\u0440\u0442\u044b\u049b\u0448\u044b\u043b\u044b\u049b\"],\n[\"\uff0c\", null],\n[\"\u5bf9\", null],\n[\"\u5357\", \"\u043e\u04a3\u0442\u04af\u0441\u0442\u0456\u043a\"],\n[\"\u8fd0\u6cb3\", \"\u043a\u0430\u043d\u0430\u043b\"],\n[\"\u4e24\u5cb8\", \"\u0436\u0430\u0493\u0430\u043b\u0430\u0443\"],\n[\"\u8fdb\u884c\u4e86\", null],\n[\"\u91cd\u65b0\", \"\u049b\u0430\u0439\u0442\u0430\"],\n[\"\u89c4\u5212\", \"\u0436\u043e\u0441\u043f\u0430\u0440\u043b\u0430\u0443\"],\n[\"\u548c\", null],\n[\"\u63d0\u5347\", \"\u0436\u0430\u049b\u0441\u0430\u0440\u0442\u0443\"],\n[\"\u6539\u9020\", \"\u049b\u0430\u0439\u0442\u0430 \u049b\u04b1\u0440\u0443\"],\n[\"\uff0c\", null],\n[\"\u6253\u9020\u51fa\", \"\u049b\u04b1\u0440\u0443\"],\n[\"\u5929\", \"\u0430\u0441\u043f\u0430\u043d\"],\n[\"\u5b50\", null],\n[\"\u6d25\", null],\n[\"\u6e21\", null],\n[\"\u9057\u5740\", \"\u0435\u0441\u043a\u0435\u0440\u0442\u043a\u0456\u0448\"],\n[\"\u516c\u56ed\", \"\u0441\u0430\u044f\u0431\u0430\u049b\"],\n[\"\u60e0\", null],\n[\"\u7075\", null],\n[\"\u987f\", null],\n[\"\u5e7f\u573a\", \"\u0430\u043b\u0430\u04a3\"],\n[\"\u65b0\", \"\u0436\u0430\u04a3\u0430\"],\n[\"\u5929\u5730\", \"\u04d9\u043b\u0435\u043c\"],\n[\"\u98ce\u60c5\", \"\u043a\u04e9\u0440\u0456\u043d\u0456\u0441\"],\n[\"\u8857\", \"\u043a\u04e9\u0448\u0435\"],\n[\"\u897f\", \"\u0431\u0430\u0442\u044b\u0441\"],\n[\"\u7ad9\", \"\u0441\u0442\u0430\u043d\u0446\u0438\u044f\"],\n[\"\u8d70\u5eca\", \"\u0434\u04d9\u043b\u0456\u0437\"],\n[\"\u6c34\", \"\u0441\u0443\"],\n[\"\u897f\", \"\u0431\u0430\u0442\u044b\u0441\"],\n[\"\u9057\", null],\n[\"\u97f5\", null],\n[\"\u7eff\", \"\u0436\u0430\u0441\u044b\u043b\"],\n[\"\u5eca\", \"\u0434\u04d9\u043b\u0456\u0437\"],\n[\"\u4e94\u4e2a\", \"\u0431\u0435\u0441\"],\n[\"\u6cbf\", \"\u0431\u043e\u0439\u044b\u043c\u0435\u043d\"],\n[\"\u6cb3\", \"\u04e9\u0437\u0435\u043d\"],\n[\"\u666f\u89c2\", \"\u043b\u0430\u043d\u0434\u0448\u0430\u0444\u0442\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u6574\u4e2a\u666f\u89c2\u5e26\u501f\u52a9\u539f\u6709\u81ea\u7136\u751f\u6001\u548c\u5386\u53f2\u6587\u5316\u8d44\u6e90\u4f18\u52bf\uff0c\u5bf9\u5357\u8fd0\u6cb3\u4e24\u5cb8\u8fdb\u884c\u4e86\u91cd\u65b0\u89c4\u5212\u548c\u63d0\u5347\u6539\u9020\uff0c\u6253\u9020\u51fa\u5929\u5b50\u6d25\u6e21\u9057\u5740\u516c\u56ed\u60e0\u7075\u987f\u5e7f\u573a\u65b0\u5929\u5730\u98ce\u60c5\u8857\u897f\u7ad9\u8d70\u5eca\u6c34\u897f\u9057\u97f5\u7eff\u5eca\u4e94\u4e2a\u6cbf\u6cb3\u666f\u89c2\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u6574\u4e2a', '\u666f\u89c2', '\u5e26', '\u501f\u52a9', '\u539f\u6709', '\u81ea\u7136', '\u751f\u6001', '\u548c', '\u5386\u53f2\u6587\u5316', '\u8d44\u6e90\u4f18\u52bf', '\uff0c', '\u5bf9', '\u5357', '\u8fd0\u6cb3', '\u4e24\u5cb8', '\u8fdb\u884c\u4e86', '\u91cd\u65b0', '\u89c4\u5212', '\u548c', '\u63d0\u5347', '\u6539\u9020', '\uff0c', '\u6253\u9020\u51fa', '\u5929', '\u5b50', '\u6d25', '\u6e21', '\u9057\u5740', '\u516c\u56ed', '\u60e0', '\u7075', '\u987f', '\u5e7f\u573a', '\u65b0', '\u5929\u5730', '\u98ce\u60c5', '\u8857', '\u897f', '\u7ad9', '\u8d70\u5eca', '\u6c34', '\u897f', '\u9057', '\u97f5', '\u7eff', '\u5eca', '\u4e94\u4e2a', '\u6cbf', '\u6cb3', '\u666f\u89c2', '\u3002']"}], "[\n[\"\u6574\u4e2a\", \"\u0432\u0435\u0441\u044c\"],\n[\"\u666f\u89c2\", \"\u043b\u0430\u043d\u0434\u0448\u0430\u0444\u0442\"],\n[\"\u5e26\", null],\n[\"\u501f\u52a9\", \"\u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e\"],\n[\"\u539f\u6709\", \"\u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0439\"],\n[\"\u81ea\u7136\", \"\u043f\u0440\u0438\u0440\u043e\u0434\u043d\u044b\u0439\"],\n[\"\u751f\u6001\", \"\u044d\u043a\u043e\u043b\u043e\u0433\u0438\u044f\"],\n[\"\u548c\", null],\n[\"\u5386\u53f2\u6587\u5316\", \"\u0438\u0441\u0442\u043e\u0440\u0438\u043a\u043e-\u043a\u0443\u043b\u044c\u0442\u0443\u0440\u043d\u044b\u0439\"],\n[\"\u8d44\u6e90\u4f18\u52bf\", \"\u0440\u0435\u0441\u0443\u0440\u0441\u043d\u044b\u0435 \u043f\u0440\u0435\u0438\u043c\u0443\u0449\u0435\u0441\u0442\u0432\u0430\"],\n[\"\uff0c\", null],\n[\"\u5bf9\", null],\n[\"\u5357\", \"\u044e\u0436\u043d\u044b\u0439\"],\n[\"\u8fd0\u6cb3\", \"\u043a\u0430\u043d\u0430\u043b\"],\n[\"\u4e24\u5cb8\", \"\u043e\u0431\u0430 \u0431\u0435\u0440\u0435\u0433\u0430\"],\n[\"\u8fdb\u884c\u4e86\", null],\n[\"\u91cd\u65b0\", \"\u0437\u0430\u043d\u043e\u0432\u043e\"],\n[\"\u89c4\u5212\", \"\u043f\u043b\u0430\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\"],\n[\"\u548c\", null],\n[\"\u63d0\u5347\", \"\u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u0435\"],\n[\"\u6539\u9020\", \"\u0440\u0435\u043a\u043e\u043d\u0441\u0442\u0440\u0443\u043a\u0446\u0438\u044f\"],\n[\"\uff0c\", null],\n[\"\u6253\u9020\u51fa\", \"\u0441\u043e\u0437\u0434\u0430\u0442\u044c\"],\n[\"\u5929\", null],\n[\"\u5b50\", null],\n[\"\u6d25\", null],\n[\"\u6e21\", null],\n[\"\u9057\u5740\", \"\u0440\u0443\u0438\u043d\u044b\"],\n[\"\u516c\u56ed\", \"\u043f\u0430\u0440\u043a\"],\n[\"\u60e0\", null],\n[\"\u7075\", null],\n[\"\u987f\", null],\n[\"\u5e7f\u573a\", \"\u043f\u043b\u043e\u0449\u0430\u0434\u044c\"],\n[\"\u65b0\", \"\u043d\u043e\u0432\u044b\u0439\"],\n[\"\u5929\u5730\", \"\u043c\u0438\u0440\"],\n[\"\u98ce\u60c5\", \"\u043a\u043e\u043b\u043e\u0440\u0438\u0442\"],\n[\"\u8857\", \"\u0443\u043b\u0438\u0446\u0430\"],\n[\"\u897f\", \"\u0437\u0430\u043f\u0430\u0434\u043d\u044b\u0439\"],\n[\"\u7ad9\", \"\u0441\u0442\u0430\u043d\u0446\u0438\u044f\"],\n[\"\u8d70\u5eca\", \"\u043a\u043e\u0440\u0438\u0434\u043e\u0440\"],\n[\"\u6c34\", \"\u0432\u043e\u0434\u0430\"],\n[\"\u897f\", null],\n[\"\u9057\", null],\n[\"\u97f5\", null],\n[\"\u7eff\", \"\u0437\u0435\u043b\u0435\u043d\u044b\u0439\"],\n[\"\u5eca\", \"\u043a\u043e\u0440\u0438\u0434\u043e\u0440\"],\n[\"\u4e94\u4e2a\", \"\u043f\u044f\u0442\u044c\"],\n[\"\u6cbf\", \"\u0432\u0434\u043e\u043b\u044c\"],\n[\"\u6cb3\", \"\u0440\u0435\u043a\u0430\"],\n[\"\u666f\u89c2\", \"\u043f\u0435\u0439\u0437\u0430\u0436\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u5c45\u6c11\u4eec\u5f3a\u8c03\uff0c\u5728\u6c61\u67d3\u5f80\u5f80\u8f83\u8f7b\u7684\u6e29\u6696\u6708\u4efd\u91cc\u5230\u6237\u5916\u6d3b\u52a8\u5f88\u91cd\u8981\uff0c\u8981\u628a\u90a3\u4e9b\u84dd\u5929\u65e5\u5f53\u4f5c\u5728\u5386\u53f2\u60a0\u4e45\u7684\u80e1\u540c\u91cc\u5f9c\u5f89\u6216\u662f\u63a2\u7d22\u5907\u53d7\u6b22\u8fce\u7684\u65e5\u575b\u516c\u56ed\u7684\u673a\u4f1a\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u5c45\u6c11', '\u4eec', '\u5f3a\u8c03', '\uff0c\u5728', '\u6c61\u67d3', '\u5f80\u5f80', '\u8f83', '\u8f7b', '\u7684', '\u6e29\u6696', '\u6708\u4efd', '\u91cc', '\u5230', '\u6237\u5916', '\u6d3b\u52a8', '\u5f88\u91cd\u8981', '\uff0c', '\u8981\u628a', '\u90a3\u4e9b', '\u84dd\u5929', '\u65e5', '\u5f53\u4f5c', '\u5728', '\u5386\u53f2\u60a0\u4e45', '\u7684', '\u80e1\u540c', '\u91cc', '\u5f9c', '\u5f89', '\u6216\u662f', '\u63a2\u7d22', '\u5907', '\u53d7\u6b22\u8fce', '\u7684\u65e5', '\u575b', '\u516c\u56ed', '\u7684\u673a\u4f1a', '\u3002']"}], "[\n[\"\u5c45\u6c11\", \"Residents\"],\n[\"\u4eec\", null],\n[\"\u5f3a\u8c03\", \"emphasize\"],\n[\"\uff0c\", null],\n[\"\u5728\", null],\n[\"\u6c61\u67d3\", \"pollution\"],\n[\"\u5f80\u5f80\", \"often\"],\n[\"\u8f83\", null],\n[\"\u8f7b\", \"lighter\"],\n[\"\u7684\", null],\n[\"\u6e29\u6696\", \"warm\"],\n[\"\u6708\u4efd\", \"months\"],\n[\"\u91cc\", null],\n[\"\u5230\", null],\n[\"\u6237\u5916\", \"outdoor\"],\n[\"\u6d3b\u52a8\", \"activities\"],\n[\"\u5f88\u91cd\u8981\", \"important\"],\n[\"\uff0c\", null],\n[\"\u8981\u628a\", \"to\"],\n[\"\u90a3\u4e9b\", \"those\"],\n[\"\u84dd\u5929\", \"blue sky\"],\n[\"\u65e5\", \"days\"],\n[\"\u5f53\u4f5c\", \"as\"],\n[\"\u5728\", null],\n[\"\u5386\u53f2\u60a0\u4e45\", \"historic\"],\n[\"\u7684\", null],\n[\"\u80e1\u540c\", \"hutongs\"],\n[\"\u91cc\", null],\n[\"\u5f9c\", null],\n[\"\u5f89\", \"stroll\"],\n[\"\u6216\u662f\", \"or\"],\n[\"\u63a2\u7d22\", \"explore\"],\n[\"\u5907\", null],\n[\"\u53d7\u6b22\u8fce\", \"popular\"],\n[\"\u7684\u65e5\", null],\n[\"\u575b\", null],\n[\"\u516c\u56ed\", \"park\"],\n[\"\u7684\u673a\u4f1a\", \"opportunity\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u5c45\u6c11\u4eec\u5f3a\u8c03\uff0c\u5728\u6c61\u67d3\u5f80\u5f80\u8f83\u8f7b\u7684\u6e29\u6696\u6708\u4efd\u91cc\u5230\u6237\u5916\u6d3b\u52a8\u5f88\u91cd\u8981\uff0c\u8981\u628a\u90a3\u4e9b\u84dd\u5929\u65e5\u5f53\u4f5c\u5728\u5386\u53f2\u60a0\u4e45\u7684\u80e1\u540c\u91cc\u5f9c\u5f89\u6216\u662f\u63a2\u7d22\u5907\u53d7\u6b22\u8fce\u7684\u65e5\u575b\u516c\u56ed\u7684\u673a\u4f1a\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u5c45\u6c11', '\u4eec', '\u5f3a\u8c03', '\uff0c\u5728', '\u6c61\u67d3', '\u5f80\u5f80', '\u8f83', '\u8f7b', '\u7684', '\u6e29\u6696', '\u6708\u4efd', '\u91cc', '\u5230', '\u6237\u5916', '\u6d3b\u52a8', '\u5f88\u91cd\u8981', '\uff0c', '\u8981\u628a', '\u90a3\u4e9b', '\u84dd\u5929', '\u65e5', '\u5f53\u4f5c', '\u5728', '\u5386\u53f2\u60a0\u4e45', '\u7684', '\u80e1\u540c', '\u91cc', '\u5f9c', '\u5f89', '\u6216\u662f', '\u63a2\u7d22', '\u5907', '\u53d7\u6b22\u8fce', '\u7684\u65e5', '\u575b', '\u516c\u56ed', '\u7684\u673a\u4f1a', '\u3002']"}], "[\n[\"\u5c45\u6c11\", \"\u0442\u04b1\u0440\u0493\u044b\u043d\u0434\u0430\u0440\"],\n[\"\u4eec\", null],\n[\"\u5f3a\u8c03\", \"\u0430\u0442\u0430\u043f \u043a\u04e9\u0440\u0441\u0435\u0442\u0442\u0456\"],\n[\"\uff0c\", null],\n[\"\u5728\", null],\n[\"\u6c61\u67d3\", \"\u043b\u0430\u0441\u0442\u0430\u043d\u0443\"],\n[\"\u5f80\u5f80\", \"\u043a\u04e9\u0431\u0456\u043d\u0435\u0441\u0435\"],\n[\"\u8f83\", null],\n[\"\u8f7b\", \"\u0436\u0435\u04a3\u0456\u043b\"],\n[\"\u7684\", null],\n[\"\u6e29\u6696\", \"\u0436\u044b\u043b\u044b\"],\n[\"\u6708\u4efd\", \"\u0430\u0439\u043b\u0430\u0440\"],\n[\"\u91cc\", null],\n[\"\u5230\", null],\n[\"\u6237\u5916\", \"\u0441\u044b\u0440\u0442\u049b\u044b\"],\n[\"\u6d3b\u52a8\", \"\u0431\u0435\u043b\u0441\u0435\u043d\u0434\u0456\u043b\u0456\u043a\"],\n[\"\u5f88\u91cd\u8981\", \"\u04e9\u0442\u0435 \u043c\u0430\u04a3\u044b\u0437\u0434\u044b\"],\n[\"\uff0c\", null],\n[\"\u8981\u628a\", \"\u043a\u0435\u0440\u0435\u043a\"],\n[\"\u90a3\u4e9b\", \"\u0441\u043e\u043b\"],\n[\"\u84dd\u5929\", \"\u0430\u0448\u044b\u049b \u0430\u0441\u043f\u0430\u043d\"],\n[\"\u65e5\", \"\u043a\u04af\u043d\u0434\u0435\u0440\u0456\"],\n[\"\u5f53\u4f5c\", null],\n[\"\u5728\", null],\n[\"\u5386\u53f2\u60a0\u4e45\", \"\u0435\u0436\u0435\u043b\u0433\u0456\"],\n[\"\u7684\", null],\n[\"\u80e1\u540c\", \"\u0445\u0443\u0442\u0443\u043d\"],\n[\"\u91cc\", null],\n[\"\u5f9c\", null],\n[\"\u5f89\", \"\u0441\u0435\u0440\u0443\u0435\u043d\u0434\u0435\u0443\"],\n[\"\u6216\u662f\", \"\u043d\u0435\u043c\u0435\u0441\u0435\"],\n[\"\u63a2\u7d22\", \"\u0437\u0435\u0440\u0442\u0442\u0435\u0443\"],\n[\"\u5907\", null],\n[\"\u53d7\u6b22\u8fce\", \"\u0442\u0430\u043d\u044b\u043c\u0430\u043b\"],\n[\"\u7684\u65e5\", null],\n[\"\u575b\", null],\n[\"\u516c\u56ed\", \"\u0441\u0430\u044f\u0431\u0430\u049b\"],\n[\"\u7684\u673a\u4f1a\", \"\u043c\u04af\u043c\u043a\u0456\u043d\u0434\u0456\u043a\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u5c45\u6c11\u4eec\u5f3a\u8c03\uff0c\u5728\u6c61\u67d3\u5f80\u5f80\u8f83\u8f7b\u7684\u6e29\u6696\u6708\u4efd\u91cc\u5230\u6237\u5916\u6d3b\u52a8\u5f88\u91cd\u8981\uff0c\u8981\u628a\u90a3\u4e9b\u84dd\u5929\u65e5\u5f53\u4f5c\u5728\u5386\u53f2\u60a0\u4e45\u7684\u80e1\u540c\u91cc\u5f9c\u5f89\u6216\u662f\u63a2\u7d22\u5907\u53d7\u6b22\u8fce\u7684\u65e5\u575b\u516c\u56ed\u7684\u673a\u4f1a\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u5c45\u6c11', '\u4eec', '\u5f3a\u8c03', '\uff0c\u5728', '\u6c61\u67d3', '\u5f80\u5f80', '\u8f83', '\u8f7b', '\u7684', '\u6e29\u6696', '\u6708\u4efd', '\u91cc', '\u5230', '\u6237\u5916', '\u6d3b\u52a8', '\u5f88\u91cd\u8981', '\uff0c', '\u8981\u628a', '\u90a3\u4e9b', '\u84dd\u5929', '\u65e5', '\u5f53\u4f5c', '\u5728', '\u5386\u53f2\u60a0\u4e45', '\u7684', '\u80e1\u540c', '\u91cc', '\u5f9c', '\u5f89', '\u6216\u662f', '\u63a2\u7d22', '\u5907', '\u53d7\u6b22\u8fce', '\u7684\u65e5', '\u575b', '\u516c\u56ed', '\u7684\u673a\u4f1a', '\u3002']"}], "[\n  [\"\u5c45\u6c11\", \"\u0436\u0438\u0442\u0435\u043b\u0438\"],\n  [\"\u4eec\", null],\n  [\"\u5f3a\u8c03\", \"\u043f\u043e\u0434\u0447\u0435\u0440\u043a\u0438\u0432\u0430\u044e\u0442\"],\n  [\"\uff0c\", null],\n  [\"\u5728\", null],\n  [\"\u6c61\u67d3\", \"\u0437\u0430\u0433\u0440\u044f\u0437\u043d\u0435\u043d\u0438\u0435\"],\n  [\"\u5f80\u5f80\", \"\u0447\u0430\u0441\u0442\u043e\"],\n  [\"\u8f83\", null],\n  [\"\u8f7b\", \"\u043b\u0435\u0433\u0447\u0435\"],\n  [\"\u7684\", null],\n  [\"\u6e29\u6696\", \"\u0442\u0435\u043f\u043b\u044b\u0435\"],\n  [\"\u6708\u4efd\", \"\u043c\u0435\u0441\u044f\u0446\u044b\"],\n  [\"\u91cc\", null],\n  [\"\u5230\", null],\n  [\"\u6237\u5916\", \"\u043d\u0430 \u0443\u043b\u0438\u0446\u0435\"],\n  [\"\u6d3b\u52a8\", \"\u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c\"],\n  [\"\u5f88\u91cd\u8981\", \"\u043e\u0447\u0435\u043d\u044c \u0432\u0430\u0436\u043d\u043e\"],\n  [\"\uff0c\", null],\n  [\"\u8981\u628a\", \"\u043d\u0443\u0436\u043d\u043e\"],\n  [\"\u90a3\u4e9b\", \"\u0442\u0435\"],\n  [\"\u84dd\u5929\", \"\u044f\u0441\u043d\u044b\u0435\"],\n  [\"\u65e5\", \"\u0434\u043d\u0438\"],\n  [\"\u5f53\u4f5c\", \"\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043a\u0430\u043a\"],\n  [\"\u5728\", null],\n  [\"\u5386\u53f2\u60a0\u4e45\", \"\u0438\u0441\u0442\u043e\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u0445\"],\n  [\"\u7684\", null],\n  [\"\u80e1\u540c\", \"\u0445\u0443\u0442\u043e\u043d\u0433\u0430\u0445\"],\n  [\"\u91cc\", null],\n  [\"\u5f9c\", null],\n  [\"\u5f89\", \"\u043f\u0440\u043e\u0433\u0443\u043b\u044f\u0442\u044c\u0441\u044f\"],\n  [\"\u6216\u662f\", \"\u0438\u043b\u0438\"],\n  [\"\u63a2\u7d22\", \"\u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u044c\"],\n  [\"\u5907\", null],\n  [\"\u53d7\u6b22\u8fce\", \"\u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0439\"],\n  [\"\u7684\u65e5\", null],\n  [\"\u575b\", null],\n  [\"\u516c\u56ed\", \"\u043f\u0430\u0440\u043a\"],\n  [\"\u7684\u673a\u4f1a\", \"\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u62c9\u624e\u9c81\u65af (Lazarus) \uff0c\u4ee5\u53ca\u5979\u7684\u3001\u5728\u5979\u53bb\u4e16\u540e\u4e3e\u884c\u6d3b\u52a8\u5c06\u5979\u7684\u8bd7\u53e5\u94ed\u523b\u5728\u57fa\u5ea7\u4e0a\u7684\u670b\u53cb\u4eec\uff0c\u5f88\u6e05\u695a\u4ed6\u4eec\u6b63\u5728\u53c2\u4e0e\u7684\u884c\u4e3a\u5177\u6709\u9ad8\u5ea6\u653f\u6cbb\u6027\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u62c9', '\u624e', '\u9c81', '\u65af', ' (', 'L', 'azar', 'us', ')', ' \uff0c', '\u4ee5\u53ca', '\u5979\u7684', '\u3001', '\u5728', '\u5979', '\u53bb\u4e16', '\u540e', '\u4e3e\u884c', '\u6d3b\u52a8', '\u5c06', '\u5979\u7684', '\u8bd7\u53e5', '\u94ed', '\u523b', '\u5728', '\u57fa', '\u5ea7', '\u4e0a\u7684', '\u670b\u53cb\u4eec', '\uff0c', '\u5f88\u6e05\u695a', '\u4ed6\u4eec', '\u6b63\u5728', '\u53c2\u4e0e', '\u7684\u884c\u4e3a', '\u5177\u6709', '\u9ad8\u5ea6', '\u653f\u6cbb', '\u6027', '\u3002']"}], "[\n[\"\u62c9\", \"Lazarus\"],\n[\"\u624e\", null],\n[\"\u9c81\", null],\n[\"\u65af\", null],\n[\" (\", null],\n[\"L\", null],\n[\"azar\", null],\n[\"us\", null],\n[\")\", null],\n[\" \uff0c\", null],\n[\"\u4ee5\u53ca\", \"and\"],\n[\"\u5979\u7684\", \"her\"],\n[\"\u3001\", null],\n[\"\u5728\", null],\n[\"\u5979\", \"she\"],\n[\"\u53bb\u4e16\", \"death\"],\n[\"\u540e\", \"after\"],\n[\"\u4e3e\u884c\", \"held\"],\n[\"\u6d3b\u52a8\", \"event\"],\n[\"\u5c06\", null],\n[\"\u5979\u7684\", \"her\"],\n[\"\u8bd7\u53e5\", \"verses\"],\n[\"\u94ed\", \"inscribed\"],\n[\"\u523b\", null],\n[\"\u5728\", \"on\"],\n[\"\u57fa\", \"pedestal\"],\n[\"\u5ea7\", null],\n[\"\u4e0a\u7684\", null],\n[\"\u670b\u53cb\u4eec\", \"friends\"],\n[\"\uff0c\", null],\n[\"\u5f88\u6e05\u695a\", \"clearly understood\"],\n[\"\u4ed6\u4eec\", \"they\"],\n[\"\u6b63\u5728\", \"were\"],\n[\"\u53c2\u4e0e\", \"participating\"],\n[\"\u7684\u884c\u4e3a\", \"in an act\"],\n[\"\u5177\u6709\", \"that had\"],\n[\"\u9ad8\u5ea6\", \"highly\"],\n[\"\u653f\u6cbb\", \"political\"],\n[\"\u6027\", null],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u62c9\u624e\u9c81\u65af (Lazarus) \uff0c\u4ee5\u53ca\u5979\u7684\u3001\u5728\u5979\u53bb\u4e16\u540e\u4e3e\u884c\u6d3b\u52a8\u5c06\u5979\u7684\u8bd7\u53e5\u94ed\u523b\u5728\u57fa\u5ea7\u4e0a\u7684\u670b\u53cb\u4eec\uff0c\u5f88\u6e05\u695a\u4ed6\u4eec\u6b63\u5728\u53c2\u4e0e\u7684\u884c\u4e3a\u5177\u6709\u9ad8\u5ea6\u653f\u6cbb\u6027\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u62c9', '\u624e', '\u9c81', '\u65af', ' (', 'L', 'azar', 'us', ')', ' \uff0c', '\u4ee5\u53ca', '\u5979\u7684', '\u3001', '\u5728', '\u5979', '\u53bb\u4e16', '\u540e', '\u4e3e\u884c', '\u6d3b\u52a8', '\u5c06', '\u5979\u7684', '\u8bd7\u53e5', '\u94ed', '\u523b', '\u5728', '\u57fa', '\u5ea7', '\u4e0a\u7684', '\u670b\u53cb\u4eec', '\uff0c', '\u5f88\u6e05\u695a', '\u4ed6\u4eec', '\u6b63\u5728', '\u53c2\u4e0e', '\u7684\u884c\u4e3a', '\u5177\u6709', '\u9ad8\u5ea6', '\u653f\u6cbb', '\u6027', '\u3002']"}], "[\n[\"\u62c9\", \"\u041b\u0430\u0437\u0430\u0440\u0443\u0441\"],\n[\"\u624e\", null],\n[\"\u9c81\", null],\n[\"\u65af\", null],\n[\" (\", null],\n[\"L\", null],\n[\"azar\", null],\n[\"us\", null],\n[\")\", null],\n[\" \uff0c\", null],\n[\"\u4ee5\u53ca\", null],\n[\"\u5979\u7684\", \"\u043e\u043d\u044b\u04a3\"],\n[\"\u3001\", null],\n[\"\u5728\", null],\n[\"\u5979\", \"\u043e\u043b\"],\n[\"\u53bb\u4e16\", \"\u049b\u0430\u0439\u0442\u044b\u0441 \u0431\u043e\u043b\u0493\u0430\u043d\u043d\u0430\u043d\"],\n[\"\u540e\", \"\u043a\u0435\u0439\u0456\u043d\"],\n[\"\u4e3e\u884c\", \"\u04e9\u0442\u043a\u0456\u0437\u0456\u043b\u0433\u0435\u043d\"],\n[\"\u6d3b\u52a8\", \"\u0456\u0441-\u0448\u0430\u0440\u0430\"],\n[\"\u5c06\", null],\n[\"\u5979\u7684\", \"\u043e\u043d\u044b\u04a3\"],\n[\"\u8bd7\u53e5\", \"\u04e9\u043b\u0435\u04a3\u0434\u0435\u0440\u0456\"],\n[\"\u94ed\", \"\u049b\u0430\u0448\u0430\u043b\u0493\u0430\u043d\"],\n[\"\u523b\", null],\n[\"\u5728\", null],\n[\"\u57fa\", \"\u0442\u04b1\u0493\u044b\u0440\u0493\u0430\"],\n[\"\u5ea7\", null],\n[\"\u4e0a\u7684\", null],\n[\"\u670b\u53cb\u4eec\", \"\u0434\u043e\u0441\u0442\u0430\u0440\u044b\"],\n[\"\uff0c\", null],\n[\"\u5f88\u6e05\u695a\", \"\u0436\u0430\u049b\u0441\u044b \u0442\u04af\u0441\u0456\u043d\u0434\u0456\"],\n[\"\u4ed6\u4eec\", \"\u043e\u043b\u0430\u0440\"],\n[\"\u6b63\u5728\", null],\n[\"\u53c2\u4e0e\", \"\u049b\u0430\u0442\u044b\u0441\u044b\u043f \u0436\u0430\u0442\u049b\u0430\u043d\"],\n[\"\u7684\u884c\u4e3a\", \"\u04d9\u0440\u0435\u043a\u0435\u0442\u0442\u0456\u04a3\"],\n[\"\u5177\u6709\", null],\n[\"\u9ad8\u5ea6\", \"\u0436\u043e\u0493\u0430\u0440\u044b\"],\n[\"\u653f\u6cbb\", \"\u0441\u0430\u044f\u0441\u0438\"],\n[\"\u6027\", null],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u62c9\u624e\u9c81\u65af (Lazarus) \uff0c\u4ee5\u53ca\u5979\u7684\u3001\u5728\u5979\u53bb\u4e16\u540e\u4e3e\u884c\u6d3b\u52a8\u5c06\u5979\u7684\u8bd7\u53e5\u94ed\u523b\u5728\u57fa\u5ea7\u4e0a\u7684\u670b\u53cb\u4eec\uff0c\u5f88\u6e05\u695a\u4ed6\u4eec\u6b63\u5728\u53c2\u4e0e\u7684\u884c\u4e3a\u5177\u6709\u9ad8\u5ea6\u653f\u6cbb\u6027\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u62c9', '\u624e', '\u9c81', '\u65af', ' (', 'L', 'azar', 'us', ')', ' \uff0c', '\u4ee5\u53ca', '\u5979\u7684', '\u3001', '\u5728', '\u5979', '\u53bb\u4e16', '\u540e', '\u4e3e\u884c', '\u6d3b\u52a8', '\u5c06', '\u5979\u7684', '\u8bd7\u53e5', '\u94ed', '\u523b', '\u5728', '\u57fa', '\u5ea7', '\u4e0a\u7684', '\u670b\u53cb\u4eec', '\uff0c', '\u5f88\u6e05\u695a', '\u4ed6\u4eec', '\u6b63\u5728', '\u53c2\u4e0e', '\u7684\u884c\u4e3a', '\u5177\u6709', '\u9ad8\u5ea6', '\u653f\u6cbb', '\u6027', '\u3002']"}], "[\n  [\"\u62c9\", \"\u041b\u0430\u0437\u0430\u0440\u044c\"],\n  [\"\u624e\", null],\n  [\"\u9c81\", null],\n  [\"\u65af\", null],\n  [\" (\", null],\n  [\"L\", null],\n  [\"azar\", null],\n  [\"us\", null],\n  [\")\", null],\n  [\" \uff0c\", null],\n  [\"\u4ee5\u53ca\", \"\u0438\"],\n  [\"\u5979\u7684\", \"\u0435\u0451\"],\n  [\"\u3001\", null],\n  [\"\u5728\", null],\n  [\"\u5979\", \"\u043e\u043d\u0430\"],\n  [\"\u53bb\u4e16\", \"\u0441\u043c\u0435\u0440\u0442\u044c\"],\n  [\"\u540e\", \"\u043f\u043e\u0441\u043b\u0435\"],\n  [\"\u4e3e\u884c\", \"\u043f\u0440\u043e\u0432\u043e\u0434\u0438\u0442\u044c\"],\n  [\"\u6d3b\u52a8\", \"\u043c\u0435\u0440\u043e\u043f\u0440\u0438\u044f\u0442\u0438\u0435\"],\n  [\"\u5c06\", null],\n  [\"\u5979\u7684\", \"\u0435\u0451\"],\n  [\"\u8bd7\u53e5\", \"\u0441\u0442\u0438\u0445\u0438\"],\n  [\"\u94ed\", \"\u0433\u0440\u0430\u0432\u0438\u0440\u043e\u0432\u0430\u0442\u044c\"],\n  [\"\u523b\", null],\n  [\"\u5728\", \"\u043d\u0430\"],\n  [\"\u57fa\", \"\u043f\u044c\u0435\u0434\u0435\u0441\u0442\u0430\u043b\"],\n  [\"\u5ea7\", null],\n  [\"\u4e0a\u7684\", null],\n  [\"\u670b\u53cb\u4eec\", \"\u0434\u0440\u0443\u0437\u044c\u044f\"],\n  [\"\uff0c\", null],\n  [\"\u5f88\u6e05\u695a\", \"\u0445\u043e\u0440\u043e\u0448\u043e \u043f\u043e\u043d\u0438\u043c\u0430\u043b\u0438\"],\n  [\"\u4ed6\u4eec\", \"\u043e\u043d\u0438\"],\n  [\"\u6b63\u5728\", null],\n  [\"\u53c2\u4e0e\", \"\u0443\u0447\u0430\u0441\u0442\u0432\u043e\u0432\u0430\u043b\u0438\"],\n  [\"\u7684\u884c\u4e3a\", \"\u0432 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438\"],\n  [\"\u5177\u6709\", \"\u0438\u043c\u0435\u044e\u0449\u0435\u043c\"],\n  [\"\u9ad8\u5ea6\", \"\u0432\u044b\u0441\u043e\u043a\u043e\"],\n  [\"\u653f\u6cbb\", \"\u043f\u043e\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439\"],\n  [\"\u6027\", null],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u6839\u636e\u5927\u4f17\u7684\u6295\u7968\uff0cFacebook CEO\u9a6c\u514b\u00b7\u624e\u514b\u4f2f\u683c\uff08Mark Zuckerberg\uff09\u81ea\u5236\u5bb6\u7528\u4eba\u5de5\u667a\u80fd\u52a9\u7406Jarvis\u5c06\u4f7f\u7528\u7f8e\u56fd\u5f71\u661f\u6469\u6839\u00b7\u5f17\u91cc\u66fc\uff08Morgan Freeman\uff09\u7684\u58f0\u97f3\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u6839\u636e', '\u5927\u4f17', '\u7684', '\u6295\u7968', '\uff0c', 'Facebook', ' CEO', '\u9a6c\u514b', '\u00b7', '\u624e', '\u514b', '\u4f2f', '\u683c', '\uff08', 'Mark', ' Zuckerberg', '\uff09', '\u81ea\u5236', '\u5bb6\u7528', '\u4eba\u5de5\u667a\u80fd', '\u52a9\u7406', 'Jar', 'vis', '\u5c06', '\u4f7f\u7528', '\u7f8e\u56fd', '\u5f71', '\u661f', '\u6469', '\u6839', '\u00b7', '\u5f17', '\u91cc', '\u66fc', '\uff08', 'M', 'organ', ' Freeman', '\uff09', '\u7684\u58f0\u97f3', '\u3002']"}], "[\n[\"\u6839\u636e\", \"According to\"],\n[\"\u5927\u4f17\", \"public\"],\n[\"\u7684\", null],\n[\"\u6295\u7968\", \"vote\"],\n[\"\uff0c\", null],\n[\"Facebook\", \"Facebook\"],\n[\" CEO\", \"CEO\"],\n[\"\u9a6c\u514b\", \"Mark\"],\n[\"\u00b7\", null],\n[\"\u624e\", null],\n[\"\u514b\", null],\n[\"\u4f2f\", null],\n[\"\u683c\", null],\n[\"\uff08\", null],\n[\"Mark\", \"Mark\"],\n[\" Zuckerberg\", \"Zuckerberg\"],\n[\"\uff09\", null],\n[\"\u81ea\u5236\", \"homemade\"],\n[\"\u5bb6\u7528\", \"home\"],\n[\"\u4eba\u5de5\u667a\u80fd\", \"artificial intelligence\"],\n[\"\u52a9\u7406\", \"assistant\"],\n[\"Jar\", \"Jarvis\"],\n[\"vis\", null],\n[\"\u5c06\", null],\n[\"\u4f7f\u7528\", \"use\"],\n[\"\u7f8e\u56fd\", \"American\"],\n[\"\u5f71\", null],\n[\"\u661f\", \"actor\"],\n[\"\u6469\", null],\n[\"\u6839\", null],\n[\"\u00b7\", null],\n[\"\u5f17\", null],\n[\"\u91cc\", null],\n[\"\u66fc\", null],\n[\"\uff08\", null],\n[\"M\", null],\n[\"organ\", null],\n[\" Freeman\", \"Freeman\"],\n[\"\uff09\", null],\n[\"\u7684\u58f0\u97f3\", \"voice\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u6839\u636e\u5927\u4f17\u7684\u6295\u7968\uff0cFacebook CEO\u9a6c\u514b\u00b7\u624e\u514b\u4f2f\u683c\uff08Mark Zuckerberg\uff09\u81ea\u5236\u5bb6\u7528\u4eba\u5de5\u667a\u80fd\u52a9\u7406Jarvis\u5c06\u4f7f\u7528\u7f8e\u56fd\u5f71\u661f\u6469\u6839\u00b7\u5f17\u91cc\u66fc\uff08Morgan Freeman\uff09\u7684\u58f0\u97f3\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u6839\u636e', '\u5927\u4f17', '\u7684', '\u6295\u7968', '\uff0c', 'Facebook', ' CEO', '\u9a6c\u514b', '\u00b7', '\u624e', '\u514b', '\u4f2f', '\u683c', '\uff08', 'Mark', ' Zuckerberg', '\uff09', '\u81ea\u5236', '\u5bb6\u7528', '\u4eba\u5de5\u667a\u80fd', '\u52a9\u7406', 'Jar', 'vis', '\u5c06', '\u4f7f\u7528', '\u7f8e\u56fd', '\u5f71', '\u661f', '\u6469', '\u6839', '\u00b7', '\u5f17', '\u91cc', '\u66fc', '\uff08', 'M', 'organ', ' Freeman', '\uff09', '\u7684\u58f0\u97f3', '\u3002']"}], "[\n[\"\u6839\u636e\", \"\u0441\u04d9\u0439\u043a\u0435\u0441\"],\n[\"\u5927\u4f17\", \"\u043a\u04e9\u043f\u0448\u0456\u043b\u0456\u043a\"],\n[\"\u7684\", null],\n[\"\u6295\u7968\", \"\u0434\u0430\u0443\u044b\u0441 \u0431\u0435\u0440\u0443\"],\n[\"\uff0c\", null],\n[\"Facebook\", \"Facebook\"],\n[\" CEO\", \"\u0431\u0430\u0441 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\"],\n[\"\u9a6c\u514b\", \"\u041c\u0430\u0440\u043a\"],\n[\"\u00b7\", null],\n[\"\u624e\", null],\n[\"\u514b\", null],\n[\"\u4f2f\", null],\n[\"\u683c\", null],\n[\"\uff08\", null],\n[\"Mark\", \"\u041c\u0430\u0440\u043a\"],\n[\" Zuckerberg\", \"\u0426\u0443\u043a\u0435\u0440\u0431\u0435\u0440\u0433\"],\n[\"\uff09\", null],\n[\"\u81ea\u5236\", \"\u04e9\u0437\u0456 \u0436\u0430\u0441\u0430\u0493\u0430\u043d\"],\n[\"\u5bb6\u7528\", \"\u04af\u0439\u0433\u0435 \u0430\u0440\u043d\u0430\u043b\u0493\u0430\u043d\"],\n[\"\u4eba\u5de5\u667a\u80fd\", \"\u0436\u0430\u0441\u0430\u043d\u0434\u044b \u0438\u043d\u0442\u0435\u043b\u043b\u0435\u043a\u0442\"],\n[\"\u52a9\u7406\", \"\u043a\u04e9\u043c\u0435\u043a\u0448\u0456\"],\n[\"Jar\", \"\u0414\u0436\u0430\u0440\u0432\u0438\u0441\"],\n[\"vis\", null],\n[\"\u5c06\", null],\n[\"\u4f7f\u7528\", \"\u043f\u0430\u0439\u0434\u0430\u043b\u0430\u043d\u0430\u0434\u044b\"],\n[\"\u7f8e\u56fd\", \"\u0410\u043c\u0435\u0440\u0438\u043a\u0430\"],\n[\"\u5f71\", null],\n[\"\u661f\", \"\u0436\u04b1\u043b\u0434\u044b\u0437\u044b\"],\n[\"\u6469\", null],\n[\"\u6839\", null],\n[\"\u00b7\", null],\n[\"\u5f17\", null],\n[\"\u91cc\", null],\n[\"\u66fc\", null],\n[\"\uff08\", null],\n[\"M\", \"\u041c\u043e\u0440\u0433\u0430\u043d\"],\n[\"organ\", null],\n[\" Freeman\", \"\u0424\u0440\u0438\u043c\u0430\u043d\"],\n[\"\uff09\", null],\n[\"\u7684\u58f0\u97f3\", \"\u0434\u0430\u0443\u044b\u0441\u044b\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u6839\u636e\u5927\u4f17\u7684\u6295\u7968\uff0cFacebook CEO\u9a6c\u514b\u00b7\u624e\u514b\u4f2f\u683c\uff08Mark Zuckerberg\uff09\u81ea\u5236\u5bb6\u7528\u4eba\u5de5\u667a\u80fd\u52a9\u7406Jarvis\u5c06\u4f7f\u7528\u7f8e\u56fd\u5f71\u661f\u6469\u6839\u00b7\u5f17\u91cc\u66fc\uff08Morgan Freeman\uff09\u7684\u58f0\u97f3\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u6839\u636e', '\u5927\u4f17', '\u7684', '\u6295\u7968', '\uff0c', 'Facebook', ' CEO', '\u9a6c\u514b', '\u00b7', '\u624e', '\u514b', '\u4f2f', '\u683c', '\uff08', 'Mark', ' Zuckerberg', '\uff09', '\u81ea\u5236', '\u5bb6\u7528', '\u4eba\u5de5\u667a\u80fd', '\u52a9\u7406', 'Jar', 'vis', '\u5c06', '\u4f7f\u7528', '\u7f8e\u56fd', '\u5f71', '\u661f', '\u6469', '\u6839', '\u00b7', '\u5f17', '\u91cc', '\u66fc', '\uff08', 'M', 'organ', ' Freeman', '\uff09', '\u7684\u58f0\u97f3', '\u3002']"}], "[\n[\"\u6839\u636e\", \"\u0421\u043e\u0433\u043b\u0430\u0441\u043d\u043e\"],\n[\"\u5927\u4f17\", \"\u043e\u0431\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u043c\u0443\"],\n[\"\u7684\", null],\n[\"\u6295\u7968\", \"\u0433\u043e\u043b\u043e\u0441\u043e\u0432\u0430\u043d\u0438\u044e\"],\n[\"\uff0c\", null],\n[\"Facebook\", \"Facebook\"],\n[\" CEO\", null],\n[\"\u9a6c\u514b\", \"\u041c\u0430\u0440\u043a\"],\n[\"\u00b7\", null],\n[\"\u624e\", null],\n[\"\u514b\", null],\n[\"\u4f2f\", null],\n[\"\u683c\", null],\n[\"\uff08\", null],\n[\"Mark\", null],\n[\" Zuckerberg\", null],\n[\"\uff09\", null],\n[\"\u81ea\u5236\", \"\u0441\u0430\u043c\u043e\u0434\u0435\u043b\u044c\u043d\u044b\u0439\"],\n[\"\u5bb6\u7528\", \"\u0434\u043e\u043c\u0430\u0448\u043d\u0438\u0439\"],\n[\"\u4eba\u5de5\u667a\u80fd\", \"\u0438\u0441\u043a\u0443\u0441\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439 \u0438\u043d\u0442\u0435\u043b\u043b\u0435\u043a\u0442\"],\n[\"\u52a9\u7406\", \"\u043f\u043e\u043c\u043e\u0449\u043d\u0438\u043a\"],\n[\"Jar\", null],\n[\"vis\", null],\n[\"\u5c06\", null],\n[\"\u4f7f\u7528\", \"\u0431\u0443\u0434\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\"],\n[\"\u7f8e\u56fd\", \"\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0441\u043a\u043e\u0433\u043e\"],\n[\"\u5f71\", null],\n[\"\u661f\", \"\u0430\u043a\u0442\u0435\u0440\u0430\"],\n[\"\u6469\", null],\n[\"\u6839\", null],\n[\"\u00b7\", null],\n[\"\u5f17\", null],\n[\"\u91cc\", null],\n[\"\u66fc\", null],\n[\"\uff08\", null],\n[\"M\", null],\n[\"organ\", null],\n[\" Freeman\", null],\n[\"\uff09\", null],\n[\"\u7684\u58f0\u97f3\", \"\u0433\u043e\u043b\u043e\u0441\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u4f5c\u4e3a\u66fe\u57282010\u5e74\u5e7f\u5dde\u4e9a\u519b\u4f1a\u65af\u8bfa\u514b\u9879\u76ee\u4e2d\u51fb\u8d25\u4e01\u4fca\u6656\uff0c\u83b7\u5f97\u8be5\u9879\u76ee\u91d1\u724c\u7684\u4e16\u754c\u7ea7\u9009\u624b\uff0c\u5085\u5bb6\u4fca\u5e94\u8be5\u662f\u9999\u6e2f\u73b0\u5f79\u8fd0\u52a8\u5458\u4e2d\u77e5\u540d\u5ea6\u6700\u9ad8\u7684\u65af\u8bfa\u514b\u7403\u624b\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u4f5c\u4e3a', '\u66fe\u5728', '2', '0', '1', '0', '\u5e74', '\u5e7f\u5dde', '\u4e9a\u519b', '\u4f1a', '\u65af', '\u8bfa', '\u514b', '\u9879\u76ee', '\u4e2d', '\u51fb\u8d25', '\u4e01', '\u4fca', '\u6656', '\uff0c', '\u83b7\u5f97', '\u8be5\u9879\u76ee', '\u91d1\u724c', '\u7684\u4e16\u754c', '\u7ea7', '\u9009\u624b', '\uff0c', '\u5085', '\u5bb6', '\u4fca', '\u5e94\u8be5\u662f', '\u9999\u6e2f', '\u73b0', '\u5f79', '\u8fd0\u52a8\u5458', '\u4e2d', '\u77e5\u540d\u5ea6', '\u6700\u9ad8\u7684', '\u65af', '\u8bfa', '\u514b', '\u7403', '\u624b', '\u3002']"}], "[\n  [\"\u4f5c\u4e3a\", \"As\"],\n  [\"\u66fe\u5728\", null],\n  [\"2\", null],\n  [\"0\", null],\n  [\"1\", null],\n  [\"0\", null],\n  [\"\u5e74\", null],\n  [\"\u5e7f\u5dde\", \"Guangzhou\"],\n  [\"\u4e9a\u519b\", \"Asian Games\"],\n  [\"\u4f1a\", null],\n  [\"\u65af\", null],\n  [\"\u8bfa\", null],\n  [\"\u514b\", null],\n  [\"\u9879\u76ee\", \"event\"],\n  [\"\u4e2d\", null],\n  [\"\u51fb\u8d25\", \"defeated\"],\n  [\"\u4e01\", null],\n  [\"\u4fca\", null],\n  [\"\u6656\", \"Ding Junhui\"],\n  [\"\uff0c\", null],\n  [\"\u83b7\u5f97\", \"won\"],\n  [\"\u8be5\u9879\u76ee\", \"the event\"],\n  [\"\u91d1\u724c\", \"gold medal\"],\n  [\"\u7684\u4e16\u754c\", \"world\"],\n  [\"\u7ea7\", null],\n  [\"\u9009\u624b\", \"player\"],\n  [\"\uff0c\", null],\n  [\"\u5085\", null],\n  [\"\u5bb6\", null],\n  [\"\u4fca\", \"Marco Fu\"],\n  [\"\u5e94\u8be5\u662f\", \"should be\"],\n  [\"\u9999\u6e2f\", \"Hong Kong\"],\n  [\"\u73b0\", null],\n  [\"\u5f79\", null],\n  [\"\u8fd0\u52a8\u5458\", \"athlete\"],\n  [\"\u4e2d\", null],\n  [\"\u77e5\u540d\u5ea6\", \"fame\"],\n  [\"\u6700\u9ad8\u7684\", \"highest\"],\n  [\"\u65af\", null],\n  [\"\u8bfa\", null],\n  [\"\u514b\", null],\n  [\"\u7403\", null],\n  [\"\u624b\", \"snooker player\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u4f5c\u4e3a\u66fe\u57282010\u5e74\u5e7f\u5dde\u4e9a\u519b\u4f1a\u65af\u8bfa\u514b\u9879\u76ee\u4e2d\u51fb\u8d25\u4e01\u4fca\u6656\uff0c\u83b7\u5f97\u8be5\u9879\u76ee\u91d1\u724c\u7684\u4e16\u754c\u7ea7\u9009\u624b\uff0c\u5085\u5bb6\u4fca\u5e94\u8be5\u662f\u9999\u6e2f\u73b0\u5f79\u8fd0\u52a8\u5458\u4e2d\u77e5\u540d\u5ea6\u6700\u9ad8\u7684\u65af\u8bfa\u514b\u7403\u624b\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u4f5c\u4e3a', '\u66fe\u5728', '2', '0', '1', '0', '\u5e74', '\u5e7f\u5dde', '\u4e9a\u519b', '\u4f1a', '\u65af', '\u8bfa', '\u514b', '\u9879\u76ee', '\u4e2d', '\u51fb\u8d25', '\u4e01', '\u4fca', '\u6656', '\uff0c', '\u83b7\u5f97', '\u8be5\u9879\u76ee', '\u91d1\u724c', '\u7684\u4e16\u754c', '\u7ea7', '\u9009\u624b', '\uff0c', '\u5085', '\u5bb6', '\u4fca', '\u5e94\u8be5\u662f', '\u9999\u6e2f', '\u73b0', '\u5f79', '\u8fd0\u52a8\u5458', '\u4e2d', '\u77e5\u540d\u5ea6', '\u6700\u9ad8\u7684', '\u65af', '\u8bfa', '\u514b', '\u7403', '\u624b', '\u3002']"}], "[\n[\"\u4f5c\u4e3a\", null],\n[\"\u66fe\u5728\", null],\n[\"2\", null],\n[\"0\", null],\n[\"1\", null],\n[\"0\", null],\n[\"\u5e74\", null],\n[\"\u5e7f\u5dde\", \"\u0413\u0443\u0430\u043d\u0447\u0436\u043e\u0443\"],\n[\"\u4e9a\u519b\", \"\u0410\u0437\u0438\u044f\"],\n[\"\u4f1a\", null],\n[\"\u65af\", null],\n[\"\u8bfa\", null],\n[\"\u514b\", null],\n[\"\u9879\u76ee\", \"\u0436\u043e\u0431\u0430\"],\n[\"\u4e2d\", null],\n[\"\u51fb\u8d25\", \"\u0436\u0435\u04a3\u0443\"],\n[\"\u4e01\", null],\n[\"\u4fca\", null],\n[\"\u6656\", null],\n[\"\uff0c\", null],\n[\"\u83b7\u5f97\", \"\u0430\u043b\u0443\"],\n[\"\u8be5\u9879\u76ee\", \"\u043e\u0441\u044b \u0436\u043e\u0431\u0430\"],\n[\"\u91d1\u724c\", \"\u0430\u043b\u0442\u044b\u043d \u043c\u0435\u0434\u0430\u043b\u044c\"],\n[\"\u7684\u4e16\u754c\", \"\u04d9\u043b\u0435\u043c\u0434\u0456\u043a\"],\n[\"\u7ea7\", null],\n[\"\u9009\u624b\", \"\u043e\u0439\u044b\u043d\u0448\u044b\"],\n[\"\uff0c\", null],\n[\"\u5085\", null],\n[\"\u5bb6\", null],\n[\"\u4fca\", null],\n[\"\u5e94\u8be5\u662f\", \"\u0431\u043e\u043b\u0443\u044b \u043a\u0435\u0440\u0435\u043a\"],\n[\"\u9999\u6e2f\", \"\u0413\u043e\u043d\u043a\u043e\u043d\u0433\"],\n[\"\u73b0\", null],\n[\"\u5f79\", null],\n[\"\u8fd0\u52a8\u5458\", \"\u0441\u043f\u043e\u0440\u0442\u0448\u044b\"],\n[\"\u4e2d\", null],\n[\"\u77e5\u540d\u5ea6\", \"\u0442\u0430\u043d\u044b\u043c\u0430\u043b\u0434\u044b\u043b\u044b\u0493\u044b\"],\n[\"\u6700\u9ad8\u7684\", \"\u0435\u04a3 \u0436\u043e\u0493\u0430\u0440\u044b\"],\n[\"\u65af\", null],\n[\"\u8bfa\", null],\n[\"\u514b\", null],\n[\"\u7403\", null],\n[\"\u624b\", null],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u4f5c\u4e3a\u66fe\u57282010\u5e74\u5e7f\u5dde\u4e9a\u519b\u4f1a\u65af\u8bfa\u514b\u9879\u76ee\u4e2d\u51fb\u8d25\u4e01\u4fca\u6656\uff0c\u83b7\u5f97\u8be5\u9879\u76ee\u91d1\u724c\u7684\u4e16\u754c\u7ea7\u9009\u624b\uff0c\u5085\u5bb6\u4fca\u5e94\u8be5\u662f\u9999\u6e2f\u73b0\u5f79\u8fd0\u52a8\u5458\u4e2d\u77e5\u540d\u5ea6\u6700\u9ad8\u7684\u65af\u8bfa\u514b\u7403\u624b\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u4f5c\u4e3a', '\u66fe\u5728', '2', '0', '1', '0', '\u5e74', '\u5e7f\u5dde', '\u4e9a\u519b', '\u4f1a', '\u65af', '\u8bfa', '\u514b', '\u9879\u76ee', '\u4e2d', '\u51fb\u8d25', '\u4e01', '\u4fca', '\u6656', '\uff0c', '\u83b7\u5f97', '\u8be5\u9879\u76ee', '\u91d1\u724c', '\u7684\u4e16\u754c', '\u7ea7', '\u9009\u624b', '\uff0c', '\u5085', '\u5bb6', '\u4fca', '\u5e94\u8be5\u662f', '\u9999\u6e2f', '\u73b0', '\u5f79', '\u8fd0\u52a8\u5458', '\u4e2d', '\u77e5\u540d\u5ea6', '\u6700\u9ad8\u7684', '\u65af', '\u8bfa', '\u514b', '\u7403', '\u624b', '\u3002']"}], "[\n[\"\u4f5c\u4e3a\", \"\u041a\u0430\u043a\"],\n[\"\u66fe\u5728\", null],\n[\"2\", null],\n[\"0\", null],\n[\"1\", null],\n[\"0\", null],\n[\"\u5e74\", null],\n[\"\u5e7f\u5dde\", \"\u0413\u0443\u0430\u043d\u0447\u0436\u043e\u0443\"],\n[\"\u4e9a\u519b\", \"\u0410\u0437\u0438\u0430\u0442\u0441\u043a\u0438\u0445\"],\n[\"\u4f1a\", null],\n[\"\u65af\", null],\n[\"\u8bfa\", null],\n[\"\u514b\", null],\n[\"\u9879\u76ee\", \"\u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u044f\u0445\"],\n[\"\u4e2d\", null],\n[\"\u51fb\u8d25\", \"\u043f\u043e\u0431\u0435\u0434\u0438\u0432\"],\n[\"\u4e01\", null],\n[\"\u4fca\", null],\n[\"\u6656\", \"\u0414\u0438\u043d \u0426\u0437\u044e\u043d\u044c\u0445\u0443\u044d\u044f\"],\n[\"\uff0c\", null],\n[\"\u83b7\u5f97\", \"\u0437\u0430\u0432\u043e\u0435\u0432\u0430\u0432\"],\n[\"\u8be5\u9879\u76ee\", \"\u0432 \u044d\u0442\u043e\u043c \u0432\u0438\u0434\u0435\"],\n[\"\u91d1\u724c\", \"\u0437\u043e\u043b\u043e\u0442\u0443\u044e \u043c\u0435\u0434\u0430\u043b\u044c\"],\n[\"\u7684\u4e16\u754c\", \"\u043c\u0438\u0440\u043e\u0432\u043e\u0433\u043e\"],\n[\"\u7ea7\", null],\n[\"\u9009\u624b\", \"\u0443\u0440\u043e\u0432\u043d\u044f\"],\n[\"\uff0c\", null],\n[\"\u5085\", null],\n[\"\u5bb6\", null],\n[\"\u4fca\", \"\u041c\u0430\u0440\u043a\u043e \u0424\u0443\"],\n[\"\u5e94\u8be5\u662f\", \"\u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\"],\n[\"\u9999\u6e2f\", \"\u0413\u043e\u043d\u043a\u043e\u043d\u0433\u0430\"],\n[\"\u73b0\", null],\n[\"\u5f79\", null],\n[\"\u8fd0\u52a8\u5458\", \"\u0441\u043f\u043e\u0440\u0442\u0441\u043c\u0435\u043d\u043e\u0432\"],\n[\"\u4e2d\", null],\n[\"\u77e5\u540d\u5ea6\", \"\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u043c\"],\n[\"\u6700\u9ad8\u7684\", \"\u0441\u0430\u043c\u044b\u043c\"],\n[\"\u65af\", null],\n[\"\u8bfa\", null],\n[\"\u514b\", null],\n[\"\u7403\", null],\n[\"\u624b\", \"\u0441\u043d\u0443\u043a\u0435\u0440\u0438\u0441\u0442\u043e\u043c\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u4e3a\u6b64\uff0c\u5979\u8058\u8bf7\u4e86\u6765\u81ea Duck\uff06Shed \u5efa\u7b51\u548c\u8bbe\u8ba1\u5de5\u4f5c\u5ba4\uff08\u4f4d\u4e8e\u8bfa\u4e01\u5c71\uff09\u7684\u57c3\u5c14\u51e1\u00b7\u963f\u624e\u8fea\u6765\u5229\u7528\u8fd9\u4e2a 12.5 \u82f1\u5c3a\u9ad8\u7684\u5929\u82b1\u677f\uff0c\u7cbe\u5fc3\u6253\u9020\u4e00\u4e2a\u5bb6\u3002\u8fd9\u4e2a\u5bb6\u4e0d\u4ec5\u662f\u5f00\u53d1\u9879\u76ee\uff0c\u8fd8\u662f\u4e54\u5b89\u59ae\u5c45\u4f4f\u7684\u5730\u65b9\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u4e3a\u6b64', '\uff0c', '\u5979', '\u8058\u8bf7', '\u4e86', '\u6765\u81ea', ' Duck', '\uff06', 'Sh', 'ed', '', '', '', '\u7b51', '\u548c', '\u8bbe\u8ba1', '\u5de5\u4f5c\u5ba4', '\uff08', '\u4f4d\u4e8e', '\u8bfa', '\u4e01', '\u5c71', '\uff09', '\u7684', '\u57c3\u5c14', '\u51e1', '\u00b7', '\u963f', '\u624e', '\u8fea', '\u6765', '\u5229\u7528', '\u8fd9\u4e2a', ' ', '1', '2', '.', '5', '', '', '\u5c3a', '\u9ad8\u7684', '\u5929\u82b1\u677f', '\uff0c', '\u7cbe\u5fc3', '\u6253\u9020', '\u4e00\u4e2a', '\u5bb6', '\u3002', '\u8fd9\u4e2a', '\u5bb6', '\u4e0d\u4ec5\u662f', '\u5f00\u53d1', '\u9879\u76ee', '\uff0c', '\u8fd8\u662f', '\u4e54', '\u5b89', '\u59ae', '\u5c45\u4f4f', '\u7684\u5730\u65b9', '\u3002']"}], "[\n[\"\u4e3a\u6b64\", \"For this\"],\n[\"\uff0c\", null],\n[\"\u5979\", \"she\"],\n[\"\u8058\u8bf7\", \"hired\"],\n[\"\u4e86\", null],\n[\"\u6765\u81ea\", \"from\"],\n[\" Duck\", \"Duck\"],\n[\"\uff06\", null],\n[\"Sh\", null],\n[\"ed\", null],\n[\"\", null],\n[\"\", null],\n[\"\", null],\n[\"\u7b51\", \"architecture\"],\n[\"\u548c\", null],\n[\"\u8bbe\u8ba1\", \"design\"],\n[\"\u5de5\u4f5c\u5ba4\", \"studio\"],\n[\"\uff08\", null],\n[\"\u4f4d\u4e8e\", \"located in\"],\n[\"\u8bfa\", null],\n[\"\u4e01\", null],\n[\"\u5c71\", \"Notting Hill\"],\n[\"\uff09\", null],\n[\"\u7684\", null],\n[\"\u57c3\u5c14\", null],\n[\"\u51e1\", null],\n[\"\u00b7\", null],\n[\"\u963f\", null],\n[\"\u624e\", null],\n[\"\u8fea\", \"Elfan Azadi\"],\n[\"\u6765\", \"to\"],\n[\"\u5229\u7528\", \"utilize\"],\n[\"\u8fd9\u4e2a\", \"this\"],\n[\" \", null],\n[\"1\", null],\n[\"2\", null],\n[\".\", null],\n[\"5\", null],\n[\"\", null],\n[\"\", null],\n[\"\u5c3a\", \"foot\"],\n[\"\u9ad8\u7684\", \"high\"],\n[\"\u5929\u82b1\u677f\", \"ceiling\"],\n[\"\uff0c\", null],\n[\"\u7cbe\u5fc3\", \"carefully\"],\n[\"\u6253\u9020\", \"create\"],\n[\"\u4e00\u4e2a\", \"a\"],\n[\"\u5bb6\", \"home\"],\n[\"\u3002\", null],\n[\"\u8fd9\u4e2a\", \"This\"],\n[\"\u5bb6\", \"home\"],\n[\"\u4e0d\u4ec5\u662f\", \"is not only\"],\n[\"\u5f00\u53d1\", \"development\"],\n[\"\u9879\u76ee\", \"project\"],\n[\"\uff0c\", null],\n[\"\u8fd8\u662f\", \"but also\"],\n[\"\u4e54\", null],\n[\"\u5b89\", null],\n[\"\u59ae\", \"Joanne\"],\n[\"\u5c45\u4f4f\", \"living\"],\n[\"\u7684\u5730\u65b9\", \"place\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u4e3a\u6b64\uff0c\u5979\u8058\u8bf7\u4e86\u6765\u81ea Duck\uff06Shed \u5efa\u7b51\u548c\u8bbe\u8ba1\u5de5\u4f5c\u5ba4\uff08\u4f4d\u4e8e\u8bfa\u4e01\u5c71\uff09\u7684\u57c3\u5c14\u51e1\u00b7\u963f\u624e\u8fea\u6765\u5229\u7528\u8fd9\u4e2a 12.5 \u82f1\u5c3a\u9ad8\u7684\u5929\u82b1\u677f\uff0c\u7cbe\u5fc3\u6253\u9020\u4e00\u4e2a\u5bb6\u3002\u8fd9\u4e2a\u5bb6\u4e0d\u4ec5\u662f\u5f00\u53d1\u9879\u76ee\uff0c\u8fd8\u662f\u4e54\u5b89\u59ae\u5c45\u4f4f\u7684\u5730\u65b9\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u4e3a\u6b64', '\uff0c', '\u5979', '\u8058\u8bf7', '\u4e86', '\u6765\u81ea', ' Duck', '\uff06', 'Sh', 'ed', '', '', '', '\u7b51', '\u548c', '\u8bbe\u8ba1', '\u5de5\u4f5c\u5ba4', '\uff08', '\u4f4d\u4e8e', '\u8bfa', '\u4e01', '\u5c71', '\uff09', '\u7684', '\u57c3\u5c14', '\u51e1', '\u00b7', '\u963f', '\u624e', '\u8fea', '\u6765', '\u5229\u7528', '\u8fd9\u4e2a', ' ', '1', '2', '.', '5', '', '', '\u5c3a', '\u9ad8\u7684', '\u5929\u82b1\u677f', '\uff0c', '\u7cbe\u5fc3', '\u6253\u9020', '\u4e00\u4e2a', '\u5bb6', '\u3002', '\u8fd9\u4e2a', '\u5bb6', '\u4e0d\u4ec5\u662f', '\u5f00\u53d1', '\u9879\u76ee', '\uff0c', '\u8fd8\u662f', '\u4e54', '\u5b89', '\u59ae', '\u5c45\u4f4f', '\u7684\u5730\u65b9', '\u3002']"}], "[\n[\"\u4e3a\u6b64\", \"\u043e\u0441\u044b\u0493\u0430\u043d\"],\n[\"\uff0c\", null],\n[\"\u5979\", \"\u043e\u043b\"],\n[\"\u8058\u8bf7\", \"\u0436\u0430\u043b\u0434\u0430\u0434\u044b\"],\n[\"\u4e86\", null],\n[\"\u6765\u81ea\", null],\n[\" Duck\", null],\n[\"\uff06\", null],\n[\"Sh\", null],\n[\"ed\", null],\n[\"\", null],\n[\"\", null],\n[\"\", null],\n[\"\u7b51\", null],\n[\"\u548c\", \"\u0436\u04d9\u043d\u0435\"],\n[\"\u8bbe\u8ba1\", \"\u0434\u0438\u0437\u0430\u0439\u043d\"],\n[\"\u5de5\u4f5c\u5ba4\", \"\u0441\u0442\u0443\u0434\u0438\u044f\u0441\u044b\"],\n[\"\uff08\", null],\n[\"\u4f4d\u4e8e\", \"\u043e\u0440\u043d\u0430\u043b\u0430\u0441\u049b\u0430\u043d\"],\n[\"\u8bfa\", null],\n[\"\u4e01\", null],\n[\"\u5c71\", null],\n[\"\uff09\", null],\n[\"\u7684\", null],\n[\"\u57c3\u5c14\", null],\n[\"\u51e1\", null],\n[\"\u00b7\", null],\n[\"\u963f\", null],\n[\"\u624e\", null],\n[\"\u8fea\", null],\n[\"\u6765\", null],\n[\"\u5229\u7528\", \"\u043f\u0430\u0439\u0434\u0430\u043b\u0430\u043d\u0443\"],\n[\"\u8fd9\u4e2a\", \"\u0431\u04b1\u043b\"],\n[\" \", null],\n[\"1\", null],\n[\"2\", null],\n[\".\", null],\n[\"5\", null],\n[\"\", null],\n[\"\", null],\n[\"\u5c3a\", \"\u0444\u0443\u0442\"],\n[\"\u9ad8\u7684\", \"\u0431\u0438\u0456\u043a\u0442\u0456\u043a\u0442\u0435\u0433\u0456\"],\n[\"\u5929\u82b1\u677f\", \"\u0442\u04e9\u0431\u0435\u0441\u0456\"],\n[\"\uff0c\", null],\n[\"\u7cbe\u5fc3\", \"\u043c\u04b1\u049b\u0438\u044f\u0442\"],\n[\"\u6253\u9020\", \"\u0436\u0430\u0441\u0430\u0443\"],\n[\"\u4e00\u4e2a\", \"\u0431\u0456\u0440\"],\n[\"\u5bb6\", \"\u04af\u0439\"],\n[\"\u3002\", null],\n[\"\u8fd9\u4e2a\", \"\u0431\u04b1\u043b\"],\n[\"\u5bb6\", \"\u04af\u0439\"],\n[\"\u4e0d\u4ec5\u662f\", \"\u0442\u0435\u043a \u049b\u0430\u043d\u0430 \u0435\u043c\u0435\u0441\"],\n[\"\u5f00\u53d1\", \"\u0434\u0430\u043c\u0443\"],\n[\"\u9879\u76ee\", \"\u0436\u043e\u0431\u0430\u0441\u044b\"],\n[\"\uff0c\", null],\n[\"\u8fd8\u662f\", \"\u0441\u043e\u043d\u044b\u043c\u0435\u043d \u049b\u0430\u0442\u0430\u0440\"],\n[\"\u4e54\", null],\n[\"\u5b89\", null],\n[\"\u59ae\", null],\n[\"\u5c45\u4f4f\", \"\u0442\u04b1\u0440\u0430\u0442\u044b\u043d\"],\n[\"\u7684\u5730\u65b9\", \"\u0436\u0435\u0440\u0456\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u4e3a\u6b64\uff0c\u5979\u8058\u8bf7\u4e86\u6765\u81ea Duck\uff06Shed \u5efa\u7b51\u548c\u8bbe\u8ba1\u5de5\u4f5c\u5ba4\uff08\u4f4d\u4e8e\u8bfa\u4e01\u5c71\uff09\u7684\u57c3\u5c14\u51e1\u00b7\u963f\u624e\u8fea\u6765\u5229\u7528\u8fd9\u4e2a 12.5 \u82f1\u5c3a\u9ad8\u7684\u5929\u82b1\u677f\uff0c\u7cbe\u5fc3\u6253\u9020\u4e00\u4e2a\u5bb6\u3002\u8fd9\u4e2a\u5bb6\u4e0d\u4ec5\u662f\u5f00\u53d1\u9879\u76ee\uff0c\u8fd8\u662f\u4e54\u5b89\u59ae\u5c45\u4f4f\u7684\u5730\u65b9\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u4e3a\u6b64', '\uff0c', '\u5979', '\u8058\u8bf7', '\u4e86', '\u6765\u81ea', ' Duck', '\uff06', 'Sh', 'ed', '', '', '', '\u7b51', '\u548c', '\u8bbe\u8ba1', '\u5de5\u4f5c\u5ba4', '\uff08', '\u4f4d\u4e8e', '\u8bfa', '\u4e01', '\u5c71', '\uff09', '\u7684', '\u57c3\u5c14', '\u51e1', '\u00b7', '\u963f', '\u624e', '\u8fea', '\u6765', '\u5229\u7528', '\u8fd9\u4e2a', ' ', '1', '2', '.', '5', '', '', '\u5c3a', '\u9ad8\u7684', '\u5929\u82b1\u677f', '\uff0c', '\u7cbe\u5fc3', '\u6253\u9020', '\u4e00\u4e2a', '\u5bb6', '\u3002', '\u8fd9\u4e2a', '\u5bb6', '\u4e0d\u4ec5\u662f', '\u5f00\u53d1', '\u9879\u76ee', '\uff0c', '\u8fd8\u662f', '\u4e54', '\u5b89', '\u59ae', '\u5c45\u4f4f', '\u7684\u5730\u65b9', '\u3002']"}], "[\n[\"\u4e3a\u6b64\", \"\u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e\"],\n[\"\uff0c\", null],\n[\"\u5979\", \"\u043e\u043d\u0430\"],\n[\"\u8058\u8bf7\", \"\u043d\u0430\u043d\u044f\u043b\u0430\"],\n[\"\u4e86\", null],\n[\"\u6765\u81ea\", \"\u0438\u0437\"],\n[\" Duck\", \"Duck\"],\n[\"\uff06\", null],\n[\"Sh\", null],\n[\"ed\", null],\n[\"\", null],\n[\"\", null],\n[\"\", null],\n[\"\u7b51\", null],\n[\"\u548c\", \"\u0438\"],\n[\"\u8bbe\u8ba1\", \"\u0434\u0438\u0437\u0430\u0439\u043d\u0430\"],\n[\"\u5de5\u4f5c\u5ba4\", \"\u0441\u0442\u0443\u0434\u0438\u0438\"],\n[\"\uff08\", null],\n[\"\u4f4d\u4e8e\", \"\u0440\u0430\u0441\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u043d\u043e\u0439\"],\n[\"\u8bfa\", null],\n[\"\u4e01\", null],\n[\"\u5c71\", null],\n[\"\uff09\", null],\n[\"\u7684\", null],\n[\"\u57c3\u5c14\", null],\n[\"\u51e1\", null],\n[\"\u00b7\", null],\n[\"\u963f\", null],\n[\"\u624e\", null],\n[\"\u8fea\", \"\u042d\u043b\u044c\u0432\u0430\u043d\u0430 \u0410\u0437\u0430\u0434\u0438\"],\n[\"\u6765\", \"\u0447\u0442\u043e\u0431\u044b\"],\n[\"\u5229\u7528\", \"\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\"],\n[\"\u8fd9\u4e2a\", \"\u044d\u0442\u043e\u0442\"],\n[\" \", null],\n[\"1\", null],\n[\"2\", null],\n[\".\", null],\n[\"5\", null],\n[\"\", null],\n[\"\", null],\n[\"\u5c3a\", \"\u0444\u0443\u0442\u043e\u0432\u044b\u0439\"],\n[\"\u9ad8\u7684\", \"\u0432\u044b\u0441\u043e\u043a\u0438\u0439\"],\n[\"\u5929\u82b1\u677f\", \"\u043f\u043e\u0442\u043e\u043b\u043e\u043a\"],\n[\"\uff0c\", null],\n[\"\u7cbe\u5fc3\", \"\u0442\u0449\u0430\u0442\u0435\u043b\u044c\u043d\u043e\"],\n[\"\u6253\u9020\", \"\u0441\u043e\u0437\u0434\u0430\u0442\u044c\"],\n[\"\u4e00\u4e2a\", \"\u0434\u043e\u043c\"],\n[\"\u5bb6\", null],\n[\"\u3002\", null],\n[\"\u8fd9\u4e2a\", \"\u042d\u0442\u043e\u0442\"],\n[\"\u5bb6\", \"\u0434\u043e\u043c\"],\n[\"\u4e0d\u4ec5\u662f\", \"\u043d\u0435 \u0442\u043e\u043b\u044c\u043a\u043e\"],\n[\"\u5f00\u53d1\", \"\u043f\u0440\u043e\u0435\u043a\u0442\"],\n[\"\u9879\u76ee\", null],\n[\"\uff0c\", null],\n[\"\u8fd8\u662f\", \"\u043d\u043e \u0438\"],\n[\"\u4e54\", null],\n[\"\u5b89\", null],\n[\"\u59ae\", \"\u043c\u0435\u0441\u0442\u043e \u0436\u0438\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u0430 \u0414\u0436\u043e\u0430\u043d\u043d\u044b\"],\n[\"\u5c45\u4f4f\", null],\n[\"\u7684\u5730\u65b9\", null],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u8fd9\u5341\u90e8\u97f3\u4e50IP\u4f5c\u54c1\uff0c\u65b9\u6587\u5c71\u4f1a\u81ea\u5df1\u6267\u5bfc\u4e00\u5230\u4e24\u90e8\uff0c\u5f53\u5468\u8463\u88ab\u95ee\u5230\u662f\u5426\u4f1a\u5e2e\u5fd9\u6267\u5bfc\u65f6\uff0c\u5468\u6770\u4f26\u5766\u8a00\u5f53\u5bfc\u6f14\u538b\u529b\u592a\u5927\uff0c\u5e0c\u671b\u627e\u4e00\u4e9b\u5e74\u8f7b\u7684\u5bfc\u6f14\uff0c\u8ba9\u4ed6\u4eec\u6709\u53d1\u6325\u7684\u7a7a\u95f4\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u8fd9', '\u5341', '\u90e8', '\u97f3\u4e50', 'IP', '\u4f5c\u54c1', '\uff0c', '\u65b9', '\u6587', '\u5c71', '\u4f1a', '\u81ea\u5df1', '\u6267\u5bfc', '\u4e00', '\u5230', '\u4e24', '\u90e8', '\uff0c', '\u5f53', '\u5468', '\u8463', '\u88ab', '\u95ee', '\u5230', '\u662f\u5426\u4f1a', '\u5e2e\u5fd9', '\u6267\u5bfc', '\u65f6', '\uff0c', '\u5468', '\u6770', '\u4f26', '\u5766\u8a00', '\u5f53', '\u5bfc\u6f14', '\u538b\u529b', '\u592a\u5927', '\uff0c', '\u5e0c\u671b', '\u627e', '\u4e00\u4e9b', '\u5e74\u8f7b\u7684', '\u5bfc\u6f14', '\uff0c', '\u8ba9\u4ed6\u4eec', '\u6709', '\u53d1\u6325', '\u7684\u7a7a\u95f4', '\u3002']"}], "[\n  [\"\u8fd9\", \"These\"],\n  [\"\u5341\", \"ten\"],\n  [\"\u90e8\", null],\n  [\"\u97f3\u4e50\", \"music\"],\n  [\"IP\", \"IP\"],\n  [\"\u4f5c\u54c1\", \"works\"],\n  [\"\uff0c\", null],\n  [\"\u65b9\", \"Fang\"],\n  [\"\u6587\", null],\n  [\"\u5c71\", null],\n  [\"\u4f1a\", null],\n  [\"\u81ea\u5df1\", \"himself\"],\n  [\"\u6267\u5bfc\", \"direct\"],\n  [\"\u4e00\", \"one\"],\n  [\"\u5230\", \"to\"],\n  [\"\u4e24\", \"two\"],\n  [\"\u90e8\", null],\n  [\"\uff0c\", null],\n  [\"\u5f53\", \"When\"],\n  [\"\u5468\", \"Zhou\"],\n  [\"\u8463\", null],\n  [\"\u88ab\", null],\n  [\"\u95ee\", \"asked\"],\n  [\"\u5230\", null],\n  [\"\u662f\u5426\u4f1a\", \"whether\"],\n  [\"\u5e2e\u5fd9\", \"help\"],\n  [\"\u6267\u5bfc\", \"direct\"],\n  [\"\u65f6\", null],\n  [\"\uff0c\", null],\n  [\"\u5468\", \"Jay\"],\n  [\"\u6770\", null],\n  [\"\u4f26\", null],\n  [\"\u5766\u8a00\", \"frankly\"],\n  [\"\u5f53\", \"being\"],\n  [\"\u5bfc\u6f14\", \"director\"],\n  [\"\u538b\u529b\", \"pressure\"],\n  [\"\u592a\u5927\", \"too great\"],\n  [\"\uff0c\", null],\n  [\"\u5e0c\u671b\", \"hopes\"],\n  [\"\u627e\", \"find\"],\n  [\"\u4e00\u4e9b\", \"some\"],\n  [\"\u5e74\u8f7b\u7684\", \"young\"],\n  [\"\u5bfc\u6f14\", \"directors\"],\n  [\"\uff0c\", null],\n  [\"\u8ba9\u4ed6\u4eec\", \"allow them\"],\n  [\"\u6709\", \"have\"],\n  [\"\u53d1\u6325\", \"demonstrate\"],\n  [\"\u7684\u7a7a\u95f4\", \"space\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u8fd9\u5341\u90e8\u97f3\u4e50IP\u4f5c\u54c1\uff0c\u65b9\u6587\u5c71\u4f1a\u81ea\u5df1\u6267\u5bfc\u4e00\u5230\u4e24\u90e8\uff0c\u5f53\u5468\u8463\u88ab\u95ee\u5230\u662f\u5426\u4f1a\u5e2e\u5fd9\u6267\u5bfc\u65f6\uff0c\u5468\u6770\u4f26\u5766\u8a00\u5f53\u5bfc\u6f14\u538b\u529b\u592a\u5927\uff0c\u5e0c\u671b\u627e\u4e00\u4e9b\u5e74\u8f7b\u7684\u5bfc\u6f14\uff0c\u8ba9\u4ed6\u4eec\u6709\u53d1\u6325\u7684\u7a7a\u95f4\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u8fd9', '\u5341', '\u90e8', '\u97f3\u4e50', 'IP', '\u4f5c\u54c1', '\uff0c', '\u65b9', '\u6587', '\u5c71', '\u4f1a', '\u81ea\u5df1', '\u6267\u5bfc', '\u4e00', '\u5230', '\u4e24', '\u90e8', '\uff0c', '\u5f53', '\u5468', '\u8463', '\u88ab', '\u95ee', '\u5230', '\u662f\u5426\u4f1a', '\u5e2e\u5fd9', '\u6267\u5bfc', '\u65f6', '\uff0c', '\u5468', '\u6770', '\u4f26', '\u5766\u8a00', '\u5f53', '\u5bfc\u6f14', '\u538b\u529b', '\u592a\u5927', '\uff0c', '\u5e0c\u671b', '\u627e', '\u4e00\u4e9b', '\u5e74\u8f7b\u7684', '\u5bfc\u6f14', '\uff0c', '\u8ba9\u4ed6\u4eec', '\u6709', '\u53d1\u6325', '\u7684\u7a7a\u95f4', '\u3002']"}], "[\n    [\"\u8fd9\", \"\u0431\u04b1\u043b\"],\n    [\"\u5341\", \"\u043e\u043d\"],\n    [\"\u90e8\", null],\n    [\"\u97f3\u4e50\", \"\u043c\u0443\u0437\u044b\u043a\u0430\u043b\u044b\u049b\"],\n    [\"IP\", \"IP\"],\n    [\"\u4f5c\u54c1\", \"\u0442\u0443\u044b\u043d\u0434\u044b\"],\n    [\"\uff0c\", null],\n    [\"\u65b9\", null],\n    [\"\u6587\", null],\n    [\"\u5c71\", \"\u0424\u0430\u043d \u0412\u0435\u043d\u0448\u0430\u043d\"],\n    [\"\u4f1a\", null],\n    [\"\u81ea\u5df1\", \"\u04e9\u0437\u0456\"],\n    [\"\u6267\u5bfc\", \"\u0440\u0435\u0436\u0438\u0441\u0441\u0435\u0440\u043b\u0456\u043a\"],\n    [\"\u4e00\", \"\u0431\u0456\u0440\"],\n    [\"\u5230\", null],\n    [\"\u4e24\", \"\u0435\u043a\u0456\"],\n    [\"\u90e8\", null],\n    [\"\uff0c\", null],\n    [\"\u5f53\", null],\n    [\"\u5468\", null],\n    [\"\u8463\", \"\u0427\u0436\u043e\u0443 \u0414\u0443\u043d\"],\n    [\"\u88ab\", null],\n    [\"\u95ee\", \"\u0441\u04b1\u0440\u0430\u043b\u0493\u0430\u043d\"],\n    [\"\u5230\", null],\n    [\"\u662f\u5426\u4f1a\", \"\u043a\u04e9\u043c\u0435\u043a\u0442\u0435\u0441\u0435\u0442\u0456\u043d\u0456\"],\n    [\"\u5e2e\u5fd9\", null],\n    [\"\u6267\u5bfc\", \"\u0440\u0435\u0436\u0438\u0441\u0441\u0435\u0440\u043b\u0456\u043a\"],\n    [\"\u65f6\", null],\n    [\"\uff0c\", null],\n    [\"\u5468\", null],\n    [\"\u6770\", null],\n    [\"\u4f26\", \"\u0427\u0436\u043e\u0443 \u0426\u0437\u0435\u043b\u0443\u043d\u044c\"],\n    [\"\u5766\u8a00\", \"\u0430\u0448\u044b\u049b \u0430\u0439\u0442\u0442\u044b\"],\n    [\"\u5f53\", null],\n    [\"\u5bfc\u6f14\", \"\u0440\u0435\u0436\u0438\u0441\u0441\u0435\u0440\"],\n    [\"\u538b\u529b\", \"\u049b\u044b\u0441\u044b\u043c\"],\n    [\"\u592a\u5927\", \"\u0442\u044b\u043c \u04af\u043b\u043a\u0435\u043d\"],\n    [\"\uff0c\", null],\n    [\"\u5e0c\u671b\", \"\u04af\u043c\u0456\u0442\u0442\u0435\u043d\u0435\u0434\u0456\"],\n    [\"\u627e\", \"\u0442\u0430\u0431\u0443\"],\n    [\"\u4e00\u4e9b\", \"\u0431\u0456\u0440\u043d\u0435\u0448\u0435\"],\n    [\"\u5e74\u8f7b\u7684\", \"\u0436\u0430\u0441\"],\n    [\"\u5bfc\u6f14\", \"\u0440\u0435\u0436\u0438\u0441\u0441\u0435\u0440\"],\n    [\"\uff0c\", null],\n    [\"\u8ba9\u4ed6\u4eec\", \"\u043e\u043b\u0430\u0440\u0493\u0430\"],\n    [\"\u6709\", null],\n    [\"\u53d1\u6325\", \"\u043a\u04e9\u0440\u0441\u0435\u0442\u0443\"],\n    [\"\u7684\u7a7a\u95f4\", \"\u043c\u04af\u043c\u043a\u0456\u043d\u0434\u0456\u043a\"],\n    [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u8fd9\u5341\u90e8\u97f3\u4e50IP\u4f5c\u54c1\uff0c\u65b9\u6587\u5c71\u4f1a\u81ea\u5df1\u6267\u5bfc\u4e00\u5230\u4e24\u90e8\uff0c\u5f53\u5468\u8463\u88ab\u95ee\u5230\u662f\u5426\u4f1a\u5e2e\u5fd9\u6267\u5bfc\u65f6\uff0c\u5468\u6770\u4f26\u5766\u8a00\u5f53\u5bfc\u6f14\u538b\u529b\u592a\u5927\uff0c\u5e0c\u671b\u627e\u4e00\u4e9b\u5e74\u8f7b\u7684\u5bfc\u6f14\uff0c\u8ba9\u4ed6\u4eec\u6709\u53d1\u6325\u7684\u7a7a\u95f4\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u8fd9', '\u5341', '\u90e8', '\u97f3\u4e50', 'IP', '\u4f5c\u54c1', '\uff0c', '\u65b9', '\u6587', '\u5c71', '\u4f1a', '\u81ea\u5df1', '\u6267\u5bfc', '\u4e00', '\u5230', '\u4e24', '\u90e8', '\uff0c', '\u5f53', '\u5468', '\u8463', '\u88ab', '\u95ee', '\u5230', '\u662f\u5426\u4f1a', '\u5e2e\u5fd9', '\u6267\u5bfc', '\u65f6', '\uff0c', '\u5468', '\u6770', '\u4f26', '\u5766\u8a00', '\u5f53', '\u5bfc\u6f14', '\u538b\u529b', '\u592a\u5927', '\uff0c', '\u5e0c\u671b', '\u627e', '\u4e00\u4e9b', '\u5e74\u8f7b\u7684', '\u5bfc\u6f14', '\uff0c', '\u8ba9\u4ed6\u4eec', '\u6709', '\u53d1\u6325', '\u7684\u7a7a\u95f4', '\u3002']"}], "[\n  [\"\u8fd9\", \"\u044d\u0442\u0438\"],\n  [\"\u5341\", \"\u0434\u0435\u0441\u044f\u0442\u044c\"],\n  [\"\u90e8\", null],\n  [\"\u97f3\u4e50\", \"\u043c\u0443\u0437\u044b\u043a\u0430\u043b\u044c\u043d\u044b\u0445\"],\n  [\"IP\", \"IP\"],\n  [\"\u4f5c\u54c1\", \"\u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u043d\u0438\u0439\"],\n  [\"\uff0c\", null],\n  [\"\u65b9\", null],\n  [\"\u6587\", null],\n  [\"\u5c71\", \"\u0424\u0430\u043d \u0412\u0435\u043d\u044c\u0448\u0430\u043d\u044c\"],\n  [\"\u4f1a\", null],\n  [\"\u81ea\u5df1\", \"\u0441\u0430\u043c\"],\n  [\"\u6267\u5bfc\", \"\u0441\u0440\u0435\u0436\u0438\u0441\u0441\u0438\u0440\u0443\u0435\u0442\"],\n  [\"\u4e00\", \"\u043e\u0434\u0438\u043d\"],\n  [\"\u5230\", null],\n  [\"\u4e24\", \"\u0434\u0432\u0430\"],\n  [\"\u90e8\", null],\n  [\"\uff0c\", null],\n  [\"\u5f53\", \"\u043a\u043e\u0433\u0434\u0430\"],\n  [\"\u5468\", null],\n  [\"\u8463\", \"\u0427\u0436\u043e\u0443 \u0414\u0443\u043d\"],\n  [\"\u88ab\", null],\n  [\"\u95ee\", \"\u0441\u043f\u0440\u043e\u0441\u0438\u043b\u0438\"],\n  [\"\u5230\", null],\n  [\"\u662f\u5426\u4f1a\", \"\u0431\u0443\u0434\u0435\u0442 \u043b\u0438\"],\n  [\"\u5e2e\u5fd9\", \"\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u044c\"],\n  [\"\u6267\u5bfc\", \"\u0440\u0435\u0436\u0438\u0441\u0441\u0438\u0440\u043e\u0432\u0430\u0442\u044c\"],\n  [\"\u65f6\", null],\n  [\"\uff0c\", null],\n  [\"\u5468\", null],\n  [\"\u6770\", null],\n  [\"\u4f26\", \"\u0414\u0436\u0435\u0439 \u0427\u043e\u0443\"],\n  [\"\u5766\u8a00\", \"\u043e\u0442\u043a\u0440\u043e\u0432\u0435\u043d\u043d\u043e \u0441\u043a\u0430\u0437\u0430\u043b\"],\n  [\"\u5f53\", \"\u0431\u044b\u0442\u044c\"],\n  [\"\u5bfc\u6f14\", \"\u0440\u0435\u0436\u0438\u0441\u0441\u0435\u0440\u043e\u043c\"],\n  [\"\u538b\u529b\", \"\u0434\u0430\u0432\u043b\u0435\u043d\u0438\u0435\"],\n  [\"\u592a\u5927\", \"\u0441\u043b\u0438\u0448\u043a\u043e\u043c \u0431\u043e\u043b\u044c\u0448\u043e\u0435\"],\n  [\"\uff0c\", null],\n  [\"\u5e0c\u671b\", \"\u043d\u0430\u0434\u0435\u0435\u0442\u0441\u044f\"],\n  [\"\u627e\", \"\u043d\u0430\u0439\u0442\u0438\"],\n  [\"\u4e00\u4e9b\", \"\u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e\"],\n  [\"\u5e74\u8f7b\u7684\", \"\u043c\u043e\u043b\u043e\u0434\u044b\u0445\"],\n  [\"\u5bfc\u6f14\", \"\u0440\u0435\u0436\u0438\u0441\u0441\u0435\u0440\u043e\u0432\"],\n  [\"\uff0c\", null],\n  [\"\u8ba9\u4ed6\u4eec\", \"\u0447\u0442\u043e\u0431\u044b \u043e\u043d\u0438\"],\n  [\"\u6709\", \"\u0438\u043c\u0435\u043b\u0438\"],\n  [\"\u53d1\u6325\", \"\u043f\u0440\u043e\u044f\u0432\u0438\u0442\u044c\"],\n  [\"\u7684\u7a7a\u95f4\", \"\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u043e \u0434\u043b\u044f\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n2016\u5e74\uff0ceMarketer\u8ddf\u8e2a\u4e86180\u5bb6\u6700\u5927\u7684\u7535\u5546\u4e4b\u540e\u4f30\u8ba1\uff0c\u7535\u5546\u603b\u9500\u552e\u989d\u7ea6\u4e3a2000\u4ebf\u7f8e\u5143\uff0c\u800c\u5176\u4e2d\u6700\u5927\u768425\u5bb6\u5360\u5230\u4e861590\u591a\u4ebf\u7f8e\u5143\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['2', '0', '1', '6', '\u5e74', '\uff0c', 'e', 'Mark', 'eter', '\u8ddf\u8e2a', '\u4e86', '1', '8', '0', '\u5bb6', '\u6700\u5927\u7684', '\u7535\u5546', '\u4e4b\u540e', '\u4f30\u8ba1', '\uff0c', '\u7535\u5546', '\u603b', '\u9500\u552e\u989d', '\u7ea6\u4e3a', '2', '0', '0', '0', '\u4ebf\u7f8e\u5143', '\uff0c', '\u800c', '\u5176\u4e2d', '\u6700\u5927\u7684', '2', '5', '\u5bb6', '\u5360', '\u5230\u4e86', '1', '5', '9', '0', '\u591a', '\u4ebf\u7f8e\u5143', '\u3002']"}], "[\n  [\"2\", null],\n  [\"0\", null],\n  [\"1\", null],\n  [\"6\", null],\n  [\"\u5e74\", \"In\"],\n  [\"\uff0c\", null],\n  [\"e\", null],\n  [\"Mark\", \"eMarketer\"],\n  [\"eter\", null],\n  [\"\u8ddf\u8e2a\", \"tracked\"],\n  [\"\u4e86\", null],\n  [\"1\", null],\n  [\"8\", null],\n  [\"0\", null],\n  [\"\u5bb6\", null],\n  [\"\u6700\u5927\u7684\", \"largest\"],\n  [\"\u7535\u5546\", \"e-commerce companies\"],\n  [\"\u4e4b\u540e\", \"after\"],\n  [\"\u4f30\u8ba1\", \"estimated\"],\n  [\"\uff0c\", null],\n  [\"\u7535\u5546\", \"e-commerce\"],\n  [\"\u603b\", \"total\"],\n  [\"\u9500\u552e\u989d\", \"sales\"],\n  [\"\u7ea6\u4e3a\", \"were approximately\"],\n  [\"2\", null],\n  [\"0\", null],\n  [\"0\", null],\n  [\"0\", null],\n  [\"\u4ebf\u7f8e\u5143\", \"billion dollars\"],\n  [\"\uff0c\", null],\n  [\"\u800c\", \"while\"],\n  [\"\u5176\u4e2d\", \"among them\"],\n  [\"\u6700\u5927\u7684\", \"the largest\"],\n  [\"2\", null],\n  [\"5\", null],\n  [\"\u5bb6\", null],\n  [\"\u5360\", \"accounted for\"],\n  [\"\u5230\u4e86\", null],\n  [\"1\", null],\n  [\"5\", null],\n  [\"9\", null],\n  [\"0\", null],\n  [\"\u591a\", \"over\"],\n  [\"\u4ebf\u7f8e\u5143\", \"billion dollars\"],\n  [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n2016\u5e74\uff0ceMarketer\u8ddf\u8e2a\u4e86180\u5bb6\u6700\u5927\u7684\u7535\u5546\u4e4b\u540e\u4f30\u8ba1\uff0c\u7535\u5546\u603b\u9500\u552e\u989d\u7ea6\u4e3a2000\u4ebf\u7f8e\u5143\uff0c\u800c\u5176\u4e2d\u6700\u5927\u768425\u5bb6\u5360\u5230\u4e861590\u591a\u4ebf\u7f8e\u5143\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['2', '0', '1', '6', '\u5e74', '\uff0c', 'e', 'Mark', 'eter', '\u8ddf\u8e2a', '\u4e86', '1', '8', '0', '\u5bb6', '\u6700\u5927\u7684', '\u7535\u5546', '\u4e4b\u540e', '\u4f30\u8ba1', '\uff0c', '\u7535\u5546', '\u603b', '\u9500\u552e\u989d', '\u7ea6\u4e3a', '2', '0', '0', '0', '\u4ebf\u7f8e\u5143', '\uff0c', '\u800c', '\u5176\u4e2d', '\u6700\u5927\u7684', '2', '5', '\u5bb6', '\u5360', '\u5230\u4e86', '1', '5', '9', '0', '\u591a', '\u4ebf\u7f8e\u5143', '\u3002']"}], "[\n[\"2\", null],\n[\"0\", null],\n[\"1\", null],\n[\"6\", null],\n[\"\u5e74\", null],\n[\"\uff0c\", null],\n[\"e\", null],\n[\"Mark\", null],\n[\"eter\", null],\n[\"\u8ddf\u8e2a\", \"\u0431\u0430\u049b\u044b\u043b\u0430\u0443\"],\n[\"\u4e86\", null],\n[\"1\", null],\n[\"8\", null],\n[\"0\", null],\n[\"\u5bb6\", null],\n[\"\u6700\u5927\u7684\", \"\u0435\u04a3 \u04af\u043b\u043a\u0435\u043d\"],\n[\"\u7535\u5546\", \"\u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u0434\u044b\u049b \u043a\u043e\u043c\u043c\u0435\u0440\u0446\u0438\u044f\"],\n[\"\u4e4b\u540e\", \"\u043a\u0435\u0439\u0456\u043d\"],\n[\"\u4f30\u8ba1\", \"\u0431\u0430\u0493\u0430\u043b\u0430\u0443\"],\n[\"\uff0c\", null],\n[\"\u7535\u5546\", \"\u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u0434\u044b\u049b \u043a\u043e\u043c\u043c\u0435\u0440\u0446\u0438\u044f\"],\n[\"\u603b\", \"\u0436\u0430\u043b\u043f\u044b\"],\n[\"\u9500\u552e\u989d\", \"\u0441\u0430\u0442\u0443 \u043a\u04e9\u043b\u0435\u043c\u0456\"],\n[\"\u7ea6\u4e3a\", \"\u0448\u0430\u043c\u0430\u043c\u0435\u043d\"],\n[\"2\", null],\n[\"0\", null],\n[\"0\", null],\n[\"0\", null],\n[\"\u4ebf\u7f8e\u5143\", \"\u043c\u0438\u043b\u043b\u0438\u0430\u0440\u0434 \u0434\u043e\u043b\u043b\u0430\u0440\"],\n[\"\uff0c\", null],\n[\"\u800c\", null],\n[\"\u5176\u4e2d\", \"\u043e\u043d\u044b\u04a3 \u0456\u0448\u0456\u043d\u0434\u0435\"],\n[\"\u6700\u5927\u7684\", \"\u0435\u04a3 \u04af\u043b\u043a\u0435\u043d\"],\n[\"2\", null],\n[\"5\", null],\n[\"\u5bb6\", null],\n[\"\u5360\", \"\u049b\u04b1\u0440\u0430\u0434\u044b\"],\n[\"\u5230\u4e86\", null],\n[\"1\", null],\n[\"5\", null],\n[\"9\", null],\n[\"0\", null],\n[\"\u591a\", \"\u0430\u0441\u0442\u0430\u043c\"],\n[\"\u4ebf\u7f8e\u5143\", \"\u043c\u0438\u043b\u043b\u0438\u0430\u0440\u0434 \u0434\u043e\u043b\u043b\u0430\u0440\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n2016\u5e74\uff0ceMarketer\u8ddf\u8e2a\u4e86180\u5bb6\u6700\u5927\u7684\u7535\u5546\u4e4b\u540e\u4f30\u8ba1\uff0c\u7535\u5546\u603b\u9500\u552e\u989d\u7ea6\u4e3a2000\u4ebf\u7f8e\u5143\uff0c\u800c\u5176\u4e2d\u6700\u5927\u768425\u5bb6\u5360\u5230\u4e861590\u591a\u4ebf\u7f8e\u5143\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['2', '0', '1', '6', '\u5e74', '\uff0c', 'e', 'Mark', 'eter', '\u8ddf\u8e2a', '\u4e86', '1', '8', '0', '\u5bb6', '\u6700\u5927\u7684', '\u7535\u5546', '\u4e4b\u540e', '\u4f30\u8ba1', '\uff0c', '\u7535\u5546', '\u603b', '\u9500\u552e\u989d', '\u7ea6\u4e3a', '2', '0', '0', '0', '\u4ebf\u7f8e\u5143', '\uff0c', '\u800c', '\u5176\u4e2d', '\u6700\u5927\u7684', '2', '5', '\u5bb6', '\u5360', '\u5230\u4e86', '1', '5', '9', '0', '\u591a', '\u4ebf\u7f8e\u5143', '\u3002']"}], "[\n    [\"2\", null],\n    [\"0\", null],\n    [\"1\", null],\n    [\"6\", null],\n    [\"\u5e74\", \"\u0412\"],\n    [\"\uff0c\", null],\n    [\"e\", null],\n    [\"Mark\", \"\u041c\u0430\u0440\u043a\u0435\u0442\u0435\u0440\"],\n    [\"eter\", null],\n    [\"\u8ddf\u8e2a\", \"\u043e\u0442\u0441\u043b\u0435\u0434\u0438\u043b\"],\n    [\"\u4e86\", null],\n    [\"1\", null],\n    [\"8\", null],\n    [\"0\", null],\n    [\"\u5bb6\", null],\n    [\"\u6700\u5927\u7684\", \"\u043a\u0440\u0443\u043f\u043d\u0435\u0439\u0448\u0438\u0445\"],\n    [\"\u7535\u5546\", \"\u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u043d\u044b\u0445 \u043c\u0430\u0433\u0430\u0437\u0438\u043d\u043e\u0432\"],\n    [\"\u4e4b\u540e\", \"\u043f\u043e\u0441\u043b\u0435\"],\n    [\"\u4f30\u8ba1\", \"\u043e\u0446\u0435\u043d\u0438\u043b\"],\n    [\"\uff0c\", null],\n    [\"\u7535\u5546\", \"\u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u043d\u043e\u0439 \u043a\u043e\u043c\u043c\u0435\u0440\u0446\u0438\u0438\"],\n    [\"\u603b\", \"\u043e\u0431\u0449\u0438\u0439\"],\n    [\"\u9500\u552e\u989d\", \"\u043e\u0431\u044a\u0435\u043c \u043f\u0440\u043e\u0434\u0430\u0436\"],\n    [\"\u7ea6\u4e3a\", \"\u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e\"],\n    [\"2\", null],\n    [\"0\", null],\n    [\"0\", null],\n    [\"0\", null],\n    [\"\u4ebf\u7f8e\u5143\", \"\u043c\u0438\u043b\u043b\u0438\u0430\u0440\u0434\u043e\u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u043e\u0432\"],\n    [\"\uff0c\", null],\n    [\"\u800c\", \"\u043f\u0440\u0438 \u044d\u0442\u043e\u043c\"],\n    [\"\u5176\u4e2d\", \"\u0438\u0437 \u043d\u0438\u0445\"],\n    [\"\u6700\u5927\u7684\", \"\u043a\u0440\u0443\u043f\u043d\u0435\u0439\u0448\u0438\u0445\"],\n    [\"2\", null],\n    [\"5\", null],\n    [\"\u5bb6\", null],\n    [\"\u5360\", \"\u0441\u043e\u0441\u0442\u0430\u0432\u0438\u043b\u0438\"],\n    [\"\u5230\u4e86\", null],\n    [\"1\", null],\n    [\"5\", null],\n    [\"9\", null],\n    [\"0\", null],\n    [\"\u591a\", \"\u0431\u043e\u043b\u0435\u0435\"],\n    [\"\u4ebf\u7f8e\u5143\", \"\u043c\u0438\u043b\u043b\u0438\u0430\u0440\u0434\u043e\u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u043e\u0432\"],\n    [\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u4ece\u800c\u6709\u4eba\u56e0\u4e3a\u6536\u5165\u8fc7\u4f4e\u652f\u4ed8\u4e0d\u8d77\u623f\u8d37\uff0c\u4e8e\u662f\u94f6\u884c\u5c31\u5f00\u59cb\u6536\u56de\u623f\u4ea7\uff0c\u5f53\u94f6\u884c\u4ece\u62d6\u6b20\u8d37\u6b3e\u7684\u8d2d\u623f\u8005\u624b\u4e2d\u6536\u56de\u623f\u4ea7\u4e4b\u540e\uff0c\u4ed6\u4eec\u4f1a\u4ee5\u6781\u4f4e\u7684\u4ef7\u683c\u552e\u51fa\uff0c\u4ece\u800c\u5bfc\u81f4\u4e86\u8be5\u5730\u533a\u7684\u623f\u4ef7\u4e00\u6b65\u6b65\u4e0b\u6ed1\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u53c8\u5f15\u53d1\u4e86\u623f\u4ef7\u8fdb\u4e00\u6b65\u7684\u4e0b\u6ed1\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u6076\u6027\u5faa\u73af\uff0c\u5f53\u623f\u4ef7\u4e0b\u8dcc\u5230\u4e00\u5b9a\u7684\u7a0b\u5ea6\u4e4b\u540e\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u8d2d\u623f\u8005\u53d1\u73b0\u81ea\u5df1\u7684\u623f\u5b50\u4ef7\u503c\u751a\u81f3\u5df2\u7ecf\u6bd4\u81ea\u5df1\u7684\u8d37\u6b3e\u8fd8\u4f4e\u4e86\uff0c\u4e8e\u662f\u5c31\u6709\u4e86\u66f4\u591a\u7684\u4eba\u505c\u6b62\u7ee7\u7eed\u8fd8\u623f\u8d37\uff0c\u53c8\u9020\u6210\u4e86\u66f4\u591a\u7684\u574f\u8d26\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u4ece\u800c', '\u6709\u4eba', '\u56e0\u4e3a', '\u6536\u5165', '\u8fc7', '\u4f4e', '\u652f\u4ed8', '\u4e0d\u8d77', '\u623f\u8d37', '\uff0c', '\u4e8e\u662f', '\u94f6\u884c', '\u5c31\u5f00\u59cb', '\u6536\u56de', '\u623f\u4ea7', '\uff0c', '\u5f53', '\u94f6\u884c', '\u4ece', '\u62d6\u6b20', '\u8d37\u6b3e', '\u7684', '\u8d2d\u623f\u8005', '\u624b\u4e2d', '\u6536\u56de', '\u623f\u4ea7', '\u4e4b\u540e', '\uff0c', '\u4ed6\u4eec\u4f1a', '\u4ee5', '\u6781', '\u4f4e', '\u7684\u4ef7\u683c', '\u552e', '\u51fa', '\uff0c', '\u4ece\u800c', '\u5bfc\u81f4', '\u4e86', '\u8be5', '\u5730\u533a\u7684', '\u623f\u4ef7', '\u4e00\u6b65\u6b65', '\u4e0b\u6ed1', '\uff0c', '\u8fd9\u4e2a', '\u8fc7\u7a0b', '\u53c8', '\u5f15\u53d1\u4e86', '\u623f\u4ef7', '\u8fdb\u4e00\u6b65', '\u7684', '\u4e0b\u6ed1', '\uff0c', '\u5f62\u6210\u4e86', '\u4e00\u4e2a', '\u6076\u6027', '\u5faa\u73af', '\uff0c', '\u5f53', '\u623f\u4ef7', '\u4e0b\u8dcc', '\u5230', '\u4e00\u5b9a\u7684', '\u7a0b\u5ea6', '\u4e4b\u540e', '\uff0c', '\u8d8a\u6765\u8d8a\u591a', '\u7684', '\u8d2d\u623f\u8005', '\u53d1\u73b0', '\u81ea\u5df1\u7684', '\u623f\u5b50', '\u4ef7\u503c', '\u751a\u81f3', '\u5df2\u7ecf', '\u6bd4', '\u81ea\u5df1\u7684', '\u8d37\u6b3e', '\u8fd8', '\u4f4e', '\u4e86', '\uff0c', '\u4e8e\u662f', '\u5c31\u6709\u4e86', '\u66f4\u591a', '\u7684\u4eba', '\u505c\u6b62', '\u7ee7\u7eed', '\u8fd8', '\u623f\u8d37', '\uff0c', '\u53c8', '\u9020\u6210\u4e86', '\u66f4\u591a\u7684', '\u574f', '\u8d26', '\u3002']"}], "[\n[\"\u4ece\u800c\", \"consequently\"],\n[\"\u6709\u4eba\", \"some people\"],\n[\"\u56e0\u4e3a\", null],\n[\"\u6536\u5165\", \"income\"],\n[\"\u8fc7\", null],\n[\"\u4f4e\", null],\n[\"\u652f\u4ed8\", \"pay\"],\n[\"\u4e0d\u8d77\", null],\n[\"\u623f\u8d37\", \"mortgage\"],\n[\"\uff0c\", null],\n[\"\u4e8e\u662f\", \"therefore\"],\n[\"\u94f6\u884c\", \"banks\"],\n[\"\u5c31\u5f00\u59cb\", \"begin to\"],\n[\"\u6536\u56de\", \"repossess\"],\n[\"\u623f\u4ea7\", \"properties\"],\n[\"\uff0c\", null],\n[\"\u5f53\", null],\n[\"\u94f6\u884c\", null],\n[\"\u4ece\", null],\n[\"\u62d6\u6b20\", \"delinquent\"],\n[\"\u8d37\u6b3e\", \"loans\"],\n[\"\u7684\", null],\n[\"\u8d2d\u623f\u8005\", \"homebuyers\"],\n[\"\u624b\u4e2d\", null],\n[\"\u6536\u56de\", null],\n[\"\u623f\u4ea7\", null],\n[\"\u4e4b\u540e\", \"after\"],\n[\"\uff0c\", null],\n[\"\u4ed6\u4eec\u4f1a\", \"they\"],\n[\"\u4ee5\", null],\n[\"\u6781\", null],\n[\"\u4f4e\", \"low\"],\n[\"\u7684\u4ef7\u683c\", \"prices\"],\n[\"\u552e\", null],\n[\"\u51fa\", null],\n[\"\uff0c\", null],\n[\"\u4ece\u800c\", \"consequently\"],\n[\"\u5bfc\u81f4\", \"lead to\"],\n[\"\u4e86\", null],\n[\"\u8be5\", null],\n[\"\u5730\u533a\u7684\", \"area's\"],\n[\"\u623f\u4ef7\", \"housing prices\"],\n[\"\u4e00\u6b65\u6b65\", \"gradually\"],\n[\"\u4e0b\u6ed1\", \"decline\"],\n[\"\uff0c\", null],\n[\"\u8fd9\u4e2a\", \"this\"],\n[\"\u8fc7\u7a0b\", \"process\"],\n[\"\u53c8\", \"further\"],\n[\"\u5f15\u53d1\u4e86\", \"triggers\"],\n[\"\u623f\u4ef7\", null],\n[\"\u8fdb\u4e00\u6b65\", \"further\"],\n[\"\u7684\", null],\n[\"\u4e0b\u6ed1\", \"decline\"],\n[\"\uff0c\", null],\n[\"\u5f62\u6210\u4e86\", \"forming\"],\n[\"\u4e00\u4e2a\", null],\n[\"\u6076\u6027\", \"vicious\"],\n[\"\u5faa\u73af\", \"cycle\"],\n[\"\uff0c\", null],\n[\"\u5f53\", \"when\"],\n[\"\u623f\u4ef7\", null],\n[\"\u4e0b\u8dcc\", \"fall\"],\n[\"\u5230\", null],\n[\"\u4e00\u5b9a\u7684\", \"certain\"],\n[\"\u7a0b\u5ea6\", \"level\"],\n[\"\u4e4b\u540e\", null],\n[\"\uff0c\", null],\n[\"\u8d8a\u6765\u8d8a\u591a\", \"more and more\"],\n[\"\u7684\", null],\n[\"\u8d2d\u623f\u8005\", \"homebuyers\"],\n[\"\u53d1\u73b0\", \"discover\"],\n[\"\u81ea\u5df1\u7684\", \"their\"],\n[\"\u623f\u5b50\", \"houses\"],\n[\"\u4ef7\u503c\", \"value\"],\n[\"\u751a\u81f3\", \"even\"],\n[\"\u5df2\u7ecf\", \"already\"],\n[\"\u6bd4\", \"than\"],\n[\"\u81ea\u5df1\u7684\", null],\n[\"\u8d37\u6b3e\", \"loans\"],\n[\"\u8fd8\", null],\n[\"\u4f4e\", \"lower\"],\n[\"\u4e86\", null],\n[\"\uff0c\", null],\n[\"\u4e8e\u662f\", \"consequently\"],\n[\"\u5c31\u6709\u4e86\", \"there are\"],\n[\"\u66f4\u591a\", \"more\"],\n[\"\u7684\u4eba\", \"people\"],\n[\"\u505c\u6b62\", \"stop\"],\n[\"\u7ee7\u7eed\", \"continuing\"],\n[\"\u8fd8\", null],\n[\"\u623f\u8d37\", \"mortgage payments\"],\n[\"\uff0c\", null],\n[\"\u53c8\", \"further\"],\n[\"\u9020\u6210\u4e86\", \"causing\"],\n[\"\u66f4\u591a\u7684\", \"more\"],\n[\"\u574f\", \"bad\"],\n[\"\u8d26\", \"debts\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u4ece\u800c\u6709\u4eba\u56e0\u4e3a\u6536\u5165\u8fc7\u4f4e\u652f\u4ed8\u4e0d\u8d77\u623f\u8d37\uff0c\u4e8e\u662f\u94f6\u884c\u5c31\u5f00\u59cb\u6536\u56de\u623f\u4ea7\uff0c\u5f53\u94f6\u884c\u4ece\u62d6\u6b20\u8d37\u6b3e\u7684\u8d2d\u623f\u8005\u624b\u4e2d\u6536\u56de\u623f\u4ea7\u4e4b\u540e\uff0c\u4ed6\u4eec\u4f1a\u4ee5\u6781\u4f4e\u7684\u4ef7\u683c\u552e\u51fa\uff0c\u4ece\u800c\u5bfc\u81f4\u4e86\u8be5\u5730\u533a\u7684\u623f\u4ef7\u4e00\u6b65\u6b65\u4e0b\u6ed1\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u53c8\u5f15\u53d1\u4e86\u623f\u4ef7\u8fdb\u4e00\u6b65\u7684\u4e0b\u6ed1\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u6076\u6027\u5faa\u73af\uff0c\u5f53\u623f\u4ef7\u4e0b\u8dcc\u5230\u4e00\u5b9a\u7684\u7a0b\u5ea6\u4e4b\u540e\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u8d2d\u623f\u8005\u53d1\u73b0\u81ea\u5df1\u7684\u623f\u5b50\u4ef7\u503c\u751a\u81f3\u5df2\u7ecf\u6bd4\u81ea\u5df1\u7684\u8d37\u6b3e\u8fd8\u4f4e\u4e86\uff0c\u4e8e\u662f\u5c31\u6709\u4e86\u66f4\u591a\u7684\u4eba\u505c\u6b62\u7ee7\u7eed\u8fd8\u623f\u8d37\uff0c\u53c8\u9020\u6210\u4e86\u66f4\u591a\u7684\u574f\u8d26\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u4ece\u800c', '\u6709\u4eba', '\u56e0\u4e3a', '\u6536\u5165', '\u8fc7', '\u4f4e', '\u652f\u4ed8', '\u4e0d\u8d77', '\u623f\u8d37', '\uff0c', '\u4e8e\u662f', '\u94f6\u884c', '\u5c31\u5f00\u59cb', '\u6536\u56de', '\u623f\u4ea7', '\uff0c', '\u5f53', '\u94f6\u884c', '\u4ece', '\u62d6\u6b20', '\u8d37\u6b3e', '\u7684', '\u8d2d\u623f\u8005', '\u624b\u4e2d', '\u6536\u56de', '\u623f\u4ea7', '\u4e4b\u540e', '\uff0c', '\u4ed6\u4eec\u4f1a', '\u4ee5', '\u6781', '\u4f4e', '\u7684\u4ef7\u683c', '\u552e', '\u51fa', '\uff0c', '\u4ece\u800c', '\u5bfc\u81f4', '\u4e86', '\u8be5', '\u5730\u533a\u7684', '\u623f\u4ef7', '\u4e00\u6b65\u6b65', '\u4e0b\u6ed1', '\uff0c', '\u8fd9\u4e2a', '\u8fc7\u7a0b', '\u53c8', '\u5f15\u53d1\u4e86', '\u623f\u4ef7', '\u8fdb\u4e00\u6b65', '\u7684', '\u4e0b\u6ed1', '\uff0c', '\u5f62\u6210\u4e86', '\u4e00\u4e2a', '\u6076\u6027', '\u5faa\u73af', '\uff0c', '\u5f53', '\u623f\u4ef7', '\u4e0b\u8dcc', '\u5230', '\u4e00\u5b9a\u7684', '\u7a0b\u5ea6', '\u4e4b\u540e', '\uff0c', '\u8d8a\u6765\u8d8a\u591a', '\u7684', '\u8d2d\u623f\u8005', '\u53d1\u73b0', '\u81ea\u5df1\u7684', '\u623f\u5b50', '\u4ef7\u503c', '\u751a\u81f3', '\u5df2\u7ecf', '\u6bd4', '\u81ea\u5df1\u7684', '\u8d37\u6b3e', '\u8fd8', '\u4f4e', '\u4e86', '\uff0c', '\u4e8e\u662f', '\u5c31\u6709\u4e86', '\u66f4\u591a', '\u7684\u4eba', '\u505c\u6b62', '\u7ee7\u7eed', '\u8fd8', '\u623f\u8d37', '\uff0c', '\u53c8', '\u9020\u6210\u4e86', '\u66f4\u591a\u7684', '\u574f', '\u8d26', '\u3002']"}], "[\n[\"\u4ece\u800c\", null],\n[\"\u6709\u4eba\", \"\u0431\u0456\u0440\u0435\u0443\"],\n[\"\u56e0\u4e3a\", null],\n[\"\u6536\u5165\", \"\u0442\u0430\u0431\u044b\u0441\"],\n[\"\u8fc7\", null],\n[\"\u4f4e\", \"\u0442\u04e9\u043c\u0435\u043d\"],\n[\"\u652f\u4ed8\", \"\u0442\u04e9\u043b\u0435\u0443\"],\n[\"\u4e0d\u8d77\", null],\n[\"\u623f\u8d37\", \"\u0438\u043f\u043e\u0442\u0435\u043a\u0430\"],\n[\"\uff0c\", null],\n[\"\u4e8e\u662f\", null],\n[\"\u94f6\u884c\", \"\u0431\u0430\u043d\u043a\"],\n[\"\u5c31\u5f00\u59cb\", null],\n[\"\u6536\u56de\", \"\u049b\u0430\u0439\u0442\u0430\u0440\u044b\u043f \u0430\u043b\u0443\"],\n[\"\u623f\u4ea7\", \"\u0436\u044b\u043b\u0436\u044b\u043c\u0430\u0439\u0442\u044b\u043d \u043c\u04af\u043b\u0456\u043a\"],\n[\"\uff0c\", null],\n[\"\u5f53\", null],\n[\"\u94f6\u884c\", \"\u0431\u0430\u043d\u043a\"],\n[\"\u4ece\", null],\n[\"\u62d6\u6b20\", \"\u043a\u0435\u0448\u0456\u043a\u0442\u0456\u0440\u0443\"],\n[\"\u8d37\u6b3e\", \"\u043d\u0435\u0441\u0438\u0435\"],\n[\"\u7684\", null],\n[\"\u8d2d\u623f\u8005\", \"\u04af\u0439 \u0441\u0430\u0442\u044b\u043f \u0430\u043b\u0443\u0448\u044b\"],\n[\"\u624b\u4e2d\", null],\n[\"\u6536\u56de\", \"\u049b\u0430\u0439\u0442\u0430\u0440\u044b\u043f \u0430\u043b\u0443\"],\n[\"\u623f\u4ea7\", \"\u0436\u044b\u043b\u0436\u044b\u043c\u0430\u0439\u0442\u044b\u043d \u043c\u04af\u043b\u0456\u043a\"],\n[\"\u4e4b\u540e\", null],\n[\"\uff0c\", null],\n[\"\u4ed6\u4eec\u4f1a\", null],\n[\"\u4ee5\", null],\n[\"\u6781\", null],\n[\"\u4f4e\", \"\u0442\u04e9\u043c\u0435\u043d\"],\n[\"\u7684\u4ef7\u683c\", \"\u0431\u0430\u0493\u0430\u043c\u0435\u043d\"],\n[\"\u552e\", \"\u0441\u0430\u0442\u0443\"],\n[\"\u51fa\", null],\n[\"\uff0c\", null],\n[\"\u4ece\u800c\", null],\n[\"\u5bfc\u81f4\", \"\u04d9\u043a\u0435\u043b\u0443\"],\n[\"\u4e86\", null],\n[\"\u8be5\", null],\n[\"\u5730\u533a\u7684\", \"\u0430\u0439\u043c\u0430\u049b\u0442\u0430\u0493\u044b\"],\n[\"\u623f\u4ef7\", \"\u04af\u0439 \u0431\u0430\u0493\u0430\u0441\u044b\"],\n[\"\u4e00\u6b65\u6b65\", \"\u0431\u0456\u0440\u0442\u0456\u043d\u0434\u0435\u043f\"],\n[\"\u4e0b\u6ed1\", \"\u0442\u04e9\u043c\u0435\u043d\u0434\u0435\u0443\"],\n[\"\uff0c\", null],\n[\"\u8fd9\u4e2a\", null],\n[\"\u8fc7\u7a0b\", \"\u043f\u0440\u043e\u0446\u0435\u0441\u0441\"],\n[\"\u53c8\", null],\n[\"\u5f15\u53d1\u4e86\", \"\u0442\u0443\u0434\u044b\u0440\u0434\u044b\"],\n[\"\u623f\u4ef7\", \"\u04af\u0439 \u0431\u0430\u0493\u0430\u0441\u044b\"],\n[\"\u8fdb\u4e00\u6b65\", \"\u043e\u0434\u0430\u043d \u04d9\u0440\u0456\"],\n[\"\u7684\", null],\n[\"\u4e0b\u6ed1\", \"\u0442\u04e9\u043c\u0435\u043d\u0434\u0435\u0443\"],\n[\"\uff0c\", null],\n[\"\u5f62\u6210\u4e86\", \"\u049b\u04b1\u0440\u0430\u0434\u044b\"],\n[\"\u4e00\u4e2a\", null],\n[\"\u6076\u6027\", \"\u049b\u0430\u0443\u0456\u043f\u0442\u0456\"],\n[\"\u5faa\u73af\", \"\u0430\u0439\u043d\u0430\u043b\u044b\u043c\"],\n[\"\uff0c\", null],\n[\"\u5f53\", null],\n[\"\u623f\u4ef7\", \"\u04af\u0439 \u0431\u0430\u0493\u0430\u0441\u044b\"],\n[\"\u4e0b\u8dcc\", \"\u049b\u04b1\u043b\u0434\u044b\u0440\u0430\u0443\"],\n[\"\u5230\", null],\n[\"\u4e00\u5b9a\u7684\", \"\u0431\u0435\u043b\u0433\u0456\u043b\u0456 \u0431\u0456\u0440\"],\n[\"\u7a0b\u5ea6\", \"\u0434\u0435\u04a3\u0433\u0435\u0439\"],\n[\"\u4e4b\u540e\", null],\n[\"\uff0c\", null],\n[\"\u8d8a\u6765\u8d8a\u591a\", \"\u043a\u04e9\u0431\u0456\u0440\u0435\u043a\"],\n[\"\u7684\", null],\n[\"\u8d2d\u623f\u8005\", \"\u04af\u0439 \u0441\u0430\u0442\u044b\u043f \u0430\u043b\u0443\u0448\u044b\"],\n[\"\u53d1\u73b0\", \"\u0431\u0430\u0439\u049b\u0430\u0443\"],\n[\"\u81ea\u5df1\u7684\", null],\n[\"\u623f\u5b50\", \"\u04af\u0439\"],\n[\"\u4ef7\u503c\", \"\u049b\u04b1\u043d\u044b\"],\n[\"\u751a\u81f3\", \"\u0442\u0456\u043f\u0442\u0456\"],\n[\"\u5df2\u7ecf\", null],\n[\"\u6bd4\", null],\n[\"\u81ea\u5df1\u7684\", null],\n[\"\u8d37\u6b3e\", \"\u043d\u0435\u0441\u0438\u0435\"],\n[\"\u8fd8\", null],\n[\"\u4f4e\", \"\u0442\u04e9\u043c\u0435\u043d\"],\n[\"\u4e86\", null],\n[\"\uff0c\", null],\n[\"\u4e8e\u662f\", null],\n[\"\u5c31\u6709\u4e86\", null],\n[\"\u66f4\u591a\", \"\u043a\u04e9\u0431\u0456\u0440\u0435\u043a\"],\n[\"\u7684\u4eba\", null],\n[\"\u505c\u6b62\", \"\u0442\u043e\u049b\u0442\u0430\u0442\u0443\"],\n[\"\u7ee7\u7eed\", null],\n[\"\u8fd8\", null],\n[\"\u623f\u8d37\", \"\u0438\u043f\u043e\u0442\u0435\u043a\u0430\"],\n[\"\uff0c\", null],\n[\"\u53c8\", null],\n[\"\u9020\u6210\u4e86\", \"\u0442\u0443\u0434\u044b\u0440\u0434\u044b\"],\n[\"\u66f4\u591a\u7684\", \"\u043a\u04e9\u0431\u0456\u0440\u0435\u043a\"],\n[\"\u574f\", \"\u0436\u0430\u043c\u0430\u043d\"],\n[\"\u8d26\", \"\u049b\u0430\u0440\u044b\u0437\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u4ece\u800c\u6709\u4eba\u56e0\u4e3a\u6536\u5165\u8fc7\u4f4e\u652f\u4ed8\u4e0d\u8d77\u623f\u8d37\uff0c\u4e8e\u662f\u94f6\u884c\u5c31\u5f00\u59cb\u6536\u56de\u623f\u4ea7\uff0c\u5f53\u94f6\u884c\u4ece\u62d6\u6b20\u8d37\u6b3e\u7684\u8d2d\u623f\u8005\u624b\u4e2d\u6536\u56de\u623f\u4ea7\u4e4b\u540e\uff0c\u4ed6\u4eec\u4f1a\u4ee5\u6781\u4f4e\u7684\u4ef7\u683c\u552e\u51fa\uff0c\u4ece\u800c\u5bfc\u81f4\u4e86\u8be5\u5730\u533a\u7684\u623f\u4ef7\u4e00\u6b65\u6b65\u4e0b\u6ed1\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u53c8\u5f15\u53d1\u4e86\u623f\u4ef7\u8fdb\u4e00\u6b65\u7684\u4e0b\u6ed1\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u6076\u6027\u5faa\u73af\uff0c\u5f53\u623f\u4ef7\u4e0b\u8dcc\u5230\u4e00\u5b9a\u7684\u7a0b\u5ea6\u4e4b\u540e\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u8d2d\u623f\u8005\u53d1\u73b0\u81ea\u5df1\u7684\u623f\u5b50\u4ef7\u503c\u751a\u81f3\u5df2\u7ecf\u6bd4\u81ea\u5df1\u7684\u8d37\u6b3e\u8fd8\u4f4e\u4e86\uff0c\u4e8e\u662f\u5c31\u6709\u4e86\u66f4\u591a\u7684\u4eba\u505c\u6b62\u7ee7\u7eed\u8fd8\u623f\u8d37\uff0c\u53c8\u9020\u6210\u4e86\u66f4\u591a\u7684\u574f\u8d26\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u4ece\u800c', '\u6709\u4eba', '\u56e0\u4e3a', '\u6536\u5165', '\u8fc7', '\u4f4e', '\u652f\u4ed8', '\u4e0d\u8d77', '\u623f\u8d37', '\uff0c', '\u4e8e\u662f', '\u94f6\u884c', '\u5c31\u5f00\u59cb', '\u6536\u56de', '\u623f\u4ea7', '\uff0c', '\u5f53', '\u94f6\u884c', '\u4ece', '\u62d6\u6b20', '\u8d37\u6b3e', '\u7684', '\u8d2d\u623f\u8005', '\u624b\u4e2d', '\u6536\u56de', '\u623f\u4ea7', '\u4e4b\u540e', '\uff0c', '\u4ed6\u4eec\u4f1a', '\u4ee5', '\u6781', '\u4f4e', '\u7684\u4ef7\u683c', '\u552e', '\u51fa', '\uff0c', '\u4ece\u800c', '\u5bfc\u81f4', '\u4e86', '\u8be5', '\u5730\u533a\u7684', '\u623f\u4ef7', '\u4e00\u6b65\u6b65', '\u4e0b\u6ed1', '\uff0c', '\u8fd9\u4e2a', '\u8fc7\u7a0b', '\u53c8', '\u5f15\u53d1\u4e86', '\u623f\u4ef7', '\u8fdb\u4e00\u6b65', '\u7684', '\u4e0b\u6ed1', '\uff0c', '\u5f62\u6210\u4e86', '\u4e00\u4e2a', '\u6076\u6027', '\u5faa\u73af', '\uff0c', '\u5f53', '\u623f\u4ef7', '\u4e0b\u8dcc', '\u5230', '\u4e00\u5b9a\u7684', '\u7a0b\u5ea6', '\u4e4b\u540e', '\uff0c', '\u8d8a\u6765\u8d8a\u591a', '\u7684', '\u8d2d\u623f\u8005', '\u53d1\u73b0', '\u81ea\u5df1\u7684', '\u623f\u5b50', '\u4ef7\u503c', '\u751a\u81f3', '\u5df2\u7ecf', '\u6bd4', '\u81ea\u5df1\u7684', '\u8d37\u6b3e', '\u8fd8', '\u4f4e', '\u4e86', '\uff0c', '\u4e8e\u662f', '\u5c31\u6709\u4e86', '\u66f4\u591a', '\u7684\u4eba', '\u505c\u6b62', '\u7ee7\u7eed', '\u8fd8', '\u623f\u8d37', '\uff0c', '\u53c8', '\u9020\u6210\u4e86', '\u66f4\u591a\u7684', '\u574f', '\u8d26', '\u3002']"}], "[\n[\"\u4ece\u800c\", null],\n[\"\u6709\u4eba\", \"\u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043b\u044e\u0434\u0438\"],\n[\"\u56e0\u4e3a\", null],\n[\"\u6536\u5165\", \"\u0434\u043e\u0445\u043e\u0434\"],\n[\"\u8fc7\", null],\n[\"\u4f4e\", \"\u043d\u0438\u0437\u043a\u0438\u0439\"],\n[\"\u652f\u4ed8\", \"\u043f\u043b\u0430\u0442\u0438\u0442\u044c\"],\n[\"\u4e0d\u8d77\", null],\n[\"\u623f\u8d37\", \"\u0438\u043f\u043e\u0442\u0435\u043a\u0430\"],\n[\"\uff0c\", null],\n[\"\u4e8e\u662f\", null],\n[\"\u94f6\u884c\", \"\u0431\u0430\u043d\u043a\u0438\"],\n[\"\u5c31\u5f00\u59cb\", null],\n[\"\u6536\u56de\", \"\u0438\u0437\u044b\u043c\u0430\u0442\u044c\"],\n[\"\u623f\u4ea7\", \"\u043d\u0435\u0434\u0432\u0438\u0436\u0438\u043c\u043e\u0441\u0442\u044c\"],\n[\"\uff0c\", null],\n[\"\u5f53\", null],\n[\"\u94f6\u884c\", \"\u0431\u0430\u043d\u043a\u0438\"],\n[\"\u4ece\", null],\n[\"\u62d6\u6b20\", \"\u043f\u0440\u043e\u0441\u0440\u043e\u0447\u0438\u0442\u044c\"],\n[\"\u8d37\u6b3e\", \"\u043a\u0440\u0435\u0434\u0438\u0442\"],\n[\"\u7684\", null],\n[\"\u8d2d\u623f\u8005\", \"\u043f\u043e\u043a\u0443\u043f\u0430\u0442\u0435\u043b\u0438 \u0436\u0438\u043b\u044c\u044f\"],\n[\"\u624b\u4e2d\", null],\n[\"\u6536\u56de\", \"\u0438\u0437\u044b\u043c\u0430\u0442\u044c\"],\n[\"\u623f\u4ea7\", \"\u043d\u0435\u0434\u0432\u0438\u0436\u0438\u043c\u043e\u0441\u0442\u044c\"],\n[\"\u4e4b\u540e\", null],\n[\"\uff0c\", null],\n[\"\u4ed6\u4eec\u4f1a\", \"\u043e\u043d\u0438\"],\n[\"\u4ee5\", null],\n[\"\u6781\", null],\n[\"\u4f4e\", \"\u043d\u0438\u0437\u043a\u0438\u0439\"],\n[\"\u7684\u4ef7\u683c\", \"\u0446\u0435\u043d\u0430\"],\n[\"\u552e\", \"\u043f\u0440\u043e\u0434\u0430\u0432\u0430\u0442\u044c\"],\n[\"\u51fa\", null],\n[\"\uff0c\", null],\n[\"\u4ece\u800c\", null],\n[\"\u5bfc\u81f4\", \"\u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438\"],\n[\"\u4e86\", null],\n[\"\u8be5\", null],\n[\"\u5730\u533a\u7684\", \"\u0440\u0435\u0433\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439\"],\n[\"\u623f\u4ef7\", \"\u0446\u0435\u043d\u044b \u043d\u0430 \u0436\u0438\u043b\u044c\u0435\"],\n[\"\u4e00\u6b65\u6b65\", \"\u043f\u043e\u0441\u0442\u0435\u043f\u0435\u043d\u043d\u043e\"],\n[\"\u4e0b\u6ed1\", \"\u0441\u043d\u0438\u0436\u0430\u0442\u044c\u0441\u044f\"],\n[\"\uff0c\", null],\n[\"\u8fd9\u4e2a\", null],\n[\"\u8fc7\u7a0b\", \"\u043f\u0440\u043e\u0446\u0435\u0441\u0441\"],\n[\"\u53c8\", \"\u0441\u043d\u043e\u0432\u0430\"],\n[\"\u5f15\u53d1\u4e86\", \"\u0432\u044b\u0437\u0432\u0430\u0442\u044c\"],\n[\"\u623f\u4ef7\", \"\u0446\u0435\u043d\u044b \u043d\u0430 \u0436\u0438\u043b\u044c\u0435\"],\n[\"\u8fdb\u4e00\u6b65\", \"\u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0438\u0439\"],\n[\"\u7684\", null],\n[\"\u4e0b\u6ed1\", \"\u0441\u043d\u0438\u0436\u0435\u043d\u0438\u0435\"],\n[\"\uff0c\", null],\n[\"\u5f62\u6210\u4e86\", \"\u0441\u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u0442\u044c\"],\n[\"\u4e00\u4e2a\", null],\n[\"\u6076\u6027\", \"\u043f\u043e\u0440\u043e\u0447\u043d\u044b\u0439\"],\n[\"\u5faa\u73af\", \"\u043a\u0440\u0443\u0433\"],\n[\"\uff0c\", null],\n[\"\u5f53\", null],\n[\"\u623f\u4ef7\", \"\u0446\u0435\u043d\u044b \u043d\u0430 \u0436\u0438\u043b\u044c\u0435\"],\n[\"\u4e0b\u8dcc\", \"\u043f\u0430\u0434\u0430\u0442\u044c\"],\n[\"\u5230\", null],\n[\"\u4e00\u5b9a\u7684\", \"\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u044b\u0439\"],\n[\"\u7a0b\u5ea6\", \"\u0443\u0440\u043e\u0432\u0435\u043d\u044c\"],\n[\"\u4e4b\u540e\", null],\n[\"\uff0c\", null],\n[\"\u8d8a\u6765\u8d8a\u591a\", \"\u0432\u0441\u0435 \u0431\u043e\u043b\u044c\u0448\u0435\"],\n[\"\u7684\", null],\n[\"\u8d2d\u623f\u8005\", \"\u043f\u043e\u043a\u0443\u043f\u0430\u0442\u0435\u043b\u0438 \u0436\u0438\u043b\u044c\u044f\"],\n[\"\u53d1\u73b0\", \"\u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0438\u0442\u044c\"],\n[\"\u81ea\u5df1\u7684\", null],\n[\"\u623f\u5b50\", \"\u0434\u043e\u043c\"],\n[\"\u4ef7\u503c\", \"\u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u044c\"],\n[\"\u751a\u81f3\", \"\u0434\u0430\u0436\u0435\"],\n[\"\u5df2\u7ecf\", null],\n[\"\u6bd4\", null],\n[\"\u81ea\u5df1\u7684\", null],\n[\"\u8d37\u6b3e\", \"\u043a\u0440\u0435\u0434\u0438\u0442\"],\n[\"\u8fd8\", null],\n[\"\u4f4e\", \"\u043d\u0438\u0436\u0435\"],\n[\"\u4e86\", null],\n[\"\uff0c\", null],\n[\"\u4e8e\u662f\", null],\n[\"\u5c31\u6709\u4e86\", null],\n[\"\u66f4\u591a\", \"\u0431\u043e\u043b\u044c\u0448\u0435\"],\n[\"\u7684\u4eba\", \"\u043b\u044e\u0434\u0435\u0439\"],\n[\"\u505c\u6b62\", \"\u043f\u0440\u0435\u043a\u0440\u0430\u0442\u0438\u0442\u044c\"],\n[\"\u7ee7\u7eed\", \"\u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0430\u0442\u044c\"],\n[\"\u8fd8\", null],\n[\"\u623f\u8d37\", \"\u0438\u043f\u043e\u0442\u0435\u043a\u0430\"],\n[\"\uff0c\", null],\n[\"\u53c8\", \"\u0441\u043d\u043e\u0432\u0430\"],\n[\"\u9020\u6210\u4e86\", \"\u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438\"],\n[\"\u66f4\u591a\u7684\", \"\u0431\u043e\u043b\u044c\u0448\u0435\"],\n[\"\u574f\", \"\u043f\u043b\u043e\u0445\u043e\u0439\"],\n[\"\u8d26\", \"\u0434\u043e\u043b\u0433\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u5409\u5c14\u4f2f\u6258\u00b7\u7f57\u5fb7\u91cc\u683c\u65af\u00b7\u6b27\u534e\u62c9\uff08\u8fbe\u7c73\u5b89\u00b7\u963f\u5c14\u5361\u624e\u9970\uff09\u662f\u5361\u5229\u96c6\u56e2\u7684\u9886\u5bfc\u4eba\uff0c\u7c73\u683c\u5c14\u00b7\u7f57\u5fb7\u91cc\u683c\u65af\u00b7\u6b27\u534e\u62c9\uff08\u5f17\u6717\u897f\u65af\u79d1\u00b7\u4e39\u5c3c\u65af\u9970\uff09\u662f\u5361\u5229\u96c6\u56e2\u7684\u667a\u56ca\uff0cPacho Herrera\u5e15\u4e54\u00b7\u8d6b\u96f7\u62c9\uff08\u963f\u5c14\u4f2f\u6258\u00b7\u963f\u66fc\u9970\uff09\u8d1f\u8d23\u8fd0\u8425\u58a8\u897f\u54e5\u7684\u7f51\u7edc\uff0c\u5f7b\u666e\u00b7\u5723\u514b\u9c81\u65af\u00b7\u4f26\u6566\u8bfa\uff08\u4f69\u4f69\u00b7\u62c9\u5e15\u4f50\u7279\u9970\uff09\u8d1f\u8d23\u7ebd\u7ea6\u7684\u8fd0\u8425\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u5409', '\u5c14', '\u4f2f', '\u6258', '\u00b7', '\u7f57', '\u5fb7', '\u91cc', '\u683c', '\u65af', '\u00b7', '\u6b27', '\u534e', '\u62c9', '\uff08', '\u8fbe', '\u7c73', '\u5b89', '\u00b7', '\u963f\u5c14', '\u5361', '\u624e', '\u9970', '\uff09', '\u662f', '\u5361', '\u5229', '\u96c6\u56e2', '\u7684', '\u9886\u5bfc\u4eba', '\uff0c', '\u7c73', '\u683c\u5c14', '\u00b7', '\u7f57', '\u5fb7', '\u91cc', '\u683c', '\u65af', '\u00b7', '\u6b27', '\u534e', '\u62c9', '\uff08', '\u5f17', '\u6717', '\u897f', '\u65af', '\u79d1', '\u00b7', '\u4e39', '\u5c3c\u65af', '\u9970', '\uff09', '\u662f', '\u5361', '\u5229', '\u96c6\u56e2', '\u7684', '\u667a', '\u56ca', '\uff0c', 'P', 'acho', ' Herrera', '\u5e15', '\u4e54', '\u00b7', '\u8d6b', '\u96f7', '\u62c9', '\uff08', '\u963f\u5c14', '\u4f2f', '\u6258', '\u00b7', '\u963f', '\u66fc', '\u9970', '\uff09', '\u8d1f\u8d23', '\u8fd0\u8425', '\u58a8\u897f\u54e5', '\u7684', '\u7f51\u7edc', '\uff0c', '\u5f7b', '\u666e', '\u00b7', '\u5723', '\u514b', '\u9c81', '\u65af', '\u00b7', '\u4f26\u6566', '\u8bfa', '\uff08', '\u4f69', '\u4f69', '\u00b7', '\u62c9', '\u5e15', '\u4f50', '\u7279', '\u9970', '\uff09', '\u8d1f\u8d23', '\u7ebd\u7ea6', '\u7684', '\u8fd0\u8425', '\u3002']"}], "[\n[\"\u5409\", null],\n[\"\u5c14\", null],\n[\"\u4f2f\", null],\n[\"\u6258\", \"Gilberto\"],\n[\"\u00b7\", null],\n[\"\u7f57\", null],\n[\"\u5fb7\", null],\n[\"\u91cc\", null],\n[\"\u683c\", null],\n[\"\u65af\", null],\n[\"\u00b7\", null],\n[\"\u6b27\", null],\n[\"\u534e\", null],\n[\"\u62c9\", \"Rodriguez Orejuela\"],\n[\"\uff08\", null],\n[\"\u8fbe\", null],\n[\"\u7c73\", null],\n[\"\u5b89\", null],\n[\"\u00b7\", null],\n[\"\u963f\u5c14\", null],\n[\"\u5361\", null],\n[\"\u624e\", \"Damian Alcazar\"],\n[\"\u9970\", null],\n[\"\uff09\", null],\n[\"\u662f\", null],\n[\"\u5361\", null],\n[\"\u5229\", null],\n[\"\u96c6\u56e2\", \"Cali Cartel\"],\n[\"\u7684\", null],\n[\"\u9886\u5bfc\u4eba\", \"leader\"],\n[\"\uff0c\", null],\n[\"\u7c73\", null],\n[\"\u683c\u5c14\", \"Miguel\"],\n[\"\u00b7\", null],\n[\"\u7f57\", null],\n[\"\u5fb7\", null],\n[\"\u91cc\", null],\n[\"\u683c\", null],\n[\"\u65af\", null],\n[\"\u00b7\", null],\n[\"\u6b27\", null],\n[\"\u534e\", null],\n[\"\u62c9\", \"Rodriguez Orejuela\"],\n[\"\uff08\", null],\n[\"\u5f17\", null],\n[\"\u6717\", null],\n[\"\u897f\", null],\n[\"\u65af\", null],\n[\"\u79d1\", null],\n[\"\u00b7\", null],\n[\"\u4e39\", null],\n[\"\u5c3c\u65af\", \"Francisco Denis\"],\n[\"\u9970\", null],\n[\"\uff09\", null],\n[\"\u662f\", null],\n[\"\u5361\", null],\n[\"\u5229\", null],\n[\"\u96c6\u56e2\", \"Cali Cartel\"],\n[\"\u7684\", null],\n[\"\u667a\", null],\n[\"\u56ca\", \"strategist\"],\n[\"\uff0c\", null],\n[\"P\", null],\n[\"acho\", null],\n[\" Herrera\", null],\n[\"\u5e15\", null],\n[\"\u4e54\", null],\n[\"\u00b7\", null],\n[\"\u8d6b\", null],\n[\"\u96f7\", null],\n[\"\u62c9\", \"Pacho Herrera\"],\n[\"\uff08\", null],\n[\"\u963f\u5c14\", null],\n[\"\u4f2f\", null],\n[\"\u6258\", null],\n[\"\u00b7\", null],\n[\"\u963f\", null],\n[\"\u66fc\", \"Alberto Ammann\"],\n[\"\u9970\", null],\n[\"\uff09\", null],\n[\"\u8d1f\u8d23\", \"is responsible for\"],\n[\"\u8fd0\u8425\", \"operating\"],\n[\"\u58a8\u897f\u54e5\", \"Mexico\"],\n[\"\u7684\", null],\n[\"\u7f51\u7edc\", \"network\"],\n[\"\uff0c\", null],\n[\"\u5f7b\", null],\n[\"\u666e\", null],\n[\"\u00b7\", null],\n[\"\u5723\", null],\n[\"\u514b\", null],\n[\"\u9c81\", null],\n[\"\u65af\", null],\n[\"\u00b7\", null],\n[\"\u4f26\u6566\", null],\n[\"\u8bfa\", \"Chepe Santacruz Londono\"],\n[\"\uff08\", null],\n[\"\u4f69\", null],\n[\"\u4f69\", null],\n[\"\u00b7\", null],\n[\"\u62c9\", null],\n[\"\u5e15\", null],\n[\"\u4f50\", null],\n[\"\u7279\", \"Pepe Rapazote\"],\n[\"\u9970\", null],\n[\"\uff09\", null],\n[\"\u8d1f\u8d23\", \"is responsible for\"],\n[\"\u7ebd\u7ea6\", \"New York\"],\n[\"\u7684\", null],\n[\"\u8fd0\u8425\", \"operations\"],\n[\"\u3002\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u5409\u5c14\u4f2f\u6258\u00b7\u7f57\u5fb7\u91cc\u683c\u65af\u00b7\u6b27\u534e\u62c9\uff08\u8fbe\u7c73\u5b89\u00b7\u963f\u5c14\u5361\u624e\u9970\uff09\u662f\u5361\u5229\u96c6\u56e2\u7684\u9886\u5bfc\u4eba\uff0c\u7c73\u683c\u5c14\u00b7\u7f57\u5fb7\u91cc\u683c\u65af\u00b7\u6b27\u534e\u62c9\uff08\u5f17\u6717\u897f\u65af\u79d1\u00b7\u4e39\u5c3c\u65af\u9970\uff09\u662f\u5361\u5229\u96c6\u56e2\u7684\u667a\u56ca\uff0cPacho Herrera\u5e15\u4e54\u00b7\u8d6b\u96f7\u62c9\uff08\u963f\u5c14\u4f2f\u6258\u00b7\u963f\u66fc\u9970\uff09\u8d1f\u8d23\u8fd0\u8425\u58a8\u897f\u54e5\u7684\u7f51\u7edc\uff0c\u5f7b\u666e\u00b7\u5723\u514b\u9c81\u65af\u00b7\u4f26\u6566\u8bfa\uff08\u4f69\u4f69\u00b7\u62c9\u5e15\u4f50\u7279\u9970\uff09\u8d1f\u8d23\u7ebd\u7ea6\u7684\u8fd0\u8425\u3002\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u5409', '\u5c14', '\u4f2f', '\u6258', '\u00b7', '\u7f57', '\u5fb7', '\u91cc', '\u683c', '\u65af', '\u00b7', '\u6b27', '\u534e', '\u62c9', '\uff08', '\u8fbe', '\u7c73', '\u5b89', '\u00b7', '\u963f\u5c14', '\u5361', '\u624e', '\u9970', '\uff09', '\u662f', '\u5361', '\u5229', '\u96c6\u56e2', '\u7684', '\u9886\u5bfc\u4eba', '\uff0c', '\u7c73', '\u683c\u5c14', '\u00b7', '\u7f57', '\u5fb7', '\u91cc', '\u683c', '\u65af', '\u00b7', '\u6b27', '\u534e', '\u62c9', '\uff08', '\u5f17', '\u6717', '\u897f', '\u65af', '\u79d1', '\u00b7', '\u4e39', '\u5c3c\u65af', '\u9970', '\uff09', '\u662f', '\u5361', '\u5229', '\u96c6\u56e2', '\u7684', '\u667a', '\u56ca', '\uff0c', 'P', 'acho', ' Herrera', '\u5e15', '\u4e54', '\u00b7', '\u8d6b', '\u96f7', '\u62c9', '\uff08', '\u963f\u5c14', '\u4f2f', '\u6258', '\u00b7', '\u963f', '\u66fc', '\u9970', '\uff09', '\u8d1f\u8d23', '\u8fd0\u8425', '\u58a8\u897f\u54e5', '\u7684', '\u7f51\u7edc', '\uff0c', '\u5f7b', '\u666e', '\u00b7', '\u5723', '\u514b', '\u9c81', '\u65af', '\u00b7', '\u4f26\u6566', '\u8bfa', '\uff08', '\u4f69', '\u4f69', '\u00b7', '\u62c9', '\u5e15', '\u4f50', '\u7279', '\u9970', '\uff09', '\u8d1f\u8d23', '\u7ebd\u7ea6', '\u7684', '\u8fd0\u8425', '\u3002']"}], "[\n[\"\u5409\",\"\u0425\u0438\u043b\u044c\u0431\u0435\u0440\u0442\u043e\"],\n[\"\u5c14\",null],\n[\"\u4f2f\",null],\n[\"\u6258\",null],\n[\"\u00b7\",null],\n[\"\u7f57\",null],\n[\"\u5fb7\",null],\n[\"\u91cc\",null],\n[\"\u683c\",null],\n[\"\u65af\",null],\n[\"\u00b7\",null],\n[\"\u6b27\",null],\n[\"\u534e\",null],\n[\"\u62c9\",null],\n[\"\uff08\",null],\n[\"\u8fbe\",\"\u0414\u044d\u043c\u0438\u0435\u043d\"],\n[\"\u7c73\",null],\n[\"\u5b89\",null],\n[\"\u00b7\",null],\n[\"\u963f\u5c14\",\"\u0410\u043b\u044c\u043a\u0430\u0441\u0430\u0440\"],\n[\"\u5361\",null],\n[\"\u624e\",null],\n[\"\u9970\",null],\n[\"\uff09\",null],\n[\"\u662f\",null],\n[\"\u5361\",\"\u041a\u0430\u043b\u0438\"],\n[\"\u5229\",null],\n[\"\u96c6\u56e2\",\"\u043a\u0430\u0440\u0442\u0435\u043b\u0456\"],\n[\"\u7684\",null],\n[\"\u9886\u5bfc\u4eba\",\"\u043a\u04e9\u0448\u0431\u0430\u0441\u0448\u044b\u0441\u044b\"],\n[\"\uff0c\",null],\n[\"\u7c73\",\"\u041c\u0438\u0433\u0435\u043b\u044c\"],\n[\"\u683c\u5c14\",null],\n[\"\u00b7\",null],\n[\"\u7f57\",null],\n[\"\u5fb7\",null],\n[\"\u91cc\",null],\n[\"\u683c\",null],\n[\"\u65af\",null],\n[\"\u00b7\",null],\n[\"\u6b27\",null],\n[\"\u534e\",null],\n[\"\u62c9\",null],\n[\"\uff08\",null],\n[\"\u5f17\",\"\u0424\u0440\u0430\u043d\u0441\u0438\u0441\u043a\u043e\"],\n[\"\u6717\",null],\n[\"\u897f\",null],\n[\"\u65af\",null],\n[\"\u79d1\",null],\n[\"\u00b7\",null],\n[\"\u4e39\",\"\u0414\u0435\u043d\u0438\u0441\"],\n[\"\u5c3c\u65af\",null],\n[\"\u9970\",null],\n[\"\uff09\",null],\n[\"\u662f\",null],\n[\"\u5361\",\"\u041a\u0430\u043b\u0438\"],\n[\"\u5229\",null],\n[\"\u96c6\u56e2\",\"\u043a\u0430\u0440\u0442\u0435\u043b\u0456\u043d\u0456\u04a3\"],\n[\"\u7684\",null],\n[\"\u667a\",\"\u0430\u049b\u044b\u043b\u0433\u04e9\u0439\u0456\"],\n[\"\u56ca\",null],\n[\"\uff0c\",null],\n[\"P\",null],\n[\"acho\",null],\n[\" Herrera\",null],\n[\"\u5e15\",\"\u041f\u0430\u0447\u043e\"],\n[\"\u4e54\",null],\n[\"\u00b7\",null],\n[\"\u8d6b\",\"\u042d\u0440\u0440\u0435\u0440\u0430\"],\n[\"\u96f7\",null],\n[\"\u62c9\",null],\n[\"\uff08\",null],\n[\"\u963f\u5c14\",\"\u0410\u043b\u044c\u0431\u0435\u0440\u0442\u043e\"],\n[\"\u4f2f\",null],\n[\"\u6258\",null],\n[\"\u00b7\",null],\n[\"\u963f\",\"\u0410\u043c\u0430\u043d\"],\n[\"\u66fc\",null],\n[\"\u9970\",null],\n[\"\uff09\",null],\n[\"\u8d1f\u8d23\",\"\u0436\u0430\u0443\u0430\u043f\u0442\u044b\"],\n[\"\u8fd0\u8425\",\"\u0431\u0430\u0441\u049b\u0430\u0440\u0430\u0434\u044b\"],\n[\"\u58a8\u897f\u54e5\",\"\u041c\u0435\u043a\u0441\u0438\u043a\u0430\u0434\u0430\u0493\u044b\"],\n[\"\u7684\",null],\n[\"\u7f51\u7edc\",\"\u0436\u0435\u043b\u0456\u043d\u0456\"],\n[\"\uff0c\",null],\n[\"\u5f7b\",\"\u0427\u0435\u043f\u0435\"],\n[\"\u666e\",null],\n[\"\u00b7\",null],\n[\"\u5723\",\"\u0421\u0430\u043d\u0442\u0430-\u041a\u0440\u0443\u0441\"],\n[\"\u514b\",null],\n[\"\u9c81\",null],\n[\"\u65af\",null],\n[\"\u00b7\",null],\n[\"\u4f26\u6566\",\"\u041b\u043e\u043d\u0434\u043e\u043d\u044c\u043e\"],\n[\"\u8bfa\",null],\n[\"\uff08\",null],\n[\"\u4f69\",\"\u041f\u0435\u043f\u0435\"],\n[\"\u4f69\",null],\n[\"\u00b7\",null],\n[\"\u62c9\",\"\u0420\u0430\u043f\u0430\u0441\u043e\u0442\u0435\"],\n[\"\u5e15\",null],\n[\"\u4f50\",null],\n[\"\u7279\",null],\n[\"\u9970\",null],\n[\"\uff09\",null],\n[\"\u8d1f\u8d23\",\"\u0436\u0430\u0443\u0430\u043f\u0442\u044b\"],\n[\"\u7ebd\u7ea6\",\"\u041d\u044c\u044e-\u0419\u043e\u0440\u043a\u0442\u0430\u0493\u044b\"],\n[\"\u7684\",null],\n[\"\u8fd0\u8425\",\"\u043e\u043f\u0435\u0440\u0430\u0446\u0438\u044f\u043b\u0430\u0440\u0493\u0430\"],\n[\"\u3002\",null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u04e8\u0437\u0456\u043d\u0456\u04a3 \u049a\u043e\u043b\u043c\u0430-\u049b\u043e\u043b \u0430\u049b\u0448\u0430 \u049b\u0430\u0440\u0493\u044b\u0441\u044b\u0430\u0442\u0442\u044b\u0436\u0430\u04a3\u0430 \u043a\u0456\u0442\u0430\u0431\u044b\u043d\u0434\u0430\u041a\u0435\u043d\u043d\u0435\u0442 \u0420\u043e\u0433\u043e\u0444\u0444 \u0441\u0430\u043b\u044b\u049b\u0442\u044b \u0442\u04e9\u043b\u0435\u0443\u0434\u0435\u043d \u0436\u0430\u043b\u0442\u0430\u0440\u0443 \u0436\u04d9\u043d\u0435 \u049b\u044b\u043b\u043c\u044b\u0441\u0442\u044b\u049b \u04d9\u0440\u0435\u043a\u0435\u0442\u043f\u0435\u043d \u043a\u04af\u0440\u0435\u0441\u0443 \u043c\u0430\u049b\u0441\u0430\u0442\u044b\u043d\u0434\u0430 \u0436\u043e\u0493\u0430\u0440\u044b \u0430\u0442\u0430\u0443\u043b\u044b \u0431\u0430\u043d\u043a\u043d\u043e\u0442\u0430\u043b\u0430\u0440\u0434\u044b \u0436\u043e\u044e\u0434\u044b \u049b\u043e\u043b\u0434\u0430\u0439\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u04e8', '\u0437', '\u0456', '\u043d', '\u0456', '\u04a3', ' ', '\u049a', '\u043e\u043b', '\u043c\u0430', '-', '\u049b', '\u043e\u043b', ' \u0430', '\u049b', '\u0448\u0430', ' ', '\u049b', '\u0430\u0440', '\u0493', '\u044b', '\u0441\u044b', '\u0430\u0442', '\u0442\u044b', '\u0436\u0430', '\u04a3', '\u0430', ' \u043a', '\u0456', '\u0442', '\u0430\u0431', '\u044b', '\u043d\u0434', '', '', '\u0435\u043d\u043d', '\u0435\u0442', ' \u0420', '\u043e\u0433\u043e', '\u0444\u0444', ' \u0441', '\u0430\u043b', '\u044b', '\u049b', '\u0442\u044b', ' \u0442', '\u04e9', '\u043b\u0435', '\u0443\u0434', '\u0435\u043d', ' \u0436', '\u0430\u043b', '\u0442\u0430\u0440', '\u0443', ' \u0436', '\u04d9', '\u043d\u0435', ' ', '\u049b', '\u044b', '\u043b', '\u043c\u044b', '\u0441\u0442\u044b', '\u049b', ' ', '\u04d9', '\u0440', '\u0435\u043a', '\u0435\u0442', '\u043f', '\u0435\u043d', ' \u043a', '\u04af', '\u0440\u0435\u0441', '\u0443', ' \u043c\u0430', '\u049b', '\u0441', '\u0430\u0442', '\u044b', '\u043d', '\u0434\u0430', ' \u0436', '\u043e', '\u0493', '\u0430\u0440', '\u044b', ' \u0430', '\u0442\u0430', '\u0443', '\u043b\u044b', ' \u0431\u0430\u043d', '\u043a', '\u043d', '\u043e\u0442', '\u0430\u043b', '\u0430\u0440', '\u0434\u044b', ' \u0436', '\u043e', '\u044e', '\u0434\u044b', ' ', '\u049b', '\u043e\u043b', '\u0434', '\u0430\u0439\u0434', '\u044b', '.']"}], "[\n[\"\u04e8\", \"In\"],\n[\"\u0437\", null],\n[\"\u0456\", null],\n[\"\u043d\", null],\n[\"\u0456\", null],\n[\"\u04a3\", null],\n[\" \", null],\n[\"\u049a\", \"Cash\"],\n[\"\u043e\u043b\", null],\n[\"\u043c\u0430\", null],\n[\"-\", null],\n[\"\u049b\", null],\n[\"\u043e\u043b\", null],\n[\" \u0430\", null],\n[\"\u049b\", null],\n[\"\u0448\u0430\", null],\n[\" \", null],\n[\"\u049b\", \"Curse\"],\n[\"\u0430\u0440\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u0441\u044b\", null],\n[\"\u0430\u0442\", null],\n[\"\u0442\u044b\", null],\n[\"\u0436\u0430\", \"new\"],\n[\"\u04a3\", null],\n[\"\u0430\", null],\n[\" \u043a\", null],\n[\"\u0456\", null],\n[\"\u0442\", null],\n[\"\u0430\u0431\", null],\n[\"\u044b\", null],\n[\"\u043d\u0434\", null],\n[\"\", null],\n[\"\", null],\n[\"\u0435\u043d\u043d\", \"Kenneth\"],\n[\"\u0435\u0442\", null],\n[\" \u0420\", null],\n[\"\u043e\u0433\u043e\", \"Rogoff\"],\n[\"\u0444\u0444\", null],\n[\" \u0441\", null],\n[\"\u0430\u043b\", \"tax\"],\n[\"\u044b\", null],\n[\"\u049b\", null],\n[\"\u0442\u044b\", null],\n[\" \u0442\", null],\n[\"\u04e9\", null],\n[\"\u043b\u0435\", null],\n[\"\u0443\u0434\", null],\n[\"\u0435\u043d\", null],\n[\" \u0436\", null],\n[\"\u0430\u043b\", \"evasion\"],\n[\"\u0442\u0430\u0440\", null],\n[\"\u0443\", null],\n[\" \u0436\", null],\n[\"\u04d9\", null],\n[\"\u043d\u0435\", null],\n[\" \", null],\n[\"\u049b\", \"criminal\"],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\"\u043c\u044b\", null],\n[\"\u0441\u0442\u044b\", null],\n[\"\u049b\", null],\n[\" \", null],\n[\"\u04d9\", \"activities\"],\n[\"\u0440\", null],\n[\"\u0435\u043a\", null],\n[\"\u0435\u0442\", null],\n[\"\u043f\", null],\n[\"\u0435\u043d\", null],\n[\" \u043a\", null],\n[\"\u04af\", \"combat\"],\n[\"\u0440\u0435\u0441\", null],\n[\"\u0443\", null],\n[\" \u043c\u0430\", null],\n[\"\u049b\", \"purpose\"],\n[\"\u0441\", null],\n[\"\u0430\u0442\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u0434\u0430\", null],\n[\" \u0436\", null],\n[\"\u043e\", \"high\"],\n[\"\u0493\", null],\n[\"\u0430\u0440\", null],\n[\"\u044b\", null],\n[\" \u0430\", null],\n[\"\u0442\u0430\", \"denomination\"],\n[\"\u0443\", null],\n[\"\u043b\u044b\", null],\n[\" \u0431\u0430\u043d\", \"banknotes\"],\n[\"\u043a\", null],\n[\"\u043d\", null],\n[\"\u043e\u0442\", null],\n[\"\u0430\u043b\", null],\n[\"\u0430\u0440\", null],\n[\"\u0434\u044b\", null],\n[\" \u0436\", null],\n[\"\u043e\", \"elimination\"],\n[\"\u044e\", null],\n[\"\u0434\u044b\", null],\n[\" \", null],\n[\"\u049b\", \"supports\"],\n[\"\u043e\u043b\", null],\n[\"\u0434\", null],\n[\"\u0430\u0439\u0434\", null],\n[\"\u044b\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u04e8\u0437\u0456\u043d\u0456\u04a3 \u049a\u043e\u043b\u043c\u0430-\u049b\u043e\u043b \u0430\u049b\u0448\u0430 \u049b\u0430\u0440\u0493\u044b\u0441\u044b\u0430\u0442\u0442\u044b\u0436\u0430\u04a3\u0430 \u043a\u0456\u0442\u0430\u0431\u044b\u043d\u0434\u0430\u041a\u0435\u043d\u043d\u0435\u0442 \u0420\u043e\u0433\u043e\u0444\u0444 \u0441\u0430\u043b\u044b\u049b\u0442\u044b \u0442\u04e9\u043b\u0435\u0443\u0434\u0435\u043d \u0436\u0430\u043b\u0442\u0430\u0440\u0443 \u0436\u04d9\u043d\u0435 \u049b\u044b\u043b\u043c\u044b\u0441\u0442\u044b\u049b \u04d9\u0440\u0435\u043a\u0435\u0442\u043f\u0435\u043d \u043a\u04af\u0440\u0435\u0441\u0443 \u043c\u0430\u049b\u0441\u0430\u0442\u044b\u043d\u0434\u0430 \u0436\u043e\u0493\u0430\u0440\u044b \u0430\u0442\u0430\u0443\u043b\u044b \u0431\u0430\u043d\u043a\u043d\u043e\u0442\u0430\u043b\u0430\u0440\u0434\u044b \u0436\u043e\u044e\u0434\u044b \u049b\u043e\u043b\u0434\u0430\u0439\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u04e8', '\u0437', '\u0456', '\u043d', '\u0456', '\u04a3', ' ', '\u049a', '\u043e\u043b', '\u043c\u0430', '-', '\u049b', '\u043e\u043b', ' \u0430', '\u049b', '\u0448\u0430', ' ', '\u049b', '\u0430\u0440', '\u0493', '\u044b', '\u0441\u044b', '\u0430\u0442', '\u0442\u044b', '\u0436\u0430', '\u04a3', '\u0430', ' \u043a', '\u0456', '\u0442', '\u0430\u0431', '\u044b', '\u043d\u0434', '', '', '\u0435\u043d\u043d', '\u0435\u0442', ' \u0420', '\u043e\u0433\u043e', '\u0444\u0444', ' \u0441', '\u0430\u043b', '\u044b', '\u049b', '\u0442\u044b', ' \u0442', '\u04e9', '\u043b\u0435', '\u0443\u0434', '\u0435\u043d', ' \u0436', '\u0430\u043b', '\u0442\u0430\u0440', '\u0443', ' \u0436', '\u04d9', '\u043d\u0435', ' ', '\u049b', '\u044b', '\u043b', '\u043c\u044b', '\u0441\u0442\u044b', '\u049b', ' ', '\u04d9', '\u0440', '\u0435\u043a', '\u0435\u0442', '\u043f', '\u0435\u043d', ' \u043a', '\u04af', '\u0440\u0435\u0441', '\u0443', ' \u043c\u0430', '\u049b', '\u0441', '\u0430\u0442', '\u044b', '\u043d', '\u0434\u0430', ' \u0436', '\u043e', '\u0493', '\u0430\u0440', '\u044b', ' \u0430', '\u0442\u0430', '\u0443', '\u043b\u044b', ' \u0431\u0430\u043d', '\u043a', '\u043d', '\u043e\u0442', '\u0430\u043b', '\u0430\u0440', '\u0434\u044b', ' \u0436', '\u043e', '\u044e', '\u0434\u044b', ' ', '\u049b', '\u043e\u043b', '\u0434', '\u0430\u0439\u0434', '\u044b', '.']"}], "[\n[\"\u04e8\", \"\u81ea\"],\n[\"\u0437\", null],\n[\"\u0456\", null],\n[\"\u043d\", null],\n[\"\u0456\", null],\n[\"\u04a3\", null],\n[\" \", null],\n[\"\u049a\", \"\u73b0\u91d1\"],\n[\"\u043e\u043b\", null],\n[\"\u043c\u0430\", null],\n[\"-\", null],\n[\"\u049b\", null],\n[\"\u043e\u043b\", null],\n[\" \u0430\", null],\n[\"\u049b\", null],\n[\"\u0448\u0430\", null],\n[\" \", null],\n[\"\u049b\", \"\u8bc5\u5492\"],\n[\"\u0430\u0440\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u0441\u044b\", null],\n[\"\u0430\u0442\", null],\n[\"\u0442\u044b\", null],\n[\"\u0436\u0430\", \"\u65b0\"],\n[\"\u04a3\", null],\n[\"\u0430\", null],\n[\" \u043a\", null],\n[\"\u0456\", null],\n[\"\u0442\", null],\n[\"\u0430\u0431\", null],\n[\"\u044b\", null],\n[\"\u043d\u0434\", null],\n[\"\", null],\n[\"\", null],\n[\"\u0435\u043d\u043d\", \"\u80af\u5c3c\u65af\"],\n[\"\u0435\u0442\", null],\n[\" \u0420\", null],\n[\"\u043e\u0433\u043e\", null],\n[\"\u0444\u0444\", null],\n[\" \u0441\", null],\n[\"\u0430\u043b\", \"\u7a0e\u6536\"],\n[\"\u044b\", null],\n[\"\u049b\", null],\n[\"\u0442\u044b\", null],\n[\" \u0442\", null],\n[\"\u04e9\", null],\n[\"\u043b\u0435\", null],\n[\"\u0443\u0434\", null],\n[\"\u0435\u043d\", null],\n[\" \u0436\", null],\n[\"\u0430\u043b\", \"\u9003\u907f\"],\n[\"\u0442\u0430\u0440\", null],\n[\"\u0443\", null],\n[\" \u0436\", null],\n[\"\u04d9\", null],\n[\"\u043d\u0435\", null],\n[\" \", null],\n[\"\u049b\", \"\u72af\u7f6a\"],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\"\u043c\u044b\", null],\n[\"\u0441\u0442\u044b\", null],\n[\"\u049b\", null],\n[\" \", null],\n[\"\u04d9\", \"\u884c\u4e3a\"],\n[\"\u0440\", null],\n[\"\u0435\u043a\", null],\n[\"\u0435\u0442\", null],\n[\"\u043f\", null],\n[\"\u0435\u043d\", null],\n[\" \u043a\", null],\n[\"\u04af\", \"\u6253\u51fb\"],\n[\"\u0440\u0435\u0441\", null],\n[\"\u0443\", null],\n[\" \u043c\u0430\", null],\n[\"\u049b\", \"\u76ee\u7684\"],\n[\"\u0441\", null],\n[\"\u0430\u0442\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u0434\u0430\", null],\n[\" \u0436\", null],\n[\"\u043e\", \"\u9ad8\u9762\u989d\"],\n[\"\u0493\", null],\n[\"\u0430\u0440\", null],\n[\"\u044b\", null],\n[\" \u0430\", null],\n[\"\u0442\u0430\", null],\n[\"\u0443\", null],\n[\"\u043b\u044b\", null],\n[\" \u0431\u0430\u043d\", \"\u7eb8\u5e01\"],\n[\"\u043a\", null],\n[\"\u043d\", null],\n[\"\u043e\u0442\", null],\n[\"\u0430\u043b\", null],\n[\"\u0430\u0440\", null],\n[\"\u0434\u044b\", null],\n[\" \u0436\", null],\n[\"\u043e\", \"\u5e9f\u9664\"],\n[\"\u044e\", null],\n[\"\u0434\u044b\", null],\n[\" \", null],\n[\"\u049b\", \"\u652f\u6301\"],\n[\"\u043e\u043b\", null],\n[\"\u0434\", null],\n[\"\u0430\u0439\u0434\", null],\n[\"\u044b\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u04e8\u0437\u0456\u043d\u0456\u04a3 \u049a\u043e\u043b\u043c\u0430-\u049b\u043e\u043b \u0430\u049b\u0448\u0430 \u049b\u0430\u0440\u0493\u044b\u0441\u044b\u0430\u0442\u0442\u044b\u0436\u0430\u04a3\u0430 \u043a\u0456\u0442\u0430\u0431\u044b\u043d\u0434\u0430\u041a\u0435\u043d\u043d\u0435\u0442 \u0420\u043e\u0433\u043e\u0444\u0444 \u0441\u0430\u043b\u044b\u049b\u0442\u044b \u0442\u04e9\u043b\u0435\u0443\u0434\u0435\u043d \u0436\u0430\u043b\u0442\u0430\u0440\u0443 \u0436\u04d9\u043d\u0435 \u049b\u044b\u043b\u043c\u044b\u0441\u0442\u044b\u049b \u04d9\u0440\u0435\u043a\u0435\u0442\u043f\u0435\u043d \u043a\u04af\u0440\u0435\u0441\u0443 \u043c\u0430\u049b\u0441\u0430\u0442\u044b\u043d\u0434\u0430 \u0436\u043e\u0493\u0430\u0440\u044b \u0430\u0442\u0430\u0443\u043b\u044b \u0431\u0430\u043d\u043a\u043d\u043e\u0442\u0430\u043b\u0430\u0440\u0434\u044b \u0436\u043e\u044e\u0434\u044b \u049b\u043e\u043b\u0434\u0430\u0439\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u04e8', '\u0437', '\u0456', '\u043d', '\u0456', '\u04a3', ' ', '\u049a', '\u043e\u043b', '\u043c\u0430', '-', '\u049b', '\u043e\u043b', ' \u0430', '\u049b', '\u0448\u0430', ' ', '\u049b', '\u0430\u0440', '\u0493', '\u044b', '\u0441\u044b', '\u0430\u0442', '\u0442\u044b', '\u0436\u0430', '\u04a3', '\u0430', ' \u043a', '\u0456', '\u0442', '\u0430\u0431', '\u044b', '\u043d\u0434', '', '', '\u0435\u043d\u043d', '\u0435\u0442', ' \u0420', '\u043e\u0433\u043e', '\u0444\u0444', ' \u0441', '\u0430\u043b', '\u044b', '\u049b', '\u0442\u044b', ' \u0442', '\u04e9', '\u043b\u0435', '\u0443\u0434', '\u0435\u043d', ' \u0436', '\u0430\u043b', '\u0442\u0430\u0440', '\u0443', ' \u0436', '\u04d9', '\u043d\u0435', ' ', '\u049b', '\u044b', '\u043b', '\u043c\u044b', '\u0441\u0442\u044b', '\u049b', ' ', '\u04d9', '\u0440', '\u0435\u043a', '\u0435\u0442', '\u043f', '\u0435\u043d', ' \u043a', '\u04af', '\u0440\u0435\u0441', '\u0443', ' \u043c\u0430', '\u049b', '\u0441', '\u0430\u0442', '\u044b', '\u043d', '\u0434\u0430', ' \u0436', '\u043e', '\u0493', '\u0430\u0440', '\u044b', ' \u0430', '\u0442\u0430', '\u0443', '\u043b\u044b', ' \u0431\u0430\u043d', '\u043a', '\u043d', '\u043e\u0442', '\u0430\u043b', '\u0430\u0440', '\u0434\u044b', ' \u0436', '\u043e', '\u044e', '\u0434\u044b', ' ', '\u049b', '\u043e\u043b', '\u0434', '\u0430\u0439\u0434', '\u044b', '.']"}], "[\n[\"\u04e8\", \"\u0421\u0432\u043e\u0435\u0439\"],\n[\"\u0437\", null],\n[\"\u0456\", null],\n[\"\u043d\", null],\n[\"\u0456\", null],\n[\"\u04a3\", null],\n[\" \", null],\n[\"\u049a\", \"\u041d\u0430\u043b\u0438\u0447\u043d\u044b\u0435\"],\n[\"\u043e\u043b\", null],\n[\"\u043c\u0430\", null],\n[\"-\", null],\n[\"\u049b\", null],\n[\"\u043e\u043b\", null],\n[\" \u0430\", null],\n[\"\u049b\", null],\n[\"\u0448\u0430\", null],\n[\" \", null],\n[\"\u049b\", \"\u043f\u0440\u043e\u043a\u043b\u044f\u0442\u0438\u0435\"],\n[\"\u0430\u0440\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u0441\u044b\", null],\n[\"\u0430\u0442\", null],\n[\"\u0442\u044b\", null],\n[\"\u0436\u0430\", \"\u043d\u043e\u0432\u043e\u0439\"],\n[\"\u04a3\", null],\n[\"\u0430\", null],\n[\" \u043a\", \"\u043a\u043d\u0438\u0433\u0435\"],\n[\"\u0456\", null],\n[\"\u0442\", null],\n[\"\u0430\u0431\", null],\n[\"\u044b\", null],\n[\"\u043d\u0434\", null],\n[\"\", null],\n[\"\", null],\n[\"\u0435\u043d\u043d\", \"\u041a\u0435\u043d\u043d\u0435\u0442\"],\n[\"\u0435\u0442\", null],\n[\" \u0420\", \"\u0420\u043e\u0433\u043e\u0444\u0444\"],\n[\"\u043e\u0433\u043e\", null],\n[\"\u0444\u0444\", null],\n[\" \u0441\", \"\u043d\u0430\u043b\u043e\u0433\u043e\u0432\"],\n[\"\u0430\u043b\", null],\n[\"\u044b\", null],\n[\"\u049b\", null],\n[\"\u0442\u044b\", null],\n[\" \u0442\", \"\u0443\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u044f\"],\n[\"\u04e9\", null],\n[\"\u043b\u0435\", null],\n[\"\u0443\u0434\", null],\n[\"\u0435\u043d\", null],\n[\" \u0436\", \"\u0438\"],\n[\"\u0430\u043b\", null],\n[\"\u0442\u0430\u0440\", null],\n[\"\u0443\", null],\n[\" \u0436\", \"\u0431\u043e\u0440\u044c\u0431\u044b\"],\n[\"\u04d9\", null],\n[\"\u043d\u0435\", null],\n[\" \", null],\n[\"\u049b\", \"\u043f\u0440\u0435\u0441\u0442\u0443\u043f\u043d\u043e\u0439\"],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\"\u043c\u044b\", null],\n[\"\u0441\u0442\u044b\", null],\n[\"\u049b\", null],\n[\" \", null],\n[\"\u04d9\", \"\u0434\u0435\u044f\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c\u044e\"],\n[\"\u0440\", null],\n[\"\u0435\u043a\", null],\n[\"\u0435\u0442\", null],\n[\"\u043f\", null],\n[\"\u0435\u043d\", null],\n[\" \u043a\", \"\u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0435\u0442\"],\n[\"\u04af\", null],\n[\"\u0440\u0435\u0441\", null],\n[\"\u0443\", null],\n[\" \u043c\u0430\", \"\u0446\u0435\u043b\u044f\u0445\"],\n[\"\u049b\", null],\n[\"\u0441\", null],\n[\"\u0430\u0442\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u0434\u0430\", null],\n[\" \u0436\", \"\u0443\u043d\u0438\u0447\u0442\u043e\u0436\u0435\u043d\u0438\u0435\"],\n[\"\u043e\", null],\n[\"\u0493\", null],\n[\"\u0430\u0440\", null],\n[\"\u044b\", null],\n[\" \u0430\", \"\u043a\u0440\u0443\u043f\u043d\u044b\u0445\"],\n[\"\u0442\u0430\", null],\n[\"\u0443\", null],\n[\"\u043b\u044b\", null],\n[\" \u0431\u0430\u043d\", \"\u0431\u0430\u043d\u043a\u043d\u043e\u0442\"],\n[\"\u043a\", null],\n[\"\u043d\", null],\n[\"\u043e\u0442\", null],\n[\"\u0430\u043b\", null],\n[\"\u0430\u0440\", null],\n[\"\u0434\u044b\", null],\n[\" \u0436\", null],\n[\"\u043e\", null],\n[\"\u044e\", null],\n[\"\u0434\u044b\", null],\n[\" \", null],\n[\"\u049b\", null],\n[\"\u043e\u043b\", null],\n[\"\u0434\", null],\n[\"\u0430\u0439\u0434\", null],\n[\"\u044b\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u041e\u0434\u0430\u043d \u0431\u0430\u0441\u049b\u0430 \u0435\u0448\u049b\u0430\u043d\u0434\u0430\u0439 \u0432\u0430\u043b\u044e\u0442\u0430 \u0435\u043d \u0431\u0430\u0439\u043b\u044b\u049b\u0442\u044b\u04a3 \u049b\u0430\u0443\u0456\u043f\u0441\u0456\u0437\u0434\u0456\u0433\u0456 \u043c\u0435\u043d \u04e9\u0442\u0456\u043c\u0434\u0456\u043b\u0456\u0433\u0456\u043d \u043e\u043d\u0434\u0430\u0439 \u0434\u0435\u04a3\u0433\u0435\u0439\u0434\u0435 \u049b\u0430\u043c\u0442\u0430\u043c\u0430\u0441\u044b\u0437 \u0435\u0442\u0435 \u0430\u043b\u0493\u0430\u043d \u0435\u043c\u0435\u0441.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u041e', '\u0434\u0430\u043d', ' \u0431', '\u0430\u0441', '\u049b', '\u0430', ' \u0435', '\u0448', '\u049b', '\u0430\u043d\u0434', '\u0430\u0439', ' \u0432\u0430\u043b', '\u044e', '\u0442\u0430', ' \u0435', '\u043d', ' \u0431', '\u0430\u0439', '\u043b\u044b', '\u049b', '\u0442\u044b', '\u04a3', ' ', '\u049b', '\u0430', '\u0443', '\u0456', '\u043f', '\u0441', '\u0456', '\u0437\u0434', '\u0456', '\u0433', '\u0456', ' \u043c\u0435\u043d', ' ', '\u04e9', '\u0442', '\u0456', '\u043c', '\u0434', '\u0456', '\u043b', '\u0456', '\u0433', '\u0456', '\u043d', ' \u043e\u043d', '\u0434', '\u0430\u0439', ' \u0434\u0435', '\u04a3', '\u0433', '\u0435\u0439', '\u0434\u0435', ' ', '\u049b', '\u0430\u043c', '\u0442', '\u0430\u043c', '\u0430\u0441', '\u044b', '\u0437', ' \u0435', '\u0442\u0435', ' \u0430\u043b', '\u0493', '\u0430\u043d', ' \u0435', '\u043c\u0435\u0441', '.']"}], "[\n[\"\u041e\", null],\n[\"\u0434\u0430\u043d\", null],\n[\" \u0431\", null],\n[\"\u0430\u0441\", null],\n[\"\u049b\", null],\n[\"\u0430\", null],\n[\" \u0435\", null],\n[\"\u0448\", null],\n[\"\u049b\", null],\n[\"\u0430\u043d\u0434\", null],\n[\"\u0430\u0439\", null],\n[\" \u0432\u0430\u043b\", null],\n[\"\u044e\", null],\n[\"\u0442\u0430\", \"currency\"],\n[\" \u0435\", null],\n[\"\u043d\", null],\n[\" \u0431\", null],\n[\"\u0430\u0439\", null],\n[\"\u043b\u044b\", null],\n[\"\u049b\", null],\n[\"\u0442\u044b\", null],\n[\"\u04a3\", null],\n[\" \", null],\n[\"\u049b\", null],\n[\"\u0430\", null],\n[\"\u0443\", null],\n[\"\u0456\", null],\n[\"\u043f\", null],\n[\"\u0441\", null],\n[\"\u0456\", null],\n[\"\u0437\u0434\", null],\n[\"\u0456\", null],\n[\"\u0433\", null],\n[\"\u0456\", \"security\"],\n[\" \u043c\u0435\u043d\", null],\n[\" \", null],\n[\"\u04e9\", null],\n[\"\u0442\", null],\n[\"\u0456\", null],\n[\"\u043c\", null],\n[\"\u0434\", null],\n[\"\u0456\", null],\n[\"\u043b\", null],\n[\"\u0456\", null],\n[\"\u0433\", null],\n[\"\u0456\", null],\n[\"\u043d\", \"liquidity\"],\n[\" \u043e\u043d\", null],\n[\"\u0434\", null],\n[\"\u0430\u0439\", null],\n[\" \u0434\u0435\", null],\n[\"\u04a3\", null],\n[\"\u0433\", null],\n[\"\u0435\u0439\", null],\n[\"\u0434\u0435\", \"level\"],\n[\" \", null],\n[\"\u049b\", null],\n[\"\u0430\u043c\", null],\n[\"\u0442\", null],\n[\"\u0430\u043c\", null],\n[\"\u0430\u0441\", null],\n[\"\u044b\", null],\n[\"\u0437\", null],\n[\" \u0435\", null],\n[\"\u0442\u0435\", null],\n[\" \u0430\u043b\", null],\n[\"\u0493\", null],\n[\"\u0430\u043d\", null],\n[\" \u0435\", null],\n[\"\u043c\u0435\u0441\", \"not\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u041e\u0434\u0430\u043d \u0431\u0430\u0441\u049b\u0430 \u0435\u0448\u049b\u0430\u043d\u0434\u0430\u0439 \u0432\u0430\u043b\u044e\u0442\u0430 \u0435\u043d \u0431\u0430\u0439\u043b\u044b\u049b\u0442\u044b\u04a3 \u049b\u0430\u0443\u0456\u043f\u0441\u0456\u0437\u0434\u0456\u0433\u0456 \u043c\u0435\u043d \u04e9\u0442\u0456\u043c\u0434\u0456\u043b\u0456\u0433\u0456\u043d \u043e\u043d\u0434\u0430\u0439 \u0434\u0435\u04a3\u0433\u0435\u0439\u0434\u0435 \u049b\u0430\u043c\u0442\u0430\u043c\u0430\u0441\u044b\u0437 \u0435\u0442\u0435 \u0430\u043b\u0493\u0430\u043d \u0435\u043c\u0435\u0441.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u041e', '\u0434\u0430\u043d', ' \u0431', '\u0430\u0441', '\u049b', '\u0430', ' \u0435', '\u0448', '\u049b', '\u0430\u043d\u0434', '\u0430\u0439', ' \u0432\u0430\u043b', '\u044e', '\u0442\u0430', ' \u0435', '\u043d', ' \u0431', '\u0430\u0439', '\u043b\u044b', '\u049b', '\u0442\u044b', '\u04a3', ' ', '\u049b', '\u0430', '\u0443', '\u0456', '\u043f', '\u0441', '\u0456', '\u0437\u0434', '\u0456', '\u0433', '\u0456', ' \u043c\u0435\u043d', ' ', '\u04e9', '\u0442', '\u0456', '\u043c', '\u0434', '\u0456', '\u043b', '\u0456', '\u0433', '\u0456', '\u043d', ' \u043e\u043d', '\u0434', '\u0430\u0439', ' \u0434\u0435', '\u04a3', '\u0433', '\u0435\u0439', '\u0434\u0435', ' ', '\u049b', '\u0430\u043c', '\u0442', '\u0430\u043c', '\u0430\u0441', '\u044b', '\u0437', ' \u0435', '\u0442\u0435', ' \u0430\u043b', '\u0493', '\u0430\u043d', ' \u0435', '\u043c\u0435\u0441', '.']"}], "[\n[\"\u041e\", \"\u9664\"],\n[\"\u0434\u0430\u043d\", null],\n[\" \u0431\", null],\n[\"\u0430\u0441\", null],\n[\"\u049b\", null],\n[\"\u0430\", null],\n[\" \u0435\", null],\n[\"\u0448\", null],\n[\"\u049b\", null],\n[\"\u0430\u043d\u0434\", null],\n[\"\u0430\u0439\", null],\n[\" \u0432\u0430\u043b\", null],\n[\"\u044e\", null],\n[\"\u0442\u0430\", null],\n[\" \u0435\", null],\n[\"\u043d\", null],\n[\" \u0431\", null],\n[\"\u0430\u0439\", null],\n[\"\u043b\u044b\", null],\n[\"\u049b\", null],\n[\"\u0442\u044b\", null],\n[\"\u04a3\", null],\n[\" \", null],\n[\"\u049b\", null],\n[\"\u0430\", null],\n[\"\u0443\", null],\n[\"\u0456\", null],\n[\"\u043f\", null],\n[\"\u0441\", null],\n[\"\u0456\", null],\n[\"\u0437\u0434\", null],\n[\"\u0456\", null],\n[\"\u0433\", null],\n[\"\u0456\", null],\n[\" \u043c\u0435\u043d\", null],\n[\" \", null],\n[\"\u04e9\", \"\u6d41\"],\n[\"\u0442\", null],\n[\"\u0456\", null],\n[\"\u043c\", null],\n[\"\u0434\", null],\n[\"\u0456\", null],\n[\"\u043b\", null],\n[\"\u0456\", null],\n[\"\u0433\", null],\n[\"\u0456\", null],\n[\"\u043d\", null],\n[\" \u043e\u043d\", null],\n[\"\u0434\", null],\n[\"\u0430\u0439\", null],\n[\" \u0434\u0435\", null],\n[\"\u04a3\", null],\n[\"\u0433\", null],\n[\"\u0435\u0439\", null],\n[\"\u0434\u0435\", null],\n[\" \", null],\n[\"\u049b\", \"\u4fdd\"],\n[\"\u0430\u043c\", null],\n[\"\u0442\", null],\n[\"\u0430\u043c\", null],\n[\"\u0430\u0441\", null],\n[\"\u044b\", null],\n[\"\u0437\", null],\n[\" \u0435\", null],\n[\"\u0442\u0435\", null],\n[\" \u0430\u043b\", null],\n[\"\u0493\", null],\n[\"\u0430\u043d\", null],\n[\" \u0435\", null],\n[\"\u043c\u0435\u0441\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u041e\u0434\u0430\u043d \u0431\u0430\u0441\u049b\u0430 \u0435\u0448\u049b\u0430\u043d\u0434\u0430\u0439 \u0432\u0430\u043b\u044e\u0442\u0430 \u0435\u043d \u0431\u0430\u0439\u043b\u044b\u049b\u0442\u044b\u04a3 \u049b\u0430\u0443\u0456\u043f\u0441\u0456\u0437\u0434\u0456\u0433\u0456 \u043c\u0435\u043d \u04e9\u0442\u0456\u043c\u0434\u0456\u043b\u0456\u0433\u0456\u043d \u043e\u043d\u0434\u0430\u0439 \u0434\u0435\u04a3\u0433\u0435\u0439\u0434\u0435 \u049b\u0430\u043c\u0442\u0430\u043c\u0430\u0441\u044b\u0437 \u0435\u0442\u0435 \u0430\u043b\u0493\u0430\u043d \u0435\u043c\u0435\u0441.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u041e', '\u0434\u0430\u043d', ' \u0431', '\u0430\u0441', '\u049b', '\u0430', ' \u0435', '\u0448', '\u049b', '\u0430\u043d\u0434', '\u0430\u0439', ' \u0432\u0430\u043b', '\u044e', '\u0442\u0430', ' \u0435', '\u043d', ' \u0431', '\u0430\u0439', '\u043b\u044b', '\u049b', '\u0442\u044b', '\u04a3', ' ', '\u049b', '\u0430', '\u0443', '\u0456', '\u043f', '\u0441', '\u0456', '\u0437\u0434', '\u0456', '\u0433', '\u0456', ' \u043c\u0435\u043d', ' ', '\u04e9', '\u0442', '\u0456', '\u043c', '\u0434', '\u0456', '\u043b', '\u0456', '\u0433', '\u0456', '\u043d', ' \u043e\u043d', '\u0434', '\u0430\u0439', ' \u0434\u0435', '\u04a3', '\u0433', '\u0435\u0439', '\u0434\u0435', ' ', '\u049b', '\u0430\u043c', '\u0442', '\u0430\u043c', '\u0430\u0441', '\u044b', '\u0437', ' \u0435', '\u0442\u0435', ' \u0430\u043b', '\u0493', '\u0430\u043d', ' \u0435', '\u043c\u0435\u0441', '.']"}], "[\n[\"\u041e\", null],\n[\"\u0434\u0430\u043d\", null],\n[\" \", null],\n[\"\u0431\", null],\n[\"\u0430\u0441\", null],\n[\"\u049b\", null],\n[\"\u0430\", null],\n[\" \", null],\n[\"\u0435\", null],\n[\"\u0448\", null],\n[\"\u049b\", null],\n[\"\u0430\u043d\u0434\", null],\n[\"\u0430\u0439\", null],\n[\" \", null],\n[\"\u0432\u0430\u043b\", \"\u0432\u0430\u043b\u044e\u0442\u0430\"],\n[\"\u044e\", null],\n[\"\u0442\u0430\", null],\n[\" \", null],\n[\"\u0435\", null],\n[\"\u043d\", null],\n[\" \", null],\n[\"\u0431\", \"\u0431\u043e\u0433\u0430\u0442\u0441\u0442\u0432\u043e\"],\n[\"\u0430\u0439\", null],\n[\"\u043b\u044b\", null],\n[\"\u049b\", null],\n[\"\u0442\u044b\", null],\n[\"\u04a3\", null],\n[\" \", null],\n[\"\u049b\", \"\u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u044c\"],\n[\"\u0430\", null],\n[\"\u0443\", null],\n[\"\u0456\", null],\n[\"\u043f\", null],\n[\"\u0441\", null],\n[\"\u0456\", null],\n[\"\u0437\u0434\", null],\n[\"\u0456\", null],\n[\"\u0433\", null],\n[\"\u0456\", null],\n[\" \u043c\u0435\u043d\", null],\n[\" \", null],\n[\"\u04e9\", \"\u043b\u0438\u043a\u0432\u0438\u0434\u043d\u043e\u0441\u0442\u044c\"],\n[\"\u0442\", null],\n[\"\u0456\", null],\n[\"\u043c\", null],\n[\"\u0434\", null],\n[\"\u0456\", null],\n[\"\u043b\", null],\n[\"\u0456\", null],\n[\"\u0433\", null],\n[\"\u0456\", null],\n[\"\u043d\", null],\n[\" \", null],\n[\"\u043e\u043d\", \"\u0442\u0430\u043a\u043e\u0439\"],\n[\"\u0434\", null],\n[\"\u0430\u0439\", null],\n[\" \", null],\n[\"\u0434\u0435\", \"\u0443\u0440\u043e\u0432\u0435\u043d\u044c\"],\n[\"\u04a3\", null],\n[\"\u0433\", null],\n[\"\u0435\u0439\", null],\n[\"\u0434\u0435\", null],\n[\" \", null],\n[\"\u049b\", \"\u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u0442\u044c\"],\n[\"\u0430\u043c\", null],\n[\"\u0442\", null],\n[\"\u0430\u043c\", null],\n[\"\u0430\u0441\", null],\n[\"\u044b\", null],\n[\"\u0437\", null],\n[\" \", null],\n[\"\u0435\", null],\n[\"\u0442\u0435\", null],\n[\" \", null],\n[\"\u0430\u043b\", \"\u043c\u043e\u0447\u044c\"],\n[\"\u0493\", null],\n[\"\u0430\u043d\", null],\n[\" \", null],\n[\"\u0435\", null],\n[\"\u043c\u0435\u0441\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u0422\u0435\u043a \u0430\u0437\u0430\u043c\u0430\u0442\u0442\u044b\u049b \u049b\u043e\u0493\u0430\u043c\u043d\u044b\u04a3 \u0493\u0430\u043d\u0430 \u0435\u043c\u0435\u0441, \u0431\u04af\u043a\u0456\u043b \u04e9\u0440\u043a\u0435\u043d\u0438\u0435\u0442\u0442\u0456\u04a3 \u0442\u0430\u0493\u0434\u044b\u0440\u044b \u049b\u044b\u043b \u04af\u0441\u0442\u0456\u043d\u0434\u0435 \u0442\u04b1\u0440.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0422', '\u0435\u043a', ' \u0430', '\u0437\u0430\u043c', '\u0430\u0442', '\u0442\u044b', '\u049b', ' ', '\u049b', '\u043e', '\u0493', '\u0430\u043c', '\u043d\u044b', '\u04a3', ' ', '\u0493', '\u0430\u043d\u0430', ' \u0435', '\u043c\u0435\u0441', ',', ' \u0431', '\u04af', '\u043a', '\u0456', '\u043b', ' ', '\u04e9', '\u0440', '\u043a', '\u0435\u043d\u0438', '\u0435\u0442', '\u0442', '\u0456', '\u04a3', ' \u0442', '\u0430', '\u0493', '\u0434\u044b', '\u0440\u044b', ' ', '\u049b', '\u044b', '\u043b', ' ', '\u04af', '\u0441\u0442', '\u0456', '\u043d', '\u0434\u0435', ' \u0442', '\u04b1', '\u0440', '.']"}], "[\n[\"\u0422\", \"Only\"],\n[\"\u0435\u043a\", null],\n[\" \", null],\n[\"\u0437\u0430\u043c\", \"civil\"],\n[\"\u0430\u0442\", null],\n[\"\u0442\u044b\", null],\n[\"\u049b\", null],\n[\" \", null],\n[\"\u049b\", \"society\"],\n[\"\u043e\", null],\n[\"\u0493\", null],\n[\"\u0430\u043c\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", null],\n[\" \", null],\n[\"\u0493\", \"only\"],\n[\"\u0430\u043d\u0430\", null],\n[\" \u0435\", \"not\"],\n[\"\u043c\u0435\u0441\", null],\n[\",\", null],\n[\" \u0431\", \"entire\"],\n[\"\u04af\", null],\n[\"\u043a\", null],\n[\"\u0456\", null],\n[\"\u043b\", null],\n[\" \", null],\n[\"\u04e9\", \"civilization\"],\n[\"\u0440\", null],\n[\"\u043a\", null],\n[\"\u0435\u043d\u0438\", null],\n[\"\u0435\u0442\", null],\n[\"\u0442\", null],\n[\"\u0456\", null],\n[\"\u04a3\", null],\n[\" \u0442\", \"fate\"],\n[\"\u0430\", null],\n[\"\u0493\", null],\n[\"\u0434\u044b\", null],\n[\"\u0440\u044b\", null],\n[\" \", null],\n[\"\u049b\", \"hangs\"],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\" \", null],\n[\"\u04af\", \"by\"],\n[\"\u0441\u0442\", null],\n[\"\u0456\", null],\n[\"\u043d\", null],\n[\"\u0434\u0435\", null],\n[\" \u0442\", \"thread\"],\n[\"\u04b1\", null],\n[\"\u0440\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u0422\u0435\u043a \u0430\u0437\u0430\u043c\u0430\u0442\u0442\u044b\u049b \u049b\u043e\u0493\u0430\u043c\u043d\u044b\u04a3 \u0493\u0430\u043d\u0430 \u0435\u043c\u0435\u0441, \u0431\u04af\u043a\u0456\u043b \u04e9\u0440\u043a\u0435\u043d\u0438\u0435\u0442\u0442\u0456\u04a3 \u0442\u0430\u0493\u0434\u044b\u0440\u044b \u049b\u044b\u043b \u04af\u0441\u0442\u0456\u043d\u0434\u0435 \u0442\u04b1\u0440.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0422', '\u0435\u043a', ' \u0430', '\u0437\u0430\u043c', '\u0430\u0442', '\u0442\u044b', '\u049b', ' ', '\u049b', '\u043e', '\u0493', '\u0430\u043c', '\u043d\u044b', '\u04a3', ' ', '\u0493', '\u0430\u043d\u0430', ' \u0435', '\u043c\u0435\u0441', ',', ' \u0431', '\u04af', '\u043a', '\u0456', '\u043b', ' ', '\u04e9', '\u0440', '\u043a', '\u0435\u043d\u0438', '\u0435\u0442', '\u0442', '\u0456', '\u04a3', ' \u0442', '\u0430', '\u0493', '\u0434\u044b', '\u0440\u044b', ' ', '\u049b', '\u044b', '\u043b', ' ', '\u04af', '\u0441\u0442', '\u0456', '\u043d', '\u0434\u0435', ' \u0442', '\u04b1', '\u0440', '.']"}], "[\n[\"\u0422\", \"\u4e0d\u4ec5\"],\n[\"\u0435\u043a\", null],\n[\" \", null],\n[\"\u0437\u0430\u043c\", \"\u516c\u6c11\"],\n[\"\u0430\u0442\", null],\n[\"\u0442\u044b\", null],\n[\"\u049b\", null],\n[\" \", null],\n[\"\u049b\", \"\u793e\u4f1a\"],\n[\"\u043e\", null],\n[\"\u0493\", null],\n[\"\u0430\u043c\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", null],\n[\" \", null],\n[\"\u0493\", \"\u800c\u5df2\"],\n[\"\u0430\u043d\u0430\", null],\n[\" \u0435\", \"\u4e0d\u662f\"],\n[\"\u043c\u0435\u0441\", null],\n[\",\", null],\n[\" \u0431\", \"\u6574\u4e2a\"],\n[\"\u04af\", null],\n[\"\u043a\", null],\n[\"\u0456\", null],\n[\"\u043b\", null],\n[\" \", null],\n[\"\u04e9\", \"\u6587\u660e\"],\n[\"\u0440\", null],\n[\"\u043a\", null],\n[\"\u0435\u043d\u0438\", null],\n[\"\u0435\u0442\", null],\n[\"\u0442\", null],\n[\"\u0456\", null],\n[\"\u04a3\", null],\n[\" \u0442\", \"\u547d\u8fd0\"],\n[\"\u0430\", null],\n[\"\u0493\", null],\n[\"\u0434\u044b\", null],\n[\"\u0440\u044b\", null],\n[\" \", null],\n[\"\u049b\", \"\u6b63\"],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\" \", null],\n[\"\u04af\", \"\u5904\u4e8e\"],\n[\"\u0441\u0442\", null],\n[\"\u0456\", null],\n[\"\u043d\", null],\n[\"\u0434\u0435\", null],\n[\" \u0442\", \"\u5371\u9669\"],\n[\"\u04b1\", null],\n[\"\u0440\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u0422\u0435\u043a \u0430\u0437\u0430\u043c\u0430\u0442\u0442\u044b\u049b \u049b\u043e\u0493\u0430\u043c\u043d\u044b\u04a3 \u0493\u0430\u043d\u0430 \u0435\u043c\u0435\u0441, \u0431\u04af\u043a\u0456\u043b \u04e9\u0440\u043a\u0435\u043d\u0438\u0435\u0442\u0442\u0456\u04a3 \u0442\u0430\u0493\u0434\u044b\u0440\u044b \u049b\u044b\u043b \u04af\u0441\u0442\u0456\u043d\u0434\u0435 \u0442\u04b1\u0440.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0422', '\u0435\u043a', ' \u0430', '\u0437\u0430\u043c', '\u0430\u0442', '\u0442\u044b', '\u049b', ' ', '\u049b', '\u043e', '\u0493', '\u0430\u043c', '\u043d\u044b', '\u04a3', ' ', '\u0493', '\u0430\u043d\u0430', ' \u0435', '\u043c\u0435\u0441', ',', ' \u0431', '\u04af', '\u043a', '\u0456', '\u043b', ' ', '\u04e9', '\u0440', '\u043a', '\u0435\u043d\u0438', '\u0435\u0442', '\u0442', '\u0456', '\u04a3', ' \u0442', '\u0430', '\u0493', '\u0434\u044b', '\u0440\u044b', ' ', '\u049b', '\u044b', '\u043b', ' ', '\u04af', '\u0441\u0442', '\u0456', '\u043d', '\u0434\u0435', ' \u0442', '\u04b1', '\u0440', '.']"}], "[\n[\"\u0422\", \"\u0441\u0443\u0434\u044c\u0431\u0430\"],\n[\"\u0435\u043a\", null],\n[\" \", null],\n[\"\u0437\u0430\u043c\", null],\n[\"\u0430\u0442\", null],\n[\"\u0442\u044b\", null],\n[\"\u049b\", null],\n[\" \", null],\n[\"\u049b\", \"\u043e\u0431\u0449\u0435\u0441\u0442\u0432\u043e\"],\n[\"\u043e\", null],\n[\"\u0493\", null],\n[\"\u0430\u043c\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", null],\n[\" \", null],\n[\"\u0493\", \"\u0442\u043e\u043b\u044c\u043a\u043e\"],\n[\"\u0430\u043d\u0430\", null],\n[\" \u0435\", null],\n[\"\u043c\u0435\u0441\", null],\n[\",\", null],\n[\" \u0431\", \"\u0432\u0435\u0441\u044c\"],\n[\"\u04af\", null],\n[\"\u043a\", null],\n[\"\u0456\", null],\n[\"\u043b\", null],\n[\" \", null],\n[\"\u04e9\", \"\u0446\u0438\u0432\u0438\u043b\u0438\u0437\u0430\u0446\u0438\u044f\"],\n[\"\u0440\", null],\n[\"\u043a\", null],\n[\"\u0435\u043d\u0438\", null],\n[\"\u0435\u0442\", null],\n[\"\u0442\", null],\n[\"\u0456\", null],\n[\"\u04a3\", null],\n[\" \u0442\", \"\u0441\u0443\u0434\u044c\u0431\u0430\"],\n[\"\u0430\", null],\n[\"\u0493\", null],\n[\"\u0434\u044b\", null],\n[\"\u0440\u044b\", null],\n[\" \", null],\n[\"\u049b\", \"\u0432\u043e\u043b\u043e\u0441\u043e\u043a\"],\n[\"\u044b\", null],\n[\"\u043b\", null],\n[\" \", null],\n[\"\u04af\", \"\u043d\u0430\"],\n[\"\u0441\u0442\", null],\n[\"\u0456\", null],\n[\"\u043d\", null],\n[\"\u0434\u0435\", null],\n[\" \u0442\", \"\u0432\u0438\u0441\u0435\u0442\u044c\"],\n[\"\u04b1\", null],\n[\"\u0440\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u0411\u0430\u0441\u049b\u0430 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u0442\u0430\u0440 \u0430\u0439\u044b\u0440\u043c\u0430\u0448\u044b\u043b\u044b\u0493\u044b \u2013 \u041c\u0430\u043a\u0440\u043e\u043d \u0415\u0443\u0440\u043e\u043f\u0430 \u043e\u0434\u0430\u0493\u044b\u043d \u0442\u04af\u0431\u0456\u0440\u0456\u043c\u0435\u043d \u04e9\u0437\u0433\u0435\u0440\u0442\u0456\u043f \u049b\u0430\u043d\u0430 \u049b\u043e\u0439\u043c\u0430\u0439, \u0431\u04af\u043a\u0456\u043b \u0415\u0443\u0440\u043e\u043f\u0430\u0434\u0430 \u044b\u043d\u0442\u044b\u043c\u0430\u049b\u0442\u0430\u0441\u0442\u044b\u049b\u0442\u044b \u0430\u0440\u0442\u0442\u044b\u0440\u0443 \u043a\u0435\u0440\u0435\u043a \u0434\u0435\u043f \u0441\u0430\u043d\u0430\u0439\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0411', '\u0430\u0441', '\u049b', '\u0430', ' \u043a', '\u0430\u043d\u0434', '\u0438\u0434', '\u0430\u0442', '\u0442\u0430\u0440', ' \u0430', '\u0439', '\u044b', '\u0440', '\u043c', '\u0430\u0448', '\u044b', '\u043b\u044b', '\u0493', '\u044b', ' \u2013', ' \u041c\u0430\u043a', '\u0440\u043e\u043d', ' \u0415', '\u0443\u0440', '\u043e\u043f', '\u0430', ' \u043e', '\u0434\u0430', '\u0493', '\u044b', '\u043d', ' \u0442', '\u04af', '\u0431', '\u0456', '\u0440', '\u0456', '\u043c\u0435\u043d', ' ', '\u04e9', '\u0437', '\u0433', '\u0435\u0440\u0442', '\u0456', '\u043f', ' ', '\u049b', '\u0430\u043d\u0430', ' ', '\u049b', '\u043e\u0439', '\u043c', '\u0430\u0439', ',', ' \u0431', '\u04af', '\u043a', '\u0456', '\u043b', ' \u0415', '\u0443\u0440', '\u043e\u043f', '\u0430\u0434', '\u0430', ' ', '\u044b', '\u043d', '\u0442\u044b', '\u043c\u0430', '\u049b', '\u0442\u0430', '\u0441\u0442\u044b', '\u049b', '\u0442\u044b', ' \u0430', '\u0440', '\u0442', '\u0442\u044b', '\u0440\u0443', ' \u043a', '\u0435\u0440', '\u0435\u043a', ' \u0434\u0435\u043f', ' \u0441', '\u0430\u043d', '\u0430\u0439\u0434', '\u044b', '.']"}], "[\n[\"\u0411\", null],\n[\"\u0430\u0441\", null],\n[\"\u049b\", null],\n[\"\u0430\", null],\n[\" \", null],\n[\"\u043a\", null],\n[\"\u0430\u043d\u0434\", null],\n[\"\u0438\u0434\", null],\n[\"\u0430\u0442\", null],\n[\"\u0442\u0430\u0440\", \"candidates\"],\n[\" \", null],\n[\"\u0430\", null],\n[\"\u0439\", null],\n[\"\u044b\", null],\n[\"\u0440\", null],\n[\"\u043c\", null],\n[\"\u0430\u0448\", null],\n[\"\u044b\", null],\n[\"\u043b\u044b\", null],\n[\"\u0493\", null],\n[\"\u044b\", \"difference\"],\n[\" \", null],\n[\"\u2013\", null],\n[\" \", null],\n[\"\u041c\u0430\u043a\", null],\n[\"\u0440\u043e\u043d\", \"Macron\"],\n[\" \", null],\n[\"\u0415\", null],\n[\"\u0443\u0440\", null],\n[\"\u043e\u043f\", null],\n[\"\u0430\", \"European\"],\n[\" \", null],\n[\"\u043e\", null],\n[\"\u0434\u0430\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u043d\", \"Union\"],\n[\" \", null],\n[\"\u0442\", null],\n[\"\u04af\", null],\n[\"\u0431\", null],\n[\"\u0456\", null],\n[\"\u0440\", null],\n[\"\u0456\", null],\n[\"\u043c\u0435\u043d\", \"radically\"],\n[\" \", null],\n[\"\u04e9\", null],\n[\"\u0437\", null],\n[\"\u0433\", null],\n[\"\u0435\u0440\u0442\", null],\n[\"\u0456\", null],\n[\"\u043f\", \"change\"],\n[\" \", null],\n[\"\u049b\", null],\n[\"\u0430\u043d\u0430\", \"only\"],\n[\" \", null],\n[\"\u049b\", null],\n[\"\u043e\u0439\", null],\n[\"\u043c\", null],\n[\"\u0430\u0439\", \"not\"],\n[\",\", null],\n[\" \", null],\n[\"\u0431\", null],\n[\"\u04af\", null],\n[\"\u043a\", null],\n[\"\u0456\", null],\n[\"\u043b\", \"entire\"],\n[\" \", null],\n[\"\u0415\", null],\n[\"\u0443\u0440\", null],\n[\"\u043e\u043f\", null],\n[\"\u0430\u0434\", null],\n[\"\u0430\", \"Europe\"],\n[\" \", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\"\u0442\u044b\", null],\n[\"\u043c\u0430\", null],\n[\"\u049b\", null],\n[\"\u0442\u0430\", null],\n[\"\u0441\u0442\u044b\", null],\n[\"\u049b\", null],\n[\"\u0442\u044b\", \"cooperation\"],\n[\" \", null],\n[\"\u0430\", null],\n[\"\u0440\", null],\n[\"\u0442\", null],\n[\"\u0442\u044b\", null],\n[\"\u0440\u0443\", \"increase\"],\n[\" \", null],\n[\"\u043a\", null],\n[\"\u0435\u0440\", null],\n[\"\u0435\u043a\", \"necessary\"],\n[\" \", null],\n[\"\u0434\u0435\u043f\", \"thinks\"],\n[\" \", null],\n[\"\u0441\", null],\n[\"\u0430\u043d\", null],\n[\"\u0430\u0439\u0434\", null],\n[\"\u044b\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u0411\u0430\u0441\u049b\u0430 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u0442\u0430\u0440 \u0430\u0439\u044b\u0440\u043c\u0430\u0448\u044b\u043b\u044b\u0493\u044b \u2013 \u041c\u0430\u043a\u0440\u043e\u043d \u0415\u0443\u0440\u043e\u043f\u0430 \u043e\u0434\u0430\u0493\u044b\u043d \u0442\u04af\u0431\u0456\u0440\u0456\u043c\u0435\u043d \u04e9\u0437\u0433\u0435\u0440\u0442\u0456\u043f \u049b\u0430\u043d\u0430 \u049b\u043e\u0439\u043c\u0430\u0439, \u0431\u04af\u043a\u0456\u043b \u0415\u0443\u0440\u043e\u043f\u0430\u0434\u0430 \u044b\u043d\u0442\u044b\u043c\u0430\u049b\u0442\u0430\u0441\u0442\u044b\u049b\u0442\u044b \u0430\u0440\u0442\u0442\u044b\u0440\u0443 \u043a\u0435\u0440\u0435\u043a \u0434\u0435\u043f \u0441\u0430\u043d\u0430\u0439\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0411', '\u0430\u0441', '\u049b', '\u0430', ' \u043a', '\u0430\u043d\u0434', '\u0438\u0434', '\u0430\u0442', '\u0442\u0430\u0440', ' \u0430', '\u0439', '\u044b', '\u0440', '\u043c', '\u0430\u0448', '\u044b', '\u043b\u044b', '\u0493', '\u044b', ' \u2013', ' \u041c\u0430\u043a', '\u0440\u043e\u043d', ' \u0415', '\u0443\u0440', '\u043e\u043f', '\u0430', ' \u043e', '\u0434\u0430', '\u0493', '\u044b', '\u043d', ' \u0442', '\u04af', '\u0431', '\u0456', '\u0440', '\u0456', '\u043c\u0435\u043d', ' ', '\u04e9', '\u0437', '\u0433', '\u0435\u0440\u0442', '\u0456', '\u043f', ' ', '\u049b', '\u0430\u043d\u0430', ' ', '\u049b', '\u043e\u0439', '\u043c', '\u0430\u0439', ',', ' \u0431', '\u04af', '\u043a', '\u0456', '\u043b', ' \u0415', '\u0443\u0440', '\u043e\u043f', '\u0430\u0434', '\u0430', ' ', '\u044b', '\u043d', '\u0442\u044b', '\u043c\u0430', '\u049b', '\u0442\u0430', '\u0441\u0442\u044b', '\u049b', '\u0442\u044b', ' \u0430', '\u0440', '\u0442', '\u0442\u044b', '\u0440\u0443', ' \u043a', '\u0435\u0440', '\u0435\u043a', ' \u0434\u0435\u043f', ' \u0441', '\u0430\u043d', '\u0430\u0439\u0434', '\u044b', '.']"}], "[\n[\"\u0411\", \"\u5176\u4ed6\"],\n[\"\u0430\u0441\", null],\n[\"\u049b\", null],\n[\"\u0430\", null],\n[\" \u043a\", null],\n[\"\u0430\u043d\u0434\", null],\n[\"\u0438\u0434\", null],\n[\"\u0430\u0442\", null],\n[\"\u0442\u0430\u0440\", null],\n[\" \u0430\", null],\n[\"\u0439\", null],\n[\"\u044b\", null],\n[\"\u0440\", null],\n[\"\u043c\", null],\n[\"\u0430\u0448\", null],\n[\"\u044b\", null],\n[\"\u043b\u044b\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\" \u2013\", null],\n[\" \u041c\u0430\u043a\", \"\u9a6c\u514b\u9f99\"],\n[\"\u0440\u043e\u043d\", null],\n[\" \u0415\", null],\n[\"\u0443\u0440\", null],\n[\"\u043e\u043f\", null],\n[\"\u0430\", null],\n[\" \u043e\", null],\n[\"\u0434\u0430\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\" \u0442\", null],\n[\"\u04af\", null],\n[\"\u0431\", null],\n[\"\u0456\", null],\n[\"\u0440\", null],\n[\"\u0456\", null],\n[\"\u043c\u0435\u043d\", null],\n[\" \", null],\n[\"\u04e9\", \"\u6539\u53d8\"],\n[\"\u0437\", null],\n[\"\u0433\", null],\n[\"\u0435\u0440\u0442\", null],\n[\"\u0456\", null],\n[\"\u043f\", null],\n[\" \", null],\n[\"\u049b\", null],\n[\"\u0430\u043d\u0430\", null],\n[\" \", null],\n[\"\u049b\", null],\n[\"\u043e\u0439\", null],\n[\"\u043c\", null],\n[\"\u0430\u0439\", null],\n[\",\", null],\n[\" \u0431\", \"\u5168\"],\n[\"\u04af\", null],\n[\"\u043a\", null],\n[\"\u0456\", null],\n[\"\u043b\", null],\n[\" \u0415\", \"\u6b27\u6d32\"],\n[\"\u0443\u0440\", null],\n[\"\u043e\u043f\", null],\n[\"\u0430\u0434\", null],\n[\"\u0430\", null],\n[\" \", null],\n[\"\u044b\", \"\u5408\u4f5c\"],\n[\"\u043d\", null],\n[\"\u0442\u044b\", null],\n[\"\u043c\u0430\", null],\n[\"\u049b\", null],\n[\"\u0442\u0430\", null],\n[\"\u0441\u0442\u044b\", null],\n[\"\u049b\", null],\n[\"\u0442\u044b\", null],\n[\" \u0430\", \"\u589e\u52a0\"],\n[\"\u0440\", null],\n[\"\u0442\", null],\n[\"\u0442\u044b\", null],\n[\"\u0440\u0443\", null],\n[\" \u043a\", \"\u9700\u8981\"],\n[\"\u0435\u0440\", null],\n[\"\u0435\u043a\", null],\n[\" \u0434\u0435\u043f\", \"\u8ba4\u4e3a\"],\n[\" \u0441\", null],\n[\"\u0430\u043d\", null],\n[\"\u0430\u0439\u0434\", null],\n[\"\u044b\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u0411\u0430\u0441\u049b\u0430 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u0442\u0430\u0440 \u0430\u0439\u044b\u0440\u043c\u0430\u0448\u044b\u043b\u044b\u0493\u044b \u2013 \u041c\u0430\u043a\u0440\u043e\u043d \u0415\u0443\u0440\u043e\u043f\u0430 \u043e\u0434\u0430\u0493\u044b\u043d \u0442\u04af\u0431\u0456\u0440\u0456\u043c\u0435\u043d \u04e9\u0437\u0433\u0435\u0440\u0442\u0456\u043f \u049b\u0430\u043d\u0430 \u049b\u043e\u0439\u043c\u0430\u0439, \u0431\u04af\u043a\u0456\u043b \u0415\u0443\u0440\u043e\u043f\u0430\u0434\u0430 \u044b\u043d\u0442\u044b\u043c\u0430\u049b\u0442\u0430\u0441\u0442\u044b\u049b\u0442\u044b \u0430\u0440\u0442\u0442\u044b\u0440\u0443 \u043a\u0435\u0440\u0435\u043a \u0434\u0435\u043f \u0441\u0430\u043d\u0430\u0439\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0411', '\u0430\u0441', '\u049b', '\u0430', ' \u043a', '\u0430\u043d\u0434', '\u0438\u0434', '\u0430\u0442', '\u0442\u0430\u0440', ' \u0430', '\u0439', '\u044b', '\u0440', '\u043c', '\u0430\u0448', '\u044b', '\u043b\u044b', '\u0493', '\u044b', ' \u2013', ' \u041c\u0430\u043a', '\u0440\u043e\u043d', ' \u0415', '\u0443\u0440', '\u043e\u043f', '\u0430', ' \u043e', '\u0434\u0430', '\u0493', '\u044b', '\u043d', ' \u0442', '\u04af', '\u0431', '\u0456', '\u0440', '\u0456', '\u043c\u0435\u043d', ' ', '\u04e9', '\u0437', '\u0433', '\u0435\u0440\u0442', '\u0456', '\u043f', ' ', '\u049b', '\u0430\u043d\u0430', ' ', '\u049b', '\u043e\u0439', '\u043c', '\u0430\u0439', ',', ' \u0431', '\u04af', '\u043a', '\u0456', '\u043b', ' \u0415', '\u0443\u0440', '\u043e\u043f', '\u0430\u0434', '\u0430', ' ', '\u044b', '\u043d', '\u0442\u044b', '\u043c\u0430', '\u049b', '\u0442\u0430', '\u0441\u0442\u044b', '\u049b', '\u0442\u044b', ' \u0430', '\u0440', '\u0442', '\u0442\u044b', '\u0440\u0443', ' \u043a', '\u0435\u0440', '\u0435\u043a', ' \u0434\u0435\u043f', ' \u0441', '\u0430\u043d', '\u0430\u0439\u0434', '\u044b', '.']"}], "[\n[\"\u0411\", \"\u0434\u0440\u0443\u0433\u043e\u0439\"],\n[\"\u0430\u0441\", null],\n[\"\u049b\", null],\n[\"\u0430\", null],\n[\" \u043a\", null],\n[\"\u0430\u043d\u0434\", null],\n[\"\u0438\u0434\", null],\n[\"\u0430\u0442\", null],\n[\"\u0442\u0430\u0440\", null],\n[\" \u0430\", null],\n[\"\u0439\", null],\n[\"\u044b\", null],\n[\"\u0440\", null],\n[\"\u043c\", null],\n[\"\u0430\u0448\", null],\n[\"\u044b\", null],\n[\"\u043b\u044b\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\" \u2013\", null],\n[\" \u041c\u0430\u043a\", \"\u041c\u0430\u043a\u0440\u043e\u043d\"],\n[\"\u0440\u043e\u043d\", null],\n[\" \u0415\", \"\u0415\u0432\u0440\u043e\u043f\u0435\u0439\u0441\u043a\u0438\u0439\"],\n[\"\u0443\u0440\", null],\n[\"\u043e\u043f\", null],\n[\"\u0430\", null],\n[\" \u043e\", \"\u0441\u043e\u044e\u0437\"],\n[\"\u0434\u0430\", null],\n[\"\u0493\", null],\n[\"\u044b\", null],\n[\"\u043d\", null],\n[\" \u0442\", \"\u043a\u043e\u0440\u0435\u043d\u043d\u044b\u043c\"],\n[\"\u04af\", null],\n[\"\u0431\", null],\n[\"\u0456\", null],\n[\"\u0440\", null],\n[\"\u0456\", null],\n[\"\u043c\u0435\u043d\", null],\n[\" \", null],\n[\"\u04e9\", \"\u0438\u0437\u043c\u0435\u043d\u0438\u0442\u044c\"],\n[\"\u0437\", null],\n[\"\u0433\", null],\n[\"\u0435\u0440\u0442\", null],\n[\"\u0456\", null],\n[\"\u043f\", null],\n[\" \", null],\n[\"\u049b\", \"\u0442\u043e\u043b\u044c\u043a\u043e\"],\n[\"\u0430\u043d\u0430\", null],\n[\" \", null],\n[\"\u049b\", \"\u043d\u0435\"],\n[\"\u043e\u0439\", null],\n[\"\u043c\", null],\n[\"\u0430\u0439\", null],\n[\",\", null],\n[\" \u0431\", \"\u0432\u0435\u0441\u044c\"],\n[\"\u04af\", null],\n[\"\u043a\", null],\n[\"\u0456\", null],\n[\"\u043b\", null],\n[\" \u0415\", \"\u0415\u0432\u0440\u043e\u043f\u0430\"],\n[\"\u0443\u0440\", null],\n[\"\u043e\u043f\", null],\n[\"\u0430\u0434\", null],\n[\"\u0430\", null],\n[\" \", null],\n[\"\u044b\", \"\u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u0447\u0435\u0441\u0442\u0432\u043e\"],\n[\"\u043d\", null],\n[\"\u0442\u044b\", null],\n[\"\u043c\u0430\", null],\n[\"\u049b\", null],\n[\"\u0442\u0430\", null],\n[\"\u0441\u0442\u044b\", null],\n[\"\u049b\", null],\n[\"\u0442\u044b\", null],\n[\" \u0430\", \"\u0443\u0441\u0438\u043b\u0438\u0442\u044c\"],\n[\"\u0440\", null],\n[\"\u0442\", null],\n[\"\u0442\u044b\", null],\n[\"\u0440\u0443\", null],\n[\" \u043a\", \"\u043d\u0443\u0436\u043d\u043e\"],\n[\"\u0435\u0440\", null],\n[\"\u0435\u043a\", null],\n[\" \u0434\u0435\u043f\", null],\n[\" \u0441\", \"\u0441\u0447\u0438\u0442\u0430\u0442\u044c\"],\n[\"\u0430\u043d\", null],\n[\"\u0430\u0439\u0434\", null],\n[\"\u044b\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u0414\u0435\u0441\u0435 \u0434\u0435, \u0441\u0430\u044f\u0441\u0438 \u0442\u04d9\u0443\u0435\u043a\u0435\u043b\u0434\u0435\u0440\u0433\u0435 \u043a\u0435\u043b\u0433\u0435\u043d\u0434\u0435 \u0438\u043d\u0432\u0435\u0441\u0442\u043e\u0440\u043b\u0430\u0440 \u04d9\u0434\u0435\u0442\u0442\u0435\u0433\u0456\u0434\u0435\u043d \u0431\u0456\u0440\u0448\u0430\u043c\u0430 \u0440\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u0434\u044b \u04d9\u0440\u0435\u043a\u0435\u0442 \u0435\u0442\u0435\u0434\u0456.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0414', '\u0435\u0441', '\u0435', ' \u0434\u0435', ',', ' \u0441', '\u0430\u044f', '\u0441\u0438', ' \u0442', '\u04d9', '\u0443', '\u0435\u043a', '\u0435\u043b', '\u0434\u0435\u0440', '\u0433\u0435', ' \u043a', '\u0435\u043b', '\u0433\u0435\u043d', '\u0434\u0435', ' \u0438\u043d', '\u0432\u0435\u0441\u0442', '\u043e\u0440', '\u043b', '\u0430\u0440', ' ', '\u04d9', '\u0434\u0435\u0442', '\u0442', '\u0435\u0433', '\u0456', '\u0434\u0435\u043d', ' \u0431', '\u0456', '\u0440', '\u0448', '\u0430\u043c', '\u0430', ' \u0440', '\u0430\u0446\u0438', '\u043e\u043d\u0430', '\u043b', '\u0434\u044b', ' ', '\u04d9', '\u0440', '\u0435\u043a', '\u0435\u0442', ' \u0435', '\u0442', '\u0435\u0434', '\u0456', '.']"}], "[\n[\"\u0414\", \"However\"],\n[\"\u0435\u0441\", null],\n[\"\u0435\", null],\n[\" \u0434\u0435\", null],\n[\",\", null],\n[\" \u0441\", null],\n[\"\u0430\u044f\", null],\n[\"\u0441\u0438\", null],\n[\" \u0442\", null],\n[\"\u04d9\", null],\n[\"\u0443\", null],\n[\"\u0435\u043a\", null],\n[\"\u0435\u043b\", null],\n[\"\u0434\u0435\u0440\", null],\n[\"\u0433\u0435\", null],\n[\" \u043a\", null],\n[\"\u0435\u043b\", null],\n[\"\u0433\u0435\u043d\", null],\n[\"\u0434\u0435\", null],\n[\" \u0438\u043d\", null],\n[\"\u0432\u0435\u0441\u0442\", null],\n[\"\u043e\u0440\", null],\n[\"\u043b\", null],\n[\"\u0430\u0440\", null],\n[\" \", null],\n[\"\u04d9\", null],\n[\"\u0434\u0435\u0442\", null],\n[\"\u0442\", null],\n[\"\u0435\u0433\", null],\n[\"\u0456\", null],\n[\"\u0434\u0435\u043d\", null],\n[\" \u0431\", null],\n[\"\u0456\", null],\n[\"\u0440\", null],\n[\"\u0448\", null],\n[\"\u0430\u043c\", null],\n[\"\u0430\", null],\n[\" \u0440\", null],\n[\"\u0430\u0446\u0438\", null],\n[\"\u043e\u043d\u0430\", null],\n[\"\u043b\", null],\n[\"\u0434\u044b\", null],\n[\" \", null],\n[\"\u04d9\", null],\n[\"\u0440\", null],\n[\"\u0435\u043a\", null],\n[\"\u0435\u0442\", null],\n[\" \u0435\", null],\n[\"\u0442\", null],\n[\"\u0435\u0434\", null],\n[\"\u0456\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u0414\u0435\u0441\u0435 \u0434\u0435, \u0441\u0430\u044f\u0441\u0438 \u0442\u04d9\u0443\u0435\u043a\u0435\u043b\u0434\u0435\u0440\u0433\u0435 \u043a\u0435\u043b\u0433\u0435\u043d\u0434\u0435 \u0438\u043d\u0432\u0435\u0441\u0442\u043e\u0440\u043b\u0430\u0440 \u04d9\u0434\u0435\u0442\u0442\u0435\u0433\u0456\u0434\u0435\u043d \u0431\u0456\u0440\u0448\u0430\u043c\u0430 \u0440\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u0434\u044b \u04d9\u0440\u0435\u043a\u0435\u0442 \u0435\u0442\u0435\u0434\u0456.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0414', '\u0435\u0441', '\u0435', ' \u0434\u0435', ',', ' \u0441', '\u0430\u044f', '\u0441\u0438', ' \u0442', '\u04d9', '\u0443', '\u0435\u043a', '\u0435\u043b', '\u0434\u0435\u0440', '\u0433\u0435', ' \u043a', '\u0435\u043b', '\u0433\u0435\u043d', '\u0434\u0435', ' \u0438\u043d', '\u0432\u0435\u0441\u0442', '\u043e\u0440', '\u043b', '\u0430\u0440', ' ', '\u04d9', '\u0434\u0435\u0442', '\u0442', '\u0435\u0433', '\u0456', '\u0434\u0435\u043d', ' \u0431', '\u0456', '\u0440', '\u0448', '\u0430\u043c', '\u0430', ' \u0440', '\u0430\u0446\u0438', '\u043e\u043d\u0430', '\u043b', '\u0434\u044b', ' ', '\u04d9', '\u0440', '\u0435\u043a', '\u0435\u0442', ' \u0435', '\u0442', '\u0435\u0434', '\u0456', '.']"}], "[\n[\"\u0414\", \"\u7136\u800c\"],\n[\"\u0435\u0441\", null],\n[\"\u0435\", null],\n[\" \u0434\u0435\", null],\n[\",\", null],\n[\" \u0441\", null],\n[\"\u0430\u044f\", null],\n[\"\u0441\u0438\", null],\n[\" \u0442\", null],\n[\"\u04d9\", null],\n[\"\u0443\", null],\n[\"\u0435\u043a\", null],\n[\"\u0435\u043b\", null],\n[\"\u0434\u0435\u0440\", null],\n[\"\u0433\u0435\", null],\n[\" \u043a\", null],\n[\"\u0435\u043b\", null],\n[\"\u0433\u0435\u043d\", null],\n[\"\u0434\u0435\", null],\n[\" \u0438\u043d\", null],\n[\"\u0432\u0435\u0441\u0442\", null],\n[\"\u043e\u0440\", null],\n[\"\u043b\", null],\n[\"\u0430\u0440\", null],\n[\" \", null],\n[\"\u04d9\", null],\n[\"\u0434\u0435\u0442\", null],\n[\"\u0442\", null],\n[\"\u0435\u0433\", null],\n[\"\u0456\", null],\n[\"\u0434\u0435\u043d\", null],\n[\" \u0431\", null],\n[\"\u0456\", null],\n[\"\u0440\", null],\n[\"\u0448\", null],\n[\"\u0430\u043c\", null],\n[\"\u0430\", null],\n[\" \u0440\", null],\n[\"\u0430\u0446\u0438\", null],\n[\"\u043e\u043d\u0430\", null],\n[\"\u043b\", null],\n[\"\u0434\u044b\", null],\n[\" \", null],\n[\"\u04d9\", null],\n[\"\u0440\", null],\n[\"\u0435\u043a\", null],\n[\"\u0435\u0442\", null],\n[\" \u0435\", null],\n[\"\u0442\", null],\n[\"\u0435\u0434\", null],\n[\"\u0456\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u0414\u0435\u0441\u0435 \u0434\u0435, \u0441\u0430\u044f\u0441\u0438 \u0442\u04d9\u0443\u0435\u043a\u0435\u043b\u0434\u0435\u0440\u0433\u0435 \u043a\u0435\u043b\u0433\u0435\u043d\u0434\u0435 \u0438\u043d\u0432\u0435\u0441\u0442\u043e\u0440\u043b\u0430\u0440 \u04d9\u0434\u0435\u0442\u0442\u0435\u0433\u0456\u0434\u0435\u043d \u0431\u0456\u0440\u0448\u0430\u043c\u0430 \u0440\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u0434\u044b \u04d9\u0440\u0435\u043a\u0435\u0442 \u0435\u0442\u0435\u0434\u0456.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0414', '\u0435\u0441', '\u0435', ' \u0434\u0435', ',', ' \u0441', '\u0430\u044f', '\u0441\u0438', ' \u0442', '\u04d9', '\u0443', '\u0435\u043a', '\u0435\u043b', '\u0434\u0435\u0440', '\u0433\u0435', ' \u043a', '\u0435\u043b', '\u0433\u0435\u043d', '\u0434\u0435', ' \u0438\u043d', '\u0432\u0435\u0441\u0442', '\u043e\u0440', '\u043b', '\u0430\u0440', ' ', '\u04d9', '\u0434\u0435\u0442', '\u0442', '\u0435\u0433', '\u0456', '\u0434\u0435\u043d', ' \u0431', '\u0456', '\u0440', '\u0448', '\u0430\u043c', '\u0430', ' \u0440', '\u0430\u0446\u0438', '\u043e\u043d\u0430', '\u043b', '\u0434\u044b', ' ', '\u04d9', '\u0440', '\u0435\u043a', '\u0435\u0442', ' \u0435', '\u0442', '\u0435\u0434', '\u0456', '.']"}], "[\n[\"\u0414\", \"\u0422\u0435\u043c\"],\n[\"\u0435\u0441\", null],\n[\"\u0435\", null],\n[\" \u0434\u0435\", null],\n[\",\", null],\n[\" \u0441\", null],\n[\"\u0430\u044f\", null],\n[\"\u0441\u0438\", null],\n[\" \u0442\", null],\n[\"\u04d9\", null],\n[\"\u0443\", null],\n[\"\u0435\u043a\", null],\n[\"\u0435\u043b\", null],\n[\"\u0434\u0435\u0440\", null],\n[\"\u0433\u0435\", null],\n[\" \u043a\", null],\n[\"\u0435\u043b\", null],\n[\"\u0433\u0435\u043d\", null],\n[\"\u0434\u0435\", null],\n[\" \u0438\u043d\", null],\n[\"\u0432\u0435\u0441\u0442\", null],\n[\"\u043e\u0440\", null],\n[\"\u043b\", null],\n[\"\u0430\u0440\", null],\n[\" \", null],\n[\"\u04d9\", null],\n[\"\u0434\u0435\u0442\", null],\n[\"\u0442\", null],\n[\"\u0435\u0433\", null],\n[\"\u0456\", null],\n[\"\u0434\u0435\u043d\", null],\n[\" \u0431\", null],\n[\"\u0456\", null],\n[\"\u0440\", null],\n[\"\u0448\", null],\n[\"\u0430\u043c\", null],\n[\"\u0430\", null],\n[\" \u0440\", null],\n[\"\u0430\u0446\u0438\", null],\n[\"\u043e\u043d\u0430\", null],\n[\"\u043b\", null],\n[\"\u0434\u044b\", null],\n[\" \", null],\n[\"\u04d9\", null],\n[\"\u0440\", null],\n[\"\u0435\u043a\", null],\n[\"\u0435\u0442\", null],\n[\" \u0435\", null],\n[\"\u0442\", null],\n[\"\u0435\u0434\", null],\n[\"\u0456\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u041d\u042c\u042e-\u0419\u041e\u0420\u041a \u2013 \u041e\u04a3\u0442\u04af\u0441\u0442\u0456\u043a \u0410\u0444\u0440\u0438\u043a\u0430 \u0420\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430\u0441\u044b\u043d\u044b\u04a3 \u04af\u043a\u0456\u043c\u0435\u0442\u0456 1997 \u0436\u044b\u043b\u044b \u0410\u0418\u0422\u0412 / \u0416\u0418\u0422\u0421-\u0442\u0456 \u0435\u043c\u0434\u0435\u0443\u0433\u0435 \u0430\u0440\u043d\u0430\u043b\u0493\u0430\u043d \u0434\u0436\u0435\u043d\u0435\u0440\u0438\u043a \u0434\u04d9\u0440\u0456-\u0434\u04d9\u0440\u043c\u0435\u043a\u0442\u0435\u0440\u0434\u0456 \u049b\u043e\u043b\u0436\u0435\u0442\u0456\u043c\u0434\u0456 \u0435\u0442\u0443 \u04af\u0448\u0456\u043d \u04e9\u0437\u0434\u0435\u0440\u0456\u043d\u0456\u04a3 \u0437\u0430\u04a3\u0434\u0430\u0440\u044b\u043d\u0430 \u04e9\u0437\u0433\u0435\u0440\u0456\u0441\u0442\u0435\u0440 \u0435\u043d\u0433\u0456\u0437\u0443\u0433\u0435 \u0442\u044b\u0440\u044b\u0441\u049b\u0430\u043d \u043a\u0435\u0437\u0434\u0435, \u0431\u04af\u043a\u0456\u043b \u04d9\u043b\u0435\u043c\u0434\u0456\u043a \u0444\u0430\u0440\u043c\u0430\u0446\u0435\u0432\u0442\u0438\u043a\u0430\u043b\u044b\u049b \u0438\u043d\u0434\u0443\u0441\u0442\u0440\u0438\u044f\u043d\u044b\u04a3 \u0437\u0430\u04a3\u0434\u044b \u043a\u04af\u0448\u0442\u0435\u0440\u0456 \u043e\u0441\u044b \u0435\u043b\u0433\u0435 \u0431\u0430\u0441 \u0441\u0430\u043b\u044b\u043f, \u043e\u043d\u044b \u0435\u043d\u0433\u0456\u0437\u0443\u0434\u0456 \u043a\u0435\u0448\u0456\u043a\u0442\u0456\u0440\u0456\u043f, \u043a\u04e9\u043f\u0442\u0435\u0433\u0435\u043d \u0430\u0434\u0430\u043c\u0434\u0430\u0440\u0434\u044b\u04a3 \u04e9\u043c\u0456\u0440\u0456\u043d \u049b\u0438\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u041d', '\u042c', '\u042e', '-', '\u0419', '\u041e\u0420', '\u041a', ' \u2013', ' \u041e', '\u04a3', '\u0442', '\u04af', '\u0441\u0442', '\u0456', '\u043a', ' \u0410', '\u0444', '\u0440\u0438', '\u043a\u0430', ' \u0420\u0435\u0441\u043f\u0443\u0431', '\u043b\u0438', '\u043a\u0430', '\u0441\u044b', '\u043d\u044b', '\u04a3', ' ', '\u04af', '\u043a', '\u0456', '\u043c\u0435\u0442', '\u0456', ' ', '1', '9', '9', '7', ' \u0436', '\u044b', '\u043b\u044b', ' \u0410', '\u0418', '\u0422', '\u0412', ' /', ' \u0416', '\u0418', '\u0422', '\u0421', '-', '\u0442', '\u0456', ' \u0435', '\u043c', '\u0434\u0435', '\u0443\u0433', '\u0435', ' \u0430', '\u0440', '\u043d\u0430\u043b', '\u0493', '\u0430\u043d', ' \u0434', '\u0436\u0435\u043d', '\u0435\u0440', '\u0438\u043a', ' \u0434', '\u04d9', '\u0440', '\u0456', '-', '\u0434', '\u04d9', '\u0440', '\u043c', '\u0435\u043a\u0442', '\u0435\u0440', '\u0434', '\u0456', ' ', '\u049b', '\u043e\u043b\u0436', '\u0435\u0442', '\u0456', '\u043c', '\u0434', '\u0456', ' \u0435', '\u0442\u0443', ' ', '\u04af', '\u0448', '\u0456', '\u043d', ' ', '\u04e9', '\u0437\u0434', '\u0435\u0440', '\u0456', '\u043d', '\u0456', '\u04a3', ' \u0437\u0430', '\u04a3', '\u0434\u0430\u0440', '\u044b', '\u043d\u0430', ' ', '\u04e9', '\u0437', '\u0433\u0435\u0440', '\u0456', '\u0441\u0442\u0435\u0440', ' \u0435', '\u043d', '\u0433', '\u0456', '\u0437', '\u0443\u0433', '\u0435', ' \u0442\u044b', '\u0440\u044b', '\u0441', '\u049b', '\u0430\u043d', ' \u043a', '\u0435\u0437', '\u0434\u0435', ',', ' \u0431', '\u04af', '\u043a', '\u0456', '\u043b', ' ', '\u04d9', '\u043b\u0435\u043c', '\u0434', '\u0456', '\u043a', ' \u0444', '\u0430\u0440', '\u043c\u0430', '\u0446\u0435\u0432', '\u0442\u0438\u043a\u0430', '\u043b\u044b', '\u049b', ' \u0438\u043d', '\u0434', '\u0443\u0441\u0442', '\u0440\u0438', '\u044f', '\u043d\u044b', '\u04a3', ' \u0437\u0430', '\u04a3', '\u0434\u044b', ' \u043a', '\u04af', '\u0448', '\u0442\u0435\u0440', '\u0456', ' \u043e', '\u0441\u044b', ' \u0435', '\u043b', '\u0433\u0435', ' \u0431', '\u0430\u0441', ' \u0441', '\u0430\u043b', '\u044b', '\u043f', ',', ' \u043e', '\u043d\u044b', ' \u0435', '\u043d', '\u0433', '\u0456', '\u0437', '\u0443\u0434', '\u0456', ' \u043a', '\u0435\u0448', '\u0456', '\u043a\u0442', '\u0456', '\u0440', '\u0456', '\u043f', ',', ' \u043a', '\u04e9', '\u043f\u0442', '\u0435\u0433', '\u0435\u043d', ' \u0430\u0434', '\u0430\u043c', '\u0434\u0430\u0440', '\u0434\u044b', '\u04a3', ' ', '\u04e9', '\u043c', '\u0456', '\u0440', '\u0456', '\u043d', ' ', '\u049b', '\u0438\u0434', '\u044b', '.']"}], "[\n[\"\u041d\", null],\n[\"\u042c\", null],\n[\"\u042e\", null],\n[\"-\", null],\n[\"\u0419\", null],\n[\"\u041e\u0420\", null],\n[\"\u041a\", \"NEW YORK\"],\n[\" \u2013\", null],\n[\" \u041e\", null],\n[\"\u04a3\", null],\n[\"\u0442\", null],\n[\"\u04af\", null],\n[\"\u0441\u0442\", null],\n[\"\u0456\", null],\n[\"\u043a\", \"South\"],\n[\" \u0410\", null],\n[\"\u0444\", null],\n[\"\u0440\u0438\", null],\n[\"\u043a\u0430\", \"Africa\"],\n[\" \u0420\u0435\u0441\u043f\u0443\u0431\", null],\n[\"\u043b\u0438\", null],\n[\"\u043a\u0430\", null],\n[\"\u0441\u044b\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", \"Republic\"],\n[\" \", null],\n[\"\u04af\", null],\n[\"\u043a\", null],\n[\"\u0456\", null],\n[\"\u043c\u0435\u0442\", null],\n[\"\u0456\", \"government\"],\n[\" \", null],\n[\"1\", null],\n[\"9\", null],\n[\"9\", null],\n[\"7\", \"1997\"],\n[\" \u0436\", null],\n[\"\u044b\", null],\n[\"\u043b\u044b\", \"in\"],\n[\" \u0410\", null],\n[\"\u0418\", null],\n[\"\u0422\", null],\n[\"\u0412\", \"HIV\"],\n[\" /\", null],\n[\" \u0416\", null],\n[\"\u0418\", null],\n[\"\u0422\", null],\n[\"\u0421\", \"AIDS\"],\n[\"-\", null],\n[\"\u0442\", null],\n[\"\u0456\", null],\n[\" \u0435\", null],\n[\"\u043c\", null],\n[\"\u0434\u0435\", null],\n[\"\u0443\u0433\", null],\n[\"\u0435\", \"treat\"],\n[\" \u0430\", null],\n[\"\u0440\", null],\n[\"\u043d\u0430\u043b\", null],\n[\"\u0493\", null],\n[\"\u0430\u043d\", \"for\"],\n[\" \u0434\", null],\n[\"\u0436\u0435\u043d\", null],\n[\"\u0435\u0440\", null],\n[\"\u0438\u043a\", \"generic\"],\n[\" \u0434\", null],\n[\"\u04d9\", null],\n[\"\u0440\", null],\n[\"\u0456\", null],\n[\"-\", null],\n[\"\u0434\", null],\n[\"\u04d9\", null],\n[\"\u0440\", null],\n[\"\u043c\", null],\n[\"\u0435\u043a\u0442\", null],\n[\"\u0435\u0440\", null],\n[\"\u0434\", null],\n[\"\u0456\", \"medications\"],\n[\" \", null],\n[\"\u049b\", null],\n[\"\u043e\u043b\u0436\", null],\n[\"\u0435\u0442\", null],\n[\"\u0456\", null],\n[\"\u043c\", null],\n[\"\u0434\", null],\n[\"\u0456\", \"accessible\"],\n[\" \u0435\", null],\n[\"\u0442\u0443\", \"make\"],\n[\" \", null],\n[\"\u04af\", null],\n[\"\u0448\", null],\n[\"\u0456\", null],\n[\"\u043d\", \"to\"],\n[\" \", null],\n[\"\u04e9\", null],\n[\"\u0437\u0434\", null],\n[\"\u0435\u0440\", null],\n[\"\u0456\", null],\n[\"\u043d\", null],\n[\"\u0456\", null],\n[\"\u04a3\", \"their\"],\n[\" \u0437\u0430\", null],\n[\"\u04a3\", null],\n[\"\u0434\u0430\u0440\", null],\n[\"\u044b\", null],\n[\"\u043d\u0430\", \"laws\"],\n[\" \", null],\n[\"\u04e9\", null],\n[\"\u0437\", null],\n[\"\u0433\u0435\u0440\", null],\n[\"\u0456\", null],\n[\"\u0441\u0442\u0435\u0440\", \"changes\"],\n[\" \u0435\", null],\n[\"\u043d\", null],\n[\"\u0433\", null],\n[\"\u0456\", null],\n[\"\u0437\", null],\n[\"\u0443\u0433\", null],\n[\"\u0435\", \"introduce\"],\n[\" \u0442\u044b\", null],\n[\"\u0440\u044b\", null],\n[\"\u0441\", null],\n[\"\u049b\", null],\n[\"\u0430\u043d\", \"attempted\"],\n[\" \u043a\", null],\n[\"\u0435\u0437\", null],\n[\"\u0434\u0435\", \"when\"],\n[\",\", null],\n[\" \u0431\", null],\n[\"\u04af\", null],\n[\"\u043a\", null],\n[\"\u0456\", null],\n[\"\u043b\", \"global\"],\n[\" \", null],\n[\"\u04d9\", null],\n[\"\u043b\u0435\u043c\", null],\n[\"\u0434\", null],\n[\"\u0456\", null],\n[\"\u043a\", \"worldwide\"],\n[\" \u0444\", null],\n[\"\u0430\u0440\", null],\n[\"\u043c\u0430\", null],\n[\"\u0446\u0435\u0432\", null],\n[\"\u0442\u0438\u043a\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u049b\", \"pharmaceutical\"],\n[\" \u0438\u043d\", null],\n[\"\u0434\", null],\n[\"\u0443\u0441\u0442\", null],\n[\"\u0440\u0438\", null],\n[\"\u044f\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", \"industry\"],\n[\" \u0437\u0430\", null],\n[\"\u04a3\", null],\n[\"\u0434\u044b\", \"legal\"],\n[\" \u043a\", null],\n[\"\u04af\", null],\n[\"\u0448\", null],\n[\"\u0442\u0435\u0440\", null],\n[\"\u0456\", \"forces\"],\n[\" \u043e\", null],\n[\"\u0441\u044b\", \"this\"],\n[\" \u0435\", null],\n[\"\u043b\", null],\n[\"\u0433\u0435\", \"country\"],\n[\" \u0431\", null],\n[\"\u0430\u0441\", \"attacked\"],\n[\" \u0441\", null],\n[\"\u0430\u043b\", null],\n[\"\u044b\", null],\n[\"\u043f\", \"pounced\"],\n[\",\", null],\n[\" \u043e\", null],\n[\"\u043d\u044b\", \"its\"],\n[\" \u0435\", null],\n[\"\u043d\", null],\n[\"\u0433\", null],\n[\"\u0456\", null],\n[\"\u0437\", null],\n[\"\u0443\u0434\", null],\n[\"\u0456\", \"introduction\"],\n[\" \u043a\", null],\n[\"\u0435\u0448\", null],\n[\"\u0456\", null],\n[\"\u043a\u0442\", null],\n[\"\u0456\", null],\n[\"\u0440\", null],\n[\"\u0456\", null],\n[\"\u043f\", \"delayed\"],\n[\",\", null],\n[\" \u043a\", null],\n[\"\u04e9\", null],\n[\"\u043f\u0442\", null],\n[\"\u0435\u0433\", null],\n[\"\u0435\u043d\", \"many\"],\n[\" \u0430\u0434\", null],\n[\"\u0430\u043c\", null],\n[\"\u0434\u0430\u0440\", null],\n[\"\u0434\u044b\", null],\n[\"\u04a3\", \"people's\"],\n[\" \", null],\n[\"\u04e9\", null],\n[\"\u043c\", null],\n[\"\u0456\", null],\n[\"\u0440\", null],\n[\"\u0456\", null],\n[\"\u043d\", \"lives\"],\n[\" \", null],\n[\"\u049b\", null],\n[\"\u0438\u0434\", null],\n[\"\u044b\", \"took\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u041d\u042c\u042e-\u0419\u041e\u0420\u041a \u2013 \u041e\u04a3\u0442\u04af\u0441\u0442\u0456\u043a \u0410\u0444\u0440\u0438\u043a\u0430 \u0420\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430\u0441\u044b\u043d\u044b\u04a3 \u04af\u043a\u0456\u043c\u0435\u0442\u0456 1997 \u0436\u044b\u043b\u044b \u0410\u0418\u0422\u0412 / \u0416\u0418\u0422\u0421-\u0442\u0456 \u0435\u043c\u0434\u0435\u0443\u0433\u0435 \u0430\u0440\u043d\u0430\u043b\u0493\u0430\u043d \u0434\u0436\u0435\u043d\u0435\u0440\u0438\u043a \u0434\u04d9\u0440\u0456-\u0434\u04d9\u0440\u043c\u0435\u043a\u0442\u0435\u0440\u0434\u0456 \u049b\u043e\u043b\u0436\u0435\u0442\u0456\u043c\u0434\u0456 \u0435\u0442\u0443 \u04af\u0448\u0456\u043d \u04e9\u0437\u0434\u0435\u0440\u0456\u043d\u0456\u04a3 \u0437\u0430\u04a3\u0434\u0430\u0440\u044b\u043d\u0430 \u04e9\u0437\u0433\u0435\u0440\u0456\u0441\u0442\u0435\u0440 \u0435\u043d\u0433\u0456\u0437\u0443\u0433\u0435 \u0442\u044b\u0440\u044b\u0441\u049b\u0430\u043d \u043a\u0435\u0437\u0434\u0435, \u0431\u04af\u043a\u0456\u043b \u04d9\u043b\u0435\u043c\u0434\u0456\u043a \u0444\u0430\u0440\u043c\u0430\u0446\u0435\u0432\u0442\u0438\u043a\u0430\u043b\u044b\u049b \u0438\u043d\u0434\u0443\u0441\u0442\u0440\u0438\u044f\u043d\u044b\u04a3 \u0437\u0430\u04a3\u0434\u044b \u043a\u04af\u0448\u0442\u0435\u0440\u0456 \u043e\u0441\u044b \u0435\u043b\u0433\u0435 \u0431\u0430\u0441 \u0441\u0430\u043b\u044b\u043f, \u043e\u043d\u044b \u0435\u043d\u0433\u0456\u0437\u0443\u0434\u0456 \u043a\u0435\u0448\u0456\u043a\u0442\u0456\u0440\u0456\u043f, \u043a\u04e9\u043f\u0442\u0435\u0433\u0435\u043d \u0430\u0434\u0430\u043c\u0434\u0430\u0440\u0434\u044b\u04a3 \u04e9\u043c\u0456\u0440\u0456\u043d \u049b\u0438\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u041d', '\u042c', '\u042e', '-', '\u0419', '\u041e\u0420', '\u041a', ' \u2013', ' \u041e', '\u04a3', '\u0442', '\u04af', '\u0441\u0442', '\u0456', '\u043a', ' \u0410', '\u0444', '\u0440\u0438', '\u043a\u0430', ' \u0420\u0435\u0441\u043f\u0443\u0431', '\u043b\u0438', '\u043a\u0430', '\u0441\u044b', '\u043d\u044b', '\u04a3', ' ', '\u04af', '\u043a', '\u0456', '\u043c\u0435\u0442', '\u0456', ' ', '1', '9', '9', '7', ' \u0436', '\u044b', '\u043b\u044b', ' \u0410', '\u0418', '\u0422', '\u0412', ' /', ' \u0416', '\u0418', '\u0422', '\u0421', '-', '\u0442', '\u0456', ' \u0435', '\u043c', '\u0434\u0435', '\u0443\u0433', '\u0435', ' \u0430', '\u0440', '\u043d\u0430\u043b', '\u0493', '\u0430\u043d', ' \u0434', '\u0436\u0435\u043d', '\u0435\u0440', '\u0438\u043a', ' \u0434', '\u04d9', '\u0440', '\u0456', '-', '\u0434', '\u04d9', '\u0440', '\u043c', '\u0435\u043a\u0442', '\u0435\u0440', '\u0434', '\u0456', ' ', '\u049b', '\u043e\u043b\u0436', '\u0435\u0442', '\u0456', '\u043c', '\u0434', '\u0456', ' \u0435', '\u0442\u0443', ' ', '\u04af', '\u0448', '\u0456', '\u043d', ' ', '\u04e9', '\u0437\u0434', '\u0435\u0440', '\u0456', '\u043d', '\u0456', '\u04a3', ' \u0437\u0430', '\u04a3', '\u0434\u0430\u0440', '\u044b', '\u043d\u0430', ' ', '\u04e9', '\u0437', '\u0433\u0435\u0440', '\u0456', '\u0441\u0442\u0435\u0440', ' \u0435', '\u043d', '\u0433', '\u0456', '\u0437', '\u0443\u0433', '\u0435', ' \u0442\u044b', '\u0440\u044b', '\u0441', '\u049b', '\u0430\u043d', ' \u043a', '\u0435\u0437', '\u0434\u0435', ',', ' \u0431', '\u04af', '\u043a', '\u0456', '\u043b', ' ', '\u04d9', '\u043b\u0435\u043c', '\u0434', '\u0456', '\u043a', ' \u0444', '\u0430\u0440', '\u043c\u0430', '\u0446\u0435\u0432', '\u0442\u0438\u043a\u0430', '\u043b\u044b', '\u049b', ' \u0438\u043d', '\u0434', '\u0443\u0441\u0442', '\u0440\u0438', '\u044f', '\u043d\u044b', '\u04a3', ' \u0437\u0430', '\u04a3', '\u0434\u044b', ' \u043a', '\u04af', '\u0448', '\u0442\u0435\u0440', '\u0456', ' \u043e', '\u0441\u044b', ' \u0435', '\u043b', '\u0433\u0435', ' \u0431', '\u0430\u0441', ' \u0441', '\u0430\u043b', '\u044b', '\u043f', ',', ' \u043e', '\u043d\u044b', ' \u0435', '\u043d', '\u0433', '\u0456', '\u0437', '\u0443\u0434', '\u0456', ' \u043a', '\u0435\u0448', '\u0456', '\u043a\u0442', '\u0456', '\u0440', '\u0456', '\u043f', ',', ' \u043a', '\u04e9', '\u043f\u0442', '\u0435\u0433', '\u0435\u043d', ' \u0430\u0434', '\u0430\u043c', '\u0434\u0430\u0440', '\u0434\u044b', '\u04a3', ' ', '\u04e9', '\u043c', '\u0456', '\u0440', '\u0456', '\u043d', ' ', '\u049b', '\u0438\u0434', '\u044b', '.']"}], "[\n[\"\u041d\", null],\n[\"\u042c\", null],\n[\"\u042e\", null],\n[\"-\", null],\n[\"\u0419\", null],\n[\"\u041e\u0420\", null],\n[\"\u041a\", null],\n[\" \u2013\", null],\n[\" \u041e\", null],\n[\"\u04a3\", null],\n[\"\u0442\", null],\n[\"\u04af\", null],\n[\"\u0441\u0442\", null],\n[\"\u0456\", null],\n[\"\u043a\", null],\n[\" \u0410\", null],\n[\"\u0444\", null],\n[\"\u0440\u0438\", null],\n[\"\u043a\u0430\", \"\u5357\u975e\"],\n[\" \u0420\u0435\u0441\u043f\u0443\u0431\", null],\n[\"\u043b\u0438\", null],\n[\"\u043a\u0430\", null],\n[\"\u0441\u044b\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", null],\n[\" \", null],\n[\"\u04af\", null],\n[\"\u043a\", null],\n[\"\u0456\", null],\n[\"\u043c\u0435\u0442\", null],\n[\"\u0456\", \"\u653f\u5e9c\"],\n[\" \", null],\n[\"1\", null],\n[\"9\", null],\n[\"9\", null],\n[\"7\", null],\n[\" \u0436\", null],\n[\"\u044b\", null],\n[\"\u043b\u044b\", \"\u5e74\"],\n[\" \u0410\", null],\n[\"\u0418\", null],\n[\"\u0422\", null],\n[\"\u0412\", \"\u827e\u6ecb\u75c5\"],\n[\" /\", null],\n[\" \u0416\", null],\n[\"\u0418\", null],\n[\"\u0422\", null],\n[\"\u0421\", null],\n[\"-\", null],\n[\"\u0442\", null],\n[\"\u0456\", null],\n[\" \u0435\", null],\n[\"\u043c\", null],\n[\"\u0434\u0435\", null],\n[\"\u0443\u0433\", null],\n[\"\u0435\", \"\u6cbb\u7597\"],\n[\" \u0430\", null],\n[\"\u0440\", null],\n[\"\u043d\u0430\u043b\", null],\n[\"\u0493\", null],\n[\"\u0430\u043d\", null],\n[\" \u0434\", null],\n[\"\u0436\u0435\u043d\", null],\n[\"\u0435\u0440\", null],\n[\"\u0438\u043a\", \"\u4eff\u5236\u836f\"],\n[\" \u0434\", null],\n[\"\u04d9\", null],\n[\"\u0440\", null],\n[\"\u0456\", null],\n[\"-\", null],\n[\"\u0434\", null],\n[\"\u04d9\", null],\n[\"\u0440\", null],\n[\"\u043c\", null],\n[\"\u0435\u043a\u0442\", null],\n[\"\u0435\u0440\", null],\n[\"\u0434\", null],\n[\"\u0456\", null],\n[\" \", null],\n[\"\u049b\", null],\n[\"\u043e\u043b\u0436\", null],\n[\"\u0435\u0442\", null],\n[\"\u0456\", null],\n[\"\u043c\", null],\n[\"\u0434\", null],\n[\"\u0456\", \"\u53ef\u53ca\"],\n[\" \u0435\", null],\n[\"\u0442\u0443\", null],\n[\" \", null],\n[\"\u04af\", null],\n[\"\u0448\", null],\n[\"\u0456\", null],\n[\"\u043d\", null],\n[\" \", null],\n[\"\u04e9\", null],\n[\"\u0437\u0434\", null],\n[\"\u0435\u0440\", null],\n[\"\u0456\", null],\n[\"\u043d\", null],\n[\"\u0456\", null],\n[\"\u04a3\", null],\n[\" \u0437\u0430\", null],\n[\"\u04a3\", null],\n[\"\u0434\u0430\u0440\", null],\n[\"\u044b\", null],\n[\"\u043d\u0430\", \"\u6cd5\u5f8b\"],\n[\" \", null],\n[\"\u04e9\", null],\n[\"\u0437\", null],\n[\"\u0433\u0435\u0440\", null],\n[\"\u0456\", null],\n[\"\u0441\u0442\u0435\u0440\", \"\u4fee\u6539\"],\n[\" \u0435\", null],\n[\"\u043d\", null],\n[\"\u0433\", null],\n[\"\u0456\", null],\n[\"\u0437\", null],\n[\"\u0443\u0433\", null],\n[\"\u0435\", null],\n[\" \u0442\u044b\", null],\n[\"\u0440\u044b\", null],\n[\"\u0441\", null],\n[\"\u049b\", null],\n[\"\u0430\u043d\", \"\u5c1d\u8bd5\"],\n[\" \u043a\", null],\n[\"\u0435\u0437\", null],\n[\"\u0434\u0435\", \"\u65f6\"],\n[\",\", null],\n[\" \u0431\", null],\n[\"\u04af\", null],\n[\"\u043a\", null],\n[\"\u0456\", null],\n[\"\u043b\", null],\n[\" \", null],\n[\"\u04d9\", null],\n[\"\u043b\u0435\u043c\", null],\n[\"\u0434\", null],\n[\"\u0456\", null],\n[\"\u043a\", \"\u5168\u7403\"],\n[\" \u0444\", null],\n[\"\u0430\u0440\", null],\n[\"\u043c\u0430\", null],\n[\"\u0446\u0435\u0432\", null],\n[\"\u0442\u0438\u043a\u0430\", null],\n[\"\u043b\u044b\", null],\n[\"\u049b\", \"\u5236\u836f\"],\n[\" \u0438\u043d\", null],\n[\"\u0434\", null],\n[\"\u0443\u0441\u0442\", null],\n[\"\u0440\u0438\", null],\n[\"\u044f\", null],\n[\"\u043d\u044b\", null],\n[\"\u04a3\", \"\u4ea7\u4e1a\"],\n[\" \u0437\u0430\", null],\n[\"\u04a3\", null],\n[\"\u0434\u044b\", \"\u6cd5\u5f8b\"],\n[\" \u043a\", null],\n[\"\u04af\", null],\n[\"\u0448\", null],\n[\"\u0442\u0435\u0440\", null],\n[\"\u0456\", \"\u529b\u91cf\"],\n[\" \u043e\", null],\n[\"\u0441\u044b\", \"\u8fd9\u4e2a\"],\n[\" \u0435\", null],\n[\"\u043b\", null],\n[\"\u0433\u0435\", \"\u56fd\u5bb6\"],\n[\" \u0431\", null],\n[\"\u0430\u0441\", null],\n[\" \u0441\", null],\n[\"\u0430\u043b\", null],\n[\"\u044b\", null],\n[\"\u043f\", \"\u653b\u51fb\"],\n[\",\", null],\n[\" \u043e\", null],\n[\"\u043d\u044b\", \"\u5b83\"],\n[\" \u0435\", null],\n[\"\u043d\", null],\n[\"\u0433\", null],\n[\"\u0456\", null],\n[\"\u0437\", null],\n[\"\u0443\u0434\", null],\n[\"\u0456\", null],\n[\" \u043a\", null],\n[\"\u0435\u0448\", null],\n[\"\u0456\", null],\n[\"\u043a\u0442\", null],\n[\"\u0456\", null],\n[\"\u0440\", null],\n[\"\u0456\", null],\n[\"\u043f\", \"\u63a8\u8fdf\"],\n[\",\", null],\n[\" \u043a\", null],\n[\"\u04e9\", null],\n[\"\u043f\u0442\", null],\n[\"\u0435\u0433\", null],\n[\"\u0435\u043d\", \"\u8bb8\u591a\"],\n[\" \u0430\u0434\", null],\n[\"\u0430\u043c\", null],\n[\"\u0434\u0430\u0440\", null],\n[\"\u0434\u044b\", null],\n[\"\u04a3\", null],\n[\" \", null],\n[\"\u04e9\", null],\n[\"\u043c\", null],\n[\"\u0456\", null],\n[\"\u0440\", null],\n[\"\u0456\", null],\n[\"\u043d\", null],\n[\" \", null],\n[\"\u049b\", null],\n[\"\u0438\u0434\", null],\n[\"\u044b\", \"\u593a\u8d70\u751f\u547d\"],\n[\".\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0418\u043c\u043c\u0443\u043d\u043d\u044b\u0435 \u043a\u043b\u0435\u0442\u043a\u0438 \u043f\u0440\u0435\u0432\u0440\u0430\u0442\u0438\u043b\u0438\u0441\u044c \u0432 \u043a\u043b\u0435\u0442\u043a\u0438, \u043f\u043e\u0432\u044b\u0448\u0430\u044e\u0449\u0438\u0435 \u0442\u043e\u043b\u0435\u0440\u0430\u043d\u0442\u043d\u043e\u0441\u0442\u044c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0418', '\u043c', '\u043c', '\u0443\u043d', '\u043d\u044b\u0435', ' \u043a\u043b', '\u0435\u0442', '\u043a\u0438', ' \u043f\u0440', '\u0435\u0432', '\u0440\u0430\u0442', '\u0438\u043b\u0438\u0441\u044c', ' \u0432', ' \u043a\u043b', '\u0435\u0442', '\u043a\u0438', ',', ' \u043f\u043e\u0432', '\u044b\u0448', '\u0430', '\u044e\u0449\u0438\u0435', ' \u0442', '\u043e\u043b', '\u0435\u0440', '\u0430\u043d\u0442', '\u043d\u043e\u0441\u0442\u044c', '.']"}], "[\n  [\"\u0418\", \"Immune\"],\n  [\"\u043c\", null],\n  [\"\u043c\", null],\n  [\"\u0443\u043d\", null],\n  [\"\u043d\u044b\u0435\", null],\n  [\" \u043a\u043b\", \"cells\"],\n  [\"\u0435\u0442\", null],\n  [\"\u043a\u0438\", null],\n  [\" \u043f\u0440\", \"transformed\"],\n  [\"\u0435\u0432\", null],\n  [\"\u0440\u0430\u0442\", null],\n  [\"\u0438\u043b\u0438\u0441\u044c\", null],\n  [\" \u0432\", \"into\"],\n  [\" \u043a\u043b\", \"cells\"],\n  [\"\u0435\u0442\", null],\n  [\"\u043a\u0438\", null],\n  [\",\", null],\n  [\" \u043f\u043e\u0432\", \"increasing\"],\n  [\"\u044b\u0448\", null],\n  [\"\u0430\", null],\n  [\"\u044e\u0449\u0438\u0435\", null],\n  [\" \u0442\", \"tolerance\"],\n  [\"\u043e\u043b\", null],\n  [\"\u0435\u0440\", null],\n  [\"\u0430\u043d\u0442\", null],\n  [\"\u043d\u043e\u0441\u0442\u044c\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u0418\u043c\u043c\u0443\u043d\u043d\u044b\u0435 \u043a\u043b\u0435\u0442\u043a\u0438 \u043f\u0440\u0435\u0432\u0440\u0430\u0442\u0438\u043b\u0438\u0441\u044c \u0432 \u043a\u043b\u0435\u0442\u043a\u0438, \u043f\u043e\u0432\u044b\u0448\u0430\u044e\u0449\u0438\u0435 \u0442\u043e\u043b\u0435\u0440\u0430\u043d\u0442\u043d\u043e\u0441\u0442\u044c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0418', '\u043c', '\u043c', '\u0443\u043d', '\u043d\u044b\u0435', ' \u043a\u043b', '\u0435\u0442', '\u043a\u0438', ' \u043f\u0440', '\u0435\u0432', '\u0440\u0430\u0442', '\u0438\u043b\u0438\u0441\u044c', ' \u0432', ' \u043a\u043b', '\u0435\u0442', '\u043a\u0438', ',', ' \u043f\u043e\u0432', '\u044b\u0448', '\u0430', '\u044e\u0449\u0438\u0435', ' \u0442', '\u043e\u043b', '\u0435\u0440', '\u0430\u043d\u0442', '\u043d\u043e\u0441\u0442\u044c', '.']"}], "[\n[\"\u0418\", \"\u514d\u75ab\"],\n[\"\u043c\", null],\n[\"\u043c\", null],\n[\"\u0443\u043d\", null],\n[\"\u043d\u044b\u0435\", null],\n[\" \u043a\u043b\", \"\u7ec6\u80de\"],\n[\"\u0435\u0442\", null],\n[\"\u043a\u0438\", null],\n[\" \u043f\u0440\", \"\u8f6c\u53d8\"],\n[\"\u0435\u0432\", null],\n[\"\u0440\u0430\u0442\", null],\n[\"\u0438\u043b\u0438\u0441\u044c\", null],\n[\" \u0432\", null],\n[\" \u043a\u043b\", \"\u7ec6\u80de\"],\n[\"\u0435\u0442\", null],\n[\"\u043a\u0438\", null],\n[\",\", null],\n[\" \u043f\u043e\u0432\", \"\u63d0\u9ad8\"],\n[\"\u044b\u0448\", null],\n[\"\u0430\", null],\n[\"\u044e\u0449\u0438\u0435\", null],\n[\" \u0442\", \"\u8010\u53d7\u6027\"],\n[\"\u043e\u043b\", null],\n[\"\u0435\u0440\", null],\n[\"\u0430\u043d\u0442\", null],\n[\"\u043d\u043e\u0441\u0442\u044c\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u0418\u043c\u043c\u0443\u043d\u043d\u044b\u0435 \u043a\u043b\u0435\u0442\u043a\u0438 \u043f\u0440\u0435\u0432\u0440\u0430\u0442\u0438\u043b\u0438\u0441\u044c \u0432 \u043a\u043b\u0435\u0442\u043a\u0438, \u043f\u043e\u0432\u044b\u0448\u0430\u044e\u0449\u0438\u0435 \u0442\u043e\u043b\u0435\u0440\u0430\u043d\u0442\u043d\u043e\u0441\u0442\u044c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0418', '\u043c', '\u043c', '\u0443\u043d', '\u043d\u044b\u0435', ' \u043a\u043b', '\u0435\u0442', '\u043a\u0438', ' \u043f\u0440', '\u0435\u0432', '\u0440\u0430\u0442', '\u0438\u043b\u0438\u0441\u044c', ' \u0432', ' \u043a\u043b', '\u0435\u0442', '\u043a\u0438', ',', ' \u043f\u043e\u0432', '\u044b\u0448', '\u0430', '\u044e\u0449\u0438\u0435', ' \u0442', '\u043e\u043b', '\u0435\u0440', '\u0430\u043d\u0442', '\u043d\u043e\u0441\u0442\u044c', '.']"}], "[\n[\"\u0418\", \"\u0418\u043c\u043c\u0443\u043d\u0434\u044b\u049b\"],\n[\"\u043c\", null],\n[\"\u043c\", null],\n[\"\u0443\u043d\", null],\n[\"\u043d\u044b\u0435\", null],\n[\" \u043a\u043b\", \"\u0436\u0430\u0441\u0443\u0448\u0430\u043b\u0430\u0440\"],\n[\"\u0435\u0442\", null],\n[\"\u043a\u0438\", null],\n[\" \u043f\u0440\", \"\u0430\u0439\u043d\u0430\u043b\u0434\u044b\"],\n[\"\u0435\u0432\", null],\n[\"\u0440\u0430\u0442\", null],\n[\"\u0438\u043b\u0438\u0441\u044c\", null],\n[\" \u0432\", null],\n[\" \u043a\u043b\", \"\u0436\u0430\u0441\u0443\u0448\u0430\u043b\u0430\u0440\u0493\u0430\"],\n[\"\u0435\u0442\", null],\n[\"\u043a\u0438\", null],\n[\",\", null],\n[\" \u043f\u043e\u0432\", \"\u0430\u0440\u0442\u0442\u044b\u0440\u0430\u0442\u044b\u043d\"],\n[\"\u044b\u0448\", null],\n[\"\u0430\", null],\n[\"\u044e\u0449\u0438\u0435\", null],\n[\" \u0442\", \"\u0442\u04e9\u0437\u0456\u043c\u0434\u0456\u043b\u0456\u043a\u0442\u0456\"],\n[\"\u043e\u043b\", null],\n[\"\u0435\u0440\", null],\n[\"\u0430\u043d\u0442\", null],\n[\"\u043d\u043e\u0441\u0442\u044c\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0421\u0442\u043e\u0438\u0442 \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c, \u0447\u0442\u043e \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044f \u0441 \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u044b\u043c\u0438 \u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u043c\u0438 \u0443\u0434\u043e\u0441\u0442\u043e\u0432\u0435\u0440\u0435\u043d\u0438\u044f\u043c\u0438 \u043e\u0441\u0442\u0430\u0435\u0442\u0441\u044f \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u0440\u0435\u0447\u0438\u0432\u043e\u0439.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0421', '\u0442\u043e', '\u0438\u0442', ' \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c', ',', ' \u0447\u0442\u043e', ' \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044f', ' \u0441', ' \u043c\u0435\u0436\u0434\u0443', '\u043d\u0430', '\u0440\u043e\u0434', '\u043d\u044b\u043c\u0438', ' \u0432\u043e\u0434', '\u0438\u0442\u0435\u043b\u044c', '\u0441\u043a', '\u0438\u043c\u0438', ' \u0443\u0434', '\u043e\u0441\u0442', '\u043e\u0432', '\u0435\u0440', '\u0435\u043d\u0438\u044f', '\u043c\u0438', ' \u043e\u0441\u0442', '\u0430\u0435\u0442\u0441\u044f', ' \u043f\u0440\u043e\u0442\u0438\u0432', '\u043e\u0440', '\u0435\u0447', '\u0438\u0432', '\u043e\u0439', '.']"}], "[\n  [\"\u0421\", \"It\"],\n  [\"\u0442\u043e\", null],\n  [\"\u0438\u0442\", null],\n  [\" \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c\", \"note\"],\n  [\",\", null],\n  [\" \u0447\u0442\u043e\", null],\n  [\" \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044f\", \"situation\"],\n  [\" \u0441\", null],\n  [\" \u043c\u0435\u0436\u0434\u0443\", \"international\"],\n  [\"\u043d\u0430\", null],\n  [\"\u0440\u043e\u0434\", null],\n  [\"\u043d\u044b\u043c\u0438\", null],\n  [\" \u0432\u043e\u0434\", \"driver's\"],\n  [\"\u0438\u0442\u0435\u043b\u044c\", null],\n  [\"\u0441\u043a\", null],\n  [\"\u0438\u043c\u0438\", null],\n  [\" \u0443\u0434\", \"licenses\"],\n  [\"\u043e\u0441\u0442\", null],\n  [\"\u043e\u0432\", null],\n  [\"\u0435\u0440\", null],\n  [\"\u0435\u043d\u0438\u044f\", null],\n  [\"\u043c\u0438\", null],\n  [\" \u043e\u0441\u0442\", \"remains\"],\n  [\"\u0430\u0435\u0442\u0441\u044f\", null],\n  [\" \u043f\u0440\u043e\u0442\u0438\u0432\", \"contradictory\"],\n  [\"\u043e\u0440\", null],\n  [\"\u0435\u0447\", null],\n  [\"\u0438\u0432\", null],\n  [\"\u043e\u0439\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u0421\u0442\u043e\u0438\u0442 \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c, \u0447\u0442\u043e \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044f \u0441 \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u044b\u043c\u0438 \u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u043c\u0438 \u0443\u0434\u043e\u0441\u0442\u043e\u0432\u0435\u0440\u0435\u043d\u0438\u044f\u043c\u0438 \u043e\u0441\u0442\u0430\u0435\u0442\u0441\u044f \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u0440\u0435\u0447\u0438\u0432\u043e\u0439.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0421', '\u0442\u043e', '\u0438\u0442', ' \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c', ',', ' \u0447\u0442\u043e', ' \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044f', ' \u0441', ' \u043c\u0435\u0436\u0434\u0443', '\u043d\u0430', '\u0440\u043e\u0434', '\u043d\u044b\u043c\u0438', ' \u0432\u043e\u0434', '\u0438\u0442\u0435\u043b\u044c', '\u0441\u043a', '\u0438\u043c\u0438', ' \u0443\u0434', '\u043e\u0441\u0442', '\u043e\u0432', '\u0435\u0440', '\u0435\u043d\u0438\u044f', '\u043c\u0438', ' \u043e\u0441\u0442', '\u0430\u0435\u0442\u0441\u044f', ' \u043f\u0440\u043e\u0442\u0438\u0432', '\u043e\u0440', '\u0435\u0447', '\u0438\u0432', '\u043e\u0439', '.']"}], "[\n[\"\u0421\", \"\u503c\u5f97\"],\n[\"\u0442\u043e\", null],\n[\"\u0438\u0442\", null],\n[\" \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c\", null],\n[\",\", null],\n[\" \u0447\u0442\u043e\", null],\n[\" \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044f\", \"\u60c5\u51b5\"],\n[\" \u0441\", null],\n[\" \u043c\u0435\u0436\u0434\u0443\", \"\u56fd\u9645\"],\n[\"\u043d\u0430\", null],\n[\"\u0440\u043e\u0434\", null],\n[\"\u043d\u044b\u043c\u0438\", null],\n[\" \u0432\u043e\u0434\", \"\u9a7e\u9a76\"],\n[\"\u0438\u0442\u0435\u043b\u044c\", null],\n[\"\u0441\u043a\", null],\n[\"\u0438\u043c\u0438\", null],\n[\" \u0443\u0434\", \"\u8bc1\"],\n[\"\u043e\u0441\u0442\", null],\n[\"\u043e\u0432\", null],\n[\"\u0435\u0440\", null],\n[\"\u0435\u043d\u0438\u044f\", null],\n[\"\u043c\u0438\", null],\n[\" \u043e\u0441\u0442\", \"\u4ecd\u7136\"],\n[\"\u0430\u0435\u0442\u0441\u044f\", null],\n[\" \u043f\u0440\u043e\u0442\u0438\u0432\", \"\u77db\u76fe\"],\n[\"\u043e\u0440\", null],\n[\"\u0435\u0447\", null],\n[\"\u0438\u0432\", null],\n[\"\u043e\u0439\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u0421\u0442\u043e\u0438\u0442 \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c, \u0447\u0442\u043e \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044f \u0441 \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u044b\u043c\u0438 \u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u043c\u0438 \u0443\u0434\u043e\u0441\u0442\u043e\u0432\u0435\u0440\u0435\u043d\u0438\u044f\u043c\u0438 \u043e\u0441\u0442\u0430\u0435\u0442\u0441\u044f \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u0440\u0435\u0447\u0438\u0432\u043e\u0439.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u0421', '\u0442\u043e', '\u0438\u0442', ' \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c', ',', ' \u0447\u0442\u043e', ' \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044f', ' \u0441', ' \u043c\u0435\u0436\u0434\u0443', '\u043d\u0430', '\u0440\u043e\u0434', '\u043d\u044b\u043c\u0438', ' \u0432\u043e\u0434', '\u0438\u0442\u0435\u043b\u044c', '\u0441\u043a', '\u0438\u043c\u0438', ' \u0443\u0434', '\u043e\u0441\u0442', '\u043e\u0432', '\u0435\u0440', '\u0435\u043d\u0438\u044f', '\u043c\u0438', ' \u043e\u0441\u0442', '\u0430\u0435\u0442\u0441\u044f', ' \u043f\u0440\u043e\u0442\u0438\u0432', '\u043e\u0440', '\u0435\u0447', '\u0438\u0432', '\u043e\u0439', '.']"}], "[\n  [\"\u0421\", \"\u0410\u0442\u0430\u043f\"],\n  [\"\u0442\u043e\", null],\n  [\"\u0438\u0442\", null],\n  [\" \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c\", \"\u04e9\u0442\u0443\"],\n  [\",\", null],\n  [\" \u0447\u0442\u043e\", null],\n  [\" \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044f\", \"\u0436\u0430\u0493\u0434\u0430\u0439\"],\n  [\" \u0441\", null],\n  [\" \u043c\u0435\u0436\u0434\u0443\", \"\u0445\u0430\u043b\u044b\u049b\u0430\u0440\u0430\u043b\u044b\u049b\"],\n  [\"\u043d\u0430\", null],\n  [\"\u0440\u043e\u0434\", null],\n  [\"\u043d\u044b\u043c\u0438\", null],\n  [\" \u0432\u043e\u0434\", \"\u0436\u04af\u0440\u0433\u0456\u0437\u0443\u0448\u0456\"],\n  [\"\u0438\u0442\u0435\u043b\u044c\", null],\n  [\"\u0441\u043a\", null],\n  [\"\u0438\u043c\u0438\", null],\n  [\" \u0443\u0434\", \"\u043a\u0443\u04d9\u043b\u0456\u043a\u0442\u0435\u0440\"],\n  [\"\u043e\u0441\u0442\", null],\n  [\"\u043e\u0432\", null],\n  [\"\u0435\u0440\", null],\n  [\"\u0435\u043d\u0438\u044f\", null],\n  [\"\u043c\u0438\", null],\n  [\" \u043e\u0441\u0442\", \"\u049b\u0430\u043b\u0430\u0434\u044b\"],\n  [\"\u0430\u0435\u0442\u0441\u044f\", null],\n  [\" \u043f\u0440\u043e\u0442\u0438\u0432\", \"\u049b\u0430\u0440\u0430\u043c\u0430-\u049b\u0430\u0439\u0448\u044b\"],\n  [\"\u043e\u0440\", null],\n  [\"\u0435\u0447\", null],\n  [\"\u0438\u0432\", null],\n  [\"\u043e\u0439\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u6d3b\u52a8\u524d\u4e00\u5929\uff0cHMX-1 \u6307\u6325\u5b98\u5728\u5411\u4e0a\u7ea7\u6d77\u519b\u9646\u6218\u961f\u98de\u884c\u526f\u53f8\u4ee4\u53d1\u9001\u7684\u7535\u5b50\u90ae\u4ef6\u4e2d\u8868\u793a\u201c\u6211\u53ea\u60f3\u786e\u8ba4\u60a8\u77e5\u6653\uff0c\u56e0\u4e3a\u8fd9\u6b64\u6b21\u6d3b\u52a8\u53d7\u5230\u9ad8\u5ea6\u91cd\u89c6\u3002\u201d\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00e6\u00b4\u00bb\u00e5\u012c\u00a8', '\u00e5\u012b\u012f\u00e4\u00b8\u0122\u00e5\u00a4\u00a9', '\u00ef\u00bc\u012e', 'H', 'MX', '-', '1', ' \u00e6', '\u012e', '\u0129', '\u00e6\u012e\u00a5', '\u00e5\u00ae\u013a', '\u00e5\u013e\u00a8', '\u00e5\u0132\u0133\u00e4\u00b8\u012c', '\u00e7\u00ba\u00a7', '\u00e6\u00b5\u00b7\u00e5\u0128\u013d', '\u00e9\u013b\u0128', '\u00e6\u012a\u013a\u00e9\u013a\u0141', '\u00e9\u00a3\u0140\u00e8\u00a1\u012e', '\u00e5\u012b\u00af', '\u00e5\u0131\u00b8\u00e4\u00bb\u00a4', '\u00e5\u0131\u0133\u00e9\u0122\u0123', '\u00e7\u013c\u0126', '\u00e7\u0136\u00b5\u00e5\u0143\u0132\u00e9\u0124\u00ae\u00e4\u00bb\u00b6', '\u00e4\u00b8\u0143', '\u00e8\u00a1\u00a8\u00e7\u00a4\u00ba', '\u00e2\u0122\u013e', '\u00e6\u012a\u0133', '\u00e5\u0131\u00aa\u00e6\u0125\u00b3', '\u00e7\u00a1\u00ae\u00e8\u00ae\u00a4', '\u00e6\u0124\u00a8', '\u00e7\u0141\u00a5\u00e6\u013b\u0135', '\u00ef\u00bc\u012e', '\u00e5\u013d\u0142\u00e4\u00b8\u00ba', '\u00e8\u00bf\u013b', '\u00e6\u0143\u00a4\u00e6\u00ac\u00a1\u00e6\u00b4\u00bb\u00e5\u012c\u00a8', '\u00e5\u0131\u0139\u00e5\u012a\u00b0', '\u00e9\u00ab\u013a\u00e5\u00ba\u00a6\u00e9\u0129\u012f\u00e8\u00a7\u0128', '\u00e3\u0122\u0124\u00e2\u0122\u013f']"}], "[\n  [\"\u00e6\u00b4\u00bb\u00e5\u012c\u00a8\", \"activity\"],\n  [\"\u00e5\u012b\u012f\u00e4\u00b8\u0122\u00e5\u00a4\u00a9\", \"day before\"],\n  [\"\u00ef\u00bc\u012e\", null],\n  [\"H\", null],\n  [\"MX\", null],\n  [\"-\", null],\n  [\"1\", null],\n  [\" \u00e6\", null],\n  [\"\u012e\", null],\n  [\"\u0129\", null],\n  [\"\u00e6\u012e\u00a5\", null],\n  [\"\u00e5\u00ae\u013a\", \"commander\"],\n  [\"\u00e5\u013e\u00a8\", null],\n  [\"\u00e5\u0132\u0133\u00e4\u00b8\u012c\", \"sending\"],\n  [\"\u00e7\u00ba\u00a7\", null],\n  [\"\u00e6\u00b5\u00b7\u00e5\u0128\u013d\", \"Marine\"],\n  [\"\u00e9\u013b\u0128\", null],\n  [\"\u00e6\u012a\u013a\u00e9\u013a\u0141\", \"Corps\"],\n  [\"\u00e9\u00a3\u0140\u00e8\u00a1\u012e\", \"aviation\"],\n  [\"\u00e5\u012b\u00af\", null],\n  [\"\u00e5\u0131\u00b8\u00e4\u00bb\u00a4\", \"deputy commander\"],\n  [\"\u00e5\u0131\u0133\u00e9\u0122\u0123\", \"sent\"],\n  [\"\u00e7\u013c\u0126\", null],\n  [\"\u00e7\u0136\u00b5\u00e5\u0143\u0132\u00e9\u0124\u00ae\u00e4\u00bb\u00b6\", \"email\"],\n  [\"\u00e4\u00b8\u0143\", null],\n  [\"\u00e8\u00a1\u00a8\u00e7\u00a4\u00ba\", \"stated\"],\n  [\"\u00e2\u0122\u013e\", null],\n  [\"\u00e6\u012a\u0133\", \"I\"],\n  [\"\u00e5\u0131\u00aa\u00e6\u0125\u00b3\", \"just want to\"],\n  [\"\u00e7\u00a1\u00ae\u00e8\u00ae\u00a4\", \"confirm\"],\n  [\"\u00e6\u0124\u00a8\", \"you\"],\n  [\"\u00e7\u0141\u00a5\u00e6\u013b\u0135\", \"know\"],\n  [\"\u00ef\u00bc\u012e\", null],\n  [\"\u00e5\u013d\u0142\u00e4\u00b8\u00ba\", \"because\"],\n  [\"\u00e8\u00bf\u013b\", \"this\"],\n  [\"\u00e6\u0143\u00a4\u00e6\u00ac\u00a1\u00e6\u00b4\u00bb\u00e5\u012c\u00a8\", \"event\"],\n  [\"\u00e5\u0131\u0139\u00e5\u012a\u00b0\", \"received\"],\n  [\"\u00e9\u00ab\u013a\u00e5\u00ba\u00a6\u00e9\u0129\u012f\u00e8\u00a7\u0128\", \"high attention\"],\n  [\"\u00e3\u0122\u0124\u00e2\u0122\u013f\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u6d3b\u52a8\u524d\u4e00\u5929\uff0cHMX-1 \u6307\u6325\u5b98\u5728\u5411\u4e0a\u7ea7\u6d77\u519b\u9646\u6218\u961f\u98de\u884c\u526f\u53f8\u4ee4\u53d1\u9001\u7684\u7535\u5b50\u90ae\u4ef6\u4e2d\u8868\u793a\u201c\u6211\u53ea\u60f3\u786e\u8ba4\u60a8\u77e5\u6653\uff0c\u56e0\u4e3a\u8fd9\u6b64\u6b21\u6d3b\u52a8\u53d7\u5230\u9ad8\u5ea6\u91cd\u89c6\u3002\u201d\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00e6\u00b4\u00bb\u00e5\u012c\u00a8', '\u00e5\u012b\u012f\u00e4\u00b8\u0122\u00e5\u00a4\u00a9', '\u00ef\u00bc\u012e', 'H', 'MX', '-', '1', ' \u00e6', '\u012e', '\u0129', '\u00e6\u012e\u00a5', '\u00e5\u00ae\u013a', '\u00e5\u013e\u00a8', '\u00e5\u0132\u0133\u00e4\u00b8\u012c', '\u00e7\u00ba\u00a7', '\u00e6\u00b5\u00b7\u00e5\u0128\u013d', '\u00e9\u013b\u0128', '\u00e6\u012a\u013a\u00e9\u013a\u0141', '\u00e9\u00a3\u0140\u00e8\u00a1\u012e', '\u00e5\u012b\u00af', '\u00e5\u0131\u00b8\u00e4\u00bb\u00a4', '\u00e5\u0131\u0133\u00e9\u0122\u0123', '\u00e7\u013c\u0126', '\u00e7\u0136\u00b5\u00e5\u0143\u0132\u00e9\u0124\u00ae\u00e4\u00bb\u00b6', '\u00e4\u00b8\u0143', '\u00e8\u00a1\u00a8\u00e7\u00a4\u00ba', '\u00e2\u0122\u013e', '\u00e6\u012a\u0133', '\u00e5\u0131\u00aa\u00e6\u0125\u00b3', '\u00e7\u00a1\u00ae\u00e8\u00ae\u00a4', '\u00e6\u0124\u00a8', '\u00e7\u0141\u00a5\u00e6\u013b\u0135', '\u00ef\u00bc\u012e', '\u00e5\u013d\u0142\u00e4\u00b8\u00ba', '\u00e8\u00bf\u013b', '\u00e6\u0143\u00a4\u00e6\u00ac\u00a1\u00e6\u00b4\u00bb\u00e5\u012c\u00a8', '\u00e5\u0131\u0139\u00e5\u012a\u00b0', '\u00e9\u00ab\u013a\u00e5\u00ba\u00a6\u00e9\u0129\u012f\u00e8\u00a7\u0128', '\u00e3\u0122\u0124\u00e2\u0122\u013f']"}], "[\n    [\"\u00e6\u00b4\u00bb\u00e5\u012c\u00a8\", \"\u0456\u0441-\u0448\u0430\u0440\u0430\"],\n    [\"\u00e5\u012b\u012f\u00e4\u00b8\u0122\u00e5\u00a4\u00a9\", \"\u0430\u043b\u0434\u044b\u043d\u0434\u0430\u0493\u044b \u043a\u04af\u043d\u0456\"],\n    [\"\u00ef\u00bc\u012e\", null],\n    [\"H\", null],\n    [\"MX\", null],\n    [\"-\", null],\n    [\"1\", null],\n    [\" \u00e6\", null],\n    [\"\u012e\", null],\n    [\"\u0129\", null],\n    [\"\u00e6\u012e\u00a5\", null],\n    [\"\u00e5\u00ae\u013a\", \"\u043a\u043e\u043c\u0430\u043d\u0434\u0438\u0440\u0456\"],\n    [\"\u00e5\u013e\u00a8\", null],\n    [\"\u00e5\u0132\u0133\u00e4\u00b8\u012c\", \"\u0436\u043e\u0493\u0430\u0440\u044b\"],\n    [\"\u00e7\u00ba\u00a7\", null],\n    [\"\u00e6\u00b5\u00b7\u00e5\u0128\u013d\", \"\u0442\u0435\u04a3\u0456\u0437\"],\n    [\"\u00e9\u013b\u0128\", null],\n    [\"\u00e6\u012a\u013a\u00e9\u013a\u0141\", \"\u0436\u0430\u044f\u0443 \u04d9\u0441\u043a\u0435\u0440\u0456\"],\n    [\"\u00e9\u00a3\u0140\u00e8\u00a1\u012e\", \"\u04b1\u0448\u0443\"],\n    [\"\u00e5\u012b\u00af\", null],\n    [\"\u00e5\u0131\u00b8\u00e4\u00bb\u00a4\", \"\u0431\u04b1\u0439\u0440\u044b\u049b\u0448\u044b\u0441\u044b\u043d\u0430\"],\n    [\"\u00e5\u0131\u0133\u00e9\u0122\u0123\", \"\u0436\u0456\u0431\u0435\u0440\u0433\u0435\u043d\"],\n    [\"\u00e7\u013c\u0126\", null],\n    [\"\u00e7\u0136\u00b5\u00e5\u0143\u0132\u00e9\u0124\u00ae\u00e4\u00bb\u00b6\", \"\u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u0434\u044b\u049b \u0445\u0430\u0442\u0442\u0430\"],\n    [\"\u00e4\u00b8\u0143\", null],\n    [\"\u00e8\u00a1\u00a8\u00e7\u00a4\u00ba\", \"\u0430\u0439\u0442\u0442\u044b\"],\n    [\"\u00e2\u0122\u013e\", null],\n    [\"\u00e6\u012a\u0133\", \"\u043c\u0435\u043d\"],\n    [\"\u00e5\u0131\u00aa\u00e6\u0125\u00b3\", \"\u0442\u0435\u043a\"],\n    [\"\u00e7\u00a1\u00ae\u00e8\u00ae\u00a4\", \"\u0440\u0430\u0441\u0442\u0430\u0493\u044b\u043c\"],\n    [\"\u00e6\u0124\u00a8\", \"\u0441\u0456\u0437\"],\n    [\"\u00e7\u0141\u00a5\u00e6\u013b\u0135\", \"\u0431\u0456\u043b\u0435\u0442\u0456\u043d\u0456\u04a3\u0456\u0437\u0434\u0456\"],\n    [\"\u00ef\u00bc\u012e\", null],\n    [\"\u00e5\u013d\u0142\u00e4\u00b8\u00ba\", \"\u0441\u0435\u0431\u0435\u0431\u0456\"],\n    [\"\u00e8\u00bf\u013b\", \"\u0431\u04b1\u043b\"],\n    [\"\u00e6\u0143\u00a4\u00e6\u00ac\u00a1\u00e6\u00b4\u00bb\u00e5\u012c\u00a8\", \"\u0456\u0441-\u0448\u0430\u0440\u0430\"],\n    [\"\u00e5\u0131\u0139\u00e5\u012a\u00b0\", null],\n    [\"\u00e9\u00ab\u013a\u00e5\u00ba\u00a6\u00e9\u0129\u012f\u00e8\u00a7\u0128\", \"\u04e9\u0442\u0435 \u043c\u0430\u04a3\u044b\u0437\u0434\u044b\"],\n    [\"\u00e3\u0122\u0124\u00e2\u0122\u013f\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u6d3b\u52a8\u524d\u4e00\u5929\uff0cHMX-1 \u6307\u6325\u5b98\u5728\u5411\u4e0a\u7ea7\u6d77\u519b\u9646\u6218\u961f\u98de\u884c\u526f\u53f8\u4ee4\u53d1\u9001\u7684\u7535\u5b50\u90ae\u4ef6\u4e2d\u8868\u793a\u201c\u6211\u53ea\u60f3\u786e\u8ba4\u60a8\u77e5\u6653\uff0c\u56e0\u4e3a\u8fd9\u6b64\u6b21\u6d3b\u52a8\u53d7\u5230\u9ad8\u5ea6\u91cd\u89c6\u3002\u201d\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00e6\u00b4\u00bb\u00e5\u012c\u00a8', '\u00e5\u012b\u012f\u00e4\u00b8\u0122\u00e5\u00a4\u00a9', '\u00ef\u00bc\u012e', 'H', 'MX', '-', '1', ' \u00e6', '\u012e', '\u0129', '\u00e6\u012e\u00a5', '\u00e5\u00ae\u013a', '\u00e5\u013e\u00a8', '\u00e5\u0132\u0133\u00e4\u00b8\u012c', '\u00e7\u00ba\u00a7', '\u00e6\u00b5\u00b7\u00e5\u0128\u013d', '\u00e9\u013b\u0128', '\u00e6\u012a\u013a\u00e9\u013a\u0141', '\u00e9\u00a3\u0140\u00e8\u00a1\u012e', '\u00e5\u012b\u00af', '\u00e5\u0131\u00b8\u00e4\u00bb\u00a4', '\u00e5\u0131\u0133\u00e9\u0122\u0123', '\u00e7\u013c\u0126', '\u00e7\u0136\u00b5\u00e5\u0143\u0132\u00e9\u0124\u00ae\u00e4\u00bb\u00b6', '\u00e4\u00b8\u0143', '\u00e8\u00a1\u00a8\u00e7\u00a4\u00ba', '\u00e2\u0122\u013e', '\u00e6\u012a\u0133', '\u00e5\u0131\u00aa\u00e6\u0125\u00b3', '\u00e7\u00a1\u00ae\u00e8\u00ae\u00a4', '\u00e6\u0124\u00a8', '\u00e7\u0141\u00a5\u00e6\u013b\u0135', '\u00ef\u00bc\u012e', '\u00e5\u013d\u0142\u00e4\u00b8\u00ba', '\u00e8\u00bf\u013b', '\u00e6\u0143\u00a4\u00e6\u00ac\u00a1\u00e6\u00b4\u00bb\u00e5\u012c\u00a8', '\u00e5\u0131\u0139\u00e5\u012a\u00b0', '\u00e9\u00ab\u013a\u00e5\u00ba\u00a6\u00e9\u0129\u012f\u00e8\u00a7\u0128', '\u00e3\u0122\u0124\u00e2\u0122\u013f']"}], "[\n    [\"\u00e6\u00b4\u00bb\u00e5\u012c\u00a8\", \"\u043c\u0435\u0440\u043e\u043f\u0440\u0438\u044f\u0442\u0438\u0435\"],\n    [\"\u00e5\u012b\u012f\u00e4\u00b8\u0122\u00e5\u00a4\u00a9\", \"\u043d\u0430\u043a\u0430\u043d\u0443\u043d\u0435\"],\n    [\"\u00ef\u00bc\u012e\", null],\n    [\"H\", null],\n    [\"MX\", null],\n    [\"-\", null],\n    [\"1\", null],\n    [\" \u00e6\", null],\n    [\"\u012e\", null],\n    [\"\u0129\", null],\n    [\"\u00e6\u012e\u00a5\", null],\n    [\"\u00e5\u00ae\u013a\", \"\u043a\u043e\u043c\u0430\u043d\u0434\u0438\u0440\"],\n    [\"\u00e5\u013e\u00a8\", null],\n    [\"\u00e5\u0132\u0133\u00e4\u00b8\u012c\", \"\u043e\u0442\u043f\u0440\u0430\u0432\u0438\u0442\u044c\"],\n    [\"\u00e7\u00ba\u00a7\", null],\n    [\"\u00e6\u00b5\u00b7\u00e5\u0128\u013d\", \"\u043c\u043e\u0440\u0441\u043a\u0430\u044f\"],\n    [\"\u00e9\u013b\u0128\", \"\u043f\u0435\u0445\u043e\u0442\u0430\"],\n    [\"\u00e6\u012a\u013a\u00e9\u013a\u0141\", null],\n    [\"\u00e9\u00a3\u0140\u00e8\u00a1\u012e\", \"\u0430\u0432\u0438\u0430\u0446\u0438\u044f\"],\n    [\"\u00e5\u012b\u00af\", null],\n    [\"\u00e5\u0131\u00b8\u00e4\u00bb\u00a4\", null],\n    [\"\u00e5\u0131\u0133\u00e9\u0122\u0123\", \"\u043e\u0442\u043f\u0440\u0430\u0432\u0438\u0442\u044c\"],\n    [\"\u00e7\u013c\u0126\", null],\n    [\"\u00e7\u0136\u00b5\u00e5\u0143\u0132\u00e9\u0124\u00ae\u00e4\u00bb\u00b6\", \"\u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u043d\u043e\u0435 \u043f\u0438\u0441\u044c\u043c\u043e\"],\n    [\"\u00e4\u00b8\u0143\", null],\n    [\"\u00e8\u00a1\u00a8\u00e7\u00a4\u00ba\", \"\u0432\u044b\u0440\u0430\u0437\u0438\u0442\u044c\"],\n    [\"\u00e2\u0122\u013e\", null],\n    [\"\u00e6\u012a\u0133\", \"\u044f\"],\n    [\"\u00e5\u0131\u00aa\u00e6\u0125\u00b3\", \"\u0442\u043e\u043b\u044c\u043a\u043e\"],\n    [\"\u00e7\u00a1\u00ae\u00e8\u00ae\u00a4\", \"\u043f\u043e\u0434\u0442\u0432\u0435\u0440\u0434\u0438\u0442\u044c\"],\n    [\"\u00e6\u0124\u00a8\", \"\u0432\u044b\"],\n    [\"\u00e7\u0141\u00a5\u00e6\u013b\u0135\", \"\u0437\u043d\u0430\u0442\u044c\"],\n    [\"\u00ef\u00bc\u012e\", null],\n    [\"\u00e5\u013d\u0142\u00e4\u00b8\u00ba\", \"\u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e\"],\n    [\"\u00e8\u00bf\u013b\", \"\u044d\u0442\u043e\"],\n    [\"\u00e6\u0143\u00a4\u00e6\u00ac\u00a1\u00e6\u00b4\u00bb\u00e5\u012c\u00a8\", \"\u043c\u0435\u0440\u043e\u043f\u0440\u0438\u044f\u0442\u0438\u0435\"],\n    [\"\u00e5\u0131\u0139\u00e5\u012a\u00b0\", \"\u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c\"],\n    [\"\u00e9\u00ab\u013a\u00e5\u00ba\u00a6\u00e9\u0129\u012f\u00e8\u00a7\u0128\", \"\u0431\u043e\u043b\u044c\u0448\u043e\u0435 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435\"],\n    [\"\u00e3\u0122\u0124\u00e2\u0122\u013f\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to English.\n\n---=== TEXT ===---\n\n\u201c\u56db\u5ddd\u9020\u201d\u65b0\u80fd\u6e90\u6c7d\u8f66\u9006\u52bf\u4e0a\u884c\u8d22\u653f\u8865\u8d34\u529b\u5ea6\u51cf\u5f31\u3001\u5145\u7535\u8bbe\u65bd\u666e\u53ca\u7387\u4e0d\u9ad8\u3001\u4e09\u5143\u9502\u7535\u6c60\u6682\u505c\u5728\u65b0\u80fd\u6e90\u5ba2\u8f66\u4e0a\u4f7f\u7528\uff0c\u9762\u5bf9\u56fd\u5185\u65b0\u80fd\u6e90\u6c7d\u8f66\u5e02\u573a\u5f53\u524d\u7684\u4e0a\u884c\u538b\u529b\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00e2\u0122\u013e', '\u00e5\u013d\u013d\u00e5\u00b7\u013f', '\u00e9\u0122\u0142', '\u00e2\u0122\u013f', '\u00e6\u0138\u00b0\u00e8\u0125\u00bd\u00e6\u00ba\u0132', '\u00e6\u00b1\u00bd\u00e8\u00bd\u00a6', '\u00e9\u0122\u0128\u00e5\u012c\u00bf', '\u00e4\u00b8\u012c', '\u00e8\u00a1\u012e', '\u00e8\u00b4\u00a2\u00e6\u0136\u00bf', '\u00e8\u00a1\u00a5\u00e8\u00b4\u00b4', '\u00e5\u012c\u013d\u00e5\u00ba\u00a6', '\u00e5\u0129\u0131\u00e5\u00bc\u00b1', '\u00e3\u0122\u0123', '\u00e5\u0127\u0127\u00e7\u0136\u00b5', '\u00e8\u00ae\u00be\u00e6\u0138\u00bd', '\u00e6\u013b\u00ae\u00e5\u0131\u012c', '\u00e7\u0130\u0129', '\u00e4\u00b8\u012f\u00e9\u00ab\u013a', '\u00e3\u0122\u0123', '\u00e4\u00b8\u012b', '\u00e5\u0127\u0125', '\u00e9\u0136\u0124\u00e7\u0136\u00b5\u00e6\u00b1\u0142', '\u00e6\u013c\u0124\u00e5\u0123\u013e', '\u00e5\u013e\u00a8', '\u00e6\u0138\u00b0\u00e8\u0125\u00bd\u00e6\u00ba\u0132', '\u00e5\u00ae\u00a2\u00e8\u00bd\u00a6', '\u00e4\u00b8\u012c', '\u00e4\u00bd\u00bf\u00e7\u0136\u00a8', '\u00ef\u00bc\u012e', '\u00e9\u013f\u00a2\u00e5\u00af\u00b9', '\u00e5\u013d\u00bd\u00e5\u0128\u0127', '\u00e6\u0138\u00b0\u00e8\u0125\u00bd\u00e6\u00ba\u0132', '\u00e6\u00b1\u00bd\u00e8\u00bd\u00a6', '\u00e5\u00b8\u0124\u00e5\u013e\u00ba', '\u00e5\u00bd\u0135\u00e5\u012b\u012f', '\u00e7\u013c\u0126', '\u00e4\u00b8\u012c', '\u00e8\u00a1\u012e', '\u00e5\u0130\u012d\u00e5\u012c\u013d']"}], "[\n[\"\u00e2\u0122\u013e\", null],\n[\"\u00e5\u013d\u013d\u00e5\u00b7\u013f\", \"Sichuan\"],\n[\"\u00e9\u0122\u0142\", null],\n[\"\u00e2\u0122\u013f\", null],\n[\"\u00e6\u0138\u00b0\u00e8\u0125\u00bd\u00e6\u00ba\u0132\", \"new energy\"],\n[\"\u00e6\u00b1\u00bd\u00e8\u00bd\u00a6\", \"vehicles\"],\n[\"\u00e9\u0122\u0128\u00e5\u012c\u00bf\", \"counter-trend\"],\n[\"\u00e4\u00b8\u012c\", null],\n[\"\u00e8\u00a1\u012e\", \"rise\"],\n[\"\u00e8\u00b4\u00a2\u00e6\u0136\u00bf\", \"financial\"],\n[\"\u00e8\u00a1\u00a5\u00e8\u00b4\u00b4\", \"subsidies\"],\n[\"\u00e5\u012c\u013d\u00e5\u00ba\u00a6\", \"intensity\"],\n[\"\u00e5\u0129\u0131\u00e5\u00bc\u00b1\", \"weakened\"],\n[\"\u00e3\u0122\u0123\", null],\n[\"\u00e5\u0127\u0127\u00e7\u0136\u00b5\", \"charging\"],\n[\"\u00e8\u00ae\u00be\u00e6\u0138\u00bd\", \"facilities\"],\n[\"\u00e6\u013b\u00ae\u00e5\u0131\u012c\", \"popularity\"],\n[\"\u00e7\u0130\u0129\", null],\n[\"\u00e4\u00b8\u012f\u00e9\u00ab\u013a\", \"not high\"],\n[\"\u00e3\u0122\u0123\", null],\n[\"\u00e4\u00b8\u012b\", \"three\"],\n[\"\u00e5\u0127\u0125\", null],\n[\"\u00e9\u0136\u0124\u00e7\u0136\u00b5\u00e6\u00b1\u0142\", \"lithium batteries\"],\n[\"\u00e6\u013c\u0124\u00e5\u0123\u013e\", \"suspended\"],\n[\"\u00e5\u013e\u00a8\", null],\n[\"\u00e6\u0138\u00b0\u00e8\u0125\u00bd\u00e6\u00ba\u0132\", \"new energy\"],\n[\"\u00e5\u00ae\u00a2\u00e8\u00bd\u00a6\", \"buses\"],\n[\"\u00e4\u00b8\u012c\", null],\n[\"\u00e4\u00bd\u00bf\u00e7\u0136\u00a8\", \"use\"],\n[\"\u00ef\u00bc\u012e\", null],\n[\"\u00e9\u013f\u00a2\u00e5\u00af\u00b9\", \"facing\"],\n[\"\u00e5\u013d\u00bd\u00e5\u0128\u0127\", \"domestic\"],\n[\"\u00e6\u0138\u00b0\u00e8\u0125\u00bd\u00e6\u00ba\u0132\", \"new energy\"],\n[\"\u00e6\u00b1\u00bd\u00e8\u00bd\u00a6\", \"vehicle\"],\n[\"\u00e5\u00b8\u0124\u00e5\u013e\u00ba\", \"market\"],\n[\"\u00e5\u00bd\u0135\u00e5\u012b\u012f\", \"current\"],\n[\"\u00e7\u013c\u0126\", null],\n[\"\u00e4\u00b8\u012c\", null],\n[\"\u00e8\u00a1\u012e\", \"upward\"],\n[\"\u00e5\u0130\u012d\u00e5\u012c\u013d\", \"pressure\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Kazakh.\n\n---=== TEXT ===---\n\n\u201c\u56db\u5ddd\u9020\u201d\u65b0\u80fd\u6e90\u6c7d\u8f66\u9006\u52bf\u4e0a\u884c\u8d22\u653f\u8865\u8d34\u529b\u5ea6\u51cf\u5f31\u3001\u5145\u7535\u8bbe\u65bd\u666e\u53ca\u7387\u4e0d\u9ad8\u3001\u4e09\u5143\u9502\u7535\u6c60\u6682\u505c\u5728\u65b0\u80fd\u6e90\u5ba2\u8f66\u4e0a\u4f7f\u7528\uff0c\u9762\u5bf9\u56fd\u5185\u65b0\u80fd\u6e90\u6c7d\u8f66\u5e02\u573a\u5f53\u524d\u7684\u4e0a\u884c\u538b\u529b\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00e2\u0122\u013e', '\u00e5\u013d\u013d\u00e5\u00b7\u013f', '\u00e9\u0122\u0142', '\u00e2\u0122\u013f', '\u00e6\u0138\u00b0\u00e8\u0125\u00bd\u00e6\u00ba\u0132', '\u00e6\u00b1\u00bd\u00e8\u00bd\u00a6', '\u00e9\u0122\u0128\u00e5\u012c\u00bf', '\u00e4\u00b8\u012c', '\u00e8\u00a1\u012e', '\u00e8\u00b4\u00a2\u00e6\u0136\u00bf', '\u00e8\u00a1\u00a5\u00e8\u00b4\u00b4', '\u00e5\u012c\u013d\u00e5\u00ba\u00a6', '\u00e5\u0129\u0131\u00e5\u00bc\u00b1', '\u00e3\u0122\u0123', '\u00e5\u0127\u0127\u00e7\u0136\u00b5', '\u00e8\u00ae\u00be\u00e6\u0138\u00bd', '\u00e6\u013b\u00ae\u00e5\u0131\u012c', '\u00e7\u0130\u0129', '\u00e4\u00b8\u012f\u00e9\u00ab\u013a', '\u00e3\u0122\u0123', '\u00e4\u00b8\u012b', '\u00e5\u0127\u0125', '\u00e9\u0136\u0124\u00e7\u0136\u00b5\u00e6\u00b1\u0142', '\u00e6\u013c\u0124\u00e5\u0123\u013e', '\u00e5\u013e\u00a8', '\u00e6\u0138\u00b0\u00e8\u0125\u00bd\u00e6\u00ba\u0132', '\u00e5\u00ae\u00a2\u00e8\u00bd\u00a6', '\u00e4\u00b8\u012c', '\u00e4\u00bd\u00bf\u00e7\u0136\u00a8', '\u00ef\u00bc\u012e', '\u00e9\u013f\u00a2\u00e5\u00af\u00b9', '\u00e5\u013d\u00bd\u00e5\u0128\u0127', '\u00e6\u0138\u00b0\u00e8\u0125\u00bd\u00e6\u00ba\u0132', '\u00e6\u00b1\u00bd\u00e8\u00bd\u00a6', '\u00e5\u00b8\u0124\u00e5\u013e\u00ba', '\u00e5\u00bd\u0135\u00e5\u012b\u012f', '\u00e7\u013c\u0126', '\u00e4\u00b8\u012c', '\u00e8\u00a1\u012e', '\u00e5\u0130\u012d\u00e5\u012c\u013d']"}], "[\n[\"\u00e2\u0122\u013e\", null],\n[\"\u00e5\u013d\u013d\u00e5\u00b7\u013f\", \"\u0421\u044b\u0447\u0443\u0430\u043d\u044c\"],\n[\"\u00e9\u0122\u0142\", null],\n[\"\u00e2\u0122\u013f\", null],\n[\"\u00e6\u0138\u00b0\u00e8\u0125\u00bd\u00e6\u00ba\u0132\", \"\u0436\u0430\u04a3\u0430 \u044d\u043d\u0435\u0440\u0433\u0438\u044f\"],\n[\"\u00e6\u00b1\u00bd\u00e8\u00bd\u00a6\", \"\u043a\u04e9\u043b\u0456\u043a\"],\n[\"\u00e9\u0122\u0128\u00e5\u012c\u00bf\", \"\u049b\u0430\u0440\u0441\u044b\"],\n[\"\u00e4\u00b8\u012c\", null],\n[\"\u00e8\u00a1\u012e\", \"\u0430\u0493\u044b\u043d\"],\n[\"\u00e8\u00b4\u00a2\u00e6\u0136\u00bf\", \"\u049b\u0430\u0440\u0436\u044b\u043b\u044b\u049b\"],\n[\"\u00e8\u00a1\u00a5\u00e8\u00b4\u00b4\", \"\u0441\u0443\u0431\u0441\u0438\u0434\u0438\u044f\"],\n[\"\u00e5\u012c\u013d\u00e5\u00ba\u00a6\", \"\u0434\u0435\u04a3\u0433\u0435\u0439\u0456\"],\n[\"\u00e5\u0129\u0131\u00e5\u00bc\u00b1\", \"\u04d9\u043b\u0441\u0456\u0440\u0435\u0443\"],\n[\"\u00e3\u0122\u0123\", null],\n[\"\u00e5\u0127\u0127\u00e7\u0136\u00b5\", \"\u0437\u0430\u0440\u044f\u0434\u0442\u0430\u0443\"],\n[\"\u00e8\u00ae\u00be\u00e6\u0138\u00bd\", \"\u049b\u04b1\u0440\u044b\u043b\u0493\u044b\u043b\u0430\u0440\u044b\"],\n[\"\u00e6\u013b\u00ae\u00e5\u0131\u012c\", \"\u0442\u0430\u0440\u0430\u043b\u0443\"],\n[\"\u00e7\u0130\u0129\", null],\n[\"\u00e4\u00b8\u012f\u00e9\u00ab\u013a\", \"\u0436\u043e\u0493\u0430\u0440\u044b \u0435\u043c\u0435\u0441\"],\n[\"\u00e3\u0122\u0123\", null],\n[\"\u00e4\u00b8\u012b\", null],\n[\"\u00e5\u0127\u0125\", \"\u04af\u0448\"],\n[\"\u00e9\u0136\u0124\u00e7\u0136\u00b5\u00e6\u00b1\u0142\", \"\u043b\u0438\u0442\u0438\u0439\"],\n[\"\u00e6\u013c\u0124\u00e5\u0123\u013e\", \"\u0431\u0430\u0442\u0430\u0440\u0435\u044f\"],\n[\"\u00e5\u013e\u00a8\", null],\n[\"\u00e6\u0138\u00b0\u00e8\u0125\u00bd\u00e6\u00ba\u0132\", \"\u0436\u0430\u04a3\u0430 \u044d\u043d\u0435\u0440\u0433\u0438\u044f\"],\n[\"\u00e5\u00ae\u00a2\u00e8\u00bd\u00a6\", \"\u0430\u0432\u0442\u043e\u0431\u0443\u0441\"],\n[\"\u00e4\u00b8\u012c\", null],\n[\"\u00e4\u00bd\u00bf\u00e7\u0136\u00a8\", \"\u049b\u043e\u043b\u0434\u0430\u043d\u0443\"],\n[\"\u00ef\u00bc\u012e\", null],\n[\"\u00e9\u013f\u00a2\u00e5\u00af\u00b9\", \"\u049b\u0430\u0440\u0441\u044b\"],\n[\"\u00e5\u013d\u00bd\u00e5\u0128\u0127\", \"\u0456\u0448\u043a\u0456\"],\n[\"\u00e6\u0138\u00b0\u00e8\u0125\u00bd\u00e6\u00ba\u0132\", \"\u0436\u0430\u04a3\u0430 \u044d\u043d\u0435\u0440\u0433\u0438\u044f\"],\n[\"\u00e6\u00b1\u00bd\u00e8\u00bd\u00a6\", \"\u043a\u04e9\u043b\u0456\u043a\"],\n[\"\u00e5\u00b8\u0124\u00e5\u013e\u00ba\", \"\u043d\u0430\u0440\u044b\u049b\"],\n[\"\u00e5\u00bd\u0135\u00e5\u012b\u012f\", \"\u049b\u0430\u0437\u0456\u0440\u0433\u0456\"],\n[\"\u00e7\u013c\u0126\", null],\n[\"\u00e4\u00b8\u012c\", null],\n[\"\u00e8\u00a1\u012e\", \"\u0430\u0493\u044b\u043d\"],\n[\"\u00e5\u0130\u012d\u00e5\u012c\u013d\", \"\u049b\u044b\u0441\u044b\u043c\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Chinese to Russian.\n\n---=== TEXT ===---\n\n\u201c\u56db\u5ddd\u9020\u201d\u65b0\u80fd\u6e90\u6c7d\u8f66\u9006\u52bf\u4e0a\u884c\u8d22\u653f\u8865\u8d34\u529b\u5ea6\u51cf\u5f31\u3001\u5145\u7535\u8bbe\u65bd\u666e\u53ca\u7387\u4e0d\u9ad8\u3001\u4e09\u5143\u9502\u7535\u6c60\u6682\u505c\u5728\u65b0\u80fd\u6e90\u5ba2\u8f66\u4e0a\u4f7f\u7528\uff0c\u9762\u5bf9\u56fd\u5185\u65b0\u80fd\u6e90\u6c7d\u8f66\u5e02\u573a\u5f53\u524d\u7684\u4e0a\u884c\u538b\u529b\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00e2\u0122\u013e', '\u00e5\u013d\u013d\u00e5\u00b7\u013f', '\u00e9\u0122\u0142', '\u00e2\u0122\u013f', '\u00e6\u0138\u00b0\u00e8\u0125\u00bd\u00e6\u00ba\u0132', '\u00e6\u00b1\u00bd\u00e8\u00bd\u00a6', '\u00e9\u0122\u0128\u00e5\u012c\u00bf', '\u00e4\u00b8\u012c', '\u00e8\u00a1\u012e', '\u00e8\u00b4\u00a2\u00e6\u0136\u00bf', '\u00e8\u00a1\u00a5\u00e8\u00b4\u00b4', '\u00e5\u012c\u013d\u00e5\u00ba\u00a6', '\u00e5\u0129\u0131\u00e5\u00bc\u00b1', '\u00e3\u0122\u0123', '\u00e5\u0127\u0127\u00e7\u0136\u00b5', '\u00e8\u00ae\u00be\u00e6\u0138\u00bd', '\u00e6\u013b\u00ae\u00e5\u0131\u012c', '\u00e7\u0130\u0129', '\u00e4\u00b8\u012f\u00e9\u00ab\u013a', '\u00e3\u0122\u0123', '\u00e4\u00b8\u012b', '\u00e5\u0127\u0125', '\u00e9\u0136\u0124\u00e7\u0136\u00b5\u00e6\u00b1\u0142', '\u00e6\u013c\u0124\u00e5\u0123\u013e', '\u00e5\u013e\u00a8', '\u00e6\u0138\u00b0\u00e8\u0125\u00bd\u00e6\u00ba\u0132', '\u00e5\u00ae\u00a2\u00e8\u00bd\u00a6', '\u00e4\u00b8\u012c', '\u00e4\u00bd\u00bf\u00e7\u0136\u00a8', '\u00ef\u00bc\u012e', '\u00e9\u013f\u00a2\u00e5\u00af\u00b9', '\u00e5\u013d\u00bd\u00e5\u0128\u0127', '\u00e6\u0138\u00b0\u00e8\u0125\u00bd\u00e6\u00ba\u0132', '\u00e6\u00b1\u00bd\u00e8\u00bd\u00a6', '\u00e5\u00b8\u0124\u00e5\u013e\u00ba', '\u00e5\u00bd\u0135\u00e5\u012b\u012f', '\u00e7\u013c\u0126', '\u00e4\u00b8\u012c', '\u00e8\u00a1\u012e', '\u00e5\u0130\u012d\u00e5\u012c\u013d']"}], "[\n[\"\u00e2\u0122\u013e\", null],\n[\"\u00e5\u013d\u013d\u00e5\u00b7\u013f\", \"\u0421\u044b\u0447\u0443\u0430\u043d\u044c\"],\n[\"\u00e9\u0122\u0142\", null],\n[\"\u00e2\u0122\u013f\", null],\n[\"\u00e6\u0138\u00b0\u00e8\u0125\u00bd\u00e6\u00ba\u0132\", \"\u043d\u043e\u0432\u043e\u044d\u043d\u0435\u0440\u0433\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439\"],\n[\"\u00e6\u00b1\u00bd\u00e8\u00bd\u00a6\", \"\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044c\"],\n[\"\u00e9\u0122\u0128\u00e5\u012c\u00bf\", \"\u043f\u0440\u043e\u0442\u0438\u0432\u043e\u0441\u0442\u043e\u044f\u0442\u044c\"],\n[\"\u00e4\u00b8\u012c\", null],\n[\"\u00e8\u00a1\u012e\", \"\u0442\u0435\u043d\u0434\u0435\u043d\u0446\u0438\u044f\"],\n[\"\u00e8\u00b4\u00a2\u00e6\u0136\u00bf\", \"\u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u044b\u0439\"],\n[\"\u00e8\u00a1\u00a5\u00e8\u00b4\u00b4\", \"\u0441\u0443\u0431\u0441\u0438\u0434\u0438\u044f\"],\n[\"\u00e5\u012c\u013d\u00e5\u00ba\u00a6\", \"\u0441\u0442\u0435\u043f\u0435\u043d\u044c\"],\n[\"\u00e5\u0129\u0131\u00e5\u00bc\u00b1\", \"\u043e\u0441\u043b\u0430\u0431\u043b\u0435\u043d\u0438\u0435\"],\n[\"\u00e3\u0122\u0123\", null],\n[\"\u00e5\u0127\u0127\u00e7\u0136\u00b5\", \"\u0437\u0430\u0440\u044f\u0434\u043a\u0430\"],\n[\"\u00e8\u00ae\u00be\u00e6\u0138\u00bd\", \"\u0438\u043d\u0444\u0440\u0430\u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430\"],\n[\"\u00e6\u013b\u00ae\u00e5\u0131\u012c\", \"\u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u0438\u0435\"],\n[\"\u00e7\u0130\u0129\", null],\n[\"\u00e4\u00b8\u012f\u00e9\u00ab\u013a\", \"\u043d\u0435\u0432\u044b\u0441\u043e\u043a\u0438\u0439\"],\n[\"\u00e3\u0122\u0123\", null],\n[\"\u00e4\u00b8\u012b\", null],\n[\"\u00e5\u0127\u0125\", \"\u0442\u0440\u043e\u0439\u043d\u043e\u0439\"],\n[\"\u00e9\u0136\u0124\u00e7\u0136\u00b5\u00e6\u00b1\u0142\", \"\u043b\u0438\u0442\u0438\u0435\u0432\u044b\u0439 \u0430\u043a\u043a\u0443\u043c\u0443\u043b\u044f\u0442\u043e\u0440\"],\n[\"\u00e6\u013c\u0124\u00e5\u0123\u013e\", \"\u043f\u0440\u0438\u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u044c\"],\n[\"\u00e5\u013e\u00a8\", null],\n[\"\u00e6\u0138\u00b0\u00e8\u0125\u00bd\u00e6\u00ba\u0132\", \"\u043d\u043e\u0432\u043e\u044d\u043d\u0435\u0440\u0433\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439\"],\n[\"\u00e5\u00ae\u00a2\u00e8\u00bd\u00a6\", \"\u0430\u0432\u0442\u043e\u0431\u0443\u0441\"],\n[\"\u00e4\u00b8\u012c\", null],\n[\"\u00e4\u00bd\u00bf\u00e7\u0136\u00a8\", \"\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\"],\n[\"\u00ef\u00bc\u012e\", null],\n[\"\u00e9\u013f\u00a2\u00e5\u00af\u00b9\", \"\u0441\u0442\u0430\u043b\u043a\u0438\u0432\u0430\u0442\u044c\u0441\u044f\"],\n[\"\u00e5\u013d\u00bd\u00e5\u0128\u0127\", \"\u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0438\u0439\"],\n[\"\u00e6\u0138\u00b0\u00e8\u0125\u00bd\u00e6\u00ba\u0132\", \"\u043d\u043e\u0432\u043e\u044d\u043d\u0435\u0440\u0433\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439\"],\n[\"\u00e6\u00b1\u00bd\u00e8\u00bd\u00a6\", \"\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044c\"],\n[\"\u00e5\u00b8\u0124\u00e5\u013e\u00ba\", \"\u0440\u044b\u043d\u043e\u043a\"],\n[\"\u00e5\u00bd\u0135\u00e5\u012b\u012f\", \"\u0442\u0435\u043a\u0443\u0449\u0438\u0439\"],\n[\"\u00e7\u013c\u0126\", null],\n[\"\u00e4\u00b8\u012c\", null],\n[\"\u00e8\u00a1\u012e\", \"\u0440\u043e\u0441\u0442\"],\n[\"\u00e5\u0130\u012d\u00e5\u012c\u013d\", \"\u0434\u0430\u0432\u043b\u0435\u043d\u0438\u0435\"]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u04e8\u0437\u0456\u043d\u0456\u04a3 \u049a\u043e\u043b\u043c\u0430-\u049b\u043e\u043b \u0430\u049b\u0448\u0430 \u049b\u0430\u0440\u0493\u044b\u0441\u044b\u0430\u0442\u0442\u044b\u0436\u0430\u04a3\u0430 \u043a\u0456\u0442\u0430\u0431\u044b\u043d\u0434\u0430\u041a\u0435\u043d\u043d\u0435\u0442 \u0420\u043e\u0433\u043e\u0444\u0444 \u0441\u0430\u043b\u044b\u049b\u0442\u044b \u0442\u04e9\u043b\u0435\u0443\u0434\u0435\u043d \u0436\u0430\u043b\u0442\u0430\u0440\u0443 \u0436\u04d9\u043d\u0435 \u049b\u044b\u043b\u043c\u044b\u0441\u0442\u044b\u049b \u04d9\u0440\u0435\u043a\u0435\u0442\u043f\u0435\u043d \u043a\u04af\u0440\u0435\u0441\u0443 \u043c\u0430\u049b\u0441\u0430\u0442\u044b\u043d\u0434\u0430 \u0436\u043e\u0493\u0430\u0440\u044b \u0430\u0442\u0430\u0443\u043b\u044b \u0431\u0430\u043d\u043a\u043d\u043e\u0442\u0430\u043b\u0430\u0440\u0434\u044b \u0436\u043e\u044e\u0434\u044b \u049b\u043e\u043b\u0434\u0430\u0439\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00d3\u00a8', '\u00d0\u00b7', '\u00d1\u0138', '\u00d0\u00bd', '\u00d1\u0138', '\u00d2\u00a3', ' ', '\u00d2\u013c', '\u00d0\u00be\u00d0\u00bb', '\u00d0\u00bc\u00d0\u00b0', '-', '\u00d2\u013d', '\u00d0\u00be\u00d0\u00bb', ' \u00d0\u00b0', '\u00d2\u013d', '\u00d1\u012a\u00d0\u00b0', ' ', '\u00d2\u013d', '\u00d0\u00b0\u00d1\u0122', '\u00d2\u0135', '\u00d1\u012d', '\u00d1\u0123\u00d1\u012d', '\u00d0\u00b0\u00d1\u0124', '\u00d1\u0124\u00d1\u012d', '\u00d0\u00b6\u00d0\u00b0', '\u00d2\u00a3', '\u00d0\u00b0', ' \u00d0\u00ba', '\u00d1\u0138', '\u00d1\u0124', '\u00d0\u00b0\u00d0\u00b1', '\u00d1\u012d', '\u00d0\u00bd\u00d0\u00b4', '\u00d0\u00b0\u00d0', '\u013c', '\u00d0\u00b5\u00d0\u00bd\u00d0\u00bd', '\u00d0\u00b5\u00d1\u0124', ' \u00d0\u0142', '\u00d0\u00be\u00d0\u00b3\u00d0\u00be', '\u00d1\u0126\u00d1\u0126', ' \u00d1\u0123', '\u00d0\u00b0\u00d0\u00bb', '\u00d1\u012d', '\u00d2\u013d', '\u00d1\u0124\u00d1\u012d', ' \u00d1\u0124', '\u00d3\u00a9', '\u00d0\u00bb\u00d0\u00b5', '\u00d1\u0125\u00d0\u00b4', '\u00d0\u00b5\u00d0\u00bd', ' \u00d0\u00b6', '\u00d0\u00b0\u00d0\u00bb', '\u00d1\u0124\u00d0\u00b0\u00d1\u0122', '\u00d1\u0125', ' \u00d0\u00b6', '\u00d3\u013b', '\u00d0\u00bd\u00d0\u00b5', ' ', '\u00d2\u013d', '\u00d1\u012d', '\u00d0\u00bb', '\u00d0\u00bc\u00d1\u012d', '\u00d1\u0123\u00d1\u0124\u00d1\u012d', '\u00d2\u013d', ' ', '\u00d3\u013b', '\u00d1\u0122', '\u00d0\u00b5\u00d0\u00ba', '\u00d0\u00b5\u00d1\u0124', '\u00d0\u00bf', '\u00d0\u00b5\u00d0\u00bd', ' \u00d0\u00ba', '\u00d2\u00af', '\u00d1\u0122\u00d0\u00b5\u00d1\u0123', '\u00d1\u0125', ' \u00d0\u00bc\u00d0\u00b0', '\u00d2\u013d', '\u00d1\u0123', '\u00d0\u00b0\u00d1\u0124', '\u00d1\u012d', '\u00d0\u00bd', '\u00d0\u00b4\u00d0\u00b0', ' \u00d0\u00b6', '\u00d0\u00be', '\u00d2\u0135', '\u00d0\u00b0\u00d1\u0122', '\u00d1\u012d', ' \u00d0\u00b0', '\u00d1\u0124\u00d0\u00b0', '\u00d1\u0125', '\u00d0\u00bb\u00d1\u012d', ' \u00d0\u00b1\u00d0\u00b0\u00d0\u00bd', '\u00d0\u00ba', '\u00d0\u00bd', '\u00d0\u00be\u00d1\u0124', '\u00d0\u00b0\u00d0\u00bb', '\u00d0\u00b0\u00d1\u0122', '\u00d0\u00b4\u00d1\u012d', ' \u00d0\u00b6', '\u00d0\u00be', '\u00d1\u0130', '\u00d0\u00b4\u00d1\u012d', ' ', '\u00d2\u013d', '\u00d0\u00be\u00d0\u00bb', '\u00d0\u00b4', '\u00d0\u00b0\u00d0\u00b9\u00d0\u00b4', '\u00d1\u012d', '.']"}], "[\n[\"\u00d3\u00a8\", null],\n[\"\u00d0\u00b7\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00bd\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d2\u00a3\", null],\n[\" \", null],\n[\"\u00d2\u013c\", null],\n[\"\u00d0\u00be\u00d0\u00bb\", null],\n[\"\u00d0\u00bc\u00d0\u00b0\", null],\n[\"-\", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d0\u00be\u00d0\u00bb\", null],\n[\" \u00d0\u00b0\", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d1\u012a\u00d0\u00b0\", null],\n[\" \", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d0\u00b0\u00d1\u0122\", null],\n[\"\u00d2\u0135\", null],\n[\"\u00d1\u012d\", null],\n[\"\u00d1\u0123\u00d1\u012d\", null],\n[\"\u00d0\u00b0\u00d1\u0124\", null],\n[\"\u00d1\u0124\u00d1\u012d\", null],\n[\"\u00d0\u00b6\u00d0\u00b0\", null],\n[\"\u00d2\u00a3\", null],\n[\"\u00d0\u00b0\", null],\n[\" \u00d0\u00ba\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d1\u0124\", null],\n[\"\u00d0\u00b0\u00d0\u00b1\", null],\n[\"\u00d1\u012d\", null],\n[\"\u00d0\u00bd\u00d0\u00b4\", null],\n[\"\u00d0\u00b0\u00d0\", null],\n[\"\u013c\", null],\n[\"\u00d0\u00b5\u00d0\u00bd\u00d0\u00bd\", \"Kenneth\"],\n[\"\u00d0\u00b5\u00d1\u0124\", null],\n[\" \u00d0\u0142\", null],\n[\"\u00d0\u00be\u00d0\u00b3\u00d0\u00be\", \"Rogoff\"],\n[\"\u00d1\u0126\u00d1\u0126\", null],\n[\" \u00d1\u0123\", null],\n[\"\u00d0\u00b0\u00d0\u00bb\", \"tax\"],\n[\"\u00d1\u012d\", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d1\u0124\u00d1\u012d\", null],\n[\" \u00d1\u0124\", null],\n[\"\u00d3\u00a9\", null],\n[\"\u00d0\u00bb\u00d0\u00b5\", null],\n[\"\u00d1\u0125\u00d0\u00b4\", null],\n[\"\u00d0\u00b5\u00d0\u00bd\", null],\n[\" \u00d0\u00b6\", null],\n[\"\u00d0\u00b0\u00d0\u00bb\", \"evasion\"],\n[\"\u00d1\u0124\u00d0\u00b0\u00d1\u0122\", null],\n[\"\u00d1\u0125\", null],\n[\" \u00d0\u00b6\", null],\n[\"\u00d3\u013b\", null],\n[\"\u00d0\u00bd\u00d0\u00b5\", null],\n[\" \", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d1\u012d\", null],\n[\"\u00d0\u00bb\", null],\n[\"\u00d0\u00bc\u00d1\u012d\", null],\n[\"\u00d1\u0123\u00d1\u0124\u00d1\u012d\", null],\n[\"\u00d2\u013d\", null],\n[\" \", null],\n[\"\u00d3\u013b\", null],\n[\"\u00d1\u0122\", null],\n[\"\u00d0\u00b5\u00d0\u00ba\", \"criminal\"],\n[\"\u00d0\u00b5\u00d1\u0124\", null],\n[\"\u00d0\u00bf\", null],\n[\"\u00d0\u00b5\u00d0\u00bd\", null],\n[\" \u00d0\u00ba\", null],\n[\"\u00d2\u00af\", null],\n[\"\u00d1\u0122\u00d0\u00b5\u00d1\u0123\", \"combat\"],\n[\"\u00d1\u0125\", null],\n[\" \u00d0\u00bc\u00d0\u00b0\", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d1\u0123\", null],\n[\"\u00d0\u00b0\u00d1\u0124\", null],\n[\"\u00d1\u012d\", null],\n[\"\u00d0\u00bd\", null],\n[\"\u00d0\u00b4\u00d0\u00b0\", null],\n[\" \u00d0\u00b6\", null],\n[\"\u00d0\u00be\", null],\n[\"\u00d2\u0135\", null],\n[\"\u00d0\u00b0\u00d1\u0122\", \"high\"],\n[\"\u00d1\u012d\", null],\n[\" \u00d0\u00b0\", null],\n[\"\u00d1\u0124\u00d0\u00b0\", null],\n[\"\u00d1\u0125\", null],\n[\"\u00d0\u00bb\u00d1\u012d\", null],\n[\" \u00d0\u00b1\u00d0\u00b0\u00d0\u00bd\", \"banknotes\"],\n[\"\u00d0\u00ba\", null],\n[\"\u00d0\u00bd\", null],\n[\"\u00d0\u00be\u00d1\u0124\", null],\n[\"\u00d0\u00b0\u00d0\u00bb\", null],\n[\"\u00d0\u00b0\u00d1\u0122\", null],\n[\"\u00d0\u00b4\u00d1\u012d\", null],\n[\" \u00d0\u00b6\", null],\n[\"\u00d0\u00be\", null],\n[\"\u00d1\u0130\", null],\n[\"\u00d0\u00b4\u00d1\u012d\", \"eliminate\"],\n[\" \", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d0\u00be\u00d0\u00bb\", null],\n[\"\u00d0\u00b4\", null],\n[\"\u00d0\u00b0\u00d0\u00b9\u00d0\u00b4\", null],\n[\"\u00d1\u012d\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u04e8\u0437\u0456\u043d\u0456\u04a3 \u049a\u043e\u043b\u043c\u0430-\u049b\u043e\u043b \u0430\u049b\u0448\u0430 \u049b\u0430\u0440\u0493\u044b\u0441\u044b\u0430\u0442\u0442\u044b\u0436\u0430\u04a3\u0430 \u043a\u0456\u0442\u0430\u0431\u044b\u043d\u0434\u0430\u041a\u0435\u043d\u043d\u0435\u0442 \u0420\u043e\u0433\u043e\u0444\u0444 \u0441\u0430\u043b\u044b\u049b\u0442\u044b \u0442\u04e9\u043b\u0435\u0443\u0434\u0435\u043d \u0436\u0430\u043b\u0442\u0430\u0440\u0443 \u0436\u04d9\u043d\u0435 \u049b\u044b\u043b\u043c\u044b\u0441\u0442\u044b\u049b \u04d9\u0440\u0435\u043a\u0435\u0442\u043f\u0435\u043d \u043a\u04af\u0440\u0435\u0441\u0443 \u043c\u0430\u049b\u0441\u0430\u0442\u044b\u043d\u0434\u0430 \u0436\u043e\u0493\u0430\u0440\u044b \u0430\u0442\u0430\u0443\u043b\u044b \u0431\u0430\u043d\u043a\u043d\u043e\u0442\u0430\u043b\u0430\u0440\u0434\u044b \u0436\u043e\u044e\u0434\u044b \u049b\u043e\u043b\u0434\u0430\u0439\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00d3\u00a8', '\u00d0\u00b7', '\u00d1\u0138', '\u00d0\u00bd', '\u00d1\u0138', '\u00d2\u00a3', ' ', '\u00d2\u013c', '\u00d0\u00be\u00d0\u00bb', '\u00d0\u00bc\u00d0\u00b0', '-', '\u00d2\u013d', '\u00d0\u00be\u00d0\u00bb', ' \u00d0\u00b0', '\u00d2\u013d', '\u00d1\u012a\u00d0\u00b0', ' ', '\u00d2\u013d', '\u00d0\u00b0\u00d1\u0122', '\u00d2\u0135', '\u00d1\u012d', '\u00d1\u0123\u00d1\u012d', '\u00d0\u00b0\u00d1\u0124', '\u00d1\u0124\u00d1\u012d', '\u00d0\u00b6\u00d0\u00b0', '\u00d2\u00a3', '\u00d0\u00b0', ' \u00d0\u00ba', '\u00d1\u0138', '\u00d1\u0124', '\u00d0\u00b0\u00d0\u00b1', '\u00d1\u012d', '\u00d0\u00bd\u00d0\u00b4', '\u00d0\u00b0\u00d0', '\u013c', '\u00d0\u00b5\u00d0\u00bd\u00d0\u00bd', '\u00d0\u00b5\u00d1\u0124', ' \u00d0\u0142', '\u00d0\u00be\u00d0\u00b3\u00d0\u00be', '\u00d1\u0126\u00d1\u0126', ' \u00d1\u0123', '\u00d0\u00b0\u00d0\u00bb', '\u00d1\u012d', '\u00d2\u013d', '\u00d1\u0124\u00d1\u012d', ' \u00d1\u0124', '\u00d3\u00a9', '\u00d0\u00bb\u00d0\u00b5', '\u00d1\u0125\u00d0\u00b4', '\u00d0\u00b5\u00d0\u00bd', ' \u00d0\u00b6', '\u00d0\u00b0\u00d0\u00bb', '\u00d1\u0124\u00d0\u00b0\u00d1\u0122', '\u00d1\u0125', ' \u00d0\u00b6', '\u00d3\u013b', '\u00d0\u00bd\u00d0\u00b5', ' ', '\u00d2\u013d', '\u00d1\u012d', '\u00d0\u00bb', '\u00d0\u00bc\u00d1\u012d', '\u00d1\u0123\u00d1\u0124\u00d1\u012d', '\u00d2\u013d', ' ', '\u00d3\u013b', '\u00d1\u0122', '\u00d0\u00b5\u00d0\u00ba', '\u00d0\u00b5\u00d1\u0124', '\u00d0\u00bf', '\u00d0\u00b5\u00d0\u00bd', ' \u00d0\u00ba', '\u00d2\u00af', '\u00d1\u0122\u00d0\u00b5\u00d1\u0123', '\u00d1\u0125', ' \u00d0\u00bc\u00d0\u00b0', '\u00d2\u013d', '\u00d1\u0123', '\u00d0\u00b0\u00d1\u0124', '\u00d1\u012d', '\u00d0\u00bd', '\u00d0\u00b4\u00d0\u00b0', ' \u00d0\u00b6', '\u00d0\u00be', '\u00d2\u0135', '\u00d0\u00b0\u00d1\u0122', '\u00d1\u012d', ' \u00d0\u00b0', '\u00d1\u0124\u00d0\u00b0', '\u00d1\u0125', '\u00d0\u00bb\u00d1\u012d', ' \u00d0\u00b1\u00d0\u00b0\u00d0\u00bd', '\u00d0\u00ba', '\u00d0\u00bd', '\u00d0\u00be\u00d1\u0124', '\u00d0\u00b0\u00d0\u00bb', '\u00d0\u00b0\u00d1\u0122', '\u00d0\u00b4\u00d1\u012d', ' \u00d0\u00b6', '\u00d0\u00be', '\u00d1\u0130', '\u00d0\u00b4\u00d1\u012d', ' ', '\u00d2\u013d', '\u00d0\u00be\u00d0\u00bb', '\u00d0\u00b4', '\u00d0\u00b0\u00d0\u00b9\u00d0\u00b4', '\u00d1\u012d', '.']"}], "[\n[\"\u00d3\u00a8\", null],\n[\"\u00d0\u00b7\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00bd\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d2\u00a3\", null],\n[\" \", null],\n[\"\u00d2\u013c\", null],\n[\"\u00d0\u00be\u00d0\u00bb\", null],\n[\"\u00d0\u00bc\u00d0\u00b0\", null],\n[\"-\", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d0\u00be\u00d0\u00bb\", null],\n[\" \u00d0\u00b0\", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d1\u012a\u00d0\u00b0\", null],\n[\" \", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d0\u00b0\u00d1\u0122\", null],\n[\"\u00d2\u0135\", null],\n[\"\u00d1\u012d\", null],\n[\"\u00d1\u0123\u00d1\u012d\", null],\n[\"\u00d0\u00b0\u00d1\u0124\", null],\n[\"\u00d1\u0124\u00d1\u012d\", null],\n[\"\u00d0\u00b6\u00d0\u00b0\", null],\n[\"\u00d2\u00a3\", null],\n[\"\u00d0\u00b0\", null],\n[\" \u00d0\u00ba\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d1\u0124\", null],\n[\"\u00d0\u00b0\u00d0\u00b1\", null],\n[\"\u00d1\u012d\", null],\n[\"\u00d0\u00bd\u00d0\u00b4\", null],\n[\"\u00d0\u00b0\u00d0\", null],\n[\"\u013c\", null],\n[\"\u00d0\u00b5\u00d0\u00bd\u00d0\u00bd\", null],\n[\"\u00d0\u00b5\u00d1\u0124\", null],\n[\" \u00d0\u0142\", null],\n[\"\u00d0\u00be\u00d0\u00b3\u00d0\u00be\", null],\n[\"\u00d1\u0126\u00d1\u0126\", null],\n[\" \u00d1\u0123\", null],\n[\"\u00d0\u00b0\u00d0\u00bb\", null],\n[\"\u00d1\u012d\", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d1\u0124\u00d1\u012d\", null],\n[\" \u00d1\u0124\", null],\n[\"\u00d3\u00a9\", null],\n[\"\u00d0\u00bb\u00d0\u00b5\", null],\n[\"\u00d1\u0125\u00d0\u00b4\", null],\n[\"\u00d0\u00b5\u00d0\u00bd\", null],\n[\" \u00d0\u00b6\", null],\n[\"\u00d0\u00b0\u00d0\u00bb\", null],\n[\"\u00d1\u0124\u00d0\u00b0\u00d1\u0122\", null],\n[\"\u00d1\u0125\", null],\n[\" \u00d0\u00b6\", null],\n[\"\u00d3\u013b\", null],\n[\"\u00d0\u00bd\u00d0\u00b5\", null],\n[\" \", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d1\u012d\", null],\n[\"\u00d0\u00bb\", null],\n[\"\u00d0\u00bc\u00d1\u012d\", null],\n[\"\u00d1\u0123\u00d1\u0124\u00d1\u012d\", null],\n[\"\u00d2\u013d\", null],\n[\" \", null],\n[\"\u00d3\u013b\", null],\n[\"\u00d1\u0122\", null],\n[\"\u00d0\u00b5\u00d0\u00ba\", null],\n[\"\u00d0\u00b5\u00d1\u0124\", null],\n[\"\u00d0\u00bf\", null],\n[\"\u00d0\u00b5\u00d0\u00bd\", null],\n[\" \u00d0\u00ba\", null],\n[\"\u00d2\u00af\", null],\n[\"\u00d1\u0122\u00d0\u00b5\u00d1\u0123\", null],\n[\"\u00d1\u0125\", null],\n[\" \u00d0\u00bc\u00d0\u00b0\", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d1\u0123\", null],\n[\"\u00d0\u00b0\u00d1\u0124\", null],\n[\"\u00d1\u012d\", null],\n[\"\u00d0\u00bd\", null],\n[\"\u00d0\u00b4\u00d0\u00b0\", null],\n[\" \u00d0\u00b6\", null],\n[\"\u00d0\u00be\", null],\n[\"\u00d2\u0135\", null],\n[\"\u00d0\u00b0\u00d1\u0122\", null],\n[\"\u00d1\u012d\", null],\n[\" \u00d0\u00b0\", null],\n[\"\u00d1\u0124\u00d0\u00b0\", null],\n[\"\u00d1\u0125\", null],\n[\"\u00d0\u00bb\u00d1\u012d\", null],\n[\" \u00d0\u00b1\u00d0\u00b0\u00d0\u00bd\", \"\u00e9\u00b6\u00e8\u00a1\u00e7\u00ba\u010c\"],\n[\"\u00d0\u00ba\", null],\n[\"\u00d0\u00bd\", null],\n[\"\u00d0\u00be\u00d1\u0124\", null],\n[\"\u00d0\u00b0\u00d0\u00bb\", null],\n[\"\u00d0\u00b0\u00d1\u0122\", null],\n[\"\u00d0\u00b4\u00d1\u012d\", null],\n[\" \u00d0\u00b6\", null],\n[\"\u00d0\u00be\", null],\n[\"\u00d1\u0130\", null],\n[\"\u00d0\u00b4\u00d1\u012d\", null],\n[\" \", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d0\u00be\u00d0\u00bb\", null],\n[\"\u00d0\u00b4\", null],\n[\"\u00d0\u00b0\u00d0\u00b9\u00d0\u00b4\", null],\n[\"\u00d1\u012d\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u04e8\u0437\u0456\u043d\u0456\u04a3 \u049a\u043e\u043b\u043c\u0430-\u049b\u043e\u043b \u0430\u049b\u0448\u0430 \u049b\u0430\u0440\u0493\u044b\u0441\u044b\u0430\u0442\u0442\u044b\u0436\u0430\u04a3\u0430 \u043a\u0456\u0442\u0430\u0431\u044b\u043d\u0434\u0430\u041a\u0435\u043d\u043d\u0435\u0442 \u0420\u043e\u0433\u043e\u0444\u0444 \u0441\u0430\u043b\u044b\u049b\u0442\u044b \u0442\u04e9\u043b\u0435\u0443\u0434\u0435\u043d \u0436\u0430\u043b\u0442\u0430\u0440\u0443 \u0436\u04d9\u043d\u0435 \u049b\u044b\u043b\u043c\u044b\u0441\u0442\u044b\u049b \u04d9\u0440\u0435\u043a\u0435\u0442\u043f\u0435\u043d \u043a\u04af\u0440\u0435\u0441\u0443 \u043c\u0430\u049b\u0441\u0430\u0442\u044b\u043d\u0434\u0430 \u0436\u043e\u0493\u0430\u0440\u044b \u0430\u0442\u0430\u0443\u043b\u044b \u0431\u0430\u043d\u043a\u043d\u043e\u0442\u0430\u043b\u0430\u0440\u0434\u044b \u0436\u043e\u044e\u0434\u044b \u049b\u043e\u043b\u0434\u0430\u0439\u0434\u044b.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00d3\u00a8', '\u00d0\u00b7', '\u00d1\u0138', '\u00d0\u00bd', '\u00d1\u0138', '\u00d2\u00a3', ' ', '\u00d2\u013c', '\u00d0\u00be\u00d0\u00bb', '\u00d0\u00bc\u00d0\u00b0', '-', '\u00d2\u013d', '\u00d0\u00be\u00d0\u00bb', ' \u00d0\u00b0', '\u00d2\u013d', '\u00d1\u012a\u00d0\u00b0', ' ', '\u00d2\u013d', '\u00d0\u00b0\u00d1\u0122', '\u00d2\u0135', '\u00d1\u012d', '\u00d1\u0123\u00d1\u012d', '\u00d0\u00b0\u00d1\u0124', '\u00d1\u0124\u00d1\u012d', '\u00d0\u00b6\u00d0\u00b0', '\u00d2\u00a3', '\u00d0\u00b0', ' \u00d0\u00ba', '\u00d1\u0138', '\u00d1\u0124', '\u00d0\u00b0\u00d0\u00b1', '\u00d1\u012d', '\u00d0\u00bd\u00d0\u00b4', '\u00d0\u00b0\u00d0', '\u013c', '\u00d0\u00b5\u00d0\u00bd\u00d0\u00bd', '\u00d0\u00b5\u00d1\u0124', ' \u00d0\u0142', '\u00d0\u00be\u00d0\u00b3\u00d0\u00be', '\u00d1\u0126\u00d1\u0126', ' \u00d1\u0123', '\u00d0\u00b0\u00d0\u00bb', '\u00d1\u012d', '\u00d2\u013d', '\u00d1\u0124\u00d1\u012d', ' \u00d1\u0124', '\u00d3\u00a9', '\u00d0\u00bb\u00d0\u00b5', '\u00d1\u0125\u00d0\u00b4', '\u00d0\u00b5\u00d0\u00bd', ' \u00d0\u00b6', '\u00d0\u00b0\u00d0\u00bb', '\u00d1\u0124\u00d0\u00b0\u00d1\u0122', '\u00d1\u0125', ' \u00d0\u00b6', '\u00d3\u013b', '\u00d0\u00bd\u00d0\u00b5', ' ', '\u00d2\u013d', '\u00d1\u012d', '\u00d0\u00bb', '\u00d0\u00bc\u00d1\u012d', '\u00d1\u0123\u00d1\u0124\u00d1\u012d', '\u00d2\u013d', ' ', '\u00d3\u013b', '\u00d1\u0122', '\u00d0\u00b5\u00d0\u00ba', '\u00d0\u00b5\u00d1\u0124', '\u00d0\u00bf', '\u00d0\u00b5\u00d0\u00bd', ' \u00d0\u00ba', '\u00d2\u00af', '\u00d1\u0122\u00d0\u00b5\u00d1\u0123', '\u00d1\u0125', ' \u00d0\u00bc\u00d0\u00b0', '\u00d2\u013d', '\u00d1\u0123', '\u00d0\u00b0\u00d1\u0124', '\u00d1\u012d', '\u00d0\u00bd', '\u00d0\u00b4\u00d0\u00b0', ' \u00d0\u00b6', '\u00d0\u00be', '\u00d2\u0135', '\u00d0\u00b0\u00d1\u0122', '\u00d1\u012d', ' \u00d0\u00b0', '\u00d1\u0124\u00d0\u00b0', '\u00d1\u0125', '\u00d0\u00bb\u00d1\u012d', ' \u00d0\u00b1\u00d0\u00b0\u00d0\u00bd', '\u00d0\u00ba', '\u00d0\u00bd', '\u00d0\u00be\u00d1\u0124', '\u00d0\u00b0\u00d0\u00bb', '\u00d0\u00b0\u00d1\u0122', '\u00d0\u00b4\u00d1\u012d', ' \u00d0\u00b6', '\u00d0\u00be', '\u00d1\u0130', '\u00d0\u00b4\u00d1\u012d', ' ', '\u00d2\u013d', '\u00d0\u00be\u00d0\u00bb', '\u00d0\u00b4', '\u00d0\u00b0\u00d0\u00b9\u00d0\u00b4', '\u00d1\u012d', '.']"}], "[\n[\"\u00d3\u00a8\", \"\u0412\"],\n[\"\u00d0\u00b7\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00bd\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d2\u00a3\", null],\n[\" \", null],\n[\"\u00d2\u013c\", null],\n[\"\u00d0\u00be\u00d0\u00bb\", null],\n[\"\u00d0\u00bc\u00d0\u00b0\", null],\n[\"-\", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d0\u00be\u00d0\u00bb\", null],\n[\" \u00d0\u00b0\", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d1\u012a\u00d0\u00b0\", null],\n[\" \", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d0\u00b0\u00d1\u0122\", null],\n[\"\u00d2\u0135\", null],\n[\"\u00d1\u012d\", null],\n[\"\u00d1\u0123\u00d1\u012d\", null],\n[\"\u00d0\u00b0\u00d1\u0124\", null],\n[\"\u00d1\u0124\u00d1\u012d\", null],\n[\"\u00d0\u00b6\u00d0\u00b0\", null],\n[\"\u00d2\u00a3\", null],\n[\"\u00d0\u00b0\", null],\n[\" \u00d0\u00ba\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d1\u0124\", null],\n[\"\u00d0\u00b0\u00d0\u00b1\", null],\n[\"\u00d1\u012d\", null],\n[\"\u00d0\u00bd\u00d0\u00b4\", null],\n[\"\u00d0\u00b0\u00d0\", null],\n[\"\u013c\", null],\n[\"\u00d0\u00b5\u00d0\u00bd\u00d0\u00bd\", null],\n[\"\u00d0\u00b5\u00d1\u0124\", null],\n[\" \u00d0\u0142\", null],\n[\"\u00d0\u00be\u00d0\u00b3\u00d0\u00be\", null],\n[\"\u00d1\u0126\u00d1\u0126\", null],\n[\" \u00d1\u0123\", null],\n[\"\u00d0\u00b0\u00d0\u00bb\", null],\n[\"\u00d1\u012d\", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d1\u0124\u00d1\u012d\", null],\n[\" \u00d1\u0124\", null],\n[\"\u00d3\u00a9\", null],\n[\"\u00d0\u00bb\u00d0\u00b5\", null],\n[\"\u00d1\u0125\u00d0\u00b4\", null],\n[\"\u00d0\u00b5\u00d0\u00bd\", null],\n[\" \u00d0\u00b6\", null],\n[\"\u00d0\u00b0\u00d0\u00bb\", null],\n[\"\u00d1\u0124\u00d0\u00b0\u00d1\u0122\", null],\n[\"\u00d1\u0125\", null],\n[\" \u00d0\u00b6\", null],\n[\"\u00d3\u013b\", null],\n[\"\u00d0\u00bd\u00d0\u00b5\", null],\n[\" \", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d1\u012d\", null],\n[\"\u00d0\u00bb\", null],\n[\"\u00d0\u00bc\u00d1\u012d\", null],\n[\"\u00d1\u0123\u00d1\u0124\u00d1\u012d\", null],\n[\"\u00d2\u013d\", null],\n[\" \", null],\n[\"\u00d3\u013b\", null],\n[\"\u00d1\u0122\", null],\n[\"\u00d0\u00b5\u00d0\u00ba\", null],\n[\"\u00d0\u00b5\u00d1\u0124\", null],\n[\"\u00d0\u00bf\", null],\n[\"\u00d0\u00b5\u00d0\u00bd\", null],\n[\" \u00d0\u00ba\", null],\n[\"\u00d2\u00af\", null],\n[\"\u00d1\u0122\u00d0\u00b5\u00d1\u0123\", null],\n[\"\u00d1\u0125\", null],\n[\" \u00d0\u00bc\u00d0\u00b0\", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d1\u0123\", null],\n[\"\u00d0\u00b0\u00d1\u0124\", null],\n[\"\u00d1\u012d\", null],\n[\"\u00d0\u00bd\", null],\n[\"\u00d0\u00b4\u00d0\u00b0\", null],\n[\" \u00d0\u00b6\", null],\n[\"\u00d0\u00be\", null],\n[\"\u00d2\u0135\", null],\n[\"\u00d0\u00b0\u00d1\u0122\", null],\n[\"\u00d1\u012d\", null],\n[\" \u00d0\u00b0\", null],\n[\"\u00d1\u0124\u00d0\u00b0\", null],\n[\"\u00d1\u0125\", null],\n[\"\u00d0\u00bb\u00d1\u012d\", null],\n[\" \u00d0\u00b1\u00d0\u00b0\u00d0\u00bd\", \"\u0431\u0430\u043d\u043a\u043d\u043e\u0442\u044b\"],\n[\"\u00d0\u00ba\", null],\n[\"\u00d0\u00bd\", null],\n[\"\u00d0\u00be\u00d1\u0124\", null],\n[\"\u00d0\u00b0\u00d0\u00bb\", null],\n[\"\u00d0\u00b0\u00d1\u0122\", null],\n[\"\u00d0\u00b4\u00d1\u012d\", null],\n[\" \u00d0\u00b6\", null],\n[\"\u00d0\u00be\", null],\n[\"\u00d1\u0130\", null],\n[\"\u00d0\u00b4\u00d1\u012d\", null],\n[\" \", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d0\u00be\u00d0\u00bb\", null],\n[\"\u00d0\u00b4\", null],\n[\"\u00d0\u00b0\u00d0\u00b9\u00d0\u00b4\", null],\n[\"\u00d1\u012d\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to English.\n\n---=== TEXT ===---\n\n\u041e\u0434\u0430\u043d \u0431\u0430\u0441\u049b\u0430 \u0435\u0448\u049b\u0430\u043d\u0434\u0430\u0439 \u0432\u0430\u043b\u044e\u0442\u0430 \u0435\u043d \u0431\u0430\u0439\u043b\u044b\u049b\u0442\u044b\u04a3 \u049b\u0430\u0443\u0456\u043f\u0441\u0456\u0437\u0434\u0456\u0433\u0456 \u043c\u0435\u043d \u04e9\u0442\u0456\u043c\u0434\u0456\u043b\u0456\u0433\u0456\u043d \u043e\u043d\u0434\u0430\u0439 \u0434\u0435\u04a3\u0433\u0435\u0439\u0434\u0435 \u049b\u0430\u043c\u0442\u0430\u043c\u0430\u0441\u044b\u0437 \u0435\u0442\u0435 \u0430\u043b\u0493\u0430\u043d \u0435\u043c\u0435\u0441.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00d0\u0140', '\u00d0\u00b4\u00d0\u00b0\u00d0\u00bd', ' \u00d0\u00b1', '\u00d0\u00b0\u00d1\u0123', '\u00d2\u013d', '\u00d0\u00b0', ' \u00d0\u00b5', '\u00d1\u012a', '\u00d2\u013d', '\u00d0\u00b0\u00d0\u00bd\u00d0\u00b4', '\u00d0\u00b0\u00d0\u00b9', ' \u00d0\u00b2\u00d0\u00b0\u00d0\u00bb', '\u00d1\u0130', '\u00d1\u0124\u00d0\u00b0', ' \u00d0\u00b5', '\u00d0\u00bd', ' \u00d0\u00b1', '\u00d0\u00b0\u00d0\u00b9', '\u00d0\u00bb\u00d1\u012d', '\u00d2\u013d', '\u00d1\u0124\u00d1\u012d', '\u00d2\u00a3', ' ', '\u00d2\u013d', '\u00d0\u00b0', '\u00d1\u0125', '\u00d1\u0138', '\u00d0\u00bf', '\u00d1\u0123', '\u00d1\u0138', '\u00d0\u00b7\u00d0\u00b4', '\u00d1\u0138', '\u00d0\u00b3', '\u00d1\u0138', ' \u00d0\u00bc\u00d0\u00b5\u00d0\u00bd', ' ', '\u00d3\u00a9', '\u00d1\u0124', '\u00d1\u0138', '\u00d0\u00bc', '\u00d0\u00b4', '\u00d1\u0138', '\u00d0\u00bb', '\u00d1\u0138', '\u00d0\u00b3', '\u00d1\u0138', '\u00d0\u00bd', ' \u00d0\u00be\u00d0\u00bd', '\u00d0\u00b4', '\u00d0\u00b0\u00d0\u00b9', ' \u00d0\u00b4\u00d0\u00b5', '\u00d2\u00a3', '\u00d0\u00b3', '\u00d0\u00b5\u00d0\u00b9', '\u00d0\u00b4\u00d0\u00b5', ' ', '\u00d2\u013d', '\u00d0\u00b0\u00d0\u00bc', '\u00d1\u0124', '\u00d0\u00b0\u00d0\u00bc', '\u00d0\u00b0\u00d1\u0123', '\u00d1\u012d', '\u00d0\u00b7', ' \u00d0\u00b5', '\u00d1\u0124\u00d0\u00b5', ' \u00d0\u00b0\u00d0\u00bb', '\u00d2\u0135', '\u00d0\u00b0\u00d0\u00bd', ' \u00d0\u00b5', '\u00d0\u00bc\u00d0\u00b5\u00d1\u0123', '.']"}], "[\n[\"\u00d0\u0140\", \"no\"],\n[\"\u00d0\u00b4\u00d0\u00b0\u00d0\u00bd\", null],\n[\" \u00d0\u00b1\", null],\n[\"\u00d0\u00b0\u00d1\u0123\", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d0\u00b0\", null],\n[\" \u00d0\u00b5\", null],\n[\"\u00d1\u012a\", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d0\u00b0\u00d0\u00bd\u00d0\u00b4\", null],\n[\"\u00d0\u00b0\u00d0\u00b9\", null],\n[\" \u00d0\u00b2\u00d0\u00b0\u00d0\u00bb\", \"currency\"],\n[\"\u00d1\u0130\", null],\n[\"\u00d1\u0124\u00d0\u00b0\", null],\n[\" \u00d0\u00b5\", null],\n[\"\u00d0\u00bd\", null],\n[\" \u00d0\u00b1\", null],\n[\"\u00d0\u00b0\u00d0\u00b9\", null],\n[\"\u00d0\u00bb\u00d1\u012d\", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d1\u0124\u00d1\u012d\", null],\n[\"\u00d2\u00a3\", null],\n[\" \", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d0\u00b0\", null],\n[\"\u00d1\u0125\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00bf\", null],\n[\"\u00d1\u0123\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00b7\u00d0\u00b4\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00b3\", null],\n[\"\u00d1\u0138\", \"security\"],\n[\" \u00d0\u00bc\u00d0\u00b5\u00d0\u00bd\", null],\n[\" \", null],\n[\"\u00d3\u00a9\", null],\n[\"\u00d1\u0124\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00bc\", null],\n[\"\u00d0\u00b4\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00bb\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00b3\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00bd\", \"liquidity\"],\n[\" \u00d0\u00be\u00d0\u00bd\", null],\n[\"\u00d0\u00b4\", null],\n[\"\u00d0\u00b0\u00d0\u00b9\", null],\n[\" \u00d0\u00b4\u00d0\u00b5\", null],\n[\"\u00d2\u00a3\", null],\n[\"\u00d0\u00b3\", null],\n[\"\u00d0\u00b5\u00d0\u00b9\", null],\n[\"\u00d0\u00b4\u00d0\u00b5\", \"level\"],\n[\" \", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d0\u00b0\u00d0\u00bc\", null],\n[\"\u00d1\u0124\", null],\n[\"\u00d0\u00b0\u00d0\u00bc\", null],\n[\"\u00d0\u00b0\u00d1\u0123\", null],\n[\"\u00d1\u012d\", null],\n[\"\u00d0\u00b7\", \"provide\"],\n[\" \u00d0\u00b5\", null],\n[\"\u00d1\u0124\u00d0\u00b5\", null],\n[\" \u00d0\u00b0\u00d0\u00bb\", null],\n[\"\u00d2\u0135\", null],\n[\"\u00d0\u00b0\u00d0\u00bd\", null],\n[\" \u00d0\u00b5\", null],\n[\"\u00d0\u00bc\u00d0\u00b5\u00d1\u0123\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Chinese.\n\n---=== TEXT ===---\n\n\u041e\u0434\u0430\u043d \u0431\u0430\u0441\u049b\u0430 \u0435\u0448\u049b\u0430\u043d\u0434\u0430\u0439 \u0432\u0430\u043b\u044e\u0442\u0430 \u0435\u043d \u0431\u0430\u0439\u043b\u044b\u049b\u0442\u044b\u04a3 \u049b\u0430\u0443\u0456\u043f\u0441\u0456\u0437\u0434\u0456\u0433\u0456 \u043c\u0435\u043d \u04e9\u0442\u0456\u043c\u0434\u0456\u043b\u0456\u0433\u0456\u043d \u043e\u043d\u0434\u0430\u0439 \u0434\u0435\u04a3\u0433\u0435\u0439\u0434\u0435 \u049b\u0430\u043c\u0442\u0430\u043c\u0430\u0441\u044b\u0437 \u0435\u0442\u0435 \u0430\u043b\u0493\u0430\u043d \u0435\u043c\u0435\u0441.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00d0\u0140', '\u00d0\u00b4\u00d0\u00b0\u00d0\u00bd', ' \u00d0\u00b1', '\u00d0\u00b0\u00d1\u0123', '\u00d2\u013d', '\u00d0\u00b0', ' \u00d0\u00b5', '\u00d1\u012a', '\u00d2\u013d', '\u00d0\u00b0\u00d0\u00bd\u00d0\u00b4', '\u00d0\u00b0\u00d0\u00b9', ' \u00d0\u00b2\u00d0\u00b0\u00d0\u00bb', '\u00d1\u0130', '\u00d1\u0124\u00d0\u00b0', ' \u00d0\u00b5', '\u00d0\u00bd', ' \u00d0\u00b1', '\u00d0\u00b0\u00d0\u00b9', '\u00d0\u00bb\u00d1\u012d', '\u00d2\u013d', '\u00d1\u0124\u00d1\u012d', '\u00d2\u00a3', ' ', '\u00d2\u013d', '\u00d0\u00b0', '\u00d1\u0125', '\u00d1\u0138', '\u00d0\u00bf', '\u00d1\u0123', '\u00d1\u0138', '\u00d0\u00b7\u00d0\u00b4', '\u00d1\u0138', '\u00d0\u00b3', '\u00d1\u0138', ' \u00d0\u00bc\u00d0\u00b5\u00d0\u00bd', ' ', '\u00d3\u00a9', '\u00d1\u0124', '\u00d1\u0138', '\u00d0\u00bc', '\u00d0\u00b4', '\u00d1\u0138', '\u00d0\u00bb', '\u00d1\u0138', '\u00d0\u00b3', '\u00d1\u0138', '\u00d0\u00bd', ' \u00d0\u00be\u00d0\u00bd', '\u00d0\u00b4', '\u00d0\u00b0\u00d0\u00b9', ' \u00d0\u00b4\u00d0\u00b5', '\u00d2\u00a3', '\u00d0\u00b3', '\u00d0\u00b5\u00d0\u00b9', '\u00d0\u00b4\u00d0\u00b5', ' ', '\u00d2\u013d', '\u00d0\u00b0\u00d0\u00bc', '\u00d1\u0124', '\u00d0\u00b0\u00d0\u00bc', '\u00d0\u00b0\u00d1\u0123', '\u00d1\u012d', '\u00d0\u00b7', ' \u00d0\u00b5', '\u00d1\u0124\u00d0\u00b5', ' \u00d0\u00b0\u00d0\u00bb', '\u00d2\u0135', '\u00d0\u00b0\u00d0\u00bd', ' \u00d0\u00b5', '\u00d0\u00bc\u00d0\u00b5\u00d1\u0123', '.']"}], "[\n[\"\u00d0\u0140\", null],\n[\"\u00d0\u00b4\u00d0\u00b0\u00d0\u00bd\", null],\n[\" \u00d0\u00b1\", null],\n[\"\u00d0\u00b0\u00d1\u0123\", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d0\u00b0\", null],\n[\" \u00d0\u00b5\", null],\n[\"\u00d1\u012a\", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d0\u00b0\u00d0\u00bd\u00d0\u00b4\", null],\n[\"\u00d0\u00b0\u00d0\u00b9\", null],\n[\" \u00d0\u00b2\u00d0\u00b0\u00d0\u00bb\", \"\u8d27\u5e01\"],\n[\"\u00d1\u0130\", null],\n[\"\u00d1\u0124\u00d0\u00b0\", null],\n[\" \u00d0\u00b5\", null],\n[\"\u00d0\u00bd\", null],\n[\" \u00d0\u00b1\", null],\n[\"\u00d0\u00b0\u00d0\u00b9\", null],\n[\"\u00d0\u00bb\u00d1\u012d\", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d1\u0124\u00d1\u012d\", null],\n[\"\u00d2\u00a3\", null],\n[\" \", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d0\u00b0\", null],\n[\"\u00d1\u0125\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00bf\", null],\n[\"\u00d1\u0123\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00b7\u00d0\u00b4\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00b3\", null],\n[\"\u00d1\u0138\", \"\u5b89\u5168\u6027\"],\n[\" \u00d0\u00bc\u00d0\u00b5\u00d0\u00bd\", \"\u548c\"],\n[\" \", null],\n[\"\u00d3\u00a9\", null],\n[\"\u00d1\u0124\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00bc\", null],\n[\"\u00d0\u00b4\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00bb\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00b3\", null],\n[\"\u00d1\u0138\", \"\u6d41\u52a8\u6027\"],\n[\"\u00d0\u00bd\", null],\n[\" \u00d0\u00be\u00d0\u00bd\", null],\n[\"\u00d0\u00b4\", null],\n[\"\u00d0\u00b0\u00d0\u00b9\", null],\n[\" \u00d0\u00b4\u00d0\u00b5\", null],\n[\"\u00d2\u00a3\", null],\n[\"\u00d0\u00b3\", null],\n[\"\u00d0\u00b5\u00d0\u00b9\", null],\n[\"\u00d0\u00b4\u00d0\u00b5\", \"\u7a0b\u5ea6\"],\n[\" \", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d0\u00b0\u00d0\u00bc\", null],\n[\"\u00d1\u0124\", null],\n[\"\u00d0\u00b0\u00d0\u00bc\", null],\n[\"\u00d0\u00b0\u00d1\u0123\", null],\n[\"\u00d1\u012d\", null],\n[\"\u00d0\u00b7\", \"\u4fdd\u8bc1\"],\n[\" \u00d0\u00b5\", null],\n[\"\u00d1\u0124\u00d0\u00b5\", null],\n[\" \u00d0\u00b0\u00d0\u00bb\", null],\n[\"\u00d2\u0135\", null],\n[\"\u00d0\u00b0\u00d0\u00bd\", null],\n[\" \u00d0\u00b5\", null],\n[\"\u00d0\u00bc\u00d0\u00b5\u00d1\u0123\", \"\u6ca1\u6709\"],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Kazakh to Russian.\n\n---=== TEXT ===---\n\n\u041e\u0434\u0430\u043d \u0431\u0430\u0441\u049b\u0430 \u0435\u0448\u049b\u0430\u043d\u0434\u0430\u0439 \u0432\u0430\u043b\u044e\u0442\u0430 \u0435\u043d \u0431\u0430\u0439\u043b\u044b\u049b\u0442\u044b\u04a3 \u049b\u0430\u0443\u0456\u043f\u0441\u0456\u0437\u0434\u0456\u0433\u0456 \u043c\u0435\u043d \u04e9\u0442\u0456\u043c\u0434\u0456\u043b\u0456\u0433\u0456\u043d \u043e\u043d\u0434\u0430\u0439 \u0434\u0435\u04a3\u0433\u0435\u0439\u0434\u0435 \u049b\u0430\u043c\u0442\u0430\u043c\u0430\u0441\u044b\u0437 \u0435\u0442\u0435 \u0430\u043b\u0493\u0430\u043d \u0435\u043c\u0435\u0441.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00d0\u0140', '\u00d0\u00b4\u00d0\u00b0\u00d0\u00bd', ' \u00d0\u00b1', '\u00d0\u00b0\u00d1\u0123', '\u00d2\u013d', '\u00d0\u00b0', ' \u00d0\u00b5', '\u00d1\u012a', '\u00d2\u013d', '\u00d0\u00b0\u00d0\u00bd\u00d0\u00b4', '\u00d0\u00b0\u00d0\u00b9', ' \u00d0\u00b2\u00d0\u00b0\u00d0\u00bb', '\u00d1\u0130', '\u00d1\u0124\u00d0\u00b0', ' \u00d0\u00b5', '\u00d0\u00bd', ' \u00d0\u00b1', '\u00d0\u00b0\u00d0\u00b9', '\u00d0\u00bb\u00d1\u012d', '\u00d2\u013d', '\u00d1\u0124\u00d1\u012d', '\u00d2\u00a3', ' ', '\u00d2\u013d', '\u00d0\u00b0', '\u00d1\u0125', '\u00d1\u0138', '\u00d0\u00bf', '\u00d1\u0123', '\u00d1\u0138', '\u00d0\u00b7\u00d0\u00b4', '\u00d1\u0138', '\u00d0\u00b3', '\u00d1\u0138', ' \u00d0\u00bc\u00d0\u00b5\u00d0\u00bd', ' ', '\u00d3\u00a9', '\u00d1\u0124', '\u00d1\u0138', '\u00d0\u00bc', '\u00d0\u00b4', '\u00d1\u0138', '\u00d0\u00bb', '\u00d1\u0138', '\u00d0\u00b3', '\u00d1\u0138', '\u00d0\u00bd', ' \u00d0\u00be\u00d0\u00bd', '\u00d0\u00b4', '\u00d0\u00b0\u00d0\u00b9', ' \u00d0\u00b4\u00d0\u00b5', '\u00d2\u00a3', '\u00d0\u00b3', '\u00d0\u00b5\u00d0\u00b9', '\u00d0\u00b4\u00d0\u00b5', ' ', '\u00d2\u013d', '\u00d0\u00b0\u00d0\u00bc', '\u00d1\u0124', '\u00d0\u00b0\u00d0\u00bc', '\u00d0\u00b0\u00d1\u0123', '\u00d1\u012d', '\u00d0\u00b7', ' \u00d0\u00b5', '\u00d1\u0124\u00d0\u00b5', ' \u00d0\u00b0\u00d0\u00bb', '\u00d2\u0135', '\u00d0\u00b0\u00d0\u00bd', ' \u00d0\u00b5', '\u00d0\u00bc\u00d0\u00b5\u00d1\u0123', '.']"}], "[\n[\"\u00d0\u0140\", \"\u041a\u0440\u043e\u043c\u0435\"],\n[\"\u00d0\u00b4\u00d0\u00b0\u00d0\u00bd\", null],\n[\" \u00d0\u00b1\", null],\n[\"\u00d0\u00b0\u00d1\u0123\", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d0\u00b0\", null],\n[\" \u00d0\u00b5\", null],\n[\"\u00d1\u012a\", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d0\u00b0\u00d0\u00bd\u00d0\u00b4\", null],\n[\"\u00d0\u00b0\u00d0\u00b9\", null],\n[\" \u00d0\u00b2\u00d0\u00b0\u00d0\u00bb\", \"\u0432\u0430\u043b\u044e\u0442\u0430\"],\n[\"\u00d1\u0130\", null],\n[\"\u00d1\u0124\u00d0\u00b0\", null],\n[\" \u00d0\u00b5\", null],\n[\"\u00d0\u00bd\", null],\n[\" \u00d0\u00b1\", \"\u0431\u043e\u0433\u0430\u0442\u0441\u0442\u0432\u043e\"],\n[\"\u00d0\u00b0\u00d0\u00b9\", null],\n[\"\u00d0\u00bb\u00d1\u012d\", null],\n[\"\u00d2\u013d\", null],\n[\"\u00d1\u0124\u00d1\u012d\", null],\n[\"\u00d2\u00a3\", null],\n[\" \", null],\n[\"\u00d2\u013d\", \"\u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u044c\"],\n[\"\u00d0\u00b0\", null],\n[\"\u00d1\u0125\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00bf\", null],\n[\"\u00d1\u0123\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00b7\u00d0\u00b4\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00b3\", null],\n[\"\u00d1\u0138\", null],\n[\" \u00d0\u00bc\u00d0\u00b5\u00d0\u00bd\", \"\u0438\"],\n[\" \", null],\n[\"\u00d3\u00a9\", \"\u043b\u0438\u043a\u0432\u0438\u0434\u043d\u043e\u0441\u0442\u044c\"],\n[\"\u00d1\u0124\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00bc\", null],\n[\"\u00d0\u00b4\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00bb\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00b3\", null],\n[\"\u00d1\u0138\", null],\n[\"\u00d0\u00bd\", null],\n[\" \u00d0\u00be\u00d0\u00bd\", \"\u0442\u0430\u043a\u043e\u0439\"],\n[\"\u00d0\u00b4\", null],\n[\"\u00d0\u00b0\u00d0\u00b9\", null],\n[\" \u00d0\u00b4\u00d0\u00b5\", \"\u0443\u0440\u043e\u0432\u0435\u043d\u044c\"],\n[\"\u00d2\u00a3\", null],\n[\"\u00d0\u00b3\", null],\n[\"\u00d0\u00b5\u00d0\u00b9\", null],\n[\"\u00d0\u00b4\u00d0\u00b5\", null],\n[\" \", null],\n[\"\u00d2\u013d\", \"\u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u0442\u044c\"],\n[\"\u00d0\u00b0\u00d0\u00bc\", null],\n[\"\u00d1\u0124\", null],\n[\"\u00d0\u00b0\u00d0\u00bc\", null],\n[\"\u00d0\u00b0\u00d1\u0123\", null],\n[\"\u00d1\u012d\", null],\n[\"\u00d0\u00b7\", null],\n[\" \u00d0\u00b5\", null],\n[\"\u00d1\u0124\u00d0\u00b5\", null],\n[\" \u00d0\u00b0\u00d0\u00bb\", null],\n[\"\u00d2\u0135\", null],\n[\"\u00d0\u00b0\u00d0\u00bd\", null],\n[\" \u00d0\u00b5\", null],\n[\"\u00d0\u00bc\u00d0\u00b5\u00d1\u0123\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0418\u043c\u043c\u0443\u043d\u043d\u044b\u0435 \u043a\u043b\u0435\u0442\u043a\u0438 \u043f\u0440\u0435\u0432\u0440\u0430\u0442\u0438\u043b\u0438\u0441\u044c \u0432 \u043a\u043b\u0435\u0442\u043a\u0438, \u043f\u043e\u0432\u044b\u0448\u0430\u044e\u0449\u0438\u0435 \u0442\u043e\u043b\u0435\u0440\u0430\u043d\u0442\u043d\u043e\u0441\u0442\u044c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00d0\u013a', '\u00d0\u00bc', '\u00d0\u00bc', '\u00d1\u0125\u00d0\u00bd', '\u00d0\u00bd\u00d1\u012d\u00d0\u00b5', ' \u00d0\u00ba\u00d0\u00bb', '\u00d0\u00b5\u00d1\u0124', '\u00d0\u00ba\u00d0\u00b8', ' \u00d0\u00bf\u00d1\u0122', '\u00d0\u00b5\u00d0\u00b2', '\u00d1\u0122\u00d0\u00b0\u00d1\u0124', '\u00d0\u00b8\u00d0\u00bb\u00d0\u00b8\u00d1\u0123\u00d1\u012e', ' \u00d0\u00b2', ' \u00d0\u00ba\u00d0\u00bb', '\u00d0\u00b5\u00d1\u0124', '\u00d0\u00ba\u00d0\u00b8', ',', ' \u00d0\u00bf\u00d0\u00be\u00d0\u00b2', '\u00d1\u012d\u00d1\u012a', '\u00d0\u00b0', '\u00d1\u0130\u00d1\u012b\u00d0\u00b8\u00d0\u00b5', ' \u00d1\u0124', '\u00d0\u00be\u00d0\u00bb', '\u00d0\u00b5\u00d1\u0122', '\u00d0\u00b0\u00d0\u00bd\u00d1\u0124', '\u00d0\u00bd\u00d0\u00be\u00d1\u0123\u00d1\u0124\u00d1\u012e', '.']"}], "[\n[\"\u00d0\u013a\", \"Immune\"],\n[\"\u00d0\u00bc\", null],\n[\"\u00d0\u00bc\", null],\n[\"\u00d1\u0125\u00d0\u00bd\", null],\n[\"\u00d0\u00bd\u00d1\u012d\u00d0\u00b5\", null],\n[\" \u00d0\u00ba\u00d0\u00bb\", \"cells\"],\n[\"\u00d0\u00b5\u00d1\u0124\", null],\n[\"\u00d0\u00ba\u00d0\u00b8\", null],\n[\" \u00d0\u00bf\u00d1\u0122\", \"turned\"],\n[\"\u00d0\u00b5\u00d0\u00b2\", null],\n[\"\u00d1\u0122\u00d0\u00b0\u00d1\u0124\", null],\n[\"\u00d0\u00b8\u00d0\u00bb\u00d0\u00b8\u00d1\u0123\u00d1\u012e\", null],\n[\" \u00d0\u00b2\", \"into\"],\n[\" \u00d0\u00ba\u00d0\u00bb\", \"cells\"],\n[\"\u00d0\u00b5\u00d1\u0124\", null],\n[\"\u00d0\u00ba\u00d0\u00b8\", null],\n[\",\", null],\n[\" \u00d0\u00bf\u00d0\u00be\u00d0\u00b2\", \"increasing\"],\n[\"\u00d1\u012d\u00d1\u012a\", null],\n[\"\u00d0\u00b0\", null],\n[\"\u00d1\u0130\u00d1\u012b\u00d0\u00b8\u00d0\u00b5\", null],\n[\" \u00d1\u0124\", \"tolerance\"],\n[\"\u00d0\u00be\u00d0\u00bb\", null],\n[\"\u00d0\u00b5\u00d1\u0122\", null],\n[\"\u00d0\u00b0\u00d0\u00bd\u00d1\u0124\", null],\n[\"\u00d0\u00bd\u00d0\u00be\u00d1\u0123\u00d1\u0124\u00d1\u012e\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u0418\u043c\u043c\u0443\u043d\u043d\u044b\u0435 \u043a\u043b\u0435\u0442\u043a\u0438 \u043f\u0440\u0435\u0432\u0440\u0430\u0442\u0438\u043b\u0438\u0441\u044c \u0432 \u043a\u043b\u0435\u0442\u043a\u0438, \u043f\u043e\u0432\u044b\u0448\u0430\u044e\u0449\u0438\u0435 \u0442\u043e\u043b\u0435\u0440\u0430\u043d\u0442\u043d\u043e\u0441\u0442\u044c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00d0\u013a', '\u00d0\u00bc', '\u00d0\u00bc', '\u00d1\u0125\u00d0\u00bd', '\u00d0\u00bd\u00d1\u012d\u00d0\u00b5', ' \u00d0\u00ba\u00d0\u00bb', '\u00d0\u00b5\u00d1\u0124', '\u00d0\u00ba\u00d0\u00b8', ' \u00d0\u00bf\u00d1\u0122', '\u00d0\u00b5\u00d0\u00b2', '\u00d1\u0122\u00d0\u00b0\u00d1\u0124', '\u00d0\u00b8\u00d0\u00bb\u00d0\u00b8\u00d1\u0123\u00d1\u012e', ' \u00d0\u00b2', ' \u00d0\u00ba\u00d0\u00bb', '\u00d0\u00b5\u00d1\u0124', '\u00d0\u00ba\u00d0\u00b8', ',', ' \u00d0\u00bf\u00d0\u00be\u00d0\u00b2', '\u00d1\u012d\u00d1\u012a', '\u00d0\u00b0', '\u00d1\u0130\u00d1\u012b\u00d0\u00b8\u00d0\u00b5', ' \u00d1\u0124', '\u00d0\u00be\u00d0\u00bb', '\u00d0\u00b5\u00d1\u0122', '\u00d0\u00b0\u00d0\u00bd\u00d1\u0124', '\u00d0\u00bd\u00d0\u00be\u00d1\u0123\u00d1\u0124\u00d1\u012e', '.']"}], "[\n[\"\u00d0\u013a\", \"\u514d\u75ab\"],\n[\"\u00d0\u00bc\", null],\n[\"\u00d0\u00bc\", null],\n[\"\u00d1\u0125\u00d0\u00bd\", null],\n[\"\u00d0\u00bd\u00d1\u012d\u00d0\u00b5\", null],\n[\" \u00d0\u00ba\u00d0\u00bb\", \"\u7ec6\u80de\"],\n[\"\u00d0\u00b5\u00d1\u0124\", null],\n[\"\u00d0\u00ba\u00d0\u00b8\", null],\n[\" \u00d0\u00bf\u00d1\u0122\", \"\u8f6c\u53d8\"],\n[\"\u00d0\u00b5\u00d0\u00b2\", null],\n[\"\u00d1\u0122\u00d0\u00b0\u00d1\u0124\", null],\n[\"\u00d0\u00b8\u00d0\u00bb\u00d0\u00b8\u00d1\u0123\u00d1\u012e\", null],\n[\" \u00d0\u00b2\", null],\n[\" \u00d0\u00ba\u00d0\u00bb\", \"\u7ec6\u80de\"],\n[\"\u00d0\u00b5\u00d1\u0124\", null],\n[\"\u00d0\u00ba\u00d0\u00b8\", null],\n[\",\", null],\n[\" \u00d0\u00bf\u00d0\u00be\u00d0\u00b2\", \"\u63d0\u9ad8\"],\n[\"\u00d1\u012d\u00d1\u012a\", null],\n[\"\u00d0\u00b0\", null],\n[\"\u00d1\u0130\u00d1\u012b\u00d0\u00b8\u00d0\u00b5\", null],\n[\" \u00d1\u0124\", \"\u8010\u53d7\u6027\"],\n[\"\u00d0\u00be\u00d0\u00bb\", null],\n[\"\u00d0\u00b5\u00d1\u0122\", null],\n[\"\u00d0\u00b0\u00d0\u00bd\u00d1\u0124\", null],\n[\"\u00d0\u00bd\u00d0\u00be\u00d1\u0123\u00d1\u0124\u00d1\u012e\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u0418\u043c\u043c\u0443\u043d\u043d\u044b\u0435 \u043a\u043b\u0435\u0442\u043a\u0438 \u043f\u0440\u0435\u0432\u0440\u0430\u0442\u0438\u043b\u0438\u0441\u044c \u0432 \u043a\u043b\u0435\u0442\u043a\u0438, \u043f\u043e\u0432\u044b\u0448\u0430\u044e\u0449\u0438\u0435 \u0442\u043e\u043b\u0435\u0440\u0430\u043d\u0442\u043d\u043e\u0441\u0442\u044c.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00d0\u013a', '\u00d0\u00bc', '\u00d0\u00bc', '\u00d1\u0125\u00d0\u00bd', '\u00d0\u00bd\u00d1\u012d\u00d0\u00b5', ' \u00d0\u00ba\u00d0\u00bb', '\u00d0\u00b5\u00d1\u0124', '\u00d0\u00ba\u00d0\u00b8', ' \u00d0\u00bf\u00d1\u0122', '\u00d0\u00b5\u00d0\u00b2', '\u00d1\u0122\u00d0\u00b0\u00d1\u0124', '\u00d0\u00b8\u00d0\u00bb\u00d0\u00b8\u00d1\u0123\u00d1\u012e', ' \u00d0\u00b2', ' \u00d0\u00ba\u00d0\u00bb', '\u00d0\u00b5\u00d1\u0124', '\u00d0\u00ba\u00d0\u00b8', ',', ' \u00d0\u00bf\u00d0\u00be\u00d0\u00b2', '\u00d1\u012d\u00d1\u012a', '\u00d0\u00b0', '\u00d1\u0130\u00d1\u012b\u00d0\u00b8\u00d0\u00b5', ' \u00d1\u0124', '\u00d0\u00be\u00d0\u00bb', '\u00d0\u00b5\u00d1\u0122', '\u00d0\u00b0\u00d0\u00bd\u00d1\u0124', '\u00d0\u00bd\u00d0\u00be\u00d1\u0123\u00d1\u0124\u00d1\u012e', '.']"}], "[\n[\"\u00d0\u013a\", \"\u0418\u043c\u043c\u0443\u043d\u0434\u044b\u049b\"],\n[\"\u00d0\u00bc\", null],\n[\"\u00d0\u00bc\", null],\n[\"\u00d1\u0125\u00d0\u00bd\", null],\n[\"\u00d0\u00bd\u00d1\u012d\u00d0\u00b5\", null],\n[\" \u00d0\u00ba\u00d0\u00bb\", \"\u0436\u0430\u0441\u0443\u0448\u0430\"],\n[\"\u00d0\u00b5\u00d1\u0124\", null],\n[\"\u00d0\u00ba\u00d0\u00b8\", null],\n[\" \u00d0\u00bf\u00d1\u0122\", \"\u0430\u0439\u043d\u0430\u043b\u0434\u044b\"],\n[\"\u00d0\u00b5\u00d0\u00b2\", null],\n[\"\u00d1\u0122\u00d0\u00b0\u00d1\u0124\", null],\n[\"\u00d0\u00b8\u00d0\u00bb\u00d0\u00b8\u00d1\u0123\u00d1\u012e\", null],\n[\" \u00d0\u00b2\", null],\n[\" \u00d0\u00ba\u00d0\u00bb\", \"\u0436\u0430\u0441\u0443\u0448\u0430\"],\n[\"\u00d0\u00b5\u00d1\u0124\", null],\n[\"\u00d0\u00ba\u00d0\u00b8\", null],\n[\",\", null],\n[\" \u00d0\u00bf\u00d0\u00be\u00d0\u00b2\", \"\u0430\u0440\u0442\u0442\u044b\u0440\u0430\u0442\u044b\u043d\"],\n[\"\u00d1\u012d\u00d1\u012a\", null],\n[\"\u00d0\u00b0\", null],\n[\"\u00d1\u0130\u00d1\u012b\u00d0\u00b8\u00d0\u00b5\", null],\n[\" \u00d1\u0124\", \"\u0442\u043e\u043b\u0435\u0440\u0430\u043d\u0442\u0442\u044b\u043b\u044b\u049b\u0442\u044b\"],\n[\"\u00d0\u00be\u00d0\u00bb\", null],\n[\"\u00d0\u00b5\u00d1\u0122\", null],\n[\"\u00d0\u00b0\u00d0\u00bd\u00d1\u0124\", null],\n[\"\u00d0\u00bd\u00d0\u00be\u00d1\u0123\u00d1\u0124\u00d1\u012e\", null],\n[\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to English.\n\n---=== TEXT ===---\n\n\u0421\u0442\u043e\u0438\u0442 \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c, \u0447\u0442\u043e \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044f \u0441 \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u044b\u043c\u0438 \u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u043c\u0438 \u0443\u0434\u043e\u0441\u0442\u043e\u0432\u0435\u0440\u0435\u043d\u0438\u044f\u043c\u0438 \u043e\u0441\u0442\u0430\u0435\u0442\u0441\u044f \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u0440\u0435\u0447\u0438\u0432\u043e\u0439.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00d0\u00a1', '\u00d1\u0124\u00d0\u00be', '\u00d0\u00b8\u00d1\u0124', ' \u00d0\u00be\u00d1\u0124\u00d0\u00bc\u00d0\u00b5\u00d1\u0124\u00d0\u00b8\u00d1\u0124\u00d1\u012e', ',', ' \u00d1\u0129\u00d1\u0124\u00d0\u00be', ' \u00d1\u0123\u00d0\u00b8\u00d1\u0124\u00d1\u0125\u00d0\u00b0\u00d1\u0128\u00d0\u00b8\u00d1\u0131', ' \u00d1\u0123', ' \u00d0\u00bc\u00d0\u00b5\u00d0\u00b6\u00d0\u00b4\u00d1\u0125', '\u00d0\u00bd\u00d0\u00b0', '\u00d1\u0122\u00d0\u00be\u00d0\u00b4', '\u00d0\u00bd\u00d1\u012d\u00d0\u00bc\u00d0\u00b8', ' \u00d0\u00b2\u00d0\u00be\u00d0\u00b4', '\u00d0\u00b8\u00d1\u0124\u00d0\u00b5\u00d0\u00bb\u00d1\u012e', '\u00d1\u0123\u00d0\u00ba', '\u00d0\u00b8\u00d0\u00bc\u00d0\u00b8', ' \u00d1\u0125\u00d0\u00b4', '\u00d0\u00be\u00d1\u0123\u00d1\u0124', '\u00d0\u00be\u00d0\u00b2', '\u00d0\u00b5\u00d1\u0122', '\u00d0\u00b5\u00d0\u00bd\u00d0\u00b8\u00d1\u0131', '\u00d0\u00bc\u00d0\u00b8', ' \u00d0\u00be\u00d1\u0123\u00d1\u0124', '\u00d0\u00b0\u00d0\u00b5\u00d1\u0124\u00d1\u0123\u00d1\u0131', ' \u00d0\u00bf\u00d1\u0122\u00d0\u00be\u00d1\u0124\u00d0\u00b8\u00d0\u00b2', '\u00d0\u00be\u00d1\u0122', '\u00d0\u00b5\u00d1\u0129', '\u00d0\u00b8\u00d0\u00b2', '\u00d0\u00be\u00d0\u00b9', '.']"}], "[\n  [\"\u00d0\u00a1\", \"It\"],\n  [\"\u00d1\u0124\u00d0\u00be\", \"should\"],\n  [\"\u00d0\u00b8\u00d1\u0124\", null],\n  [\" \u00d0\u00be\u00d1\u0124\u00d0\u00bc\u00d0\u00b5\u00d1\u0124\u00d0\u00b8\u00d1\u0124\u00d1\u012e\", \"noted\"],\n  [\",\", null],\n  [\" \u00d1\u0129\u00d1\u0124\u00d0\u00be\", null],\n  [\" \u00d1\u0123\u00d0\u00b8\u00d1\u0124\u00d1\u0125\u00d0\u00b0\u00d1\u0128\u00d0\u00b8\u00d1\u0131\", \"situation\"],\n  [\" \u00d1\u0123\", null],\n  [\" \u00d0\u00bc\u00d0\u00b5\u00d0\u00b6\u00d0\u00b4\u00d1\u0125\", \"international\"],\n  [\"\u00d0\u00bd\u00d0\u00b0\", null],\n  [\"\u00d1\u0122\u00d0\u00be\u00d0\u00b4\", null],\n  [\"\u00d0\u00bd\u00d1\u012d\u00d0\u00bc\u00d0\u00b8\", null],\n  [\" \u00d0\u00b2\u00d0\u00be\u00d0\u00b4\", \"driver's\"],\n  [\"\u00d0\u00b8\u00d1\u0124\u00d0\u00b5\u00d0\u00bb\u00d1\u012e\", null],\n  [\"\u00d1\u0123\u00d0\u00ba\", null],\n  [\"\u00d0\u00b8\u00d0\u00bc\u00d0\u00b8\", null],\n  [\" \u00d1\u0125\u00d0\u00b4\", \"licenses\"],\n  [\"\u00d0\u00be\u00d1\u0123\u00d1\u0124\", null],\n  [\"\u00d0\u00be\u00d0\u00b2\", null],\n  [\"\u00d0\u00b5\u00d1\u0122\", null],\n  [\"\u00d0\u00b5\u00d0\u00bd\u00d0\u00b8\u00d1\u0131\", null],\n  [\"\u00d0\u00bc\u00d0\u00b8\", null],\n  [\" \u00d0\u00be\u00d1\u0123\u00d1\u0124\", \"remains\"],\n  [\"\u00d0\u00b0\u00d0\u00b5\u00d1\u0124\u00d1\u0123\u00d1\u0131\", null],\n  [\" \u00d0\u00bf\u00d1\u0122\u00d0\u00be\u00d1\u0124\u00d0\u00b8\u00d0\u00b2\", \"contradictory\"],\n  [\"\u00d0\u00be\u00d1\u0122\", null],\n  [\"\u00d0\u00b5\u00d1\u0129\", null],\n  [\"\u00d0\u00b8\u00d0\u00b2\", null],\n  [\"\u00d0\u00be\u00d0\u00b9\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Chinese.\n\n---=== TEXT ===---\n\n\u0421\u0442\u043e\u0438\u0442 \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c, \u0447\u0442\u043e \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044f \u0441 \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u044b\u043c\u0438 \u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u043c\u0438 \u0443\u0434\u043e\u0441\u0442\u043e\u0432\u0435\u0440\u0435\u043d\u0438\u044f\u043c\u0438 \u043e\u0441\u0442\u0430\u0435\u0442\u0441\u044f \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u0440\u0435\u0447\u0438\u0432\u043e\u0439.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00d0\u00a1', '\u00d1\u0124\u00d0\u00be', '\u00d0\u00b8\u00d1\u0124', ' \u00d0\u00be\u00d1\u0124\u00d0\u00bc\u00d0\u00b5\u00d1\u0124\u00d0\u00b8\u00d1\u0124\u00d1\u012e', ',', ' \u00d1\u0129\u00d1\u0124\u00d0\u00be', ' \u00d1\u0123\u00d0\u00b8\u00d1\u0124\u00d1\u0125\u00d0\u00b0\u00d1\u0128\u00d0\u00b8\u00d1\u0131', ' \u00d1\u0123', ' \u00d0\u00bc\u00d0\u00b5\u00d0\u00b6\u00d0\u00b4\u00d1\u0125', '\u00d0\u00bd\u00d0\u00b0', '\u00d1\u0122\u00d0\u00be\u00d0\u00b4', '\u00d0\u00bd\u00d1\u012d\u00d0\u00bc\u00d0\u00b8', ' \u00d0\u00b2\u00d0\u00be\u00d0\u00b4', '\u00d0\u00b8\u00d1\u0124\u00d0\u00b5\u00d0\u00bb\u00d1\u012e', '\u00d1\u0123\u00d0\u00ba', '\u00d0\u00b8\u00d0\u00bc\u00d0\u00b8', ' \u00d1\u0125\u00d0\u00b4', '\u00d0\u00be\u00d1\u0123\u00d1\u0124', '\u00d0\u00be\u00d0\u00b2', '\u00d0\u00b5\u00d1\u0122', '\u00d0\u00b5\u00d0\u00bd\u00d0\u00b8\u00d1\u0131', '\u00d0\u00bc\u00d0\u00b8', ' \u00d0\u00be\u00d1\u0123\u00d1\u0124', '\u00d0\u00b0\u00d0\u00b5\u00d1\u0124\u00d1\u0123\u00d1\u0131', ' \u00d0\u00bf\u00d1\u0122\u00d0\u00be\u00d1\u0124\u00d0\u00b8\u00d0\u00b2', '\u00d0\u00be\u00d1\u0122', '\u00d0\u00b5\u00d1\u0129', '\u00d0\u00b8\u00d0\u00b2', '\u00d0\u00be\u00d0\u00b9', '.']"}], "[\n  [\"\u00d0\u00a1\", \"\u5e94\"],\n  [\"\u00d1\u0124\u00d0\u00be\", null],\n  [\"\u00d0\u00b8\u00d1\u0124\", null],\n  [\" \u00d0\u00be\u00d1\u0124\u00d0\u00bc\u00d0\u00b5\u00d1\u0124\u00d0\u00b8\u00d1\u0124\u00d1\u012e\", \"\u6307\u51fa\"],\n  [\",\", null],\n  [\" \u00d1\u0129\u00d1\u0124\u00d0\u00be\", null],\n  [\" \u00d1\u0123\u00d0\u00b8\u00d1\u0124\u00d1\u0125\u00d0\u00b0\u00d1\u0128\u00d0\u00b8\u00d1\u0131\", \"\u60c5\u51b5\"],\n  [\" \u00d1\u0123\", null],\n  [\" \u00d0\u00bc\u00d0\u00b5\u00d0\u00b6\u00d0\u00b4\u00d1\u0125\", \"\u56fd\u9645\"],\n  [\"\u00d0\u00bd\u00d0\u00b0\", null],\n  [\"\u00d1\u0122\u00d0\u00be\u00d0\u00b4\", null],\n  [\"\u00d0\u00bd\u00d1\u012d\u00d0\u00bc\u00d0\u00b8\", null],\n  [\" \u00d0\u00b2\u00d0\u00be\u00d0\u00b4\", \"\u9a7e\u9a76\"],\n  [\"\u00d0\u00b8\u00d1\u0124\u00d0\u00b5\u00d0\u00bb\u00d1\u012e\", null],\n  [\"\u00d1\u0123\u00d0\u00ba\", null],\n  [\"\u00d0\u00b8\u00d0\u00bc\u00d0\u00b8\", null],\n  [\" \u00d1\u0125\u00d0\u00b4\", \"\u6267\u7167\"],\n  [\"\u00d0\u00be\u00d1\u0123\u00d1\u0124\", null],\n  [\"\u00d0\u00be\u00d0\u00b2\", null],\n  [\"\u00d0\u00b5\u00d1\u0122\", null],\n  [\"\u00d0\u00b5\u00d0\u00bd\u00d0\u00b8\u00d1\u0131\", null],\n  [\"\u00d0\u00bc\u00d0\u00b8\", null],\n  [\" \u00d0\u00be\u00d1\u0123\u00d1\u0124\", \"\u4ecd\u7136\"],\n  [\"\u00d0\u00b0\u00d0\u00b5\u00d1\u0124\u00d1\u0123\u00d1\u0131\", null],\n  [\" \u00d0\u00bf\u00d1\u0122\u00d0\u00be\u00d1\u0124\u00d0\u00b8\u00d0\u00b2\", \"\u77db\u76fe\"],\n  [\"\u00d0\u00be\u00d1\u0122\", null],\n  [\"\u00d0\u00b5\u00d1\u0129\", null],\n  [\"\u00d0\u00b8\u00d0\u00b2\", null],\n  [\"\u00d0\u00be\u00d0\u00b9\", null],\n  [\".\", null]\n]"], [[{"role": "system", "content": "You are presented with a tokenized text in some source langauge. Please translate the text the another target language. You should output a list of pairs. The first element of each pair should be the token in the source language. The second element should be:\n* If the token in the source language is either a stop word or not a word at all, `null`.\n* If the token in the source language is a whole word, or the first token of a word, the closest translation of the whole word to the target language, taking the context of the whole text into consideration.\n* If the token in the source language belongs to a word, but is not the first token of this word, `null`.\nPlease output the list and no other text whatsoever. The output should be valid JSON."}, {"role": "user", "content": "Please translate from Russian to Kazakh.\n\n---=== TEXT ===---\n\n\u0421\u0442\u043e\u0438\u0442 \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c, \u0447\u0442\u043e \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044f \u0441 \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u044b\u043c\u0438 \u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u043c\u0438 \u0443\u0434\u043e\u0441\u0442\u043e\u0432\u0435\u0440\u0435\u043d\u0438\u044f\u043c\u0438 \u043e\u0441\u0442\u0430\u0435\u0442\u0441\u044f \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u0440\u0435\u0447\u0438\u0432\u043e\u0439.\n\n\n---=== TOKENIZED TEXT ===---\n\n['\u00d0\u00a1', '\u00d1\u0124\u00d0\u00be', '\u00d0\u00b8\u00d1\u0124', ' \u00d0\u00be\u00d1\u0124\u00d0\u00bc\u00d0\u00b5\u00d1\u0124\u00d0\u00b8\u00d1\u0124\u00d1\u012e', ',', ' \u00d1\u0129\u00d1\u0124\u00d0\u00be', ' \u00d1\u0123\u00d0\u00b8\u00d1\u0124\u00d1\u0125\u00d0\u00b0\u00d1\u0128\u00d0\u00b8\u00d1\u0131', ' \u00d1\u0123', ' \u00d0\u00bc\u00d0\u00b5\u00d0\u00b6\u00d0\u00b4\u00d1\u0125', '\u00d0\u00bd\u00d0\u00b0', '\u00d1\u0122\u00d0\u00be\u00d0\u00b4', '\u00d0\u00bd\u00d1\u012d\u00d0\u00bc\u00d0\u00b8', ' \u00d0\u00b2\u00d0\u00be\u00d0\u00b4', '\u00d0\u00b8\u00d1\u0124\u00d0\u00b5\u00d0\u00bb\u00d1\u012e', '\u00d1\u0123\u00d0\u00ba', '\u00d0\u00b8\u00d0\u00bc\u00d0\u00b8', ' \u00d1\u0125\u00d0\u00b4', '\u00d0\u00be\u00d1\u0123\u00d1\u0124', '\u00d0\u00be\u00d0\u00b2', '\u00d0\u00b5\u00d1\u0122', '\u00d0\u00b5\u00d0\u00bd\u00d0\u00b8\u00d1\u0131', '\u00d0\u00bc\u00d0\u00b8', ' \u00d0\u00be\u00d1\u0123\u00d1\u0124', '\u00d0\u00b0\u00d0\u00b5\u00d1\u0124\u00d1\u0123\u00d1\u0131', ' \u00d0\u00bf\u00d1\u0122\u00d0\u00be\u00d1\u0124\u00d0\u00b8\u00d0\u00b2', '\u00d0\u00be\u00d1\u0122', '\u00d0\u00b5\u00d1\u0129', '\u00d0\u00b8\u00d0\u00b2', '\u00d0\u00be\u00d0\u00b9', '.']"}], "[\n[\"\u00d0\u00a1\", \"\u0410\u0439\u0442\u0430\"],\n[\"\u00d1\u0124\u00d0\u00be\", null],\n[\"\u00d0\u00b8\u00d1\u0124\", null],\n[\" \u00d0\u00be\u00d1\u0124\u00d0\u00bc\u00d0\u00b5\u00d1\u0124\u00d0\u00b8\u00d1\u0124\u00d1\u012e\", null],\n[\",\", null],\n[\" \u00d1\u0129\u00d1\u0124\u00d0\u00be\", null],\n[\" \u00d1\u0123\u00d0\u00b8\u00d1\u0124\u00d1\u0125\u00d0\u00b0\u00d1\u0128\u00d0\u00b8\u00d1\u0131\", \"\u0436\u0430\u0493\u0434\u0430\u0439\"],\n[\" \u00d1\u0123\", null],\n[\" \u00d0\u00bc\u00d0\u00b5\u00d0\u00b6\u00d0\u00b4\u00d1\u0125\", \"\u0445\u0430\u043b\u044b\u049b\u0430\u0440\u0430\u043b\u044b\u049b\"],\n[\"\u00d0\u00bd\u00d0\u00b0\", null],\n[\"\u00d1\u0122\u00d0\u00be\u00d0\u00b4\", null],\n[\"\u00d0\u00bd\u00d1\u012d\u00d0\u00bc\u00d0\u00b8\", null],\n[\" \u00d0\u00b2\u00d0\u00be\u00d0\u00b4\", \"\u0436\u04af\u0440\u0433\u0456\u0437\u0443\u0448\u0456\"],\n[\"\u00d0\u00b8\u00d1\u0124\u00d0\u00b5\u00d0\u00bb\u00d1\u012e\", null],\n[\"\u00d1\u0123\u00d0\u00ba\", null],\n[\"\u00d0\u00b8\u00d0\u00bc\u00d0\u00b8\", null],\n[\" \u00d1\u0125\u00d0\u00b4\", \"\u043a\u0443\u04d9\u043b\u0456\u043a\u0442\u0435\u0440\"],\n[\"\u00d0\u00be\u00d1\u0123\u00d1\u0124\", null],\n[\"\u00d0\u00be\u00d0\u00b2\", null],\n[\"\u00d0\u00b5\u00d1\u0122\", null],\n[\"\u00d0\u00b5\u00d0\u00bd\u00d0\u00b8\u00d1\u0131\", null],\n[\"\u00d0\u00bc\u00d0\u00b8\", null],\n[\" \u00d0\u00be\u00d1\u0123\u00d1\u0124\", \"\u049b\u0430\u043b\u0430\u0434\u044b\"],\n[\"\u00d0\u00b0\u00d0\u00b5\u00d1\u0124\u00d1\u0123\u00d1\u0131\", null],\n[\" \u00d0\u00bf\u00d1\u0122\u00d0\u00be\u00d1\u0124\u00d0\u00b8\u00d0\u00b2\", \"\u049b\u0430\u0440\u0430\u043c\u0430-\u049b\u0430\u0439\u0448\u044b\"],\n[\"\u00d0\u00be\u00d1\u0122\", null],\n[\"\u00d0\u00b5\u00d1\u0129\", null],\n[\"\u00d0\u00b8\u00d0\u00b2\", null],\n[\"\u00d0\u00be\u00d0\u00b9\", null],\n[\".\", null]\n]"]]